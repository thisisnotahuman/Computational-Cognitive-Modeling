PopSpikeActor(
  (encoder): PopSpikeEncoderRegularSpike()
  (snn): SpikeMLP(
    (hidden_layers): ModuleList(
      (0): Linear(in_features=340, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=128, bias=True)
      (2): Linear(in_features=128, out_features=64, bias=True)
    )
    (out_pop_layer): Linear(in_features=64, out_features=90, bias=True)
  )
  (decoder): PopSpikeDecoder(
    (decoder): Conv1d(9, 9, kernel_size=(10,), stride=(1,), groups=9)
    (output_activation): ELU(alpha=1.0)
  )
)
Sequential(
  (0): Linear(in_features=34, out_features=256, bias=True)
  (1): SELU()
  (2): Linear(in_features=256, out_features=128, bias=True)
  (3): SELU()
  (4): Linear(in_features=128, out_features=64, bias=True)
  (5): SELU()
  (6): Linear(in_features=64, out_features=1, bias=True)
)
################################################################################
                     [1m Learning iteration 0/200000 [0m

                       Computation: 2840 steps/s (collection: 0.652s, learning 2.232s)
               Value function loss: 3.1074
                    Surrogate loss: 0.0091
             Mean action noise std: 1.00
                       Mean reward: 4.67
               Mean episode length: 14.96
                 Mean success rate: 0.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 22.08
--------------------------------------------------------------------------------
                   Total timesteps: 8192
                    Iteration time: 2.88s
                        Total time: 2.88s
                               ETA: 576796.0s

################################################################################
                     [1m Learning iteration 1/200000 [0m

                       Computation: 3276 steps/s (collection: 0.434s, learning 2.066s)
               Value function loss: 3.7404
                    Surrogate loss: 0.0079
             Mean action noise std: 1.00
                       Mean reward: 6.49
               Mean episode length: 23.09
                 Mean success rate: 0.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 13.98
--------------------------------------------------------------------------------
                   Total timesteps: 16384
                    Iteration time: 2.50s
                        Total time: 5.38s
                               ETA: 538397.7s

################################################################################
                     [1m Learning iteration 2/200000 [0m

                       Computation: 3231 steps/s (collection: 0.421s, learning 2.114s)
               Value function loss: 4.0236
                    Surrogate loss: 0.0099
             Mean action noise std: 1.00
                       Mean reward: 6.71
               Mean episode length: 22.61
                 Mean success rate: 0.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 14.08
--------------------------------------------------------------------------------
                   Total timesteps: 24576
                    Iteration time: 2.53s
                        Total time: 7.92s
                               ETA: 527920.2s

################################################################################
                     [1m Learning iteration 3/200000 [0m

                       Computation: 3230 steps/s (collection: 0.442s, learning 2.094s)
               Value function loss: 3.5712
                    Surrogate loss: 0.0077
             Mean action noise std: 1.00
                       Mean reward: 7.36
               Mean episode length: 25.88
                 Mean success rate: 0.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 16.35
--------------------------------------------------------------------------------
                   Total timesteps: 32768
                    Iteration time: 2.54s
                        Total time: 10.45s
                               ETA: 522714.9s

################################################################################
                     [1m Learning iteration 4/200000 [0m

                       Computation: 3205 steps/s (collection: 0.491s, learning 2.064s)
               Value function loss: 3.7689
                    Surrogate loss: 0.0077
             Mean action noise std: 1.00
                       Mean reward: 7.47
               Mean episode length: 27.15
                 Mean success rate: 0.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 16.19
--------------------------------------------------------------------------------
                   Total timesteps: 40960
                    Iteration time: 2.56s
                        Total time: 13.01s
                               ETA: 520378.3s

################################################################################
                     [1m Learning iteration 5/200000 [0m

                       Computation: 3214 steps/s (collection: 0.489s, learning 2.060s)
               Value function loss: 3.6477
                    Surrogate loss: 0.0083
             Mean action noise std: 1.00
                       Mean reward: 8.76
               Mean episode length: 32.36
                 Mean success rate: 0.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 17.96
--------------------------------------------------------------------------------
                   Total timesteps: 49152
                    Iteration time: 2.55s
                        Total time: 15.56s
                               ETA: 518595.7s

################################################################################
                     [1m Learning iteration 6/200000 [0m

                       Computation: 3265 steps/s (collection: 0.447s, learning 2.062s)
               Value function loss: 3.3704
                    Surrogate loss: 0.0067
             Mean action noise std: 1.00
                       Mean reward: 8.92
               Mean episode length: 32.77
                 Mean success rate: 0.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 19.60
--------------------------------------------------------------------------------
                   Total timesteps: 57344
                    Iteration time: 2.51s
                        Total time: 18.07s
                               ETA: 516190.3s

################################################################################
                     [1m Learning iteration 7/200000 [0m

                       Computation: 3157 steps/s (collection: 0.456s, learning 2.138s)
               Value function loss: 3.1869
                    Surrogate loss: 0.0065
             Mean action noise std: 1.00
                       Mean reward: 9.11
               Mean episode length: 31.91
                 Mean success rate: 0.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 21.22
--------------------------------------------------------------------------------
                   Total timesteps: 65536
                    Iteration time: 2.59s
                        Total time: 20.66s
                               ETA: 516518.6s

################################################################################
                     [1m Learning iteration 8/200000 [0m

                       Computation: 3259 steps/s (collection: 0.451s, learning 2.063s)
               Value function loss: 2.8615
                    Surrogate loss: 0.0065
             Mean action noise std: 1.00
                       Mean reward: 10.15
               Mean episode length: 36.52
                 Mean success rate: 0.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 23.47
--------------------------------------------------------------------------------
                   Total timesteps: 73728
                    Iteration time: 2.51s
                        Total time: 23.17s
                               ETA: 514977.4s

################################################################################
                     [1m Learning iteration 9/200000 [0m

                       Computation: 3321 steps/s (collection: 0.420s, learning 2.046s)
               Value function loss: 2.3525
                    Surrogate loss: 0.0058
             Mean action noise std: 1.00
                       Mean reward: 10.07
               Mean episode length: 38.70
                 Mean success rate: 0.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 25.76
--------------------------------------------------------------------------------
                   Total timesteps: 81920
                    Iteration time: 2.47s
                        Total time: 25.64s
                               ETA: 512805.5s

################################################################################
                     [1m Learning iteration 10/200000 [0m

                       Computation: 3305 steps/s (collection: 0.443s, learning 2.036s)
               Value function loss: 1.6075
                    Surrogate loss: 0.0051
             Mean action noise std: 1.00
                       Mean reward: 10.92
               Mean episode length: 43.72
                 Mean success rate: 0.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 90112
                    Iteration time: 2.48s
                        Total time: 28.12s
                               ETA: 511247.4s

################################################################################
                     [1m Learning iteration 11/200000 [0m

                       Computation: 3252 steps/s (collection: 0.464s, learning 2.055s)
               Value function loss: 2.5112
                    Surrogate loss: 0.0062
             Mean action noise std: 1.00
                       Mean reward: 12.44
               Mean episode length: 54.84
                 Mean success rate: 0.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 2.52s
                        Total time: 30.64s
                               ETA: 510619.8s

################################################################################
                     [1m Learning iteration 12/200000 [0m

                       Computation: 3214 steps/s (collection: 0.419s, learning 2.129s)
               Value function loss: 2.6438
                    Surrogate loss: 0.0066
             Mean action noise std: 1.00
                       Mean reward: 14.45
               Mean episode length: 67.08
                 Mean success rate: 0.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 106496
                    Iteration time: 2.55s
                        Total time: 33.19s
                               ETA: 510540.1s

################################################################################
                     [1m Learning iteration 13/200000 [0m

                       Computation: 3320 steps/s (collection: 0.407s, learning 2.060s)
               Value function loss: 3.7492
                    Surrogate loss: 0.0086
             Mean action noise std: 1.00
                       Mean reward: 17.81
               Mean episode length: 86.53
                 Mean success rate: 0.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 26.43
--------------------------------------------------------------------------------
                   Total timesteps: 114688
                    Iteration time: 2.47s
                        Total time: 35.65s
                               ETA: 509309.9s

################################################################################
                     [1m Learning iteration 14/200000 [0m

                       Computation: 3220 steps/s (collection: 0.494s, learning 2.050s)
               Value function loss: 2.9639
                    Surrogate loss: 0.0070
             Mean action noise std: 1.00
                       Mean reward: 21.32
               Mean episode length: 105.97
                 Mean success rate: 0.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 122880
                    Iteration time: 2.54s
                        Total time: 38.20s
                               ETA: 509268.0s

################################################################################
                     [1m Learning iteration 15/200000 [0m

                       Computation: 3220 steps/s (collection: 0.441s, learning 2.103s)
               Value function loss: 3.6169
                    Surrogate loss: 0.0056
             Mean action noise std: 1.00
                       Mean reward: 22.78
               Mean episode length: 112.97
                 Mean success rate: 0.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 25.84
--------------------------------------------------------------------------------
                   Total timesteps: 131072
                    Iteration time: 2.54s
                        Total time: 40.74s
                               ETA: 509234.9s

################################################################################
                     [1m Learning iteration 16/200000 [0m

                       Computation: 3250 steps/s (collection: 0.473s, learning 2.047s)
               Value function loss: 3.6674
                    Surrogate loss: 0.0059
             Mean action noise std: 1.00
                       Mean reward: 24.74
               Mean episode length: 124.56
                 Mean success rate: 0.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 26.09
--------------------------------------------------------------------------------
                   Total timesteps: 139264
                    Iteration time: 2.52s
                        Total time: 43.26s
                               ETA: 508926.8s

################################################################################
                     [1m Learning iteration 17/200000 [0m

                       Computation: 3267 steps/s (collection: 0.448s, learning 2.059s)
               Value function loss: 2.6989
                    Surrogate loss: 0.0044
             Mean action noise std: 1.00
                       Mean reward: 25.41
               Mean episode length: 129.28
                 Mean success rate: 0.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 26.51
--------------------------------------------------------------------------------
                   Total timesteps: 147456
                    Iteration time: 2.51s
                        Total time: 45.77s
                               ETA: 508505.7s

################################################################################
                     [1m Learning iteration 18/200000 [0m

                       Computation: 3282 steps/s (collection: 0.440s, learning 2.056s)
               Value function loss: 3.0415
                    Surrogate loss: 0.0030
             Mean action noise std: 1.00
                       Mean reward: 25.45
               Mean episode length: 131.82
                 Mean success rate: 0.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 155648
                    Iteration time: 2.50s
                        Total time: 48.27s
                               ETA: 508010.9s

################################################################################
                     [1m Learning iteration 19/200000 [0m

                       Computation: 3307 steps/s (collection: 0.416s, learning 2.061s)
               Value function loss: 2.7931
                    Surrogate loss: 0.0040
             Mean action noise std: 1.00
                       Mean reward: 27.33
               Mean episode length: 142.42
                 Mean success rate: 0.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 163840
                    Iteration time: 2.48s
                        Total time: 50.74s
                               ETA: 507374.2s

################################################################################
                     [1m Learning iteration 20/200000 [0m

                       Computation: 3276 steps/s (collection: 0.432s, learning 2.068s)
               Value function loss: 2.9863
                    Surrogate loss: 0.0008
             Mean action noise std: 1.00
                       Mean reward: 31.29
               Mean episode length: 164.14
                 Mean success rate: 0.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 172032
                    Iteration time: 2.50s
                        Total time: 53.24s
                               ETA: 507023.2s

################################################################################
                     [1m Learning iteration 21/200000 [0m

                       Computation: 3249 steps/s (collection: 0.452s, learning 2.069s)
               Value function loss: 3.6717
                    Surrogate loss: 0.0043
             Mean action noise std: 1.00
                       Mean reward: 34.98
               Mean episode length: 182.40
                 Mean success rate: 0.00
                  Mean reward/step: 0.18
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 180224
                    Iteration time: 2.52s
                        Total time: 55.76s
                               ETA: 506892.4s

################################################################################
                     [1m Learning iteration 22/200000 [0m

                       Computation: 3208 steps/s (collection: 0.460s, learning 2.093s)
               Value function loss: 2.8266
                    Surrogate loss: 0.0056
             Mean action noise std: 1.00
                       Mean reward: 35.67
               Mean episode length: 188.88
                 Mean success rate: 0.00
                  Mean reward/step: 0.19
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 188416
                    Iteration time: 2.55s
                        Total time: 58.32s
                               ETA: 507048.0s

################################################################################
                     [1m Learning iteration 23/200000 [0m

                       Computation: 3357 steps/s (collection: 0.400s, learning 2.040s)
               Value function loss: 3.4709
                    Surrogate loss: 0.0039
             Mean action noise std: 1.00
                       Mean reward: 37.07
               Mean episode length: 198.16
                 Mean success rate: 0.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 2.44s
                        Total time: 60.76s
                               ETA: 506251.8s

################################################################################
                     [1m Learning iteration 24/200000 [0m

                       Computation: 3285 steps/s (collection: 0.463s, learning 2.031s)
               Value function loss: 4.3380
                    Surrogate loss: 0.0047
             Mean action noise std: 1.00
                       Mean reward: 39.49
               Mean episode length: 210.75
                 Mean success rate: 0.00
                  Mean reward/step: 0.20
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 204800
                    Iteration time: 2.49s
                        Total time: 63.25s
                               ETA: 505946.5s

################################################################################
                     [1m Learning iteration 25/200000 [0m

                       Computation: 3175 steps/s (collection: 0.507s, learning 2.073s)
               Value function loss: 7.2077
                    Surrogate loss: 0.0065
             Mean action noise std: 1.00
                       Mean reward: 36.02
               Mean episode length: 189.10
                 Mean success rate: 0.00
                  Mean reward/step: 0.22
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 212992
                    Iteration time: 2.58s
                        Total time: 65.83s
                               ETA: 506327.4s

################################################################################
                     [1m Learning iteration 26/200000 [0m

                       Computation: 3301 steps/s (collection: 0.464s, learning 2.017s)
               Value function loss: 6.1712
                    Surrogate loss: 0.0055
             Mean action noise std: 1.00
                       Mean reward: 31.87
               Mean episode length: 164.62
                 Mean success rate: 0.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 26.43
--------------------------------------------------------------------------------
                   Total timesteps: 221184
                    Iteration time: 2.48s
                        Total time: 68.31s
                               ETA: 505949.1s

################################################################################
                     [1m Learning iteration 27/200000 [0m

                       Computation: 3235 steps/s (collection: 0.482s, learning 2.050s)
               Value function loss: 6.7876
                    Surrogate loss: 0.0038
             Mean action noise std: 1.00
                       Mean reward: 32.36
               Mean episode length: 160.81
                 Mean success rate: 0.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 229376
                    Iteration time: 2.53s
                        Total time: 70.84s
                               ETA: 505957.9s

################################################################################
                     [1m Learning iteration 28/200000 [0m

                       Computation: 3196 steps/s (collection: 0.521s, learning 2.042s)
               Value function loss: 7.9395
                    Surrogate loss: 0.0045
             Mean action noise std: 1.00
                       Mean reward: 31.27
               Mean episode length: 151.31
                 Mean success rate: 0.00
                  Mean reward/step: 0.24
       Mean episode length/episode: 26.17
--------------------------------------------------------------------------------
                   Total timesteps: 237568
                    Iteration time: 2.56s
                        Total time: 73.41s
                               ETA: 506178.7s

################################################################################
                     [1m Learning iteration 29/200000 [0m

                       Computation: 3251 steps/s (collection: 0.457s, learning 2.062s)
               Value function loss: 6.5562
                    Surrogate loss: 0.0044
             Mean action noise std: 1.00
                       Mean reward: 35.78
               Mean episode length: 174.59
                 Mean success rate: 0.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 245760
                    Iteration time: 2.52s
                        Total time: 75.93s
                               ETA: 506097.8s

################################################################################
                     [1m Learning iteration 30/200000 [0m

                       Computation: 3274 steps/s (collection: 0.484s, learning 2.018s)
               Value function loss: 4.1228
                    Surrogate loss: 0.0043
             Mean action noise std: 1.00
                       Mean reward: 36.91
               Mean episode length: 181.57
                 Mean success rate: 0.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 253952
                    Iteration time: 2.50s
                        Total time: 78.43s
                               ETA: 505909.0s

################################################################################
                     [1m Learning iteration 31/200000 [0m

                       Computation: 3343 steps/s (collection: 0.423s, learning 2.027s)
               Value function loss: 5.9976
                    Surrogate loss: 0.0044
             Mean action noise std: 1.00
                       Mean reward: 41.91
               Mean episode length: 206.62
                 Mean success rate: 0.00
                  Mean reward/step: 0.25
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 262144
                    Iteration time: 2.45s
                        Total time: 80.88s
                               ETA: 505407.1s

################################################################################
                     [1m Learning iteration 32/200000 [0m

                       Computation: 3280 steps/s (collection: 0.463s, learning 2.034s)
               Value function loss: 5.3115
                    Surrogate loss: 0.0025
             Mean action noise std: 1.00
                       Mean reward: 44.48
               Mean episode length: 217.95
                 Mean success rate: 0.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 270336
                    Iteration time: 2.50s
                        Total time: 83.37s
                               ETA: 505219.9s

################################################################################
                     [1m Learning iteration 33/200000 [0m

                       Computation: 3240 steps/s (collection: 0.474s, learning 2.054s)
               Value function loss: 6.6219
                    Surrogate loss: 0.0047
             Mean action noise std: 1.00
                       Mean reward: 49.70
               Mean episode length: 239.32
                 Mean success rate: 0.00
                  Mean reward/step: 0.27
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 278528
                    Iteration time: 2.53s
                        Total time: 85.90s
                               ETA: 505225.6s

################################################################################
                     [1m Learning iteration 34/200000 [0m

                       Computation: 3267 steps/s (collection: 0.478s, learning 2.029s)
               Value function loss: 10.7496
                    Surrogate loss: 0.0019
             Mean action noise std: 1.00
                       Mean reward: 49.90
               Mean episode length: 232.05
                 Mean success rate: 0.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 286720
                    Iteration time: 2.51s
                        Total time: 88.41s
                               ETA: 505113.7s

################################################################################
                     [1m Learning iteration 35/200000 [0m

                       Computation: 3184 steps/s (collection: 0.473s, learning 2.099s)
               Value function loss: 16.2281
                    Surrogate loss: 0.0044
             Mean action noise std: 1.00
                       Mean reward: 54.32
               Mean episode length: 242.53
                 Mean success rate: 0.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 2.57s
                        Total time: 90.98s
                               ETA: 505368.2s

################################################################################
                     [1m Learning iteration 36/200000 [0m

                       Computation: 3172 steps/s (collection: 0.525s, learning 2.057s)
               Value function loss: 12.1226
                    Surrogate loss: 0.0033
             Mean action noise std: 1.00
                       Mean reward: 55.64
               Mean episode length: 237.99
                 Mean success rate: 0.00
                  Mean reward/step: 0.28
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 303104
                    Iteration time: 2.58s
                        Total time: 93.56s
                               ETA: 505663.0s

################################################################################
                     [1m Learning iteration 37/200000 [0m

                       Computation: 3295 steps/s (collection: 0.449s, learning 2.036s)
               Value function loss: 12.2812
                    Surrogate loss: 0.0023
             Mean action noise std: 1.00
                       Mean reward: 54.31
               Mean episode length: 221.94
                 Mean success rate: 0.00
                  Mean reward/step: 0.29
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 311296
                    Iteration time: 2.49s
                        Total time: 96.05s
                               ETA: 505432.8s

################################################################################
                     [1m Learning iteration 38/200000 [0m

                       Computation: 3336 steps/s (collection: 0.446s, learning 2.009s)
               Value function loss: 9.1905
                    Surrogate loss: 0.0034
             Mean action noise std: 1.00
                       Mean reward: 51.34
               Mean episode length: 205.23
                 Mean success rate: 0.00
                  Mean reward/step: 0.30
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 319488
                    Iteration time: 2.46s
                        Total time: 98.51s
                               ETA: 505060.1s

################################################################################
                     [1m Learning iteration 39/200000 [0m

                       Computation: 3297 steps/s (collection: 0.463s, learning 2.021s)
               Value function loss: 8.4269
                    Surrogate loss: 0.0032
             Mean action noise std: 1.00
                       Mean reward: 52.55
               Mean episode length: 207.96
                 Mean success rate: 0.00
                  Mean reward/step: 0.31
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 327680
                    Iteration time: 2.48s
                        Total time: 100.99s
                               ETA: 504851.7s

################################################################################
                     [1m Learning iteration 40/200000 [0m

                       Computation: 3381 steps/s (collection: 0.406s, learning 2.017s)
               Value function loss: 13.5028
                    Surrogate loss: 0.0054
             Mean action noise std: 1.00
                       Mean reward: 52.23
               Mean episode length: 202.34
                 Mean success rate: 0.00
                  Mean reward/step: 0.32
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 335872
                    Iteration time: 2.42s
                        Total time: 103.41s
                               ETA: 504351.9s

################################################################################
                     [1m Learning iteration 41/200000 [0m

                       Computation: 3269 steps/s (collection: 0.444s, learning 2.062s)
               Value function loss: 10.8908
                    Surrogate loss: 0.0030
             Mean action noise std: 1.00
                       Mean reward: 59.21
               Mean episode length: 222.49
                 Mean success rate: 0.00
                  Mean reward/step: 0.32
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 344064
                    Iteration time: 2.51s
                        Total time: 105.92s
                               ETA: 504269.4s

################################################################################
                     [1m Learning iteration 42/200000 [0m

                       Computation: 3217 steps/s (collection: 0.508s, learning 2.038s)
               Value function loss: 11.9463
                    Surrogate loss: 0.0038
             Mean action noise std: 1.00
                       Mean reward: 65.27
               Mean episode length: 240.19
                 Mean success rate: 0.00
                  Mean reward/step: 0.32
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 352256
                    Iteration time: 2.55s
                        Total time: 108.46s
                               ETA: 504381.0s

################################################################################
                     [1m Learning iteration 43/200000 [0m

                       Computation: 3347 steps/s (collection: 0.410s, learning 2.037s)
               Value function loss: 11.1431
                    Surrogate loss: 0.0021
             Mean action noise std: 1.00
                       Mean reward: 68.20
               Mean episode length: 249.72
                 Mean success rate: 0.00
                  Mean reward/step: 0.33
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 360448
                    Iteration time: 2.45s
                        Total time: 110.91s
                               ETA: 504037.9s

################################################################################
                     [1m Learning iteration 44/200000 [0m

                       Computation: 3174 steps/s (collection: 0.453s, learning 2.128s)
               Value function loss: 18.3227
                    Surrogate loss: 0.0020
             Mean action noise std: 1.00
                       Mean reward: 76.03
               Mean episode length: 272.33
                 Mean success rate: 0.00
                  Mean reward/step: 0.34
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 368640
                    Iteration time: 2.58s
                        Total time: 113.49s
                               ETA: 504303.0s

################################################################################
                     [1m Learning iteration 45/200000 [0m

                       Computation: 3286 steps/s (collection: 0.447s, learning 2.046s)
               Value function loss: 14.9906
                    Surrogate loss: 0.0037
             Mean action noise std: 1.00
                       Mean reward: 78.94
               Mean episode length: 278.65
                 Mean success rate: 0.00
                  Mean reward/step: 0.35
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 376832
                    Iteration time: 2.49s
                        Total time: 115.99s
                               ETA: 504172.9s

################################################################################
                     [1m Learning iteration 46/200000 [0m

                       Computation: 3353 steps/s (collection: 0.411s, learning 2.032s)
               Value function loss: 10.3300
                    Surrogate loss: 0.0027
             Mean action noise std: 1.00
                       Mean reward: 82.62
               Mean episode length: 291.20
                 Mean success rate: 0.00
                  Mean reward/step: 0.37
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 385024
                    Iteration time: 2.44s
                        Total time: 118.43s
                               ETA: 503837.5s

################################################################################
                     [1m Learning iteration 47/200000 [0m

                       Computation: 3339 steps/s (collection: 0.414s, learning 2.038s)
               Value function loss: 16.9358
                    Surrogate loss: 0.0028
             Mean action noise std: 1.00
                       Mean reward: 86.34
               Mean episode length: 298.75
                 Mean success rate: 0.00
                  Mean reward/step: 0.38
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 2.45s
                        Total time: 120.88s
                               ETA: 503556.5s

################################################################################
                     [1m Learning iteration 48/200000 [0m

                       Computation: 3358 steps/s (collection: 0.408s, learning 2.032s)
               Value function loss: 18.6199
                    Surrogate loss: 0.0036
             Mean action noise std: 1.00
                       Mean reward: 82.50
               Mean episode length: 280.21
                 Mean success rate: 0.00
                  Mean reward/step: 0.39
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 401408
                    Iteration time: 2.44s
                        Total time: 123.32s
                               ETA: 503231.8s

################################################################################
                     [1m Learning iteration 49/200000 [0m

                       Computation: 3306 steps/s (collection: 0.457s, learning 2.021s)
               Value function loss: 18.8130
                    Surrogate loss: 0.0060
             Mean action noise std: 1.00
                       Mean reward: 79.09
               Mean episode length: 264.88
                 Mean success rate: 0.00
                  Mean reward/step: 0.41
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 409600
                    Iteration time: 2.48s
                        Total time: 125.80s
                               ETA: 503073.6s

################################################################################
                     [1m Learning iteration 50/200000 [0m

                       Computation: 3302 steps/s (collection: 0.464s, learning 2.016s)
               Value function loss: 19.5013
                    Surrogate loss: 0.0049
             Mean action noise std: 1.00
                       Mean reward: 76.19
               Mean episode length: 249.59
                 Mean success rate: 0.00
                  Mean reward/step: 0.41
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 417792
                    Iteration time: 2.48s
                        Total time: 128.28s
                               ETA: 502931.0s

################################################################################
                     [1m Learning iteration 51/200000 [0m

                       Computation: 3387 steps/s (collection: 0.407s, learning 2.011s)
               Value function loss: 27.0066
                    Surrogate loss: 0.0058
             Mean action noise std: 1.00
                       Mean reward: 81.83
               Mean episode length: 250.62
                 Mean success rate: 0.00
                  Mean reward/step: 0.41
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 425984
                    Iteration time: 2.42s
                        Total time: 130.70s
                               ETA: 502555.6s

################################################################################
                     [1m Learning iteration 52/200000 [0m

                       Computation: 3289 steps/s (collection: 0.452s, learning 2.038s)
               Value function loss: 22.4524
                    Surrogate loss: 0.0039
             Mean action noise std: 1.00
                       Mean reward: 87.87
               Mean episode length: 261.20
                 Mean success rate: 0.00
                  Mean reward/step: 0.42
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 434176
                    Iteration time: 2.49s
                        Total time: 133.19s
                               ETA: 502465.2s

################################################################################
                     [1m Learning iteration 53/200000 [0m

                       Computation: 3190 steps/s (collection: 0.541s, learning 2.027s)
               Value function loss: 31.2459
                    Surrogate loss: 0.0040
             Mean action noise std: 1.00
                       Mean reward: 96.65
               Mean episode length: 276.88
                 Mean success rate: 0.00
                  Mean reward/step: 0.42
       Mean episode length/episode: 25.76
--------------------------------------------------------------------------------
                   Total timesteps: 442368
                    Iteration time: 2.57s
                        Total time: 135.76s
                               ETA: 502666.4s

################################################################################
                     [1m Learning iteration 54/200000 [0m

                       Computation: 3284 steps/s (collection: 0.448s, learning 2.046s)
               Value function loss: 19.9843
                    Surrogate loss: 0.0041
             Mean action noise std: 1.00
                       Mean reward: 94.23
               Mean episode length: 264.89
                 Mean success rate: 0.00
                  Mean reward/step: 0.43
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 450560
                    Iteration time: 2.49s
                        Total time: 138.25s
                               ETA: 502590.8s

################################################################################
                     [1m Learning iteration 55/200000 [0m

                       Computation: 3350 steps/s (collection: 0.416s, learning 2.029s)
               Value function loss: 15.4915
                    Surrogate loss: 0.0040
             Mean action noise std: 1.00
                       Mean reward: 88.55
               Mean episode length: 242.21
                 Mean success rate: 0.00
                  Mean reward/step: 0.42
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 458752
                    Iteration time: 2.44s
                        Total time: 140.69s
                               ETA: 502342.0s

################################################################################
                     [1m Learning iteration 56/200000 [0m

                       Computation: 3361 steps/s (collection: 0.421s, learning 2.016s)
               Value function loss: 17.1074
                    Surrogate loss: 0.0047
             Mean action noise std: 1.00
                       Mean reward: 87.93
               Mean episode length: 240.27
                 Mean success rate: 0.00
                  Mean reward/step: 0.41
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 466944
                    Iteration time: 2.44s
                        Total time: 143.13s
                               ETA: 502074.2s

################################################################################
                     [1m Learning iteration 57/200000 [0m

                       Computation: 3274 steps/s (collection: 0.437s, learning 2.065s)
               Value function loss: 10.2259
                    Surrogate loss: 0.0040
             Mean action noise std: 1.00
                       Mean reward: 89.64
               Mean episode length: 243.60
                 Mean success rate: 0.00
                  Mean reward/step: 0.41
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 475136
                    Iteration time: 2.50s
                        Total time: 145.63s
                               ETA: 502039.6s

################################################################################
                     [1m Learning iteration 58/200000 [0m

                       Computation: 3329 steps/s (collection: 0.424s, learning 2.036s)
               Value function loss: 10.4673
                    Surrogate loss: 0.0048
             Mean action noise std: 1.00
                       Mean reward: 92.56
               Mean episode length: 245.99
                 Mean success rate: 0.00
                  Mean reward/step: 0.42
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 483328
                    Iteration time: 2.46s
                        Total time: 148.09s
                               ETA: 501865.5s

################################################################################
                     [1m Learning iteration 59/200000 [0m

                       Computation: 3306 steps/s (collection: 0.415s, learning 2.062s)
               Value function loss: 15.3232
                    Surrogate loss: 0.0035
             Mean action noise std: 1.00
                       Mean reward: 94.23
               Mean episode length: 245.02
                 Mean success rate: 0.00
                  Mean reward/step: 0.41
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 2.48s
                        Total time: 150.57s
                               ETA: 501754.7s

################################################################################
                     [1m Learning iteration 60/200000 [0m

                       Computation: 3224 steps/s (collection: 0.486s, learning 2.055s)
               Value function loss: 20.5326
                    Surrogate loss: 0.0040
             Mean action noise std: 1.00
                       Mean reward: 96.28
               Mean episode length: 244.71
                 Mean success rate: 0.00
                  Mean reward/step: 0.41
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 499712
                    Iteration time: 2.54s
                        Total time: 153.11s
                               ETA: 501853.3s

################################################################################
                     [1m Learning iteration 61/200000 [0m

                       Computation: 3212 steps/s (collection: 0.477s, learning 2.074s)
               Value function loss: 19.4677
                    Surrogate loss: 0.0049
             Mean action noise std: 1.00
                       Mean reward: 98.72
               Mean episode length: 248.11
                 Mean success rate: 0.00
                  Mean reward/step: 0.43
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 507904
                    Iteration time: 2.55s
                        Total time: 155.66s
                               ETA: 501980.9s

################################################################################
                     [1m Learning iteration 62/200000 [0m

                       Computation: 3248 steps/s (collection: 0.470s, learning 2.052s)
               Value function loss: 15.0622
                    Surrogate loss: 0.0052
             Mean action noise std: 1.00
                       Mean reward: 102.34
               Mean episode length: 255.28
                 Mean success rate: 0.00
                  Mean reward/step: 0.42
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 516096
                    Iteration time: 2.52s
                        Total time: 158.18s
                               ETA: 502012.9s

################################################################################
                     [1m Learning iteration 63/200000 [0m

                       Computation: 3266 steps/s (collection: 0.439s, learning 2.069s)
               Value function loss: 17.7448
                    Surrogate loss: 0.0046
             Mean action noise std: 1.00
                       Mean reward: 108.25
               Mean episode length: 267.04
                 Mean success rate: 0.00
                  Mean reward/step: 0.42
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 524288
                    Iteration time: 2.51s
                        Total time: 160.69s
                               ETA: 502000.2s

################################################################################
                     [1m Learning iteration 64/200000 [0m

                       Computation: 3275 steps/s (collection: 0.443s, learning 2.058s)
               Value function loss: 10.8803
                    Surrogate loss: 0.0037
             Mean action noise std: 1.00
                       Mean reward: 113.78
               Mean episode length: 278.00
                 Mean success rate: 0.00
                  Mean reward/step: 0.42
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 532480
                    Iteration time: 2.50s
                        Total time: 163.19s
                               ETA: 501967.4s

################################################################################
                     [1m Learning iteration 65/200000 [0m

                       Computation: 3338 steps/s (collection: 0.427s, learning 2.027s)
               Value function loss: 17.4027
                    Surrogate loss: 0.0042
             Mean action noise std: 1.00
                       Mean reward: 119.13
               Mean episode length: 290.00
                 Mean success rate: 0.00
                  Mean reward/step: 0.41
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 540672
                    Iteration time: 2.45s
                        Total time: 165.65s
                               ETA: 501792.6s

################################################################################
                     [1m Learning iteration 66/200000 [0m

                       Computation: 3315 steps/s (collection: 0.444s, learning 2.026s)
               Value function loss: 29.8093
                    Surrogate loss: 0.0070
             Mean action noise std: 1.00
                       Mean reward: 122.72
               Mean episode length: 298.62
                 Mean success rate: 0.00
                  Mean reward/step: 0.42
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 548864
                    Iteration time: 2.47s
                        Total time: 168.12s
                               ETA: 501672.9s

################################################################################
                     [1m Learning iteration 67/200000 [0m

                       Computation: 3298 steps/s (collection: 0.422s, learning 2.061s)
               Value function loss: 32.1765
                    Surrogate loss: 0.0038
             Mean action noise std: 1.00
                       Mean reward: 125.42
               Mean episode length: 303.96
                 Mean success rate: 0.00
                  Mean reward/step: 0.42
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 557056
                    Iteration time: 2.48s
                        Total time: 170.60s
                               ETA: 501595.8s

################################################################################
                     [1m Learning iteration 68/200000 [0m

                       Computation: 3254 steps/s (collection: 0.469s, learning 2.048s)
               Value function loss: 28.0156
                    Surrogate loss: 0.0046
             Mean action noise std: 1.00
                       Mean reward: 135.84
               Mean episode length: 325.92
                 Mean success rate: 0.00
                  Mean reward/step: 0.43
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 565248
                    Iteration time: 2.52s
                        Total time: 173.12s
                               ETA: 501616.7s

################################################################################
                     [1m Learning iteration 69/200000 [0m

                       Computation: 3247 steps/s (collection: 0.448s, learning 2.074s)
               Value function loss: 28.8950
                    Surrogate loss: 0.0067
             Mean action noise std: 1.00
                       Mean reward: 143.44
               Mean episode length: 344.00
                 Mean success rate: 0.00
                  Mean reward/step: 0.43
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 573440
                    Iteration time: 2.52s
                        Total time: 175.64s
                               ETA: 501652.9s

################################################################################
                     [1m Learning iteration 70/200000 [0m

                       Computation: 3249 steps/s (collection: 0.477s, learning 2.045s)
               Value function loss: 23.2376
                    Surrogate loss: 0.0055
             Mean action noise std: 1.00
                       Mean reward: 137.64
               Mean episode length: 330.67
                 Mean success rate: 0.00
                  Mean reward/step: 0.44
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 581632
                    Iteration time: 2.52s
                        Total time: 178.16s
                               ETA: 501684.2s

################################################################################
                     [1m Learning iteration 71/200000 [0m

                       Computation: 3170 steps/s (collection: 0.444s, learning 2.139s)
               Value function loss: 16.8725
                    Surrogate loss: 0.0055
             Mean action noise std: 1.00
                       Mean reward: 131.92
               Mean episode length: 311.95
                 Mean success rate: 0.00
                  Mean reward/step: 0.44
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 2.58s
                        Total time: 180.74s
                               ETA: 501887.5s

################################################################################
                     [1m Learning iteration 72/200000 [0m

                       Computation: 3180 steps/s (collection: 0.472s, learning 2.103s)
               Value function loss: 25.2496
                    Surrogate loss: 0.0045
             Mean action noise std: 1.00
                       Mean reward: 129.55
               Mean episode length: 303.90
                 Mean success rate: 0.00
                  Mean reward/step: 0.43
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 598016
                    Iteration time: 2.58s
                        Total time: 183.32s
                               ETA: 502062.9s

################################################################################
                     [1m Learning iteration 73/200000 [0m

                       Computation: 3187 steps/s (collection: 0.476s, learning 2.093s)
               Value function loss: 27.5441
                    Surrogate loss: 0.0043
             Mean action noise std: 1.00
                       Mean reward: 112.39
               Mean episode length: 264.04
                 Mean success rate: 0.00
                  Mean reward/step: 0.45
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 606208
                    Iteration time: 2.57s
                        Total time: 185.89s
                               ETA: 502218.4s

################################################################################
                     [1m Learning iteration 74/200000 [0m

                       Computation: 3263 steps/s (collection: 0.426s, learning 2.084s)
               Value function loss: 14.9375
                    Surrogate loss: 0.0063
             Mean action noise std: 1.00
                       Mean reward: 108.90
               Mean episode length: 256.87
                 Mean success rate: 0.00
                  Mean reward/step: 0.44
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 614400
                    Iteration time: 2.51s
                        Total time: 188.40s
                               ETA: 502210.5s

################################################################################
                     [1m Learning iteration 75/200000 [0m

                       Computation: 3230 steps/s (collection: 0.431s, learning 2.105s)
               Value function loss: 20.6133
                    Surrogate loss: 0.0046
             Mean action noise std: 1.00
                       Mean reward: 108.95
               Mean episode length: 256.84
                 Mean success rate: 0.00
                  Mean reward/step: 0.46
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 622592
                    Iteration time: 2.54s
                        Total time: 190.93s
                               ETA: 502270.3s

################################################################################
                     [1m Learning iteration 76/200000 [0m

                       Computation: 3205 steps/s (collection: 0.452s, learning 2.104s)
               Value function loss: 27.8513
                    Surrogate loss: 0.0063
             Mean action noise std: 1.00
                       Mean reward: 111.18
               Mean episode length: 256.63
                 Mean success rate: 0.00
                  Mean reward/step: 0.47
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 630784
                    Iteration time: 2.56s
                        Total time: 193.49s
                               ETA: 502380.7s

################################################################################
                     [1m Learning iteration 77/200000 [0m

                       Computation: 3123 steps/s (collection: 0.461s, learning 2.161s)
               Value function loss: 18.7602
                    Surrogate loss: 0.0038
             Mean action noise std: 1.00
                       Mean reward: 110.39
               Mean episode length: 254.97
                 Mean success rate: 0.00
                  Mean reward/step: 0.47
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 638976
                    Iteration time: 2.62s
                        Total time: 196.11s
                               ETA: 502659.5s

################################################################################
                     [1m Learning iteration 78/200000 [0m

                       Computation: 3282 steps/s (collection: 0.434s, learning 2.062s)
               Value function loss: 25.5057
                    Surrogate loss: 0.0048
             Mean action noise std: 1.00
                       Mean reward: 115.03
               Mean episode length: 261.03
                 Mean success rate: 0.00
                  Mean reward/step: 0.48
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 647168
                    Iteration time: 2.50s
                        Total time: 198.61s
                               ETA: 502610.8s

################################################################################
                     [1m Learning iteration 79/200000 [0m

                       Computation: 3297 steps/s (collection: 0.399s, learning 2.085s)
               Value function loss: 31.7538
                    Surrogate loss: 0.0044
             Mean action noise std: 1.00
                       Mean reward: 110.40
               Mean episode length: 248.75
                 Mean success rate: 0.00
                  Mean reward/step: 0.49
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 655360
                    Iteration time: 2.48s
                        Total time: 201.09s
                               ETA: 502534.0s

################################################################################
                     [1m Learning iteration 80/200000 [0m

                       Computation: 3172 steps/s (collection: 0.474s, learning 2.109s)
               Value function loss: 24.2005
                    Surrogate loss: 0.0061
             Mean action noise std: 1.00
                       Mean reward: 113.19
               Mean episode length: 251.03
                 Mean success rate: 0.00
                  Mean reward/step: 0.49
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 663552
                    Iteration time: 2.58s
                        Total time: 203.68s
                               ETA: 502701.2s

################################################################################
                     [1m Learning iteration 81/200000 [0m

                       Computation: 3057 steps/s (collection: 0.537s, learning 2.143s)
               Value function loss: 39.2589
                    Surrogate loss: 0.0055
             Mean action noise std: 1.00
                       Mean reward: 120.16
               Mean episode length: 260.79
                 Mean success rate: 0.00
                  Mean reward/step: 0.50
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 671744
                    Iteration time: 2.68s
                        Total time: 206.36s
                               ETA: 503101.2s

################################################################################
                     [1m Learning iteration 82/200000 [0m

                       Computation: 3237 steps/s (collection: 0.469s, learning 2.061s)
               Value function loss: 46.0684
                    Surrogate loss: 0.0053
             Mean action noise std: 1.00
                       Mean reward: 120.92
               Mean episode length: 263.65
                 Mean success rate: 0.00
                  Mean reward/step: 0.51
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 679936
                    Iteration time: 2.53s
                        Total time: 208.89s
                               ETA: 503131.5s

################################################################################
                     [1m Learning iteration 83/200000 [0m

                       Computation: 3308 steps/s (collection: 0.439s, learning 2.037s)
               Value function loss: 39.8574
                    Surrogate loss: 0.0058
             Mean action noise std: 1.00
                       Mean reward: 117.52
               Mean episode length: 253.13
                 Mean success rate: 0.00
                  Mean reward/step: 0.52
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 2.48s
                        Total time: 211.36s
                               ETA: 503032.5s

################################################################################
                     [1m Learning iteration 84/200000 [0m

                       Computation: 3315 steps/s (collection: 0.436s, learning 2.035s)
               Value function loss: 59.0279
                    Surrogate loss: 0.0070
             Mean action noise std: 1.00
                       Mean reward: 108.27
               Mean episode length: 236.19
                 Mean success rate: 0.00
                  Mean reward/step: 0.52
       Mean episode length/episode: 25.05
--------------------------------------------------------------------------------
                   Total timesteps: 696320
                    Iteration time: 2.47s
                        Total time: 213.83s
                               ETA: 502922.5s

################################################################################
                     [1m Learning iteration 85/200000 [0m

                       Computation: 3255 steps/s (collection: 0.488s, learning 2.028s)
               Value function loss: 45.5461
                    Surrogate loss: 0.0055
             Mean action noise std: 1.00
                       Mean reward: 104.15
               Mean episode length: 222.38
                 Mean success rate: 0.00
                  Mean reward/step: 0.52
       Mean episode length/episode: 26.34
--------------------------------------------------------------------------------
                   Total timesteps: 704512
                    Iteration time: 2.52s
                        Total time: 216.35s
                               ETA: 502921.3s

################################################################################
                     [1m Learning iteration 86/200000 [0m

                       Computation: 3220 steps/s (collection: 0.474s, learning 2.070s)
               Value function loss: 36.9121
                    Surrogate loss: 0.0058
             Mean action noise std: 1.00
                       Mean reward: 85.51
               Mean episode length: 184.37
                 Mean success rate: 0.00
                  Mean reward/step: 0.51
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 712704
                    Iteration time: 2.54s
                        Total time: 218.89s
                               ETA: 502982.9s

################################################################################
                     [1m Learning iteration 87/200000 [0m

                       Computation: 3272 steps/s (collection: 0.455s, learning 2.049s)
               Value function loss: 24.4200
                    Surrogate loss: 0.0056
             Mean action noise std: 1.00
                       Mean reward: 87.99
               Mean episode length: 189.43
                 Mean success rate: 0.00
                  Mean reward/step: 0.52
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 720896
                    Iteration time: 2.50s
                        Total time: 221.39s
                               ETA: 502951.5s

################################################################################
                     [1m Learning iteration 88/200000 [0m

                       Computation: 3285 steps/s (collection: 0.482s, learning 2.012s)
               Value function loss: 44.7255
                    Surrogate loss: 0.0063
             Mean action noise std: 1.00
                       Mean reward: 84.96
               Mean episode length: 173.03
                 Mean success rate: 0.00
                  Mean reward/step: 0.50
       Mean episode length/episode: 26.17
--------------------------------------------------------------------------------
                   Total timesteps: 729088
                    Iteration time: 2.49s
                        Total time: 223.89s
                               ETA: 502898.4s

################################################################################
                     [1m Learning iteration 89/200000 [0m

                       Computation: 3283 steps/s (collection: 0.454s, learning 2.041s)
               Value function loss: 24.5398
                    Surrogate loss: 0.0056
             Mean action noise std: 1.00
                       Mean reward: 89.69
               Mean episode length: 181.81
                 Mean success rate: 0.00
                  Mean reward/step: 0.50
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 737280
                    Iteration time: 2.50s
                        Total time: 226.38s
                               ETA: 502850.4s

################################################################################
                     [1m Learning iteration 90/200000 [0m

                       Computation: 3254 steps/s (collection: 0.487s, learning 2.030s)
               Value function loss: 20.8775
                    Surrogate loss: 0.0070
             Mean action noise std: 1.00
                       Mean reward: 95.24
               Mean episode length: 189.25
                 Mean success rate: 0.00
                  Mean reward/step: 0.52
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 745472
                    Iteration time: 2.52s
                        Total time: 228.90s
                               ETA: 502852.2s

################################################################################
                     [1m Learning iteration 91/200000 [0m

                       Computation: 3405 steps/s (collection: 0.406s, learning 2.000s)
               Value function loss: 21.1982
                    Surrogate loss: 0.0053
             Mean action noise std: 1.00
                       Mean reward: 100.78
               Mean episode length: 199.14
                 Mean success rate: 0.00
                  Mean reward/step: 0.52
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 753664
                    Iteration time: 2.41s
                        Total time: 231.31s
                               ETA: 502611.3s

################################################################################
                     [1m Learning iteration 92/200000 [0m

                       Computation: 3374 steps/s (collection: 0.410s, learning 2.018s)
               Value function loss: 15.0264
                    Surrogate loss: 0.0051
             Mean action noise std: 1.00
                       Mean reward: 102.61
               Mean episode length: 198.62
                 Mean success rate: 0.00
                  Mean reward/step: 0.51
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 761856
                    Iteration time: 2.43s
                        Total time: 233.73s
                               ETA: 502423.2s

################################################################################
                     [1m Learning iteration 93/200000 [0m

                       Computation: 3252 steps/s (collection: 0.462s, learning 2.057s)
               Value function loss: 24.9989
                    Surrogate loss: 0.0053
             Mean action noise std: 1.00
                       Mean reward: 111.66
               Mean episode length: 212.11
                 Mean success rate: 0.00
                  Mean reward/step: 0.52
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 770048
                    Iteration time: 2.52s
                        Total time: 236.25s
                               ETA: 502432.4s

################################################################################
                     [1m Learning iteration 94/200000 [0m

                       Computation: 3228 steps/s (collection: 0.458s, learning 2.080s)
               Value function loss: 29.7175
                    Surrogate loss: 0.0045
             Mean action noise std: 1.00
                       Mean reward: 120.73
               Mean episode length: 231.04
                 Mean success rate: 0.00
                  Mean reward/step: 0.51
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 778240
                    Iteration time: 2.54s
                        Total time: 238.79s
                               ETA: 502480.8s

################################################################################
                     [1m Learning iteration 95/200000 [0m

                       Computation: 3202 steps/s (collection: 0.458s, learning 2.100s)
               Value function loss: 30.4372
                    Surrogate loss: 0.0045
             Mean action noise std: 1.00
                       Mean reward: 125.37
               Mean episode length: 240.09
                 Mean success rate: 0.00
                  Mean reward/step: 0.52
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 2.56s
                        Total time: 241.35s
                               ETA: 502570.6s

################################################################################
                     [1m Learning iteration 96/200000 [0m

                       Computation: 3187 steps/s (collection: 0.460s, learning 2.110s)
               Value function loss: 29.2509
                    Surrogate loss: 0.0056
             Mean action noise std: 1.00
                       Mean reward: 136.23
               Mean episode length: 260.26
                 Mean success rate: 0.00
                  Mean reward/step: 0.52
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 794624
                    Iteration time: 2.57s
                        Total time: 243.92s
                               ETA: 502684.1s

################################################################################
                     [1m Learning iteration 97/200000 [0m

                       Computation: 3276 steps/s (collection: 0.436s, learning 2.064s)
               Value function loss: 28.8585
                    Surrogate loss: 0.0038
             Mean action noise std: 1.00
                       Mean reward: 141.42
               Mean episode length: 273.53
                 Mean success rate: 0.00
                  Mean reward/step: 0.52
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 802816
                    Iteration time: 2.50s
                        Total time: 246.42s
                               ETA: 502652.7s

################################################################################
                     [1m Learning iteration 98/200000 [0m

                       Computation: 3244 steps/s (collection: 0.464s, learning 2.061s)
               Value function loss: 35.9171
                    Surrogate loss: 0.0071
             Mean action noise std: 1.00
                       Mean reward: 149.10
               Mean episode length: 286.74
                 Mean success rate: 0.00
                  Mean reward/step: 0.52
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 811008
                    Iteration time: 2.53s
                        Total time: 248.94s
                               ETA: 502671.6s

################################################################################
                     [1m Learning iteration 99/200000 [0m

                       Computation: 3216 steps/s (collection: 0.476s, learning 2.071s)
               Value function loss: 42.8126
                    Surrogate loss: 0.0058
             Mean action noise std: 1.00
                       Mean reward: 143.24
               Mean episode length: 279.10
                 Mean success rate: 0.00
                  Mean reward/step: 0.53
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 819200
                    Iteration time: 2.55s
                        Total time: 251.49s
                               ETA: 502733.4s

################################################################################
                    [1m Learning iteration 100/200000 [0m

                       Computation: 3176 steps/s (collection: 0.487s, learning 2.092s)
               Value function loss: 65.2690
                    Surrogate loss: 0.0057
             Mean action noise std: 1.00
                       Mean reward: 147.85
               Mean episode length: 289.67
                 Mean success rate: 0.00
                  Mean reward/step: 0.54
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 827392
                    Iteration time: 2.58s
                        Total time: 254.07s
                               ETA: 502857.5s

################################################################################
                    [1m Learning iteration 101/200000 [0m

                       Computation: 3205 steps/s (collection: 0.501s, learning 2.055s)
               Value function loss: 39.0351
                    Surrogate loss: 0.0061
             Mean action noise std: 1.00
                       Mean reward: 144.02
               Mean episode length: 287.51
                 Mean success rate: 0.00
                  Mean reward/step: 0.55
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 835584
                    Iteration time: 2.56s
                        Total time: 256.63s
                               ETA: 502933.8s

################################################################################
                    [1m Learning iteration 102/200000 [0m

                       Computation: 3217 steps/s (collection: 0.496s, learning 2.050s)
               Value function loss: 49.3875
                    Surrogate loss: 0.0060
             Mean action noise std: 1.00
                       Mean reward: 136.26
               Mean episode length: 267.92
                 Mean success rate: 0.00
                  Mean reward/step: 0.57
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 843776
                    Iteration time: 2.55s
                        Total time: 259.17s
                               ETA: 502989.4s

################################################################################
                    [1m Learning iteration 103/200000 [0m

                       Computation: 3248 steps/s (collection: 0.441s, learning 2.081s)
               Value function loss: 46.8619
                    Surrogate loss: 0.0063
             Mean action noise std: 1.00
                       Mean reward: 139.09
               Mean episode length: 271.29
                 Mean success rate: 0.00
                  Mean reward/step: 0.56
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 851968
                    Iteration time: 2.52s
                        Total time: 261.69s
                               ETA: 502997.5s

################################################################################
                    [1m Learning iteration 104/200000 [0m

                       Computation: 3204 steps/s (collection: 0.455s, learning 2.102s)
               Value function loss: 53.1879
                    Surrogate loss: 0.0046
             Mean action noise std: 1.00
                       Mean reward: 142.31
               Mean episode length: 268.37
                 Mean success rate: 0.00
                  Mean reward/step: 0.53
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 860160
                    Iteration time: 2.56s
                        Total time: 264.25s
                               ETA: 503071.4s

################################################################################
                    [1m Learning iteration 105/200000 [0m

                       Computation: 3238 steps/s (collection: 0.463s, learning 2.067s)
               Value function loss: 27.6639
                    Surrogate loss: 0.0049
             Mean action noise std: 1.00
                       Mean reward: 135.59
               Mean episode length: 254.03
                 Mean success rate: 0.00
                  Mean reward/step: 0.53
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 868352
                    Iteration time: 2.53s
                        Total time: 266.78s
                               ETA: 503092.7s

################################################################################
                    [1m Learning iteration 106/200000 [0m

                       Computation: 3270 steps/s (collection: 0.475s, learning 2.030s)
               Value function loss: 22.2472
                    Surrogate loss: 0.0043
             Mean action noise std: 1.00
                       Mean reward: 132.03
               Mean episode length: 242.18
                 Mean success rate: 0.00
                  Mean reward/step: 0.54
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 876544
                    Iteration time: 2.50s
                        Total time: 269.28s
                               ETA: 503067.8s

################################################################################
                    [1m Learning iteration 107/200000 [0m

                       Computation: 3213 steps/s (collection: 0.461s, learning 2.088s)
               Value function loss: 33.5088
                    Surrogate loss: 0.0062
             Mean action noise std: 1.00
                       Mean reward: 130.45
               Mean episode length: 236.89
                 Mean success rate: 0.00
                  Mean reward/step: 0.54
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 2.55s
                        Total time: 271.83s
                               ETA: 503125.4s

################################################################################
                    [1m Learning iteration 108/200000 [0m

                       Computation: 3156 steps/s (collection: 0.468s, learning 2.128s)
               Value function loss: 34.5958
                    Surrogate loss: 0.0049
             Mean action noise std: 1.00
                       Mean reward: 128.12
               Mean episode length: 230.18
                 Mean success rate: 0.00
                  Mean reward/step: 0.54
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 892928
                    Iteration time: 2.60s
                        Total time: 274.43s
                               ETA: 503267.1s

################################################################################
                    [1m Learning iteration 109/200000 [0m

                       Computation: 3143 steps/s (collection: 0.475s, learning 2.131s)
               Value function loss: 21.9331
                    Surrogate loss: 0.0057
             Mean action noise std: 1.00
                       Mean reward: 127.81
               Mean episode length: 228.45
                 Mean success rate: 0.00
                  Mean reward/step: 0.56
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 901120
                    Iteration time: 2.61s
                        Total time: 277.03s
                               ETA: 503424.6s

################################################################################
                    [1m Learning iteration 110/200000 [0m

                       Computation: 3105 steps/s (collection: 0.535s, learning 2.104s)
               Value function loss: 30.7787
                    Surrogate loss: 0.0057
             Mean action noise std: 1.00
                       Mean reward: 130.50
               Mean episode length: 235.23
                 Mean success rate: 0.00
                  Mean reward/step: 0.54
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 909312
                    Iteration time: 2.64s
                        Total time: 279.67s
                               ETA: 503637.7s

################################################################################
                    [1m Learning iteration 111/200000 [0m

                       Computation: 3274 steps/s (collection: 0.469s, learning 2.033s)
               Value function loss: 23.8991
                    Surrogate loss: 0.0049
             Mean action noise std: 1.00
                       Mean reward: 125.94
               Mean episode length: 228.24
                 Mean success rate: 0.00
                  Mean reward/step: 0.52
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 917504
                    Iteration time: 2.50s
                        Total time: 282.17s
                               ETA: 503603.7s

################################################################################
                    [1m Learning iteration 112/200000 [0m

                       Computation: 3131 steps/s (collection: 0.548s, learning 2.068s)
               Value function loss: 29.1532
                    Surrogate loss: 0.0065
             Mean action noise std: 1.00
                       Mean reward: 126.73
               Mean episode length: 233.97
                 Mean success rate: 0.00
                  Mean reward/step: 0.52
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 925696
                    Iteration time: 2.62s
                        Total time: 284.79s
                               ETA: 503772.6s

################################################################################
                    [1m Learning iteration 113/200000 [0m

                       Computation: 3058 steps/s (collection: 0.575s, learning 2.103s)
               Value function loss: 36.6859
                    Surrogate loss: 0.0065
             Mean action noise std: 1.00
                       Mean reward: 129.42
               Mean episode length: 239.19
                 Mean success rate: 0.00
                  Mean reward/step: 0.52
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 933888
                    Iteration time: 2.68s
                        Total time: 287.47s
                               ETA: 504047.3s

################################################################################
                    [1m Learning iteration 114/200000 [0m

                       Computation: 3167 steps/s (collection: 0.515s, learning 2.072s)
               Value function loss: 38.2106
                    Surrogate loss: 0.0078
             Mean action noise std: 1.00
                       Mean reward: 137.81
               Mean episode length: 252.99
                 Mean success rate: 0.00
                  Mean reward/step: 0.52
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 942080
                    Iteration time: 2.59s
                        Total time: 290.06s
                               ETA: 504156.9s

################################################################################
                    [1m Learning iteration 115/200000 [0m

                       Computation: 3229 steps/s (collection: 0.487s, learning 2.049s)
               Value function loss: 52.5115
                    Surrogate loss: 0.0085
             Mean action noise std: 1.00
                       Mean reward: 148.50
               Mean episode length: 280.38
                 Mean success rate: 0.00
                  Mean reward/step: 0.51
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 950272
                    Iteration time: 2.54s
                        Total time: 292.59s
                               ETA: 504178.7s

################################################################################
                    [1m Learning iteration 116/200000 [0m

                       Computation: 3183 steps/s (collection: 0.502s, learning 2.071s)
               Value function loss: 70.5807
                    Surrogate loss: 0.0062
             Mean action noise std: 1.00
                       Mean reward: 157.87
               Mean episode length: 294.89
                 Mean success rate: 0.00
                  Mean reward/step: 0.52
       Mean episode length/episode: 26.51
--------------------------------------------------------------------------------
                   Total timesteps: 958464
                    Iteration time: 2.57s
                        Total time: 295.17s
                               ETA: 504263.0s

################################################################################
                    [1m Learning iteration 117/200000 [0m

                       Computation: 3202 steps/s (collection: 0.492s, learning 2.066s)
               Value function loss: 50.2628
                    Surrogate loss: 0.0059
             Mean action noise std: 1.00
                       Mean reward: 162.59
               Mean episode length: 303.30
                 Mean success rate: 0.00
                  Mean reward/step: 0.53
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 966656
                    Iteration time: 2.56s
                        Total time: 297.72s
                               ETA: 504320.1s

################################################################################
                    [1m Learning iteration 118/200000 [0m

                       Computation: 3177 steps/s (collection: 0.496s, learning 2.082s)
               Value function loss: 51.4983
                    Surrogate loss: 0.0068
             Mean action noise std: 1.00
                       Mean reward: 156.74
               Mean episode length: 296.82
                 Mean success rate: 0.00
                  Mean reward/step: 0.53
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 974848
                    Iteration time: 2.58s
                        Total time: 300.30s
                               ETA: 504409.3s

################################################################################
                    [1m Learning iteration 119/200000 [0m

                       Computation: 3159 steps/s (collection: 0.505s, learning 2.087s)
               Value function loss: 60.6553
                    Surrogate loss: 0.0057
             Mean action noise std: 1.00
                       Mean reward: 144.26
               Mean episode length: 269.69
                 Mean success rate: 0.00
                  Mean reward/step: 0.54
       Mean episode length/episode: 26.17
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 2.59s
                        Total time: 302.89s
                               ETA: 504521.8s

################################################################################
                    [1m Learning iteration 120/200000 [0m

                       Computation: 3180 steps/s (collection: 0.512s, learning 2.064s)
               Value function loss: 44.6398
                    Surrogate loss: 0.0056
             Mean action noise std: 1.00
                       Mean reward: 128.94
               Mean episode length: 245.47
                 Mean success rate: 0.00
                  Mean reward/step: 0.55
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 991232
                    Iteration time: 2.58s
                        Total time: 305.47s
                               ETA: 504604.6s

################################################################################
                    [1m Learning iteration 121/200000 [0m

                       Computation: 3122 steps/s (collection: 0.541s, learning 2.083s)
               Value function loss: 37.4460
                    Surrogate loss: 0.0066
             Mean action noise std: 1.00
                       Mean reward: 126.06
               Mean episode length: 240.62
                 Mean success rate: 0.00
                  Mean reward/step: 0.56
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 999424
                    Iteration time: 2.62s
                        Total time: 308.09s
                               ETA: 504764.8s

################################################################################
                    [1m Learning iteration 122/200000 [0m

                       Computation: 3233 steps/s (collection: 0.481s, learning 2.052s)
               Value function loss: 27.6380
                    Surrogate loss: 0.0073
             Mean action noise std: 1.00
                       Mean reward: 125.79
               Mean episode length: 238.15
                 Mean success rate: 0.00
                  Mean reward/step: 0.55
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 1007616
                    Iteration time: 2.53s
                        Total time: 310.63s
                               ETA: 504775.4s

################################################################################
                    [1m Learning iteration 123/200000 [0m

                       Computation: 3178 steps/s (collection: 0.485s, learning 2.092s)
               Value function loss: 29.0751
                    Surrogate loss: 0.0070
             Mean action noise std: 1.00
                       Mean reward: 117.38
               Mean episode length: 218.76
                 Mean success rate: 0.00
                  Mean reward/step: 0.55
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1015808
                    Iteration time: 2.58s
                        Total time: 313.20s
                               ETA: 504856.6s

################################################################################
                    [1m Learning iteration 124/200000 [0m

                       Computation: 3176 steps/s (collection: 0.493s, learning 2.085s)
               Value function loss: 38.4934
                    Surrogate loss: 0.0068
             Mean action noise std: 1.00
                       Mean reward: 113.13
               Mean episode length: 213.34
                 Mean success rate: 0.00
                  Mean reward/step: 0.57
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 1024000
                    Iteration time: 2.58s
                        Total time: 315.78s
                               ETA: 504938.5s

################################################################################
                    [1m Learning iteration 125/200000 [0m

                       Computation: 3014 steps/s (collection: 0.629s, learning 2.089s)
               Value function loss: 48.2109
                    Surrogate loss: 0.0062
             Mean action noise std: 1.00
                       Mean reward: 112.24
               Mean episode length: 204.06
                 Mean success rate: 0.00
                  Mean reward/step: 0.59
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 1032192
                    Iteration time: 2.72s
                        Total time: 318.50s
                               ETA: 505239.8s

################################################################################
                    [1m Learning iteration 126/200000 [0m

                       Computation: 3201 steps/s (collection: 0.509s, learning 2.049s)
               Value function loss: 44.4985
                    Surrogate loss: 0.0064
             Mean action noise std: 1.00
                       Mean reward: 106.72
               Mean episode length: 188.56
                 Mean success rate: 0.00
                  Mean reward/step: 0.65
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 1040384
                    Iteration time: 2.56s
                        Total time: 321.06s
                               ETA: 505285.8s

################################################################################
                    [1m Learning iteration 127/200000 [0m

                       Computation: 3167 steps/s (collection: 0.496s, learning 2.090s)
               Value function loss: 47.2885
                    Surrogate loss: 0.0082
             Mean action noise std: 1.00
                       Mean reward: 105.56
               Mean episode length: 186.97
                 Mean success rate: 0.00
                  Mean reward/step: 0.67
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 1048576
                    Iteration time: 2.59s
                        Total time: 323.64s
                               ETA: 505374.2s

################################################################################
                    [1m Learning iteration 128/200000 [0m

                       Computation: 3186 steps/s (collection: 0.519s, learning 2.052s)
               Value function loss: 62.0728
                    Surrogate loss: 0.0072
             Mean action noise std: 1.00
                       Mean reward: 99.52
               Mean episode length: 173.99
                 Mean success rate: 0.00
                  Mean reward/step: 0.66
       Mean episode length/episode: 26.34
--------------------------------------------------------------------------------
                   Total timesteps: 1056768
                    Iteration time: 2.57s
                        Total time: 326.22s
                               ETA: 505437.0s

################################################################################
                    [1m Learning iteration 129/200000 [0m

                       Computation: 3130 steps/s (collection: 0.544s, learning 2.072s)
               Value function loss: 32.4045
                    Surrogate loss: 0.0057
             Mean action noise std: 1.00
                       Mean reward: 102.28
               Mean episode length: 178.88
                 Mean success rate: 0.00
                  Mean reward/step: 0.66
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1064960
                    Iteration time: 2.62s
                        Total time: 328.83s
                               ETA: 505569.2s

################################################################################
                    [1m Learning iteration 130/200000 [0m

                       Computation: 3184 steps/s (collection: 0.511s, learning 2.062s)
               Value function loss: 43.3452
                    Surrogate loss: 0.0079
             Mean action noise std: 1.00
                       Mean reward: 101.88
               Mean episode length: 178.90
                 Mean success rate: 0.00
                  Mean reward/step: 0.67
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1073152
                    Iteration time: 2.57s
                        Total time: 331.40s
                               ETA: 505632.2s

################################################################################
                    [1m Learning iteration 131/200000 [0m

                       Computation: 3204 steps/s (collection: 0.502s, learning 2.054s)
               Value function loss: 47.2233
                    Surrogate loss: 0.0061
             Mean action noise std: 1.00
                       Mean reward: 117.36
               Mean episode length: 199.82
                 Mean success rate: 0.00
                  Mean reward/step: 0.66
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 2.56s
                        Total time: 333.96s
                               ETA: 505670.0s

################################################################################
                    [1m Learning iteration 132/200000 [0m

                       Computation: 3192 steps/s (collection: 0.490s, learning 2.077s)
               Value function loss: 69.7521
                    Surrogate loss: 0.0059
             Mean action noise std: 1.00
                       Mean reward: 125.83
               Mean episode length: 206.72
                 Mean success rate: 0.00
                  Mean reward/step: 0.68
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 1089536
                    Iteration time: 2.57s
                        Total time: 336.53s
                               ETA: 505722.1s

################################################################################
                    [1m Learning iteration 133/200000 [0m

                       Computation: 3050 steps/s (collection: 0.595s, learning 2.090s)
               Value function loss: 90.8661
                    Surrogate loss: 0.0070
             Mean action noise std: 1.00
                       Mean reward: 124.97
               Mean episode length: 198.47
                 Mean success rate: 0.00
                  Mean reward/step: 0.67
       Mean episode length/episode: 26.34
--------------------------------------------------------------------------------
                   Total timesteps: 1097728
                    Iteration time: 2.69s
                        Total time: 339.21s
                               ETA: 505951.1s

################################################################################
                    [1m Learning iteration 134/200000 [0m

                       Computation: 3199 steps/s (collection: 0.502s, learning 2.058s)
               Value function loss: 50.3671
                    Surrogate loss: 0.0064
             Mean action noise std: 1.00
                       Mean reward: 134.30
               Mean episode length: 208.01
                 Mean success rate: 0.00
                  Mean reward/step: 0.64
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 1105920
                    Iteration time: 2.56s
                        Total time: 341.77s
                               ETA: 505991.1s

################################################################################
                    [1m Learning iteration 135/200000 [0m

                       Computation: 3208 steps/s (collection: 0.497s, learning 2.056s)
               Value function loss: 45.8494
                    Surrogate loss: 0.0058
             Mean action noise std: 1.00
                       Mean reward: 128.06
               Mean episode length: 195.73
                 Mean success rate: 0.00
                  Mean reward/step: 0.64
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 1114112
                    Iteration time: 2.55s
                        Total time: 344.33s
                               ETA: 506019.9s

################################################################################
                    [1m Learning iteration 136/200000 [0m

                       Computation: 3156 steps/s (collection: 0.523s, learning 2.072s)
               Value function loss: 51.9803
                    Surrogate loss: 0.0069
             Mean action noise std: 1.00
                       Mean reward: 131.99
               Mean episode length: 202.34
                 Mean success rate: 0.00
                  Mean reward/step: 0.65
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1122304
                    Iteration time: 2.59s
                        Total time: 346.92s
                               ETA: 506109.5s

################################################################################
                    [1m Learning iteration 137/200000 [0m

                       Computation: 3133 steps/s (collection: 0.517s, learning 2.097s)
               Value function loss: 50.4404
                    Surrogate loss: 0.0071
             Mean action noise std: 1.00
                       Mean reward: 130.22
               Mean episode length: 203.31
                 Mean success rate: 0.00
                  Mean reward/step: 0.69
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1130496
                    Iteration time: 2.61s
                        Total time: 349.53s
                               ETA: 506225.2s

################################################################################
                    [1m Learning iteration 138/200000 [0m

                       Computation: 3226 steps/s (collection: 0.497s, learning 2.042s)
               Value function loss: 77.0926
                    Surrogate loss: 0.0064
             Mean action noise std: 1.00
                       Mean reward: 116.57
               Mean episode length: 177.50
                 Mean success rate: 0.00
                  Mean reward/step: 0.72
       Mean episode length/episode: 25.52
--------------------------------------------------------------------------------
                   Total timesteps: 1138688
                    Iteration time: 2.54s
                        Total time: 352.07s
                               ETA: 506231.5s

################################################################################
                    [1m Learning iteration 139/200000 [0m

                       Computation: 3187 steps/s (collection: 0.507s, learning 2.062s)
               Value function loss: 54.6361
                    Surrogate loss: 0.0062
             Mean action noise std: 1.00
                       Mean reward: 108.93
               Mean episode length: 168.51
                 Mean success rate: 0.00
                  Mean reward/step: 0.74
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 1146880
                    Iteration time: 2.57s
                        Total time: 354.64s
                               ETA: 506281.5s

################################################################################
                    [1m Learning iteration 140/200000 [0m

                       Computation: 3068 steps/s (collection: 0.602s, learning 2.068s)
               Value function loss: 77.1114
                    Surrogate loss: 0.0054
             Mean action noise std: 1.00
                       Mean reward: 100.16
               Mean episode length: 149.63
                 Mean success rate: 0.00
                  Mean reward/step: 0.75
       Mean episode length/episode: 25.13
--------------------------------------------------------------------------------
                   Total timesteps: 1155072
                    Iteration time: 2.67s
                        Total time: 357.31s
                               ETA: 506473.0s

################################################################################
                    [1m Learning iteration 141/200000 [0m

                       Computation: 3239 steps/s (collection: 0.511s, learning 2.018s)
               Value function loss: 72.5914
                    Surrogate loss: 0.0086
             Mean action noise std: 1.00
                       Mean reward: 101.39
               Mean episode length: 153.49
                 Mean success rate: 0.00
                  Mean reward/step: 0.75
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 1163264
                    Iteration time: 2.53s
                        Total time: 359.84s
                               ETA: 506462.8s

################################################################################
                    [1m Learning iteration 142/200000 [0m

                       Computation: 3173 steps/s (collection: 0.508s, learning 2.074s)
               Value function loss: 70.1990
                    Surrogate loss: 0.0071
             Mean action noise std: 1.00
                       Mean reward: 112.06
               Mean episode length: 173.59
                 Mean success rate: 0.00
                  Mean reward/step: 0.75
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 1171456
                    Iteration time: 2.58s
                        Total time: 362.42s
                               ETA: 506526.6s

################################################################################
                    [1m Learning iteration 143/200000 [0m

                       Computation: 3263 steps/s (collection: 0.488s, learning 2.023s)
               Value function loss: 59.9770
                    Surrogate loss: 0.0075
             Mean action noise std: 1.00
                       Mean reward: 113.20
               Mean episode length: 170.89
                 Mean success rate: 0.00
                  Mean reward/step: 0.74
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 2.51s
                        Total time: 364.93s
                               ETA: 506490.9s

################################################################################
                    [1m Learning iteration 144/200000 [0m

                       Computation: 3103 steps/s (collection: 0.544s, learning 2.096s)
               Value function loss: 66.0097
                    Surrogate loss: 0.0067
             Mean action noise std: 1.00
                       Mean reward: 116.68
               Mean episode length: 176.26
                 Mean success rate: 0.00
                  Mean reward/step: 0.74
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1187840
                    Iteration time: 2.64s
                        Total time: 367.57s
                               ETA: 506633.6s

################################################################################
                    [1m Learning iteration 145/200000 [0m

                       Computation: 3082 steps/s (collection: 0.572s, learning 2.086s)
               Value function loss: 80.4836
                    Surrogate loss: 0.0078
             Mean action noise std: 1.00
                       Mean reward: 114.69
               Mean episode length: 170.19
                 Mean success rate: 0.00
                  Mean reward/step: 0.75
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 1196032
                    Iteration time: 2.66s
                        Total time: 370.23s
                               ETA: 506798.8s

################################################################################
                    [1m Learning iteration 146/200000 [0m

                       Computation: 3255 steps/s (collection: 0.480s, learning 2.037s)
               Value function loss: 67.5109
                    Surrogate loss: 0.0080
             Mean action noise std: 1.00
                       Mean reward: 123.14
               Mean episode length: 178.41
                 Mean success rate: 0.00
                  Mean reward/step: 0.75
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 1204224
                    Iteration time: 2.52s
                        Total time: 372.75s
                               ETA: 506769.8s

################################################################################
                    [1m Learning iteration 147/200000 [0m

                       Computation: 3182 steps/s (collection: 0.492s, learning 2.083s)
               Value function loss: 68.3942
                    Surrogate loss: 0.0078
             Mean action noise std: 1.00
                       Mean reward: 122.62
               Mean episode length: 172.01
                 Mean success rate: 0.00
                  Mean reward/step: 0.76
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 1212416
                    Iteration time: 2.57s
                        Total time: 375.32s
                               ETA: 506819.6s

################################################################################
                    [1m Learning iteration 148/200000 [0m

                       Computation: 3128 steps/s (collection: 0.564s, learning 2.055s)
               Value function loss: 70.4488
                    Surrogate loss: 0.0072
             Mean action noise std: 1.00
                       Mean reward: 123.13
               Mean episode length: 173.38
                 Mean success rate: 0.00
                  Mean reward/step: 0.73
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 1220608
                    Iteration time: 2.62s
                        Total time: 377.94s
                               ETA: 506928.3s

################################################################################
                    [1m Learning iteration 149/200000 [0m

                       Computation: 3246 steps/s (collection: 0.501s, learning 2.022s)
               Value function loss: 82.6221
                    Surrogate loss: 0.0067
             Mean action noise std: 1.00
                       Mean reward: 130.30
               Mean episode length: 177.70
                 Mean success rate: 0.00
                  Mean reward/step: 0.74
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 1228800
                    Iteration time: 2.52s
                        Total time: 380.46s
                               ETA: 506908.7s

################################################################################
                    [1m Learning iteration 150/200000 [0m

                       Computation: 3232 steps/s (collection: 0.487s, learning 2.048s)
               Value function loss: 79.7242
                    Surrogate loss: 0.0065
             Mean action noise std: 1.00
                       Mean reward: 137.26
               Mean episode length: 186.88
                 Mean success rate: 0.00
                  Mean reward/step: 0.81
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1236992
                    Iteration time: 2.53s
                        Total time: 383.00s
                               ETA: 506903.6s

################################################################################
                    [1m Learning iteration 151/200000 [0m

                       Computation: 3224 steps/s (collection: 0.472s, learning 2.069s)
               Value function loss: 62.0816
                    Surrogate loss: 0.0069
             Mean action noise std: 1.00
                       Mean reward: 144.08
               Mean episode length: 190.15
                 Mean success rate: 0.00
                  Mean reward/step: 0.77
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1245184
                    Iteration time: 2.54s
                        Total time: 385.54s
                               ETA: 506906.9s

################################################################################
                    [1m Learning iteration 152/200000 [0m

                       Computation: 3224 steps/s (collection: 0.502s, learning 2.038s)
               Value function loss: 52.4647
                    Surrogate loss: 0.0072
             Mean action noise std: 1.00
                       Mean reward: 144.30
               Mean episode length: 190.28
                 Mean success rate: 0.00
                  Mean reward/step: 0.79
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 1253376
                    Iteration time: 2.54s
                        Total time: 388.08s
                               ETA: 506909.3s

################################################################################
                    [1m Learning iteration 153/200000 [0m

                       Computation: 3149 steps/s (collection: 0.523s, learning 2.078s)
               Value function loss: 87.7015
                    Surrogate loss: 0.0066
             Mean action noise std: 1.00
                       Mean reward: 141.52
               Mean episode length: 184.25
                 Mean success rate: 0.00
                  Mean reward/step: 0.88
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1261568
                    Iteration time: 2.60s
                        Total time: 390.68s
                               ETA: 506990.3s

################################################################################
                    [1m Learning iteration 154/200000 [0m

                       Computation: 3142 steps/s (collection: 0.554s, learning 2.053s)
               Value function loss: 101.3591
                    Surrogate loss: 0.0050
             Mean action noise std: 1.00
                       Mean reward: 153.06
               Mean episode length: 201.31
                 Mean success rate: 0.00
                  Mean reward/step: 0.83
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 1269760
                    Iteration time: 2.61s
                        Total time: 393.29s
                               ETA: 507077.4s

################################################################################
                    [1m Learning iteration 155/200000 [0m

                       Computation: 3141 steps/s (collection: 0.505s, learning 2.103s)
               Value function loss: 100.9945
                    Surrogate loss: 0.0063
             Mean action noise std: 1.00
                       Mean reward: 157.82
               Mean episode length: 207.69
                 Mean success rate: 0.00
                  Mean reward/step: 0.79
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 2.61s
                        Total time: 395.90s
                               ETA: 507164.6s

################################################################################
                    [1m Learning iteration 156/200000 [0m

                       Computation: 3087 steps/s (collection: 0.587s, learning 2.066s)
               Value function loss: 89.9694
                    Surrogate loss: 0.0079
             Mean action noise std: 1.00
                       Mean reward: 170.19
               Mean episode length: 221.00
                 Mean success rate: 0.00
                  Mean reward/step: 0.81
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 1286144
                    Iteration time: 2.65s
                        Total time: 398.55s
                               ETA: 507309.3s

################################################################################
                    [1m Learning iteration 157/200000 [0m

                       Computation: 3203 steps/s (collection: 0.521s, learning 2.036s)
               Value function loss: 85.3397
                    Surrogate loss: 0.0077
             Mean action noise std: 1.00
                       Mean reward: 176.91
               Mean episode length: 230.83
                 Mean success rate: 0.00
                  Mean reward/step: 0.85
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1294336
                    Iteration time: 2.56s
                        Total time: 401.11s
                               ETA: 507330.1s

################################################################################
                    [1m Learning iteration 158/200000 [0m

                       Computation: 3136 steps/s (collection: 0.502s, learning 2.109s)
               Value function loss: 118.7698
                    Surrogate loss: 0.0067
             Mean action noise std: 1.00
                       Mean reward: 194.38
               Mean episode length: 249.91
                 Mean success rate: 0.00
                  Mean reward/step: 0.88
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 1302528
                    Iteration time: 2.61s
                        Total time: 403.72s
                               ETA: 507419.5s

################################################################################
                    [1m Learning iteration 159/200000 [0m

                       Computation: 3190 steps/s (collection: 0.503s, learning 2.065s)
               Value function loss: 117.0740
                    Surrogate loss: 0.0082
             Mean action noise std: 1.00
                       Mean reward: 195.80
               Mean episode length: 248.07
                 Mean success rate: 0.00
                  Mean reward/step: 0.92
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 1310720
                    Iteration time: 2.57s
                        Total time: 406.29s
                               ETA: 507453.0s

################################################################################
                    [1m Learning iteration 160/200000 [0m

                       Computation: 3070 steps/s (collection: 0.588s, learning 2.080s)
               Value function loss: 148.6263
                    Surrogate loss: 0.0073
             Mean action noise std: 1.00
                       Mean reward: 200.34
               Mean episode length: 250.36
                 Mean success rate: 0.00
                  Mean reward/step: 0.92
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1318912
                    Iteration time: 2.67s
                        Total time: 408.95s
                               ETA: 507610.5s

################################################################################
                    [1m Learning iteration 161/200000 [0m

                       Computation: 3087 steps/s (collection: 0.589s, learning 2.065s)
               Value function loss: 163.7232
                    Surrogate loss: 0.0092
             Mean action noise std: 1.00
                       Mean reward: 191.43
               Mean episode length: 230.66
                 Mean success rate: 0.00
                  Mean reward/step: 0.93
       Mean episode length/episode: 26.01
--------------------------------------------------------------------------------
                   Total timesteps: 1327104
                    Iteration time: 2.65s
                        Total time: 411.61s
                               ETA: 507748.0s

################################################################################
                    [1m Learning iteration 162/200000 [0m

                       Computation: 3250 steps/s (collection: 0.487s, learning 2.033s)
               Value function loss: 143.6504
                    Surrogate loss: 0.0075
             Mean action noise std: 1.00
                       Mean reward: 167.53
               Mean episode length: 200.83
                 Mean success rate: 0.00
                  Mean reward/step: 0.91
       Mean episode length/episode: 26.09
--------------------------------------------------------------------------------
                   Total timesteps: 1335296
                    Iteration time: 2.52s
                        Total time: 414.13s
                               ETA: 507719.8s

################################################################################
                    [1m Learning iteration 163/200000 [0m

                       Computation: 3141 steps/s (collection: 0.530s, learning 2.077s)
               Value function loss: 116.1354
                    Surrogate loss: 0.0083
             Mean action noise std: 1.00
                       Mean reward: 159.49
               Mean episode length: 189.54
                 Mean success rate: 0.00
                  Mean reward/step: 0.91
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 1343488
                    Iteration time: 2.61s
                        Total time: 416.73s
                               ETA: 507798.8s

################################################################################
                    [1m Learning iteration 164/200000 [0m

                       Computation: 3233 steps/s (collection: 0.476s, learning 2.058s)
               Value function loss: 111.0317
                    Surrogate loss: 0.0088
             Mean action noise std: 1.00
                       Mean reward: 153.26
               Mean episode length: 178.82
                 Mean success rate: 0.00
                  Mean reward/step: 0.93
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 1351680
                    Iteration time: 2.53s
                        Total time: 419.27s
                               ETA: 507787.5s

################################################################################
                    [1m Learning iteration 165/200000 [0m

                       Computation: 3192 steps/s (collection: 0.494s, learning 2.072s)
               Value function loss: 119.3457
                    Surrogate loss: 0.0086
             Mean action noise std: 1.00
                       Mean reward: 153.64
               Mean episode length: 178.43
                 Mean success rate: 0.00
                  Mean reward/step: 0.95
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 1359872
                    Iteration time: 2.57s
                        Total time: 421.83s
                               ETA: 507814.8s

################################################################################
                    [1m Learning iteration 166/200000 [0m

                       Computation: 3214 steps/s (collection: 0.504s, learning 2.045s)
               Value function loss: 104.1160
                    Surrogate loss: 0.0061
             Mean action noise std: 1.00
                       Mean reward: 164.93
               Mean episode length: 187.28
                 Mean success rate: 0.00
                  Mean reward/step: 0.98
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1368064
                    Iteration time: 2.55s
                        Total time: 424.38s
                               ETA: 507821.4s

################################################################################
                    [1m Learning iteration 167/200000 [0m

                       Computation: 3102 steps/s (collection: 0.554s, learning 2.086s)
               Value function loss: 111.7353
                    Surrogate loss: 0.0077
             Mean action noise std: 1.00
                       Mean reward: 167.94
               Mean episode length: 188.46
                 Mean success rate: 0.00
                  Mean reward/step: 1.00
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 2.64s
                        Total time: 427.02s
                               ETA: 507936.9s

################################################################################
                    [1m Learning iteration 168/200000 [0m

                       Computation: 3101 steps/s (collection: 0.559s, learning 2.083s)
               Value function loss: 125.6749
                    Surrogate loss: 0.0058
             Mean action noise std: 1.00
                       Mean reward: 174.04
               Mean episode length: 192.08
                 Mean success rate: 0.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1384448
                    Iteration time: 2.64s
                        Total time: 429.67s
                               ETA: 508052.4s

################################################################################
                    [1m Learning iteration 169/200000 [0m

                       Computation: 3242 steps/s (collection: 0.476s, learning 2.050s)
               Value function loss: 120.0694
                    Surrogate loss: 0.0083
             Mean action noise std: 1.00
                       Mean reward: 173.06
               Mean episode length: 191.72
                 Mean success rate: 0.00
                  Mean reward/step: 1.01
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 1392640
                    Iteration time: 2.53s
                        Total time: 432.19s
                               ETA: 508031.2s

################################################################################
                    [1m Learning iteration 170/200000 [0m

                       Computation: 3152 steps/s (collection: 0.520s, learning 2.078s)
               Value function loss: 134.3122
                    Surrogate loss: 0.0087
             Mean action noise std: 1.00
                       Mean reward: 159.82
               Mean episode length: 175.39
                 Mean success rate: 0.00
                  Mean reward/step: 0.96
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 1400832
                    Iteration time: 2.60s
                        Total time: 434.79s
                               ETA: 508094.3s

################################################################################
                    [1m Learning iteration 171/200000 [0m

                       Computation: 3118 steps/s (collection: 0.533s, learning 2.094s)
               Value function loss: 91.3462
                    Surrogate loss: 0.0085
             Mean action noise std: 1.00
                       Mean reward: 159.11
               Mean episode length: 173.75
                 Mean success rate: 0.00
                  Mean reward/step: 0.96
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1409024
                    Iteration time: 2.63s
                        Total time: 437.42s
                               ETA: 508189.6s

################################################################################
                    [1m Learning iteration 172/200000 [0m

                       Computation: 3123 steps/s (collection: 0.531s, learning 2.092s)
               Value function loss: 114.4413
                    Surrogate loss: 0.0109
             Mean action noise std: 1.00
                       Mean reward: 180.10
               Mean episode length: 193.19
                 Mean success rate: 0.00
                  Mean reward/step: 0.99
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 1417216
                    Iteration time: 2.62s
                        Total time: 440.04s
                               ETA: 508279.2s

################################################################################
                    [1m Learning iteration 173/200000 [0m

                       Computation: 3166 steps/s (collection: 0.527s, learning 2.060s)
               Value function loss: 89.1870
                    Surrogate loss: 0.0093
             Mean action noise std: 1.00
                       Mean reward: 181.39
               Mean episode length: 193.47
                 Mean success rate: 0.00
                  Mean reward/step: 1.01
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 1425408
                    Iteration time: 2.59s
                        Total time: 442.63s
                               ETA: 508326.5s

################################################################################
                    [1m Learning iteration 174/200000 [0m

                       Computation: 3231 steps/s (collection: 0.478s, learning 2.057s)
               Value function loss: 98.4481
                    Surrogate loss: 0.0107
             Mean action noise std: 0.99
                       Mean reward: 197.10
               Mean episode length: 210.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.00
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1433600
                    Iteration time: 2.53s
                        Total time: 445.16s
                               ETA: 508313.6s

################################################################################
                    [1m Learning iteration 175/200000 [0m

                       Computation: 3063 steps/s (collection: 0.604s, learning 2.070s)
               Value function loss: 136.9458
                    Surrogate loss: 0.0112
             Mean action noise std: 0.99
                       Mean reward: 205.04
               Mean episode length: 217.97
                 Mean success rate: 0.00
                  Mean reward/step: 1.02
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1441792
                    Iteration time: 2.67s
                        Total time: 447.84s
                               ETA: 508458.7s

################################################################################
                    [1m Learning iteration 176/200000 [0m

                       Computation: 3165 steps/s (collection: 0.512s, learning 2.077s)
               Value function loss: 134.8353
                    Surrogate loss: 0.0112
             Mean action noise std: 0.99
                       Mean reward: 214.72
               Mean episode length: 226.46
                 Mean success rate: 0.00
                  Mean reward/step: 1.01
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 1449984
                    Iteration time: 2.59s
                        Total time: 450.42s
                               ETA: 508505.6s

################################################################################
                    [1m Learning iteration 177/200000 [0m

                       Computation: 3253 steps/s (collection: 0.488s, learning 2.030s)
               Value function loss: 144.6167
                    Surrogate loss: 0.0099
             Mean action noise std: 0.99
                       Mean reward: 237.56
               Mean episode length: 249.52
                 Mean success rate: 0.00
                  Mean reward/step: 1.04
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1458176
                    Iteration time: 2.52s
                        Total time: 452.94s
                               ETA: 508473.2s

################################################################################
                    [1m Learning iteration 178/200000 [0m

                       Computation: 3162 steps/s (collection: 0.516s, learning 2.074s)
               Value function loss: 155.8255
                    Surrogate loss: 0.0102
             Mean action noise std: 0.99
                       Mean reward: 251.23
               Mean episode length: 259.61
                 Mean success rate: 0.00
                  Mean reward/step: 0.98
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1466368
                    Iteration time: 2.59s
                        Total time: 455.53s
                               ETA: 508521.3s

################################################################################
                    [1m Learning iteration 179/200000 [0m

                       Computation: 2968 steps/s (collection: 0.648s, learning 2.112s)
               Value function loss: 124.2373
                    Surrogate loss: 0.0096
             Mean action noise std: 0.99
                       Mean reward: 254.23
               Mean episode length: 262.07
                 Mean success rate: 0.00
                  Mean reward/step: 1.00
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 2.76s
                        Total time: 458.29s
                               ETA: 508757.2s

################################################################################
                    [1m Learning iteration 180/200000 [0m

                       Computation: 3087 steps/s (collection: 0.600s, learning 2.053s)
               Value function loss: 113.0637
                    Surrogate loss: 0.0108
             Mean action noise std: 0.99
                       Mean reward: 264.97
               Mean episode length: 271.68
                 Mean success rate: 0.00
                  Mean reward/step: 0.98
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1482752
                    Iteration time: 2.65s
                        Total time: 460.94s
                               ETA: 508872.8s

################################################################################
                    [1m Learning iteration 181/200000 [0m

                       Computation: 3236 steps/s (collection: 0.482s, learning 2.050s)
               Value function loss: 135.2038
                    Surrogate loss: 0.0101
             Mean action noise std: 0.99
                       Mean reward: 269.76
               Mean episode length: 274.93
                 Mean success rate: 0.00
                  Mean reward/step: 1.04
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 1490944
                    Iteration time: 2.53s
                        Total time: 463.48s
                               ETA: 508853.4s

################################################################################
                    [1m Learning iteration 182/200000 [0m

                       Computation: 3173 steps/s (collection: 0.525s, learning 2.056s)
               Value function loss: 126.0222
                    Surrogate loss: 0.0078
             Mean action noise std: 0.99
                       Mean reward: 278.30
               Mean episode length: 280.93
                 Mean success rate: 0.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 1499136
                    Iteration time: 2.58s
                        Total time: 466.06s
                               ETA: 508889.1s

################################################################################
                    [1m Learning iteration 183/200000 [0m

                       Computation: 3090 steps/s (collection: 0.590s, learning 2.060s)
               Value function loss: 141.5156
                    Surrogate loss: 0.0080
             Mean action noise std: 0.99
                       Mean reward: 285.45
               Mean episode length: 286.72
                 Mean success rate: 0.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1507328
                    Iteration time: 2.65s
                        Total time: 468.71s
                               ETA: 508999.4s

################################################################################
                    [1m Learning iteration 184/200000 [0m

                       Computation: 3214 steps/s (collection: 0.502s, learning 2.046s)
               Value function loss: 135.3548
                    Surrogate loss: 0.0098
             Mean action noise std: 0.99
                       Mean reward: 283.96
               Mean episode length: 284.89
                 Mean success rate: 0.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1515520
                    Iteration time: 2.55s
                        Total time: 471.26s
                               ETA: 508998.3s

################################################################################
                    [1m Learning iteration 185/200000 [0m

                       Computation: 3187 steps/s (collection: 0.507s, learning 2.063s)
               Value function loss: 159.7114
                    Surrogate loss: 0.0092
             Mean action noise std: 0.99
                       Mean reward: 286.42
               Mean episode length: 286.73
                 Mean success rate: 0.00
                  Mean reward/step: 1.04
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 1523712
                    Iteration time: 2.57s
                        Total time: 473.83s
                               ETA: 509020.1s

################################################################################
                    [1m Learning iteration 186/200000 [0m

                       Computation: 3226 steps/s (collection: 0.500s, learning 2.039s)
               Value function loss: 108.5046
                    Surrogate loss: 0.0106
             Mean action noise std: 0.99
                       Mean reward: 286.47
               Mean episode length: 284.15
                 Mean success rate: 0.00
                  Mean reward/step: 1.02
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1531904
                    Iteration time: 2.54s
                        Total time: 476.37s
                               ETA: 509008.6s

################################################################################
                    [1m Learning iteration 187/200000 [0m

                       Computation: 3221 steps/s (collection: 0.496s, learning 2.047s)
               Value function loss: 144.5339
                    Surrogate loss: 0.0103
             Mean action noise std: 0.99
                       Mean reward: 282.13
               Mean episode length: 276.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1540096
                    Iteration time: 2.54s
                        Total time: 478.91s
                               ETA: 509001.0s

################################################################################
                    [1m Learning iteration 188/200000 [0m

                       Computation: 3188 steps/s (collection: 0.519s, learning 2.050s)
               Value function loss: 134.1297
                    Surrogate loss: 0.0103
             Mean action noise std: 0.99
                       Mean reward: 264.86
               Mean episode length: 260.30
                 Mean success rate: 0.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1548288
                    Iteration time: 2.57s
                        Total time: 481.48s
                               ETA: 509021.2s

################################################################################
                    [1m Learning iteration 189/200000 [0m

                       Computation: 3247 steps/s (collection: 0.496s, learning 2.026s)
               Value function loss: 104.0785
                    Surrogate loss: 0.0112
             Mean action noise std: 0.99
                       Mean reward: 265.74
               Mean episode length: 256.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 1556480
                    Iteration time: 2.52s
                        Total time: 484.00s
                               ETA: 508992.3s

################################################################################
                    [1m Learning iteration 190/200000 [0m

                       Computation: 3152 steps/s (collection: 0.547s, learning 2.051s)
               Value function loss: 145.8132
                    Surrogate loss: 0.0106
             Mean action noise std: 0.99
                       Mean reward: 267.87
               Mean episode length: 256.31
                 Mean success rate: 0.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1564672
                    Iteration time: 2.60s
                        Total time: 486.60s
                               ETA: 509043.6s

################################################################################
                    [1m Learning iteration 191/200000 [0m

                       Computation: 3149 steps/s (collection: 0.543s, learning 2.058s)
               Value function loss: 164.6684
                    Surrogate loss: 0.0102
             Mean action noise std: 0.99
                       Mean reward: 264.13
               Mean episode length: 249.22
                 Mean success rate: 0.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 2.60s
                        Total time: 489.20s
                               ETA: 509096.6s

################################################################################
                    [1m Learning iteration 192/200000 [0m

                       Computation: 3214 steps/s (collection: 0.505s, learning 2.043s)
               Value function loss: 183.0314
                    Surrogate loss: 0.0111
             Mean action noise std: 0.99
                       Mean reward: 282.16
               Mean episode length: 263.73
                 Mean success rate: 0.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1581056
                    Iteration time: 2.55s
                        Total time: 491.75s
                               ETA: 509094.3s

################################################################################
                    [1m Learning iteration 193/200000 [0m

                       Computation: 3164 steps/s (collection: 0.532s, learning 2.057s)
               Value function loss: 161.2200
                    Surrogate loss: 0.0124
             Mean action noise std: 0.99
                       Mean reward: 280.21
               Mean episode length: 262.23
                 Mean success rate: 0.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1589248
                    Iteration time: 2.59s
                        Total time: 494.34s
                               ETA: 509133.7s

################################################################################
                    [1m Learning iteration 194/200000 [0m

                       Computation: 3164 steps/s (collection: 0.536s, learning 2.052s)
               Value function loss: 157.7089
                    Surrogate loss: 0.0094
             Mean action noise std: 0.99
                       Mean reward: 292.88
               Mean episode length: 270.65
                 Mean success rate: 0.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 1597440
                    Iteration time: 2.59s
                        Total time: 496.93s
                               ETA: 509172.3s

################################################################################
                    [1m Learning iteration 195/200000 [0m

                       Computation: 3172 steps/s (collection: 0.524s, learning 2.058s)
               Value function loss: 140.1793
                    Surrogate loss: 0.0113
             Mean action noise std: 0.99
                       Mean reward: 295.34
               Mean episode length: 270.10
                 Mean success rate: 0.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1605632
                    Iteration time: 2.58s
                        Total time: 499.51s
                               ETA: 509204.4s

################################################################################
                    [1m Learning iteration 196/200000 [0m

                       Computation: 3234 steps/s (collection: 0.501s, learning 2.032s)
               Value function loss: 130.2045
                    Surrogate loss: 0.0115
             Mean action noise std: 0.99
                       Mean reward: 292.89
               Mean episode length: 265.50
                 Mean success rate: 0.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1613824
                    Iteration time: 2.53s
                        Total time: 502.04s
                               ETA: 509185.7s

################################################################################
                    [1m Learning iteration 197/200000 [0m

                       Computation: 3257 steps/s (collection: 0.488s, learning 2.026s)
               Value function loss: 165.4462
                    Surrogate loss: 0.0110
             Mean action noise std: 0.99
                       Mean reward: 294.37
               Mean episode length: 265.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 1622016
                    Iteration time: 2.51s
                        Total time: 504.55s
                               ETA: 509149.0s

################################################################################
                    [1m Learning iteration 198/200000 [0m

                       Computation: 3199 steps/s (collection: 0.511s, learning 2.049s)
               Value function loss: 175.4147
                    Surrogate loss: 0.0108
             Mean action noise std: 0.99
                       Mean reward: 288.42
               Mean episode length: 258.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1630208
                    Iteration time: 2.56s
                        Total time: 507.11s
                               ETA: 509158.7s

################################################################################
                    [1m Learning iteration 199/200000 [0m

                       Computation: 3125 steps/s (collection: 0.523s, learning 2.097s)
               Value function loss: 200.4464
                    Surrogate loss: 0.0128
             Mean action noise std: 0.99
                       Mean reward: 285.39
               Mean episode length: 255.09
                 Mean success rate: 0.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 1638400
                    Iteration time: 2.62s
                        Total time: 509.74s
                               ETA: 509228.4s

################################################################################
                    [1m Learning iteration 200/200000 [0m

                       Computation: 3182 steps/s (collection: 0.470s, learning 2.104s)
               Value function loss: 201.4315
                    Surrogate loss: 0.0128
             Mean action noise std: 0.99
                       Mean reward: 273.10
               Mean episode length: 243.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 1646592
                    Iteration time: 2.57s
                        Total time: 512.31s
                               ETA: 509250.7s

################################################################################
                    [1m Learning iteration 201/200000 [0m

                       Computation: 3202 steps/s (collection: 0.486s, learning 2.072s)
               Value function loss: 171.2636
                    Surrogate loss: 0.0102
             Mean action noise std: 0.99
                       Mean reward: 262.36
               Mean episode length: 233.35
                 Mean success rate: 0.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 1654784
                    Iteration time: 2.56s
                        Total time: 514.87s
                               ETA: 509257.0s

################################################################################
                    [1m Learning iteration 202/200000 [0m

                       Computation: 3137 steps/s (collection: 0.541s, learning 2.070s)
               Value function loss: 155.6506
                    Surrogate loss: 0.0107
             Mean action noise std: 0.99
                       Mean reward: 258.56
               Mean episode length: 227.97
                 Mean success rate: 0.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1662976
                    Iteration time: 2.61s
                        Total time: 517.48s
                               ETA: 509315.5s

################################################################################
                    [1m Learning iteration 203/200000 [0m

                       Computation: 3103 steps/s (collection: 0.565s, learning 2.074s)
               Value function loss: 178.0227
                    Surrogate loss: 0.0094
             Mean action noise std: 0.99
                       Mean reward: 253.96
               Mean episode length: 220.29
                 Mean success rate: 0.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 2.64s
                        Total time: 520.12s
                               ETA: 509401.4s

################################################################################
                    [1m Learning iteration 204/200000 [0m

                       Computation: 3142 steps/s (collection: 0.540s, learning 2.066s)
               Value function loss: 150.3413
                    Surrogate loss: 0.0122
             Mean action noise std: 0.99
                       Mean reward: 260.92
               Mean episode length: 224.47
                 Mean success rate: 0.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1679360
                    Iteration time: 2.61s
                        Total time: 522.72s
                               ETA: 509454.4s

################################################################################
                    [1m Learning iteration 205/200000 [0m

                       Computation: 3246 steps/s (collection: 0.485s, learning 2.038s)
               Value function loss: 150.5144
                    Surrogate loss: 0.0121
             Mean action noise std: 0.99
                       Mean reward: 259.37
               Mean episode length: 220.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1687552
                    Iteration time: 2.52s
                        Total time: 525.25s
                               ETA: 509426.0s

################################################################################
                    [1m Learning iteration 206/200000 [0m

                       Computation: 3169 steps/s (collection: 0.503s, learning 2.081s)
               Value function loss: 141.8618
                    Surrogate loss: 0.0107
             Mean action noise std: 0.99
                       Mean reward: 256.18
               Mean episode length: 214.92
                 Mean success rate: 0.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1695744
                    Iteration time: 2.58s
                        Total time: 527.83s
                               ETA: 509457.3s

################################################################################
                    [1m Learning iteration 207/200000 [0m

                       Computation: 3127 steps/s (collection: 0.564s, learning 2.055s)
               Value function loss: 150.0997
                    Surrogate loss: 0.0127
             Mean action noise std: 0.99
                       Mean reward: 267.50
               Mean episode length: 224.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1703936
                    Iteration time: 2.62s
                        Total time: 530.45s
                               ETA: 509521.4s

################################################################################
                    [1m Learning iteration 208/200000 [0m

                       Computation: 3206 steps/s (collection: 0.498s, learning 2.057s)
               Value function loss: 191.1815
                    Surrogate loss: 0.0111
             Mean action noise std: 0.99
                       Mean reward: 273.34
               Mean episode length: 230.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1712128
                    Iteration time: 2.55s
                        Total time: 533.01s
                               ETA: 509523.1s

################################################################################
                    [1m Learning iteration 209/200000 [0m

                       Computation: 3150 steps/s (collection: 0.516s, learning 2.085s)
               Value function loss: 175.1147
                    Surrogate loss: 0.0117
             Mean action noise std: 0.99
                       Mean reward: 270.36
               Mean episode length: 223.82
                 Mean success rate: 0.50
                  Mean reward/step: 1.32
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 1720320
                    Iteration time: 2.60s
                        Total time: 535.61s
                               ETA: 509568.1s

################################################################################
                    [1m Learning iteration 210/200000 [0m

                       Computation: 3056 steps/s (collection: 0.597s, learning 2.083s)
               Value function loss: 134.0371
                    Surrogate loss: 0.0138
             Mean action noise std: 0.99
                       Mean reward: 270.91
               Mean episode length: 225.95
                 Mean success rate: 0.50
                  Mean reward/step: 1.25
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 1728512
                    Iteration time: 2.68s
                        Total time: 538.29s
                               ETA: 509688.0s

################################################################################
                    [1m Learning iteration 211/200000 [0m

                       Computation: 3129 steps/s (collection: 0.517s, learning 2.101s)
               Value function loss: 207.6300
                    Surrogate loss: 0.0092
             Mean action noise std: 0.99
                       Mean reward: 277.22
               Mean episode length: 229.16
                 Mean success rate: 0.50
                  Mean reward/step: 1.29
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1736704
                    Iteration time: 2.62s
                        Total time: 540.90s
                               ETA: 509748.2s

################################################################################
                    [1m Learning iteration 212/200000 [0m

                       Computation: 3080 steps/s (collection: 0.506s, learning 2.153s)
               Value function loss: 192.2290
                    Surrogate loss: 0.0110
             Mean action noise std: 0.99
                       Mean reward: 264.42
               Mean episode length: 218.01
                 Mean success rate: 0.50
                  Mean reward/step: 1.28
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 1744896
                    Iteration time: 2.66s
                        Total time: 543.56s
                               ETA: 509846.4s

################################################################################
                    [1m Learning iteration 213/200000 [0m

                       Computation: 3011 steps/s (collection: 0.583s, learning 2.137s)
               Value function loss: 196.3700
                    Surrogate loss: 0.0137
             Mean action noise std: 0.99
                       Mean reward: 268.86
               Mean episode length: 217.17
                 Mean success rate: 0.50
                  Mean reward/step: 1.30
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1753088
                    Iteration time: 2.72s
                        Total time: 546.28s
                               ETA: 510000.6s

################################################################################
                    [1m Learning iteration 214/200000 [0m

                       Computation: 3079 steps/s (collection: 0.559s, learning 2.101s)
               Value function loss: 174.0865
                    Surrogate loss: 0.0130
             Mean action noise std: 0.99
                       Mean reward: 267.01
               Mean episode length: 215.47
                 Mean success rate: 0.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 1761280
                    Iteration time: 2.66s
                        Total time: 548.94s
                               ETA: 510098.1s

################################################################################
                    [1m Learning iteration 215/200000 [0m

                       Computation: 3092 steps/s (collection: 0.520s, learning 2.129s)
               Value function loss: 244.8578
                    Surrogate loss: 0.0095
             Mean action noise std: 0.99
                       Mean reward: 271.10
               Mean episode length: 214.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 2.65s
                        Total time: 551.59s
                               ETA: 510184.0s

################################################################################
                    [1m Learning iteration 216/200000 [0m

                       Computation: 3031 steps/s (collection: 0.577s, learning 2.125s)
               Value function loss: 253.4131
                    Surrogate loss: 0.0125
             Mean action noise std: 0.99
                       Mean reward: 273.14
               Mean episode length: 219.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 25.92
--------------------------------------------------------------------------------
                   Total timesteps: 1777664
                    Iteration time: 2.70s
                        Total time: 554.29s
                               ETA: 510318.0s

################################################################################
                    [1m Learning iteration 217/200000 [0m

                       Computation: 3068 steps/s (collection: 0.569s, learning 2.101s)
               Value function loss: 242.8593
                    Surrogate loss: 0.0099
             Mean action noise std: 0.99
                       Mean reward: 265.69
               Mean episode length: 214.28
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 1785856
                    Iteration time: 2.67s
                        Total time: 556.96s
                               ETA: 510421.5s

################################################################################
                    [1m Learning iteration 218/200000 [0m

                       Computation: 2993 steps/s (collection: 0.622s, learning 2.114s)
               Value function loss: 129.3576
                    Surrogate loss: 0.0122
             Mean action noise std: 0.99
                       Mean reward: 260.03
               Mean episode length: 210.04
                 Mean success rate: 0.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 1794048
                    Iteration time: 2.74s
                        Total time: 559.70s
                               ETA: 510584.5s

################################################################################
                    [1m Learning iteration 219/200000 [0m

                       Computation: 3165 steps/s (collection: 0.513s, learning 2.075s)
               Value function loss: 173.7639
                    Surrogate loss: 0.0103
             Mean action noise std: 0.99
                       Mean reward: 257.19
               Mean episode length: 207.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 1802240
                    Iteration time: 2.59s
                        Total time: 562.29s
                               ETA: 510611.5s

################################################################################
                    [1m Learning iteration 220/200000 [0m

                       Computation: 3069 steps/s (collection: 0.525s, learning 2.144s)
               Value function loss: 168.2200
                    Surrogate loss: 0.0095
             Mean action noise std: 0.99
                       Mean reward: 237.97
               Mean episode length: 190.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 1810432
                    Iteration time: 2.67s
                        Total time: 564.96s
                               ETA: 510711.2s

################################################################################
                    [1m Learning iteration 221/200000 [0m

                       Computation: 3111 steps/s (collection: 0.531s, learning 2.101s)
               Value function loss: 157.0426
                    Surrogate loss: 0.0124
             Mean action noise std: 0.99
                       Mean reward: 241.37
               Mean episode length: 189.59
                 Mean success rate: 0.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1818624
                    Iteration time: 2.63s
                        Total time: 567.59s
                               ETA: 510777.3s

################################################################################
                    [1m Learning iteration 222/200000 [0m

                       Computation: 3238 steps/s (collection: 0.497s, learning 2.032s)
               Value function loss: 173.3428
                    Surrogate loss: 0.0121
             Mean action noise std: 0.99
                       Mean reward: 238.55
               Mean episode length: 187.20
                 Mean success rate: 0.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 1826816
                    Iteration time: 2.53s
                        Total time: 570.12s
                               ETA: 510750.6s

################################################################################
                    [1m Learning iteration 223/200000 [0m

                       Computation: 3110 steps/s (collection: 0.540s, learning 2.093s)
               Value function loss: 127.1789
                    Surrogate loss: 0.0150
             Mean action noise std: 0.99
                       Mean reward: 244.07
               Mean episode length: 190.18
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 1835008
                    Iteration time: 2.63s
                        Total time: 572.75s
                               ETA: 510816.7s

################################################################################
                    [1m Learning iteration 224/200000 [0m

                       Computation: 3225 steps/s (collection: 0.504s, learning 2.035s)
               Value function loss: 154.0355
                    Surrogate loss: 0.0113
             Mean action noise std: 0.99
                       Mean reward: 253.86
               Mean episode length: 198.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 1843200
                    Iteration time: 2.54s
                        Total time: 575.29s
                               ETA: 510798.6s

################################################################################
                    [1m Learning iteration 225/200000 [0m

                       Computation: 3101 steps/s (collection: 0.582s, learning 2.059s)
               Value function loss: 163.7282
                    Surrogate loss: 0.0121
             Mean action noise std: 0.99
                       Mean reward: 246.95
               Mean episode length: 195.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 1851392
                    Iteration time: 2.64s
                        Total time: 577.93s
                               ETA: 510871.0s

################################################################################
                    [1m Learning iteration 226/200000 [0m

                       Computation: 3120 steps/s (collection: 0.596s, learning 2.029s)
               Value function loss: 210.9954
                    Surrogate loss: 0.0128
             Mean action noise std: 0.99
                       Mean reward: 259.89
               Mean episode length: 202.60
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 26.51
--------------------------------------------------------------------------------
                   Total timesteps: 1859584
                    Iteration time: 2.63s
                        Total time: 580.56s
                               ETA: 510928.5s

################################################################################
                    [1m Learning iteration 227/200000 [0m

                       Computation: 3210 steps/s (collection: 0.491s, learning 2.060s)
               Value function loss: 136.9028
                    Surrogate loss: 0.0129
             Mean action noise std: 0.99
                       Mean reward: 259.01
               Mean episode length: 203.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 2.55s
                        Total time: 583.11s
                               ETA: 510920.9s

################################################################################
                    [1m Learning iteration 228/200000 [0m

                       Computation: 3171 steps/s (collection: 0.541s, learning 2.042s)
               Value function loss: 170.0050
                    Surrogate loss: 0.0124
             Mean action noise std: 0.99
                       Mean reward: 260.40
               Mean episode length: 206.05
                 Mean success rate: 0.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 1875968
                    Iteration time: 2.58s
                        Total time: 585.69s
                               ETA: 510940.7s

################################################################################
                    [1m Learning iteration 229/200000 [0m

                       Computation: 3120 steps/s (collection: 0.565s, learning 2.060s)
               Value function loss: 163.2055
                    Surrogate loss: 0.0133
             Mean action noise std: 0.99
                       Mean reward: 263.37
               Mean episode length: 204.10
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 1884160
                    Iteration time: 2.63s
                        Total time: 588.32s
                               ETA: 510996.9s

################################################################################
                    [1m Learning iteration 230/200000 [0m

                       Computation: 3158 steps/s (collection: 0.535s, learning 2.059s)
               Value function loss: 187.9093
                    Surrogate loss: 0.0113
             Mean action noise std: 0.99
                       Mean reward: 277.67
               Mean episode length: 213.28
                 Mean success rate: 0.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 1892352
                    Iteration time: 2.59s
                        Total time: 590.91s
                               ETA: 511025.4s

################################################################################
                    [1m Learning iteration 231/200000 [0m

                       Computation: 3181 steps/s (collection: 0.508s, learning 2.067s)
               Value function loss: 223.7156
                    Surrogate loss: 0.0118
             Mean action noise std: 0.99
                       Mean reward: 294.21
               Mean episode length: 228.82
                 Mean success rate: 0.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 1900544
                    Iteration time: 2.57s
                        Total time: 593.49s
                               ETA: 511037.1s

################################################################################
                    [1m Learning iteration 232/200000 [0m

                       Computation: 3171 steps/s (collection: 0.510s, learning 2.072s)
               Value function loss: 157.9428
                    Surrogate loss: 0.0136
             Mean action noise std: 0.99
                       Mean reward: 298.15
               Mean episode length: 231.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 1908736
                    Iteration time: 2.58s
                        Total time: 596.07s
                               ETA: 511055.6s

################################################################################
                    [1m Learning iteration 233/200000 [0m

                       Computation: 3093 steps/s (collection: 0.563s, learning 2.085s)
               Value function loss: 154.7950
                    Surrogate loss: 0.0149
             Mean action noise std: 0.99
                       Mean reward: 299.76
               Mean episode length: 232.82
                 Mean success rate: 0.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 1916928
                    Iteration time: 2.65s
                        Total time: 598.72s
                               ETA: 511130.1s

################################################################################
                    [1m Learning iteration 234/200000 [0m

                       Computation: 3117 steps/s (collection: 0.542s, learning 2.086s)
               Value function loss: 186.3511
                    Surrogate loss: 0.0125
             Mean action noise std: 0.99
                       Mean reward: 301.70
               Mean episode length: 237.72
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 1925120
                    Iteration time: 2.63s
                        Total time: 601.35s
                               ETA: 511186.1s

################################################################################
                    [1m Learning iteration 235/200000 [0m

                       Computation: 3185 steps/s (collection: 0.494s, learning 2.077s)
               Value function loss: 197.0130
                    Surrogate loss: 0.0131
             Mean action noise std: 0.99
                       Mean reward: 294.78
               Mean episode length: 233.58
                 Mean success rate: 0.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 1933312
                    Iteration time: 2.57s
                        Total time: 603.92s
                               ETA: 511194.4s

################################################################################
                    [1m Learning iteration 236/200000 [0m

                       Computation: 3146 steps/s (collection: 0.514s, learning 2.090s)
               Value function loss: 182.1207
                    Surrogate loss: 0.0136
             Mean action noise std: 0.99
                       Mean reward: 277.35
               Mean episode length: 219.90
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 1941504
                    Iteration time: 2.60s
                        Total time: 606.52s
                               ETA: 511229.7s

################################################################################
                    [1m Learning iteration 237/200000 [0m

                       Computation: 3065 steps/s (collection: 0.576s, learning 2.096s)
               Value function loss: 132.4566
                    Surrogate loss: 0.0171
             Mean action noise std: 0.99
                       Mean reward: 264.04
               Mean episode length: 208.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 1949696
                    Iteration time: 2.67s
                        Total time: 609.19s
                               ETA: 511321.9s

################################################################################
                    [1m Learning iteration 238/200000 [0m

                       Computation: 3127 steps/s (collection: 0.523s, learning 2.096s)
               Value function loss: 176.2322
                    Surrogate loss: 0.0127
             Mean action noise std: 0.99
                       Mean reward: 279.85
               Mean episode length: 215.17
                 Mean success rate: 0.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 1957888
                    Iteration time: 2.62s
                        Total time: 611.81s
                               ETA: 511369.1s

################################################################################
                    [1m Learning iteration 239/200000 [0m

                       Computation: 3138 steps/s (collection: 0.537s, learning 2.073s)
               Value function loss: 195.0093
                    Surrogate loss: 0.0136
             Mean action noise std: 0.99
                       Mean reward: 279.77
               Mean episode length: 212.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 2.61s
                        Total time: 614.42s
                               ETA: 511408.3s

################################################################################
                    [1m Learning iteration 240/200000 [0m

                       Computation: 3065 steps/s (collection: 0.576s, learning 2.096s)
               Value function loss: 219.8872
                    Surrogate loss: 0.0140
             Mean action noise std: 0.99
                       Mean reward: 263.94
               Mean episode length: 199.22
                 Mean success rate: 0.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 1974272
                    Iteration time: 2.67s
                        Total time: 617.10s
                               ETA: 511498.7s

################################################################################
                    [1m Learning iteration 241/200000 [0m

                       Computation: 3026 steps/s (collection: 0.609s, learning 2.098s)
               Value function loss: 213.9858
                    Surrogate loss: 0.0111
             Mean action noise std: 0.99
                       Mean reward: 269.08
               Mean episode length: 200.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 1982464
                    Iteration time: 2.71s
                        Total time: 619.80s
                               ETA: 511616.9s

################################################################################
                    [1m Learning iteration 242/200000 [0m

                       Computation: 3169 steps/s (collection: 0.504s, learning 2.080s)
               Value function loss: 203.5767
                    Surrogate loss: 0.0149
             Mean action noise std: 0.99
                       Mean reward: 267.46
               Mean episode length: 200.10
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 1990656
                    Iteration time: 2.58s
                        Total time: 622.39s
                               ETA: 511633.5s

################################################################################
                    [1m Learning iteration 243/200000 [0m

                       Computation: 3089 steps/s (collection: 0.531s, learning 2.121s)
               Value function loss: 238.9545
                    Surrogate loss: 0.0120
             Mean action noise std: 0.99
                       Mean reward: 252.23
               Mean episode length: 189.73
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 1998848
                    Iteration time: 2.65s
                        Total time: 625.04s
                               ETA: 511705.1s

################################################################################
                    [1m Learning iteration 244/200000 [0m

                       Computation: 3031 steps/s (collection: 0.610s, learning 2.093s)
               Value function loss: 154.2205
                    Surrogate loss: 0.0125
             Mean action noise std: 0.99
                       Mean reward: 264.18
               Mean episode length: 199.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2007040
                    Iteration time: 2.70s
                        Total time: 627.74s
                               ETA: 511817.4s

################################################################################
                    [1m Learning iteration 245/200000 [0m

                       Computation: 3183 steps/s (collection: 0.524s, learning 2.050s)
               Value function loss: 146.1345
                    Surrogate loss: 0.0153
             Mean action noise std: 0.99
                       Mean reward: 277.92
               Mean episode length: 211.58
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2015232
                    Iteration time: 2.57s
                        Total time: 630.32s
                               ETA: 511823.8s

################################################################################
                    [1m Learning iteration 246/200000 [0m

                       Computation: 3151 steps/s (collection: 0.533s, learning 2.066s)
               Value function loss: 155.7528
                    Surrogate loss: 0.0133
             Mean action noise std: 0.99
                       Mean reward: 276.28
               Mean episode length: 209.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2023424
                    Iteration time: 2.60s
                        Total time: 632.91s
                               ETA: 511851.2s

################################################################################
                    [1m Learning iteration 247/200000 [0m

                       Computation: 3167 steps/s (collection: 0.548s, learning 2.038s)
               Value function loss: 150.6878
                    Surrogate loss: 0.0163
             Mean action noise std: 0.99
                       Mean reward: 276.82
               Mean episode length: 209.90
                 Mean success rate: 0.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 2031616
                    Iteration time: 2.59s
                        Total time: 635.50s
                               ETA: 511867.9s

################################################################################
                    [1m Learning iteration 248/200000 [0m

                       Computation: 3126 steps/s (collection: 0.514s, learning 2.106s)
               Value function loss: 150.1482
                    Surrogate loss: 0.0156
             Mean action noise std: 0.99
                       Mean reward: 282.17
               Mean episode length: 213.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2039808
                    Iteration time: 2.62s
                        Total time: 638.12s
                               ETA: 511911.8s

################################################################################
                    [1m Learning iteration 249/200000 [0m

                       Computation: 3093 steps/s (collection: 0.563s, learning 2.084s)
               Value function loss: 259.7487
                    Surrogate loss: 0.0097
             Mean action noise std: 0.99
                       Mean reward: 285.22
               Mean episode length: 217.09
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2048000
                    Iteration time: 2.65s
                        Total time: 640.77s
                               ETA: 511977.1s

################################################################################
                    [1m Learning iteration 250/200000 [0m

                       Computation: 3159 steps/s (collection: 0.532s, learning 2.061s)
               Value function loss: 174.0961
                    Surrogate loss: 0.0148
             Mean action noise std: 0.99
                       Mean reward: 284.03
               Mean episode length: 215.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 2056192
                    Iteration time: 2.59s
                        Total time: 643.36s
                               ETA: 511998.5s

################################################################################
                    [1m Learning iteration 251/200000 [0m

                       Computation: 3163 steps/s (collection: 0.531s, learning 2.058s)
               Value function loss: 191.6679
                    Surrogate loss: 0.0150
             Mean action noise std: 0.99
                       Mean reward: 286.05
               Mean episode length: 212.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 2.59s
                        Total time: 645.95s
                               ETA: 512016.8s

################################################################################
                    [1m Learning iteration 252/200000 [0m

                       Computation: 3055 steps/s (collection: 0.586s, learning 2.095s)
               Value function loss: 196.8338
                    Surrogate loss: 0.0137
             Mean action noise std: 0.99
                       Mean reward: 283.97
               Mean episode length: 209.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 2072576
                    Iteration time: 2.68s
                        Total time: 648.63s
                               ETA: 512107.0s

################################################################################
                    [1m Learning iteration 253/200000 [0m

                       Computation: 3057 steps/s (collection: 0.598s, learning 2.081s)
               Value function loss: 128.0972
                    Surrogate loss: 0.0135
             Mean action noise std: 0.99
                       Mean reward: 275.51
               Mean episode length: 202.48
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2080768
                    Iteration time: 2.68s
                        Total time: 651.31s
                               ETA: 512195.1s

################################################################################
                    [1m Learning iteration 254/200000 [0m

                       Computation: 3160 steps/s (collection: 0.522s, learning 2.070s)
               Value function loss: 178.3407
                    Surrogate loss: 0.0115
             Mean action noise std: 0.99
                       Mean reward: 276.59
               Mean episode length: 203.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2088960
                    Iteration time: 2.59s
                        Total time: 653.90s
                               ETA: 512214.1s

################################################################################
                    [1m Learning iteration 255/200000 [0m

                       Computation: 3100 steps/s (collection: 0.534s, learning 2.109s)
               Value function loss: 163.7432
                    Surrogate loss: 0.0145
             Mean action noise std: 0.99
                       Mean reward: 275.75
               Mean episode length: 206.66
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 2097152
                    Iteration time: 2.64s
                        Total time: 656.55s
                               ETA: 512272.4s

################################################################################
                    [1m Learning iteration 256/200000 [0m

                       Computation: 3039 steps/s (collection: 0.539s, learning 2.156s)
               Value function loss: 217.8331
                    Surrogate loss: 0.0123
             Mean action noise std: 0.98
                       Mean reward: 271.34
               Mean episode length: 205.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 2105344
                    Iteration time: 2.70s
                        Total time: 659.24s
                               ETA: 512371.3s

################################################################################
                    [1m Learning iteration 257/200000 [0m

                       Computation: 2950 steps/s (collection: 0.612s, learning 2.164s)
               Value function loss: 282.5888
                    Surrogate loss: 0.0100
             Mean action noise std: 0.98
                       Mean reward: 270.08
               Mean episode length: 205.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 2113536
                    Iteration time: 2.78s
                        Total time: 662.02s
                               ETA: 512532.1s

################################################################################
                    [1m Learning iteration 258/200000 [0m

                       Computation: 2955 steps/s (collection: 0.691s, learning 2.081s)
               Value function loss: 173.1265
                    Surrogate loss: 0.0135
             Mean action noise std: 0.99
                       Mean reward: 284.32
               Mean episode length: 216.49
                 Mean success rate: 0.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2121728
                    Iteration time: 2.77s
                        Total time: 664.79s
                               ETA: 512688.5s

################################################################################
                    [1m Learning iteration 259/200000 [0m

                       Computation: 2908 steps/s (collection: 0.619s, learning 2.198s)
               Value function loss: 257.3831
                    Surrogate loss: 0.0172
             Mean action noise std: 0.99
                       Mean reward: 290.73
               Mean episode length: 222.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 2129920
                    Iteration time: 2.82s
                        Total time: 667.61s
                               ETA: 512878.2s

################################################################################
                    [1m Learning iteration 260/200000 [0m

                       Computation: 3053 steps/s (collection: 0.586s, learning 2.097s)
               Value function loss: 164.0374
                    Surrogate loss: 0.0147
             Mean action noise std: 0.98
                       Mean reward: 285.99
               Mean episode length: 218.43
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2138112
                    Iteration time: 2.68s
                        Total time: 670.29s
                               ETA: 512963.5s

################################################################################
                    [1m Learning iteration 261/200000 [0m

                       Computation: 3120 steps/s (collection: 0.537s, learning 2.088s)
               Value function loss: 155.6410
                    Surrogate loss: 0.0138
             Mean action noise std: 0.98
                       Mean reward: 279.19
               Mean episode length: 212.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 2146304
                    Iteration time: 2.63s
                        Total time: 672.91s
                               ETA: 513004.3s

################################################################################
                    [1m Learning iteration 262/200000 [0m

                       Computation: 3092 steps/s (collection: 0.547s, learning 2.102s)
               Value function loss: 192.9609
                    Surrogate loss: 0.0154
             Mean action noise std: 0.98
                       Mean reward: 256.45
               Mean episode length: 198.28
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 2154496
                    Iteration time: 2.65s
                        Total time: 675.56s
                               ETA: 513062.7s

################################################################################
                    [1m Learning iteration 263/200000 [0m

                       Computation: 3063 steps/s (collection: 0.570s, learning 2.105s)
               Value function loss: 166.0542
                    Surrogate loss: 0.0145
             Mean action noise std: 0.98
                       Mean reward: 243.33
               Mean episode length: 189.84
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 2.67s
                        Total time: 678.24s
                               ETA: 513140.2s

################################################################################
                    [1m Learning iteration 264/200000 [0m

                       Computation: 3024 steps/s (collection: 0.579s, learning 2.129s)
               Value function loss: 179.8450
                    Surrogate loss: 0.0122
             Mean action noise std: 0.98
                       Mean reward: 225.85
               Mean episode length: 177.12
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 2170880
                    Iteration time: 2.71s
                        Total time: 680.95s
                               ETA: 513242.8s

################################################################################
                    [1m Learning iteration 265/200000 [0m

                       Computation: 3052 steps/s (collection: 0.556s, learning 2.128s)
               Value function loss: 207.9697
                    Surrogate loss: 0.0127
             Mean action noise std: 0.98
                       Mean reward: 235.15
               Mean episode length: 184.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2179072
                    Iteration time: 2.68s
                        Total time: 683.63s
                               ETA: 513325.9s

################################################################################
                    [1m Learning iteration 266/200000 [0m

                       Computation: 3134 steps/s (collection: 0.546s, learning 2.067s)
               Value function loss: 150.8230
                    Surrogate loss: 0.0149
             Mean action noise std: 0.99
                       Mean reward: 262.93
               Mean episode length: 207.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2187264
                    Iteration time: 2.61s
                        Total time: 686.24s
                               ETA: 513355.7s

################################################################################
                    [1m Learning iteration 267/200000 [0m

                       Computation: 3072 steps/s (collection: 0.555s, learning 2.111s)
               Value function loss: 159.7330
                    Surrogate loss: 0.0153
             Mean action noise std: 0.99
                       Mean reward: 273.86
               Mean episode length: 215.99
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2195456
                    Iteration time: 2.67s
                        Total time: 688.91s
                               ETA: 513424.4s

################################################################################
                    [1m Learning iteration 268/200000 [0m

                       Computation: 3187 steps/s (collection: 0.501s, learning 2.068s)
               Value function loss: 169.5706
                    Surrogate loss: 0.0136
             Mean action noise std: 0.99
                       Mean reward: 291.95
               Mean episode length: 223.60
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2203648
                    Iteration time: 2.57s
                        Total time: 691.48s
                               ETA: 513421.3s

################################################################################
                    [1m Learning iteration 269/200000 [0m

                       Computation: 3081 steps/s (collection: 0.555s, learning 2.103s)
               Value function loss: 137.6483
                    Surrogate loss: 0.0140
             Mean action noise std: 0.99
                       Mean reward: 279.87
               Mean episode length: 212.90
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2211840
                    Iteration time: 2.66s
                        Total time: 694.14s
                               ETA: 513483.6s

################################################################################
                    [1m Learning iteration 270/200000 [0m

                       Computation: 2854 steps/s (collection: 0.642s, learning 2.228s)
               Value function loss: 154.5318
                    Surrogate loss: 0.0124
             Mean action noise std: 0.99
                       Mean reward: 297.69
               Mean episode length: 226.79
                 Mean success rate: 0.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2220032
                    Iteration time: 2.87s
                        Total time: 697.01s
                               ETA: 513701.6s

################################################################################
                    [1m Learning iteration 271/200000 [0m

                       Computation: 2762 steps/s (collection: 0.825s, learning 2.140s)
               Value function loss: 352.6869
                    Surrogate loss: 0.0100
             Mean action noise std: 0.99
                       Mean reward: 278.53
               Mean episode length: 210.70
                 Mean success rate: 0.50
                  Mean reward/step: 1.30
       Mean episode length/episode: 26.09
--------------------------------------------------------------------------------
                   Total timesteps: 2228224
                    Iteration time: 2.97s
                        Total time: 699.97s
                               ETA: 513988.1s

################################################################################
                    [1m Learning iteration 272/200000 [0m

                       Computation: 2959 steps/s (collection: 0.629s, learning 2.140s)
               Value function loss: 174.9088
                    Surrogate loss: 0.0141
             Mean action noise std: 0.99
                       Mean reward: 282.91
               Mean episode length: 214.59
                 Mean success rate: 0.50
                  Mean reward/step: 1.28
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 2236416
                    Iteration time: 2.77s
                        Total time: 702.74s
                               ETA: 514128.0s

################################################################################
                    [1m Learning iteration 273/200000 [0m

                       Computation: 2997 steps/s (collection: 0.599s, learning 2.134s)
               Value function loss: 144.4018
                    Surrogate loss: 0.0157
             Mean action noise std: 0.99
                       Mean reward: 286.36
               Mean episode length: 220.07
                 Mean success rate: 0.50
                  Mean reward/step: 1.27
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 2244608
                    Iteration time: 2.73s
                        Total time: 705.47s
                               ETA: 514241.3s

################################################################################
                    [1m Learning iteration 274/200000 [0m

                       Computation: 2693 steps/s (collection: 0.685s, learning 2.356s)
               Value function loss: 175.2396
                    Surrogate loss: 0.0147
             Mean action noise std: 0.99
                       Mean reward: 283.60
               Mean episode length: 215.97
                 Mean success rate: 0.50
                  Mean reward/step: 1.34
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 2252800
                    Iteration time: 3.04s
                        Total time: 708.51s
                               ETA: 514577.5s

################################################################################
                    [1m Learning iteration 275/200000 [0m

                       Computation: 2747 steps/s (collection: 0.702s, learning 2.280s)
               Value function loss: 247.7938
                    Surrogate loss: 0.0134
             Mean action noise std: 0.99
                       Mean reward: 277.18
               Mean episode length: 212.60
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 25.76
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 2.98s
                        Total time: 711.50s
                               ETA: 514868.5s

################################################################################
                    [1m Learning iteration 276/200000 [0m

                       Computation: 2942 steps/s (collection: 0.647s, learning 2.137s)
               Value function loss: 144.8671
                    Surrogate loss: 0.0119
             Mean action noise std: 0.98
                       Mean reward: 259.31
               Mean episode length: 199.46
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 2269184
                    Iteration time: 2.78s
                        Total time: 714.28s
                               ETA: 515014.7s

################################################################################
                    [1m Learning iteration 277/200000 [0m

                       Computation: 2989 steps/s (collection: 0.573s, learning 2.167s)
               Value function loss: 178.2365
                    Surrogate loss: 0.0137
             Mean action noise std: 0.98
                       Mean reward: 253.72
               Mean episode length: 193.43
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 2277376
                    Iteration time: 2.74s
                        Total time: 717.02s
                               ETA: 515128.4s

################################################################################
                    [1m Learning iteration 278/200000 [0m

                       Computation: 2734 steps/s (collection: 0.649s, learning 2.347s)
               Value function loss: 134.2015
                    Surrogate loss: 0.0143
             Mean action noise std: 0.98
                       Mean reward: 261.99
               Mean episode length: 200.88
                 Mean success rate: 0.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2285568
                    Iteration time: 3.00s
                        Total time: 720.02s
                               ETA: 515423.7s

################################################################################
                    [1m Learning iteration 279/200000 [0m

                       Computation: 2774 steps/s (collection: 0.674s, learning 2.279s)
               Value function loss: 114.6416
                    Surrogate loss: 0.0159
             Mean action noise std: 0.98
                       Mean reward: 262.00
               Mean episode length: 201.76
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2293760
                    Iteration time: 2.95s
                        Total time: 722.97s
                               ETA: 515686.7s

################################################################################
                    [1m Learning iteration 280/200000 [0m

                       Computation: 2977 steps/s (collection: 0.661s, learning 2.091s)
               Value function loss: 193.2640
                    Surrogate loss: 0.0151
             Mean action noise std: 0.98
                       Mean reward: 250.95
               Mean episode length: 190.15
                 Mean success rate: 0.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 2301952
                    Iteration time: 2.75s
                        Total time: 725.72s
                               ETA: 515804.4s

################################################################################
                    [1m Learning iteration 281/200000 [0m

                       Computation: 3054 steps/s (collection: 0.573s, learning 2.110s)
               Value function loss: 112.2786
                    Surrogate loss: 0.0183
             Mean action noise std: 0.98
                       Mean reward: 257.06
               Mean episode length: 194.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2310144
                    Iteration time: 2.68s
                        Total time: 728.40s
                               ETA: 515872.5s

################################################################################
                    [1m Learning iteration 282/200000 [0m

                       Computation: 3047 steps/s (collection: 0.555s, learning 2.133s)
               Value function loss: 132.7881
                    Surrogate loss: 0.0156
             Mean action noise std: 0.98
                       Mean reward: 256.04
               Mean episode length: 193.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2318336
                    Iteration time: 2.69s
                        Total time: 731.09s
                               ETA: 515944.3s

################################################################################
                    [1m Learning iteration 283/200000 [0m

                       Computation: 2589 steps/s (collection: 0.871s, learning 2.293s)
               Value function loss: 127.0624
                    Surrogate loss: 0.0158
             Mean action noise std: 0.98
                       Mean reward: 259.38
               Mean episode length: 198.49
                 Mean success rate: 0.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 2326528
                    Iteration time: 3.16s
                        Total time: 734.26s
                               ETA: 516350.0s

################################################################################
                    [1m Learning iteration 284/200000 [0m

                       Computation: 2817 steps/s (collection: 0.625s, learning 2.282s)
               Value function loss: 125.6799
                    Surrogate loss: 0.0162
             Mean action noise std: 0.98
                       Mean reward: 264.40
               Mean episode length: 201.84
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2334720
                    Iteration time: 2.91s
                        Total time: 737.16s
                               ETA: 516573.2s

################################################################################
                    [1m Learning iteration 285/200000 [0m

                       Computation: 2790 steps/s (collection: 0.720s, learning 2.216s)
               Value function loss: 199.1016
                    Surrogate loss: 0.0128
             Mean action noise std: 0.98
                       Mean reward: 250.09
               Mean episode length: 191.97
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 2342912
                    Iteration time: 2.94s
                        Total time: 740.10s
                               ETA: 516814.3s

################################################################################
                    [1m Learning iteration 286/200000 [0m

                       Computation: 2839 steps/s (collection: 0.666s, learning 2.218s)
               Value function loss: 135.2267
                    Surrogate loss: 0.0169
             Mean action noise std: 0.98
                       Mean reward: 258.63
               Mean episode length: 202.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2351104
                    Iteration time: 2.88s
                        Total time: 742.98s
                               ETA: 517018.3s

################################################################################
                    [1m Learning iteration 287/200000 [0m

                       Computation: 2477 steps/s (collection: 1.009s, learning 2.298s)
               Value function loss: 168.5342
                    Surrogate loss: 0.0138
             Mean action noise std: 0.98
                       Mean reward: 276.84
               Mean episode length: 218.29
                 Mean success rate: 0.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 3.31s
                        Total time: 746.29s
                               ETA: 517513.5s

################################################################################
                    [1m Learning iteration 288/200000 [0m

                       Computation: 3114 steps/s (collection: 0.576s, learning 2.054s)
               Value function loss: 155.5136
                    Surrogate loss: 0.0132
             Mean action noise std: 0.98
                       Mean reward: 304.63
               Mean episode length: 241.66
                 Mean success rate: 0.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2367488
                    Iteration time: 2.63s
                        Total time: 748.92s
                               ETA: 517537.6s

################################################################################
                    [1m Learning iteration 289/200000 [0m

                       Computation: 3088 steps/s (collection: 0.580s, learning 2.072s)
               Value function loss: 118.3388
                    Surrogate loss: 0.0154
             Mean action noise std: 0.98
                       Mean reward: 315.66
               Mean episode length: 250.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2375680
                    Iteration time: 2.65s
                        Total time: 751.57s
                               ETA: 517577.2s

################################################################################
                    [1m Learning iteration 290/200000 [0m

                       Computation: 3126 steps/s (collection: 0.522s, learning 2.098s)
               Value function loss: 157.9309
                    Surrogate loss: 0.0176
             Mean action noise std: 0.98
                       Mean reward: 317.93
               Mean episode length: 253.57
                 Mean success rate: 0.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2383872
                    Iteration time: 2.62s
                        Total time: 754.19s
                               ETA: 517594.0s

################################################################################
                    [1m Learning iteration 291/200000 [0m

                       Computation: 2973 steps/s (collection: 0.644s, learning 2.111s)
               Value function loss: 175.1092
                    Surrogate loss: 0.0166
             Mean action noise std: 0.98
                       Mean reward: 347.23
               Mean episode length: 279.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2392064
                    Iteration time: 2.76s
                        Total time: 756.95s
                               ETA: 517703.1s

################################################################################
                    [1m Learning iteration 292/200000 [0m

                       Computation: 3119 steps/s (collection: 0.590s, learning 2.036s)
               Value function loss: 174.1246
                    Surrogate loss: 0.0169
             Mean action noise std: 0.98
                       Mean reward: 333.34
               Mean episode length: 268.86
                 Mean success rate: 0.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2400256
                    Iteration time: 2.63s
                        Total time: 759.57s
                               ETA: 517723.6s

################################################################################
                    [1m Learning iteration 293/200000 [0m

                       Computation: 2841 steps/s (collection: 0.700s, learning 2.183s)
               Value function loss: 170.8838
                    Surrogate loss: 0.0162
             Mean action noise std: 0.98
                       Mean reward: 340.43
               Mean episode length: 272.57
                 Mean success rate: 0.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 2408448
                    Iteration time: 2.88s
                        Total time: 762.46s
                               ETA: 517918.5s

################################################################################
                    [1m Learning iteration 294/200000 [0m

                       Computation: 3028 steps/s (collection: 0.588s, learning 2.117s)
               Value function loss: 196.8250
                    Surrogate loss: 0.0137
             Mean action noise std: 0.98
                       Mean reward: 318.25
               Mean episode length: 255.65
                 Mean success rate: 0.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 2416640
                    Iteration time: 2.71s
                        Total time: 765.16s
                               ETA: 517991.5s

################################################################################
                    [1m Learning iteration 295/200000 [0m

                       Computation: 3017 steps/s (collection: 0.576s, learning 2.138s)
               Value function loss: 131.1813
                    Surrogate loss: 0.0192
             Mean action noise std: 0.98
                       Mean reward: 314.28
               Mean episode length: 251.13
                 Mean success rate: 0.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 2424832
                    Iteration time: 2.71s
                        Total time: 767.88s
                               ETA: 518070.3s

################################################################################
                    [1m Learning iteration 296/200000 [0m

                       Computation: 2958 steps/s (collection: 0.615s, learning 2.154s)
               Value function loss: 136.1961
                    Surrogate loss: 0.0170
             Mean action noise std: 0.98
                       Mean reward: 320.09
               Mean episode length: 253.59
                 Mean success rate: 0.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2433024
                    Iteration time: 2.77s
                        Total time: 770.65s
                               ETA: 518185.1s

################################################################################
                    [1m Learning iteration 297/200000 [0m

                       Computation: 2862 steps/s (collection: 0.648s, learning 2.214s)
               Value function loss: 141.3204
                    Surrogate loss: 0.0147
             Mean action noise std: 0.98
                       Mean reward: 313.18
               Mean episode length: 243.77
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2441216
                    Iteration time: 2.86s
                        Total time: 773.51s
                               ETA: 518361.5s

################################################################################
                    [1m Learning iteration 298/200000 [0m

                       Computation: 2852 steps/s (collection: 0.679s, learning 2.193s)
               Value function loss: 161.0301
                    Surrogate loss: 0.0177
             Mean action noise std: 0.98
                       Mean reward: 327.83
               Mean episode length: 252.18
                 Mean success rate: 0.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 2449408
                    Iteration time: 2.87s
                        Total time: 776.38s
                               ETA: 518543.3s

################################################################################
                    [1m Learning iteration 299/200000 [0m

                       Computation: 2837 steps/s (collection: 0.663s, learning 2.224s)
               Value function loss: 130.1199
                    Surrogate loss: 0.0160
             Mean action noise std: 0.98
                       Mean reward: 325.58
               Mean episode length: 250.74
                 Mean success rate: 0.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 2.89s
                        Total time: 779.27s
                               ETA: 518734.0s

################################################################################
                    [1m Learning iteration 300/200000 [0m

                       Computation: 2927 steps/s (collection: 0.615s, learning 2.183s)
               Value function loss: 118.8267
                    Surrogate loss: 0.0167
             Mean action noise std: 0.98
                       Mean reward: 329.56
               Mean episode length: 250.17
                 Mean success rate: 0.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2465792
                    Iteration time: 2.80s
                        Total time: 782.06s
                               ETA: 518864.5s

################################################################################
                    [1m Learning iteration 301/200000 [0m

                       Computation: 2923 steps/s (collection: 0.664s, learning 2.138s)
               Value function loss: 143.0243
                    Surrogate loss: 0.0145
             Mean action noise std: 0.98
                       Mean reward: 343.86
               Mean episode length: 260.66
                 Mean success rate: 0.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2473984
                    Iteration time: 2.80s
                        Total time: 784.87s
                               ETA: 518996.4s

################################################################################
                    [1m Learning iteration 302/200000 [0m

                       Computation: 3036 steps/s (collection: 0.645s, learning 2.053s)
               Value function loss: 169.4537
                    Surrogate loss: 0.0167
             Mean action noise std: 0.98
                       Mean reward: 345.02
               Mean episode length: 258.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2482176
                    Iteration time: 2.70s
                        Total time: 787.56s
                               ETA: 519059.1s

################################################################################
                    [1m Learning iteration 303/200000 [0m

                       Computation: 3172 steps/s (collection: 0.538s, learning 2.045s)
               Value function loss: 123.6427
                    Surrogate loss: 0.0181
             Mean action noise std: 0.98
                       Mean reward: 352.62
               Mean episode length: 265.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2490368
                    Iteration time: 2.58s
                        Total time: 790.15s
                               ETA: 519045.6s

################################################################################
                    [1m Learning iteration 304/200000 [0m

                       Computation: 3093 steps/s (collection: 0.586s, learning 2.062s)
               Value function loss: 173.1165
                    Surrogate loss: 0.0161
             Mean action noise std: 0.98
                       Mean reward: 352.19
               Mean episode length: 268.89
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2498560
                    Iteration time: 2.65s
                        Total time: 792.79s
                               ETA: 519075.1s

################################################################################
                    [1m Learning iteration 305/200000 [0m

                       Computation: 3037 steps/s (collection: 0.622s, learning 2.075s)
               Value function loss: 123.8069
                    Surrogate loss: 0.0180
             Mean action noise std: 0.98
                       Mean reward: 362.16
               Mean episode length: 275.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2506752
                    Iteration time: 2.70s
                        Total time: 795.49s
                               ETA: 519136.2s

################################################################################
                    [1m Learning iteration 306/200000 [0m

                       Computation: 3206 steps/s (collection: 0.507s, learning 2.048s)
               Value function loss: 157.4907
                    Surrogate loss: 0.0128
             Mean action noise std: 0.98
                       Mean reward: 355.17
               Mean episode length: 271.67
                 Mean success rate: 0.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2514944
                    Iteration time: 2.55s
                        Total time: 798.05s
                               ETA: 519104.5s

################################################################################
                    [1m Learning iteration 307/200000 [0m

                       Computation: 3148 steps/s (collection: 0.558s, learning 2.044s)
               Value function loss: 145.0244
                    Surrogate loss: 0.0182
             Mean action noise std: 0.98
                       Mean reward: 349.40
               Mean episode length: 268.39
                 Mean success rate: 0.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2523136
                    Iteration time: 2.60s
                        Total time: 800.65s
                               ETA: 519103.1s

################################################################################
                    [1m Learning iteration 308/200000 [0m

                       Computation: 3156 steps/s (collection: 0.549s, learning 2.047s)
               Value function loss: 176.4420
                    Surrogate loss: 0.0191
             Mean action noise std: 0.98
                       Mean reward: 341.80
               Mean episode length: 261.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 2531328
                    Iteration time: 2.60s
                        Total time: 803.24s
                               ETA: 519098.1s

################################################################################
                    [1m Learning iteration 309/200000 [0m

                       Computation: 3097 steps/s (collection: 0.568s, learning 2.077s)
               Value function loss: 168.1234
                    Surrogate loss: 0.0204
             Mean action noise std: 0.98
                       Mean reward: 353.87
               Mean episode length: 272.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2539520
                    Iteration time: 2.64s
                        Total time: 805.89s
                               ETA: 519124.6s

################################################################################
                    [1m Learning iteration 310/200000 [0m

                       Computation: 3132 steps/s (collection: 0.546s, learning 2.069s)
               Value function loss: 208.2399
                    Surrogate loss: 0.0172
             Mean action noise std: 0.98
                       Mean reward: 357.06
               Mean episode length: 271.11
                 Mean success rate: 0.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2547712
                    Iteration time: 2.61s
                        Total time: 808.50s
                               ETA: 519131.8s

################################################################################
                    [1m Learning iteration 311/200000 [0m

                       Computation: 3109 steps/s (collection: 0.560s, learning 2.075s)
               Value function loss: 164.6870
                    Surrogate loss: 0.0179
             Mean action noise std: 0.98
                       Mean reward: 340.06
               Mean episode length: 257.71
                 Mean success rate: 0.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 2.63s
                        Total time: 811.14s
                               ETA: 519151.5s

################################################################################
                    [1m Learning iteration 312/200000 [0m

                       Computation: 3057 steps/s (collection: 0.602s, learning 2.077s)
               Value function loss: 194.9080
                    Surrogate loss: 0.0171
             Mean action noise std: 0.98
                       Mean reward: 353.08
               Mean episode length: 267.75
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2564096
                    Iteration time: 2.68s
                        Total time: 813.82s
                               ETA: 519199.5s

################################################################################
                    [1m Learning iteration 313/200000 [0m

                       Computation: 3111 steps/s (collection: 0.559s, learning 2.074s)
               Value function loss: 158.3234
                    Surrogate loss: 0.0184
             Mean action noise std: 0.98
                       Mean reward: 368.26
               Mean episode length: 276.20
                 Mean success rate: 0.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2572288
                    Iteration time: 2.63s
                        Total time: 816.45s
                               ETA: 519217.8s

################################################################################
                    [1m Learning iteration 314/200000 [0m

                       Computation: 3198 steps/s (collection: 0.496s, learning 2.065s)
               Value function loss: 145.5069
                    Surrogate loss: 0.0205
             Mean action noise std: 0.98
                       Mean reward: 376.31
               Mean episode length: 281.64
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 2580480
                    Iteration time: 2.56s
                        Total time: 819.01s
                               ETA: 519190.7s

################################################################################
                    [1m Learning iteration 315/200000 [0m

                       Computation: 3184 steps/s (collection: 0.542s, learning 2.030s)
               Value function loss: 193.3185
                    Surrogate loss: 0.0208
             Mean action noise std: 0.98
                       Mean reward: 350.67
               Mean episode length: 262.69
                 Mean success rate: 0.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 2588672
                    Iteration time: 2.57s
                        Total time: 821.58s
                               ETA: 519170.5s

################################################################################
                    [1m Learning iteration 316/200000 [0m

                       Computation: 3217 steps/s (collection: 0.499s, learning 2.048s)
               Value function loss: 185.9826
                    Surrogate loss: 0.0199
             Mean action noise std: 0.98
                       Mean reward: 336.24
               Mean episode length: 250.02
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2596864
                    Iteration time: 2.55s
                        Total time: 824.13s
                               ETA: 519134.2s

################################################################################
                    [1m Learning iteration 317/200000 [0m

                       Computation: 3089 steps/s (collection: 0.571s, learning 2.081s)
               Value function loss: 163.5130
                    Surrogate loss: 0.0160
             Mean action noise std: 0.98
                       Mean reward: 331.06
               Mean episode length: 246.13
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 2605056
                    Iteration time: 2.65s
                        Total time: 826.78s
                               ETA: 519163.9s

################################################################################
                    [1m Learning iteration 318/200000 [0m

                       Computation: 3102 steps/s (collection: 0.543s, learning 2.097s)
               Value function loss: 219.6219
                    Surrogate loss: 0.0140
             Mean action noise std: 0.98
                       Mean reward: 328.73
               Mean episode length: 238.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 2613248
                    Iteration time: 2.64s
                        Total time: 829.42s
                               ETA: 519186.5s

################################################################################
                    [1m Learning iteration 319/200000 [0m

                       Computation: 3148 steps/s (collection: 0.564s, learning 2.038s)
               Value function loss: 225.6526
                    Surrogate loss: 0.0143
             Mean action noise std: 0.98
                       Mean reward: 322.15
               Mean episode length: 229.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2621440
                    Iteration time: 2.60s
                        Total time: 832.02s
                               ETA: 519185.1s

################################################################################
                    [1m Learning iteration 320/200000 [0m

                       Computation: 3179 steps/s (collection: 0.517s, learning 2.059s)
               Value function loss: 183.2340
                    Surrogate loss: 0.0192
             Mean action noise std: 0.98
                       Mean reward: 314.67
               Mean episode length: 222.19
                 Mean success rate: 0.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 2629632
                    Iteration time: 2.58s
                        Total time: 834.60s
                               ETA: 519167.6s

################################################################################
                    [1m Learning iteration 321/200000 [0m

                       Computation: 3068 steps/s (collection: 0.563s, learning 2.106s)
               Value function loss: 229.8257
                    Surrogate loss: 0.0185
             Mean action noise std: 0.98
                       Mean reward: 334.28
               Mean episode length: 231.95
                 Mean success rate: 0.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 2637824
                    Iteration time: 2.67s
                        Total time: 837.27s
                               ETA: 519208.2s

################################################################################
                    [1m Learning iteration 322/200000 [0m

                       Computation: 3050 steps/s (collection: 0.581s, learning 2.105s)
               Value function loss: 168.8453
                    Surrogate loss: 0.0157
             Mean action noise std: 0.98
                       Mean reward: 345.42
               Mean episode length: 241.35
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2646016
                    Iteration time: 2.69s
                        Total time: 839.95s
                               ETA: 519258.1s

################################################################################
                    [1m Learning iteration 323/200000 [0m

                       Computation: 2964 steps/s (collection: 0.590s, learning 2.173s)
               Value function loss: 230.8747
                    Surrogate loss: 0.0145
             Mean action noise std: 0.98
                       Mean reward: 359.39
               Mean episode length: 248.09
                 Mean success rate: 0.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 2.76s
                        Total time: 842.72s
                               ETA: 519355.6s

################################################################################
                    [1m Learning iteration 324/200000 [0m

                       Computation: 2837 steps/s (collection: 0.679s, learning 2.207s)
               Value function loss: 234.1474
                    Surrogate loss: 0.0166
             Mean action noise std: 0.98
                       Mean reward: 370.06
               Mean episode length: 253.96
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2662400
                    Iteration time: 2.89s
                        Total time: 845.60s
                               ETA: 519528.7s

################################################################################
                    [1m Learning iteration 325/200000 [0m

                       Computation: 2701 steps/s (collection: 0.659s, learning 2.373s)
               Value function loss: 257.4381
                    Surrogate loss: 0.0178
             Mean action noise std: 0.98
                       Mean reward: 368.95
               Mean episode length: 253.41
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 2670592
                    Iteration time: 3.03s
                        Total time: 848.64s
                               ETA: 519789.6s

################################################################################
                    [1m Learning iteration 326/200000 [0m

                       Computation: 2831 steps/s (collection: 0.676s, learning 2.217s)
               Value function loss: 262.2913
                    Surrogate loss: 0.0158
             Mean action noise std: 0.98
                       Mean reward: 370.91
               Mean episode length: 253.60
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2678784
                    Iteration time: 2.89s
                        Total time: 851.53s
                               ETA: 519964.3s

################################################################################
                    [1m Learning iteration 327/200000 [0m

                       Computation: 3042 steps/s (collection: 0.577s, learning 2.115s)
               Value function loss: 185.1861
                    Surrogate loss: 0.0176
             Mean action noise std: 0.98
                       Mean reward: 361.95
               Mean episode length: 248.62
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 2686976
                    Iteration time: 2.69s
                        Total time: 854.22s
                               ETA: 520015.2s

################################################################################
                    [1m Learning iteration 328/200000 [0m

                       Computation: 3090 steps/s (collection: 0.549s, learning 2.101s)
               Value function loss: 191.9773
                    Surrogate loss: 0.0166
             Mean action noise std: 0.98
                       Mean reward: 357.02
               Mean episode length: 241.38
                 Mean success rate: 0.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2695168
                    Iteration time: 2.65s
                        Total time: 856.87s
                               ETA: 520040.9s

################################################################################
                    [1m Learning iteration 329/200000 [0m

                       Computation: 3050 steps/s (collection: 0.576s, learning 2.109s)
               Value function loss: 213.2822
                    Surrogate loss: 0.0163
             Mean action noise std: 0.98
                       Mean reward: 340.12
               Mean episode length: 234.95
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 2703360
                    Iteration time: 2.69s
                        Total time: 859.56s
                               ETA: 520087.1s

################################################################################
                    [1m Learning iteration 330/200000 [0m

                       Computation: 2968 steps/s (collection: 0.609s, learning 2.151s)
               Value function loss: 261.0108
                    Surrogate loss: 0.0162
             Mean action noise std: 0.98
                       Mean reward: 332.78
               Mean episode length: 226.97
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 2711552
                    Iteration time: 2.76s
                        Total time: 862.32s
                               ETA: 520178.2s

################################################################################
                    [1m Learning iteration 331/200000 [0m

                       Computation: 2957 steps/s (collection: 0.630s, learning 2.140s)
               Value function loss: 164.9051
                    Surrogate loss: 0.0213
             Mean action noise std: 0.98
                       Mean reward: 317.59
               Mean episode length: 216.59
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2719744
                    Iteration time: 2.77s
                        Total time: 865.09s
                               ETA: 520274.7s

################################################################################
                    [1m Learning iteration 332/200000 [0m

                       Computation: 3003 steps/s (collection: 0.600s, learning 2.127s)
               Value function loss: 225.6028
                    Surrogate loss: 0.0183
             Mean action noise std: 0.98
                       Mean reward: 327.86
               Mean episode length: 221.86
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2727936
                    Iteration time: 2.73s
                        Total time: 867.82s
                               ETA: 520345.2s

################################################################################
                    [1m Learning iteration 333/200000 [0m

                       Computation: 2992 steps/s (collection: 0.612s, learning 2.125s)
               Value function loss: 184.7958
                    Surrogate loss: 0.0217
             Mean action noise std: 0.98
                       Mean reward: 337.59
               Mean episode length: 227.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 2736128
                    Iteration time: 2.74s
                        Total time: 870.55s
                               ETA: 520420.9s

################################################################################
                    [1m Learning iteration 334/200000 [0m

                       Computation: 2993 steps/s (collection: 0.612s, learning 2.125s)
               Value function loss: 309.7439
                    Surrogate loss: 0.0161
             Mean action noise std: 0.98
                       Mean reward: 354.83
               Mean episode length: 238.61
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 2744320
                    Iteration time: 2.74s
                        Total time: 873.29s
                               ETA: 520496.1s

################################################################################
                    [1m Learning iteration 335/200000 [0m

                       Computation: 3000 steps/s (collection: 0.584s, learning 2.147s)
               Value function loss: 296.7101
                    Surrogate loss: 0.0176
             Mean action noise std: 0.98
                       Mean reward: 363.91
               Mean episode length: 243.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 2.73s
                        Total time: 876.02s
                               ETA: 520567.1s

################################################################################
                    [1m Learning iteration 336/200000 [0m

                       Computation: 2990 steps/s (collection: 0.617s, learning 2.122s)
               Value function loss: 260.1264
                    Surrogate loss: 0.0191
             Mean action noise std: 0.98
                       Mean reward: 377.92
               Mean episode length: 251.59
                 Mean success rate: 0.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 2760704
                    Iteration time: 2.74s
                        Total time: 878.76s
                               ETA: 520642.9s

################################################################################
                    [1m Learning iteration 337/200000 [0m

                       Computation: 3034 steps/s (collection: 0.585s, learning 2.115s)
               Value function loss: 217.4826
                    Surrogate loss: 0.0173
             Mean action noise std: 0.98
                       Mean reward: 400.49
               Mean episode length: 261.77
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2768896
                    Iteration time: 2.70s
                        Total time: 881.46s
                               ETA: 520694.8s

################################################################################
                    [1m Learning iteration 338/200000 [0m

                       Computation: 2949 steps/s (collection: 0.620s, learning 2.157s)
               Value function loss: 202.5300
                    Surrogate loss: 0.0192
             Mean action noise std: 0.98
                       Mean reward: 401.75
               Mean episode length: 262.11
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2777088
                    Iteration time: 2.78s
                        Total time: 884.24s
                               ETA: 520792.1s

################################################################################
                    [1m Learning iteration 339/200000 [0m

                       Computation: 3034 steps/s (collection: 0.604s, learning 2.096s)
               Value function loss: 265.7949
                    Surrogate loss: 0.0168
             Mean action noise std: 0.98
                       Mean reward: 411.26
               Mean episode length: 268.24
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2785280
                    Iteration time: 2.70s
                        Total time: 886.94s
                               ETA: 520842.9s

################################################################################
                    [1m Learning iteration 340/200000 [0m

                       Computation: 2989 steps/s (collection: 0.614s, learning 2.126s)
               Value function loss: 289.7801
                    Surrogate loss: 0.0165
             Mean action noise std: 0.98
                       Mean reward: 419.32
               Mean episode length: 268.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 2793472
                    Iteration time: 2.74s
                        Total time: 889.68s
                               ETA: 520917.2s

################################################################################
                    [1m Learning iteration 341/200000 [0m

                       Computation: 2992 steps/s (collection: 0.586s, learning 2.151s)
               Value function loss: 268.1552
                    Surrogate loss: 0.0146
             Mean action noise std: 0.98
                       Mean reward: 395.70
               Mean episode length: 255.55
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2801664
                    Iteration time: 2.74s
                        Total time: 892.41s
                               ETA: 520989.7s

################################################################################
                    [1m Learning iteration 342/200000 [0m

                       Computation: 2917 steps/s (collection: 0.635s, learning 2.173s)
               Value function loss: 298.4007
                    Surrogate loss: 0.0140
             Mean action noise std: 0.98
                       Mean reward: 397.80
               Mean episode length: 258.26
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 2809856
                    Iteration time: 2.81s
                        Total time: 895.22s
                               ETA: 521102.9s

################################################################################
                    [1m Learning iteration 343/200000 [0m

                       Computation: 2960 steps/s (collection: 0.572s, learning 2.195s)
               Value function loss: 275.8656
                    Surrogate loss: 0.0138
             Mean action noise std: 0.98
                       Mean reward: 388.91
               Mean episode length: 253.94
                 Mean success rate: 0.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 2818048
                    Iteration time: 2.77s
                        Total time: 897.99s
                               ETA: 521191.7s

################################################################################
                    [1m Learning iteration 344/200000 [0m

                       Computation: 2971 steps/s (collection: 0.604s, learning 2.152s)
               Value function loss: 255.8442
                    Surrogate loss: 0.0181
             Mean action noise std: 0.98
                       Mean reward: 389.30
               Mean episode length: 252.81
                 Mean success rate: 0.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2826240
                    Iteration time: 2.76s
                        Total time: 900.75s
                               ETA: 521273.8s

################################################################################
                    [1m Learning iteration 345/200000 [0m

                       Computation: 2913 steps/s (collection: 0.636s, learning 2.175s)
               Value function loss: 260.1521
                    Surrogate loss: 0.0170
             Mean action noise std: 0.98
                       Mean reward: 371.22
               Mean episode length: 239.85
                 Mean success rate: 0.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 2834432
                    Iteration time: 2.81s
                        Total time: 903.56s
                               ETA: 521387.0s

################################################################################
                    [1m Learning iteration 346/200000 [0m

                       Computation: 2934 steps/s (collection: 0.672s, learning 2.120s)
               Value function loss: 192.5284
                    Surrogate loss: 0.0164
             Mean action noise std: 0.98
                       Mean reward: 383.92
               Mean episode length: 247.00
                 Mean success rate: 0.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 2842624
                    Iteration time: 2.79s
                        Total time: 906.35s
                               ETA: 521488.1s

################################################################################
                    [1m Learning iteration 347/200000 [0m

                       Computation: 3005 steps/s (collection: 0.611s, learning 2.114s)
               Value function loss: 218.8719
                    Surrogate loss: 0.0201
             Mean action noise std: 0.98
                       Mean reward: 375.24
               Mean episode length: 241.40
                 Mean success rate: 0.00
                  Mean reward/step: 1.65
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 2.73s
                        Total time: 909.08s
                               ETA: 521550.4s

################################################################################
                    [1m Learning iteration 348/200000 [0m

                       Computation: 2999 steps/s (collection: 0.583s, learning 2.148s)
               Value function loss: 353.8914
                    Surrogate loss: 0.0141
             Mean action noise std: 0.98
                       Mean reward: 400.15
               Mean episode length: 251.22
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 2859008
                    Iteration time: 2.73s
                        Total time: 911.81s
                               ETA: 521615.9s

################################################################################
                    [1m Learning iteration 349/200000 [0m

                       Computation: 3018 steps/s (collection: 0.595s, learning 2.119s)
               Value function loss: 327.2106
                    Surrogate loss: 0.0150
             Mean action noise std: 0.98
                       Mean reward: 402.72
               Mean episode length: 253.66
                 Mean success rate: 0.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2867200
                    Iteration time: 2.71s
                        Total time: 914.52s
                               ETA: 521670.8s

################################################################################
                    [1m Learning iteration 350/200000 [0m

                       Computation: 3046 steps/s (collection: 0.589s, learning 2.100s)
               Value function loss: 365.7886
                    Surrogate loss: 0.0157
             Mean action noise std: 0.98
                       Mean reward: 406.80
               Mean episode length: 253.91
                 Mean success rate: 0.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 2875392
                    Iteration time: 2.69s
                        Total time: 917.21s
                               ETA: 521711.4s

################################################################################
                    [1m Learning iteration 351/200000 [0m

                       Computation: 2996 steps/s (collection: 0.615s, learning 2.119s)
               Value function loss: 342.0273
                    Surrogate loss: 0.0198
             Mean action noise std: 0.98
                       Mean reward: 393.04
               Mean episode length: 249.55
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 2883584
                    Iteration time: 2.73s
                        Total time: 919.94s
                               ETA: 521777.1s

################################################################################
                    [1m Learning iteration 352/200000 [0m

                       Computation: 2965 steps/s (collection: 0.628s, learning 2.135s)
               Value function loss: 303.6636
                    Surrogate loss: 0.0151
             Mean action noise std: 0.98
                       Mean reward: 406.80
               Mean episode length: 255.06
                 Mean success rate: 0.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2891776
                    Iteration time: 2.76s
                        Total time: 922.70s
                               ETA: 521858.8s

################################################################################
                    [1m Learning iteration 353/200000 [0m

                       Computation: 2994 steps/s (collection: 0.606s, learning 2.130s)
               Value function loss: 560.9406
                    Surrogate loss: 0.0132
             Mean action noise std: 0.98
                       Mean reward: 414.19
               Mean episode length: 260.02
                 Mean success rate: 0.00
                  Mean reward/step: 1.77
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2899968
                    Iteration time: 2.74s
                        Total time: 925.44s
                               ETA: 521925.1s

################################################################################
                    [1m Learning iteration 354/200000 [0m

                       Computation: 3054 steps/s (collection: 0.581s, learning 2.101s)
               Value function loss: 380.7676
                    Surrogate loss: 0.0157
             Mean action noise std: 0.98
                       Mean reward: 411.54
               Mean episode length: 261.98
                 Mean success rate: 0.00
                  Mean reward/step: 1.75
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 2908160
                    Iteration time: 2.68s
                        Total time: 928.12s
                               ETA: 521960.6s

################################################################################
                    [1m Learning iteration 355/200000 [0m

                       Computation: 3006 steps/s (collection: 0.603s, learning 2.122s)
               Value function loss: 383.4767
                    Surrogate loss: 0.0185
             Mean action noise std: 0.98
                       Mean reward: 428.67
               Mean episode length: 266.09
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 2916352
                    Iteration time: 2.72s
                        Total time: 930.85s
                               ETA: 522019.9s

################################################################################
                    [1m Learning iteration 356/200000 [0m

                       Computation: 3037 steps/s (collection: 0.588s, learning 2.109s)
               Value function loss: 270.7638
                    Surrogate loss: 0.0151
             Mean action noise std: 0.98
                       Mean reward: 440.85
               Mean episode length: 272.12
                 Mean success rate: 0.50
                  Mean reward/step: 1.60
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2924544
                    Iteration time: 2.70s
                        Total time: 933.54s
                               ETA: 522063.1s

################################################################################
                    [1m Learning iteration 357/200000 [0m

                       Computation: 3014 steps/s (collection: 0.605s, learning 2.112s)
               Value function loss: 319.6058
                    Surrogate loss: 0.0158
             Mean action noise std: 0.98
                       Mean reward: 442.08
               Mean episode length: 269.50
                 Mean success rate: 0.50
                  Mean reward/step: 1.66
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 2932736
                    Iteration time: 2.72s
                        Total time: 936.26s
                               ETA: 522117.6s

################################################################################
                    [1m Learning iteration 358/200000 [0m

                       Computation: 3030 steps/s (collection: 0.609s, learning 2.094s)
               Value function loss: 387.8777
                    Surrogate loss: 0.0167
             Mean action noise std: 0.98
                       Mean reward: 440.19
               Mean episode length: 268.68
                 Mean success rate: 0.50
                  Mean reward/step: 1.60
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2940928
                    Iteration time: 2.70s
                        Total time: 938.96s
                               ETA: 522163.8s

################################################################################
                    [1m Learning iteration 359/200000 [0m

                       Computation: 2974 steps/s (collection: 0.628s, learning 2.126s)
               Value function loss: 259.3264
                    Surrogate loss: 0.0193
             Mean action noise std: 0.98
                       Mean reward: 425.31
               Mean episode length: 259.53
                 Mean success rate: 0.50
                  Mean reward/step: 1.61
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 2.75s
                        Total time: 941.72s
                               ETA: 522237.9s

################################################################################
                    [1m Learning iteration 360/200000 [0m

                       Computation: 3018 steps/s (collection: 0.599s, learning 2.115s)
               Value function loss: 363.5800
                    Surrogate loss: 0.0178
             Mean action noise std: 0.98
                       Mean reward: 426.10
               Mean episode length: 254.85
                 Mean success rate: 0.50
                  Mean reward/step: 1.67
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 2957312
                    Iteration time: 2.71s
                        Total time: 944.43s
                               ETA: 522289.6s

################################################################################
                    [1m Learning iteration 361/200000 [0m

                       Computation: 2983 steps/s (collection: 0.634s, learning 2.111s)
               Value function loss: 363.9492
                    Surrogate loss: 0.0148
             Mean action noise std: 0.98
                       Mean reward: 412.60
               Mean episode length: 247.25
                 Mean success rate: 1.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 2965504
                    Iteration time: 2.75s
                        Total time: 947.18s
                               ETA: 522358.3s

################################################################################
                    [1m Learning iteration 362/200000 [0m

                       Computation: 2981 steps/s (collection: 0.587s, learning 2.160s)
               Value function loss: 296.0404
                    Surrogate loss: 0.0166
             Mean action noise std: 0.98
                       Mean reward: 419.21
               Mean episode length: 251.10
                 Mean success rate: 0.50
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 2973696
                    Iteration time: 2.75s
                        Total time: 949.93s
                               ETA: 522427.8s

################################################################################
                    [1m Learning iteration 363/200000 [0m

                       Computation: 3032 steps/s (collection: 0.577s, learning 2.125s)
               Value function loss: 244.2509
                    Surrogate loss: 0.0191
             Mean action noise std: 0.98
                       Mean reward: 406.46
               Mean episode length: 246.04
                 Mean success rate: 0.50
                  Mean reward/step: 1.75
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 2981888
                    Iteration time: 2.70s
                        Total time: 952.63s
                               ETA: 522471.7s

################################################################################
                    [1m Learning iteration 364/200000 [0m

                       Computation: 2952 steps/s (collection: 0.644s, learning 2.130s)
               Value function loss: 355.2878
                    Surrogate loss: 0.0154
             Mean action noise std: 0.98
                       Mean reward: 402.97
               Mean episode length: 239.62
                 Mean success rate: 0.50
                  Mean reward/step: 1.74
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 2990080
                    Iteration time: 2.77s
                        Total time: 955.40s
                               ETA: 522555.3s

################################################################################
                    [1m Learning iteration 365/200000 [0m

                       Computation: 3041 steps/s (collection: 0.618s, learning 2.076s)
               Value function loss: 572.3237
                    Surrogate loss: 0.0145
             Mean action noise std: 0.98
                       Mean reward: 415.39
               Mean episode length: 244.84
                 Mean success rate: 0.50
                  Mean reward/step: 1.77
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 2998272
                    Iteration time: 2.69s
                        Total time: 958.10s
                               ETA: 522594.3s

################################################################################
                    [1m Learning iteration 366/200000 [0m

                       Computation: 3128 steps/s (collection: 0.550s, learning 2.068s)
               Value function loss: 488.4559
                    Surrogate loss: 0.0145
             Mean action noise std: 0.98
                       Mean reward: 406.85
               Mean episode length: 242.57
                 Mean success rate: 0.50
                  Mean reward/step: 1.74
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 3006464
                    Iteration time: 2.62s
                        Total time: 960.71s
                               ETA: 522592.1s

################################################################################
                    [1m Learning iteration 367/200000 [0m

                       Computation: 3052 steps/s (collection: 0.563s, learning 2.120s)
               Value function loss: 385.4041
                    Surrogate loss: 0.0170
             Mean action noise std: 0.98
                       Mean reward: 402.64
               Mean episode length: 243.44
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 3014656
                    Iteration time: 2.68s
                        Total time: 963.40s
                               ETA: 522625.4s

################################################################################
                    [1m Learning iteration 368/200000 [0m

                       Computation: 3025 steps/s (collection: 0.577s, learning 2.130s)
               Value function loss: 236.1821
                    Surrogate loss: 0.0191
             Mean action noise std: 0.98
                       Mean reward: 404.73
               Mean episode length: 246.49
                 Mean success rate: 0.00
                  Mean reward/step: 1.66
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3022848
                    Iteration time: 2.71s
                        Total time: 966.11s
                               ETA: 522671.3s

################################################################################
                    [1m Learning iteration 369/200000 [0m

                       Computation: 2995 steps/s (collection: 0.597s, learning 2.138s)
               Value function loss: 496.9608
                    Surrogate loss: 0.0135
             Mean action noise std: 0.98
                       Mean reward: 403.21
               Mean episode length: 247.09
                 Mean success rate: 0.00
                  Mean reward/step: 1.78
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 3031040
                    Iteration time: 2.73s
                        Total time: 968.84s
                               ETA: 522731.5s

################################################################################
                    [1m Learning iteration 370/200000 [0m

                       Computation: 3013 steps/s (collection: 0.593s, learning 2.126s)
               Value function loss: 392.5212
                    Surrogate loss: 0.0174
             Mean action noise std: 0.98
                       Mean reward: 419.92
               Mean episode length: 257.58
                 Mean success rate: 0.00
                  Mean reward/step: 1.67
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 3039232
                    Iteration time: 2.72s
                        Total time: 971.56s
                               ETA: 522782.7s

################################################################################
                    [1m Learning iteration 371/200000 [0m

                       Computation: 2958 steps/s (collection: 0.629s, learning 2.139s)
               Value function loss: 413.8984
                    Surrogate loss: 0.0176
             Mean action noise std: 0.98
                       Mean reward: 425.50
               Mean episode length: 256.25
                 Mean success rate: 0.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 2.77s
                        Total time: 974.33s
                               ETA: 522860.6s

################################################################################
                    [1m Learning iteration 372/200000 [0m

                       Computation: 2873 steps/s (collection: 0.657s, learning 2.194s)
               Value function loss: 288.8228
                    Surrogate loss: 0.0189
             Mean action noise std: 0.98
                       Mean reward: 429.69
               Mean episode length: 252.47
                 Mean success rate: 0.50
                  Mean reward/step: 1.69
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3055616
                    Iteration time: 2.85s
                        Total time: 977.18s
                               ETA: 522982.1s

################################################################################
                    [1m Learning iteration 373/200000 [0m

                       Computation: 2962 steps/s (collection: 0.632s, learning 2.133s)
               Value function loss: 268.3477
                    Surrogate loss: 0.0200
             Mean action noise std: 0.98
                       Mean reward: 435.84
               Mean episode length: 256.40
                 Mean success rate: 0.50
                  Mean reward/step: 1.59
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3063808
                    Iteration time: 2.77s
                        Total time: 979.94s
                               ETA: 523057.2s

################################################################################
                    [1m Learning iteration 374/200000 [0m

                       Computation: 3117 steps/s (collection: 0.539s, learning 2.089s)
               Value function loss: 275.2484
                    Surrogate loss: 0.0193
             Mean action noise std: 0.98
                       Mean reward: 446.94
               Mean episode length: 257.30
                 Mean success rate: 2.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3072000
                    Iteration time: 2.63s
                        Total time: 982.57s
                               ETA: 523058.5s

################################################################################
                    [1m Learning iteration 375/200000 [0m

                       Computation: 3036 steps/s (collection: 0.601s, learning 2.097s)
               Value function loss: 451.1568
                    Surrogate loss: 0.0209
             Mean action noise std: 0.98
                       Mean reward: 464.68
               Mean episode length: 264.72
                 Mean success rate: 2.00
                  Mean reward/step: 1.74
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 3080192
                    Iteration time: 2.70s
                        Total time: 985.27s
                               ETA: 523097.2s

################################################################################
                    [1m Learning iteration 376/200000 [0m

                       Computation: 3095 steps/s (collection: 0.563s, learning 2.084s)
               Value function loss: 471.7145
                    Surrogate loss: 0.0158
             Mean action noise std: 0.98
                       Mean reward: 493.50
               Mean episode length: 282.62
                 Mean success rate: 2.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3088384
                    Iteration time: 2.65s
                        Total time: 987.92s
                               ETA: 523108.4s

################################################################################
                    [1m Learning iteration 377/200000 [0m

                       Computation: 3080 steps/s (collection: 0.566s, learning 2.093s)
               Value function loss: 430.8742
                    Surrogate loss: 0.0194
             Mean action noise std: 0.98
                       Mean reward: 482.33
               Mean episode length: 275.45
                 Mean success rate: 2.50
                  Mean reward/step: 1.73
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 3096576
                    Iteration time: 2.66s
                        Total time: 990.58s
                               ETA: 523126.2s

################################################################################
                    [1m Learning iteration 378/200000 [0m

                       Computation: 3119 steps/s (collection: 0.550s, learning 2.075s)
               Value function loss: 405.1171
                    Surrogate loss: 0.0170
             Mean action noise std: 0.98
                       Mean reward: 459.71
               Mean episode length: 271.46
                 Mean success rate: 2.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3104768
                    Iteration time: 2.63s
                        Total time: 993.20s
                               ETA: 523126.3s

################################################################################
                    [1m Learning iteration 379/200000 [0m

                       Computation: 3012 steps/s (collection: 0.575s, learning 2.145s)
               Value function loss: 461.6526
                    Surrogate loss: 0.0155
             Mean action noise std: 0.98
                       Mean reward: 448.19
               Mean episode length: 265.81
                 Mean success rate: 1.50
                  Mean reward/step: 1.76
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3112960
                    Iteration time: 2.72s
                        Total time: 995.92s
                               ETA: 523175.8s

################################################################################
                    [1m Learning iteration 380/200000 [0m

                       Computation: 2958 steps/s (collection: 0.628s, learning 2.141s)
               Value function loss: 494.2758
                    Surrogate loss: 0.0157
             Mean action noise std: 0.98
                       Mean reward: 425.30
               Mean episode length: 254.32
                 Mean success rate: 1.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3121152
                    Iteration time: 2.77s
                        Total time: 998.69s
                               ETA: 523250.7s

################################################################################
                    [1m Learning iteration 381/200000 [0m

                       Computation: 3082 steps/s (collection: 0.571s, learning 2.086s)
               Value function loss: 358.5174
                    Surrogate loss: 0.0158
             Mean action noise std: 0.98
                       Mean reward: 423.26
               Mean episode length: 250.09
                 Mean success rate: 1.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3129344
                    Iteration time: 2.66s
                        Total time: 1001.35s
                               ETA: 523266.9s

################################################################################
                    [1m Learning iteration 382/200000 [0m

                       Computation: 3020 steps/s (collection: 0.608s, learning 2.104s)
               Value function loss: 330.8542
                    Surrogate loss: 0.0180
             Mean action noise std: 0.98
                       Mean reward: 400.51
               Mean episode length: 235.95
                 Mean success rate: 1.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 3137536
                    Iteration time: 2.71s
                        Total time: 1004.06s
                               ETA: 523311.5s

################################################################################
                    [1m Learning iteration 383/200000 [0m

                       Computation: 2950 steps/s (collection: 0.580s, learning 2.196s)
               Value function loss: 483.9723
                    Surrogate loss: 0.0136
             Mean action noise std: 0.98
                       Mean reward: 408.93
               Mean episode length: 237.47
                 Mean success rate: 1.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 2.78s
                        Total time: 1006.84s
                               ETA: 523389.2s

################################################################################
                    [1m Learning iteration 384/200000 [0m

                       Computation: 2958 steps/s (collection: 0.605s, learning 2.165s)
               Value function loss: 375.5160
                    Surrogate loss: 0.0201
             Mean action noise std: 0.97
                       Mean reward: 420.27
               Mean episode length: 245.76
                 Mean success rate: 0.50
                  Mean reward/step: 1.68
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3153920
                    Iteration time: 2.77s
                        Total time: 1009.60s
                               ETA: 523463.0s

################################################################################
                    [1m Learning iteration 385/200000 [0m

                       Computation: 3034 steps/s (collection: 0.599s, learning 2.100s)
               Value function loss: 491.3033
                    Surrogate loss: 0.0162
             Mean action noise std: 0.97
                       Mean reward: 444.41
               Mean episode length: 259.67
                 Mean success rate: 0.50
                  Mean reward/step: 1.64
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3162112
                    Iteration time: 2.70s
                        Total time: 1012.30s
                               ETA: 523500.4s

################################################################################
                    [1m Learning iteration 386/200000 [0m

                       Computation: 3051 steps/s (collection: 0.578s, learning 2.106s)
               Value function loss: 384.0909
                    Surrogate loss: 0.0169
             Mean action noise std: 0.97
                       Mean reward: 431.08
               Mean episode length: 256.07
                 Mean success rate: 0.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 3170304
                    Iteration time: 2.68s
                        Total time: 1014.99s
                               ETA: 523529.7s

################################################################################
                    [1m Learning iteration 387/200000 [0m

                       Computation: 2990 steps/s (collection: 0.645s, learning 2.095s)
               Value function loss: 533.4384
                    Surrogate loss: 0.0171
             Mean action noise std: 0.97
                       Mean reward: 436.73
               Mean episode length: 258.12
                 Mean success rate: 0.50
                  Mean reward/step: 1.74
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 3178496
                    Iteration time: 2.74s
                        Total time: 1017.73s
                               ETA: 523587.3s

################################################################################
                    [1m Learning iteration 388/200000 [0m

                       Computation: 3078 steps/s (collection: 0.556s, learning 2.105s)
               Value function loss: 479.0192
                    Surrogate loss: 0.0149
             Mean action noise std: 0.97
                       Mean reward: 446.05
               Mean episode length: 265.14
                 Mean success rate: 0.50
                  Mean reward/step: 1.75
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3186688
                    Iteration time: 2.66s
                        Total time: 1020.39s
                               ETA: 523604.1s

################################################################################
                    [1m Learning iteration 389/200000 [0m

                       Computation: 3058 steps/s (collection: 0.575s, learning 2.104s)
               Value function loss: 612.9541
                    Surrogate loss: 0.0135
             Mean action noise std: 0.97
                       Mean reward: 436.52
               Mean episode length: 264.25
                 Mean success rate: 0.50
                  Mean reward/step: 1.78
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 3194880
                    Iteration time: 2.68s
                        Total time: 1023.07s
                               ETA: 523629.7s

################################################################################
                    [1m Learning iteration 390/200000 [0m

                       Computation: 3015 steps/s (collection: 0.598s, learning 2.118s)
               Value function loss: 455.9370
                    Surrogate loss: 0.0185
             Mean action noise std: 0.97
                       Mean reward: 413.54
               Mean episode length: 254.95
                 Mean success rate: 0.50
                  Mean reward/step: 1.79
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3203072
                    Iteration time: 2.72s
                        Total time: 1025.78s
                               ETA: 523674.8s

################################################################################
                    [1m Learning iteration 391/200000 [0m

                       Computation: 2949 steps/s (collection: 0.610s, learning 2.168s)
               Value function loss: 553.0812
                    Surrogate loss: 0.0149
             Mean action noise std: 0.97
                       Mean reward: 408.86
               Mean episode length: 249.94
                 Mean success rate: 0.50
                  Mean reward/step: 1.81
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 3211264
                    Iteration time: 2.78s
                        Total time: 1028.56s
                               ETA: 523750.6s

################################################################################
                    [1m Learning iteration 392/200000 [0m

                       Computation: 3101 steps/s (collection: 0.545s, learning 2.097s)
               Value function loss: 450.8523
                    Surrogate loss: 0.0171
             Mean action noise std: 0.97
                       Mean reward: 404.91
               Mean episode length: 244.41
                 Mean success rate: 1.50
                  Mean reward/step: 1.79
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3219456
                    Iteration time: 2.64s
                        Total time: 1031.20s
                               ETA: 523756.9s

################################################################################
                    [1m Learning iteration 393/200000 [0m

                       Computation: 3055 steps/s (collection: 0.567s, learning 2.114s)
               Value function loss: 334.0623
                    Surrogate loss: 0.0170
             Mean action noise std: 0.97
                       Mean reward: 417.63
               Mean episode length: 251.07
                 Mean success rate: 1.50
                  Mean reward/step: 1.73
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3227648
                    Iteration time: 2.68s
                        Total time: 1033.88s
                               ETA: 523783.3s

################################################################################
                    [1m Learning iteration 394/200000 [0m

                       Computation: 2960 steps/s (collection: 0.565s, learning 2.202s)
               Value function loss: 419.0905
                    Surrogate loss: 0.0163
             Mean action noise std: 0.97
                       Mean reward: 417.08
               Mean episode length: 252.54
                 Mean success rate: 1.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3235840
                    Iteration time: 2.77s
                        Total time: 1036.65s
                               ETA: 523853.1s

################################################################################
                    [1m Learning iteration 395/200000 [0m

                       Computation: 3002 steps/s (collection: 0.633s, learning 2.096s)
               Value function loss: 490.5157
                    Surrogate loss: 0.0181
             Mean action noise std: 0.97
                       Mean reward: 433.48
               Mean episode length: 257.42
                 Mean success rate: 1.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 2.73s
                        Total time: 1039.38s
                               ETA: 523903.0s

################################################################################
                    [1m Learning iteration 396/200000 [0m

                       Computation: 3039 steps/s (collection: 0.600s, learning 2.095s)
               Value function loss: 467.1667
                    Surrogate loss: 0.0174
             Mean action noise std: 0.97
                       Mean reward: 439.15
               Mean episode length: 258.55
                 Mean success rate: 1.00
                  Mean reward/step: 1.73
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 3252224
                    Iteration time: 2.69s
                        Total time: 1042.08s
                               ETA: 523935.6s

################################################################################
                    [1m Learning iteration 397/200000 [0m

                       Computation: 3038 steps/s (collection: 0.608s, learning 2.088s)
               Value function loss: 489.9602
                    Surrogate loss: 0.0150
             Mean action noise std: 0.97
                       Mean reward: 437.13
               Mean episode length: 253.47
                 Mean success rate: 1.50
                  Mean reward/step: 1.77
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3260416
                    Iteration time: 2.70s
                        Total time: 1044.77s
                               ETA: 523968.9s

################################################################################
                    [1m Learning iteration 398/200000 [0m

                       Computation: 3080 steps/s (collection: 0.581s, learning 2.078s)
               Value function loss: 493.7024
                    Surrogate loss: 0.0145
             Mean action noise std: 0.97
                       Mean reward: 458.54
               Mean episode length: 264.29
                 Mean success rate: 0.50
                  Mean reward/step: 1.74
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 3268608
                    Iteration time: 2.66s
                        Total time: 1047.43s
                               ETA: 523983.3s

################################################################################
                    [1m Learning iteration 399/200000 [0m

                       Computation: 3077 steps/s (collection: 0.559s, learning 2.102s)
               Value function loss: 449.7416
                    Surrogate loss: 0.0192
             Mean action noise std: 0.97
                       Mean reward: 418.06
               Mean episode length: 238.76
                 Mean success rate: 0.50
                  Mean reward/step: 1.70
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 3276800
                    Iteration time: 2.66s
                        Total time: 1050.09s
                               ETA: 523998.9s

################################################################################
                    [1m Learning iteration 400/200000 [0m

                       Computation: 3012 steps/s (collection: 0.596s, learning 2.123s)
               Value function loss: 561.0413
                    Surrogate loss: 0.0182
             Mean action noise std: 0.97
                       Mean reward: 397.53
               Mean episode length: 225.91
                 Mean success rate: 1.00
                  Mean reward/step: 1.71
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 3284992
                    Iteration time: 2.72s
                        Total time: 1052.81s
                               ETA: 524043.2s

################################################################################
                    [1m Learning iteration 401/200000 [0m

                       Computation: 2998 steps/s (collection: 0.604s, learning 2.128s)
               Value function loss: 590.3018
                    Surrogate loss: 0.0185
             Mean action noise std: 0.97
                       Mean reward: 421.75
               Mean episode length: 232.55
                 Mean success rate: 1.50
                  Mean reward/step: 1.72
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 3293184
                    Iteration time: 2.73s
                        Total time: 1055.54s
                               ETA: 524093.4s

################################################################################
                    [1m Learning iteration 402/200000 [0m

                       Computation: 2907 steps/s (collection: 0.664s, learning 2.153s)
               Value function loss: 433.5409
                    Surrogate loss: 0.0206
             Mean action noise std: 0.97
                       Mean reward: 376.52
               Mean episode length: 210.88
                 Mean success rate: 1.00
                  Mean reward/step: 1.69
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 3301376
                    Iteration time: 2.82s
                        Total time: 1058.36s
                               ETA: 524185.6s

################################################################################
                    [1m Learning iteration 403/200000 [0m

                       Computation: 3110 steps/s (collection: 0.564s, learning 2.070s)
               Value function loss: 305.1424
                    Surrogate loss: 0.0225
             Mean action noise std: 0.97
                       Mean reward: 367.49
               Mean episode length: 211.82
                 Mean success rate: 1.00
                  Mean reward/step: 1.70
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 3309568
                    Iteration time: 2.63s
                        Total time: 1061.00s
                               ETA: 524186.8s

################################################################################
                    [1m Learning iteration 404/200000 [0m

                       Computation: 3028 steps/s (collection: 0.611s, learning 2.094s)
               Value function loss: 472.3930
                    Surrogate loss: 0.0197
             Mean action noise std: 0.97
                       Mean reward: 375.19
               Mean episode length: 211.37
                 Mean success rate: 0.50
                  Mean reward/step: 1.78
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 3317760
                    Iteration time: 2.70s
                        Total time: 1063.70s
                               ETA: 524222.9s

################################################################################
                    [1m Learning iteration 405/200000 [0m

                       Computation: 2935 steps/s (collection: 0.648s, learning 2.143s)
               Value function loss: 570.9570
                    Surrogate loss: 0.0178
             Mean action noise std: 0.97
                       Mean reward: 365.40
               Mean episode length: 207.34
                 Mean success rate: 0.50
                  Mean reward/step: 1.83
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 3325952
                    Iteration time: 2.79s
                        Total time: 1066.49s
                               ETA: 524301.2s

################################################################################
                    [1m Learning iteration 406/200000 [0m

                       Computation: 2969 steps/s (collection: 0.628s, learning 2.131s)
               Value function loss: 657.6533
                    Surrogate loss: 0.0145
             Mean action noise std: 0.97
                       Mean reward: 346.01
               Mean episode length: 201.14
                 Mean success rate: 0.50
                  Mean reward/step: 1.91
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3334144
                    Iteration time: 2.76s
                        Total time: 1069.25s
                               ETA: 524363.2s

################################################################################
                    [1m Learning iteration 407/200000 [0m

                       Computation: 3010 steps/s (collection: 0.596s, learning 2.126s)
               Value function loss: 498.3525
                    Surrogate loss: 0.0154
             Mean action noise std: 0.97
                       Mean reward: 336.78
               Mean episode length: 196.07
                 Mean success rate: 0.50
                  Mean reward/step: 1.70
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 2.72s
                        Total time: 1071.97s
                               ETA: 524406.7s

################################################################################
                    [1m Learning iteration 408/200000 [0m

                       Computation: 2919 steps/s (collection: 0.675s, learning 2.130s)
               Value function loss: 460.8731
                    Surrogate loss: 0.0188
             Mean action noise std: 0.97
                       Mean reward: 362.35
               Mean episode length: 202.20
                 Mean success rate: 1.00
                  Mean reward/step: 1.72
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 3350528
                    Iteration time: 2.81s
                        Total time: 1074.78s
                               ETA: 524491.0s

################################################################################
                    [1m Learning iteration 409/200000 [0m

                       Computation: 2947 steps/s (collection: 0.651s, learning 2.128s)
               Value function loss: 484.9793
                    Surrogate loss: 0.0178
             Mean action noise std: 0.97
                       Mean reward: 352.54
               Mean episode length: 199.87
                 Mean success rate: 1.00
                  Mean reward/step: 1.84
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 3358720
                    Iteration time: 2.78s
                        Total time: 1077.56s
                               ETA: 524562.1s

################################################################################
                    [1m Learning iteration 410/200000 [0m

                       Computation: 3010 steps/s (collection: 0.582s, learning 2.138s)
               Value function loss: 557.5750
                    Surrogate loss: 0.0170
             Mean action noise std: 0.97
                       Mean reward: 353.57
               Mean episode length: 198.93
                 Mean success rate: 1.50
                  Mean reward/step: 1.84
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 3366912
                    Iteration time: 2.72s
                        Total time: 1080.28s
                               ETA: 524604.5s

################################################################################
                    [1m Learning iteration 411/200000 [0m

                       Computation: 3021 steps/s (collection: 0.587s, learning 2.125s)
               Value function loss: 664.9732
                    Surrogate loss: 0.0160
             Mean action noise std: 0.97
                       Mean reward: 343.89
               Mean episode length: 192.95
                 Mean success rate: 1.00
                  Mean reward/step: 1.76
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 3375104
                    Iteration time: 2.71s
                        Total time: 1082.99s
                               ETA: 524642.0s

################################################################################
                    [1m Learning iteration 412/200000 [0m

                       Computation: 2947 steps/s (collection: 0.604s, learning 2.176s)
               Value function loss: 591.2425
                    Surrogate loss: 0.0161
             Mean action noise std: 0.97
                       Mean reward: 343.96
               Mean episode length: 194.34
                 Mean success rate: 1.00
                  Mean reward/step: 1.92
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3383296
                    Iteration time: 2.78s
                        Total time: 1085.77s
                               ETA: 524712.2s

################################################################################
                    [1m Learning iteration 413/200000 [0m

                       Computation: 2961 steps/s (collection: 0.607s, learning 2.159s)
               Value function loss: 473.1354
                    Surrogate loss: 0.0153
             Mean action noise std: 0.97
                       Mean reward: 331.46
               Mean episode length: 188.86
                 Mean success rate: 1.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3391488
                    Iteration time: 2.77s
                        Total time: 1088.53s
                               ETA: 524775.7s

################################################################################
                    [1m Learning iteration 414/200000 [0m

                       Computation: 2974 steps/s (collection: 0.639s, learning 2.115s)
               Value function loss: 596.5382
                    Surrogate loss: 0.0164
             Mean action noise std: 0.97
                       Mean reward: 337.38
               Mean episode length: 196.07
                 Mean success rate: 1.00
                  Mean reward/step: 1.81
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3399680
                    Iteration time: 2.75s
                        Total time: 1091.29s
                               ETA: 524833.3s

################################################################################
                    [1m Learning iteration 415/200000 [0m

                       Computation: 3030 steps/s (collection: 0.587s, learning 2.116s)
               Value function loss: 876.0271
                    Surrogate loss: 0.0168
             Mean action noise std: 0.97
                       Mean reward: 341.13
               Mean episode length: 195.93
                 Mean success rate: 1.00
                  Mean reward/step: 1.93
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 3407872
                    Iteration time: 2.70s
                        Total time: 1093.99s
                               ETA: 524866.1s

################################################################################
                    [1m Learning iteration 416/200000 [0m

                       Computation: 2964 steps/s (collection: 0.606s, learning 2.157s)
               Value function loss: 695.9531
                    Surrogate loss: 0.0155
             Mean action noise std: 0.97
                       Mean reward: 355.31
               Mean episode length: 201.06
                 Mean success rate: 0.50
                  Mean reward/step: 1.90
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 3416064
                    Iteration time: 2.76s
                        Total time: 1096.75s
                               ETA: 524927.2s

################################################################################
                    [1m Learning iteration 417/200000 [0m

                       Computation: 2928 steps/s (collection: 0.629s, learning 2.169s)
               Value function loss: 708.0136
                    Surrogate loss: 0.0189
             Mean action noise std: 0.97
                       Mean reward: 365.66
               Mean episode length: 211.41
                 Mean success rate: 0.50
                  Mean reward/step: 1.76
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 3424256
                    Iteration time: 2.80s
                        Total time: 1099.55s
                               ETA: 525004.5s

################################################################################
                    [1m Learning iteration 418/200000 [0m

                       Computation: 3010 steps/s (collection: 0.594s, learning 2.127s)
               Value function loss: 575.4941
                    Surrogate loss: 0.0153
             Mean action noise std: 0.97
                       Mean reward: 384.94
               Mean episode length: 219.35
                 Mean success rate: 0.50
                  Mean reward/step: 1.77
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 3432448
                    Iteration time: 2.72s
                        Total time: 1102.27s
                               ETA: 525045.2s

################################################################################
                    [1m Learning iteration 419/200000 [0m

                       Computation: 2983 steps/s (collection: 0.600s, learning 2.146s)
               Value function loss: 657.0919
                    Surrogate loss: 0.0160
             Mean action noise std: 0.97
                       Mean reward: 382.53
               Mean episode length: 212.30
                 Mean success rate: 0.50
                  Mean reward/step: 1.83
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 2.75s
                        Total time: 1105.02s
                               ETA: 525097.2s

################################################################################
                    [1m Learning iteration 420/200000 [0m

                       Computation: 2930 steps/s (collection: 0.654s, learning 2.141s)
               Value function loss: 697.1562
                    Surrogate loss: 0.0174
             Mean action noise std: 0.97
                       Mean reward: 401.29
               Mean episode length: 217.69
                 Mean success rate: 1.00
                  Mean reward/step: 1.79
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3448832
                    Iteration time: 2.80s
                        Total time: 1107.81s
                               ETA: 525172.4s

################################################################################
                    [1m Learning iteration 421/200000 [0m

                       Computation: 2986 steps/s (collection: 0.637s, learning 2.107s)
               Value function loss: 694.6386
                    Surrogate loss: 0.0181
             Mean action noise std: 0.97
                       Mean reward: 422.63
               Mean episode length: 229.35
                 Mean success rate: 1.50
                  Mean reward/step: 1.96
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3457024
                    Iteration time: 2.74s
                        Total time: 1110.56s
                               ETA: 525222.7s

################################################################################
                    [1m Learning iteration 422/200000 [0m

                       Computation: 2989 steps/s (collection: 0.641s, learning 2.099s)
               Value function loss: 648.1616
                    Surrogate loss: 0.0186
             Mean action noise std: 0.97
                       Mean reward: 420.24
               Mean episode length: 226.84
                 Mean success rate: 1.50
                  Mean reward/step: 1.84
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3465216
                    Iteration time: 2.74s
                        Total time: 1113.30s
                               ETA: 525271.4s

################################################################################
                    [1m Learning iteration 423/200000 [0m

                       Computation: 3021 steps/s (collection: 0.598s, learning 2.113s)
               Value function loss: 729.5431
                    Surrogate loss: 0.0149
             Mean action noise std: 0.97
                       Mean reward: 438.92
               Mean episode length: 230.30
                 Mean success rate: 2.00
                  Mean reward/step: 1.83
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3473408
                    Iteration time: 2.71s
                        Total time: 1116.01s
                               ETA: 525306.0s

################################################################################
                    [1m Learning iteration 424/200000 [0m

                       Computation: 2951 steps/s (collection: 0.625s, learning 2.151s)
               Value function loss: 980.0011
                    Surrogate loss: 0.0143
             Mean action noise std: 0.97
                       Mean reward: 427.15
               Mean episode length: 233.59
                 Mean success rate: 1.50
                  Mean reward/step: 2.00
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 3481600
                    Iteration time: 2.78s
                        Total time: 1118.78s
                               ETA: 525370.8s

################################################################################
                    [1m Learning iteration 425/200000 [0m

                       Computation: 3045 steps/s (collection: 0.595s, learning 2.095s)
               Value function loss: 817.3725
                    Surrogate loss: 0.0145
             Mean action noise std: 0.97
                       Mean reward: 440.02
               Mean episode length: 245.16
                 Mean success rate: 1.50
                  Mean reward/step: 1.84
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 3489792
                    Iteration time: 2.69s
                        Total time: 1121.48s
                               ETA: 525395.3s

################################################################################
                    [1m Learning iteration 426/200000 [0m

                       Computation: 3012 steps/s (collection: 0.596s, learning 2.123s)
               Value function loss: 756.3297
                    Surrogate loss: 0.0149
             Mean action noise std: 0.97
                       Mean reward: 437.81
               Mean episode length: 249.20
                 Mean success rate: 1.00
                  Mean reward/step: 1.83
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 3497984
                    Iteration time: 2.72s
                        Total time: 1124.19s
                               ETA: 525433.0s

################################################################################
                    [1m Learning iteration 427/200000 [0m

                       Computation: 2919 steps/s (collection: 0.611s, learning 2.195s)
               Value function loss: 941.7280
                    Surrogate loss: 0.0125
             Mean action noise std: 0.97
                       Mean reward: 441.50
               Mean episode length: 251.89
                 Mean success rate: 1.00
                  Mean reward/step: 1.91
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3506176
                    Iteration time: 2.81s
                        Total time: 1127.00s
                               ETA: 525511.2s

################################################################################
                    [1m Learning iteration 428/200000 [0m

                       Computation: 2880 steps/s (collection: 0.666s, learning 2.178s)
               Value function loss: 978.4641
                    Surrogate loss: 0.0158
             Mean action noise std: 0.97
                       Mean reward: 411.33
               Mean episode length: 240.09
                 Mean success rate: 0.50
                  Mean reward/step: 2.00
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 3514368
                    Iteration time: 2.84s
                        Total time: 1129.84s
                               ETA: 525606.5s

################################################################################
                    [1m Learning iteration 429/200000 [0m

                       Computation: 2993 steps/s (collection: 0.582s, learning 2.155s)
               Value function loss: 1458.4288
                    Surrogate loss: 0.0139
             Mean action noise std: 0.97
                       Mean reward: 426.02
               Mean episode length: 238.23
                 Mean success rate: 1.50
                  Mean reward/step: 2.10
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 3522560
                    Iteration time: 2.74s
                        Total time: 1132.58s
                               ETA: 525651.7s

################################################################################
                    [1m Learning iteration 430/200000 [0m

                       Computation: 2979 steps/s (collection: 0.606s, learning 2.143s)
               Value function loss: 1062.3199
                    Surrogate loss: 0.0138
             Mean action noise std: 0.97
                       Mean reward: 468.51
               Mean episode length: 242.07
                 Mean success rate: 3.00
                  Mean reward/step: 1.99
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3530752
                    Iteration time: 2.75s
                        Total time: 1135.33s
                               ETA: 525702.5s

################################################################################
                    [1m Learning iteration 431/200000 [0m

                       Computation: 2935 steps/s (collection: 0.648s, learning 2.142s)
               Value function loss: 1488.1285
                    Surrogate loss: 0.0144
             Mean action noise std: 0.97
                       Mean reward: 487.85
               Mean episode length: 240.54
                 Mean success rate: 4.50
                  Mean reward/step: 2.05
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 2.79s
                        Total time: 1138.12s
                               ETA: 525772.3s

################################################################################
                    [1m Learning iteration 432/200000 [0m

                       Computation: 3044 steps/s (collection: 0.571s, learning 2.120s)
               Value function loss: 1138.8113
                    Surrogate loss: 0.0117
             Mean action noise std: 0.97
                       Mean reward: 501.93
               Mean episode length: 240.01
                 Mean success rate: 5.00
                  Mean reward/step: 1.88
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3547136
                    Iteration time: 2.69s
                        Total time: 1140.81s
                               ETA: 525795.7s

################################################################################
                    [1m Learning iteration 433/200000 [0m

                       Computation: 2984 steps/s (collection: 0.603s, learning 2.142s)
               Value function loss: 1162.3302
                    Surrogate loss: 0.0121
             Mean action noise std: 0.97
                       Mean reward: 507.90
               Mean episode length: 244.84
                 Mean success rate: 5.00
                  Mean reward/step: 1.97
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3555328
                    Iteration time: 2.74s
                        Total time: 1143.56s
                               ETA: 525843.7s

################################################################################
                    [1m Learning iteration 434/200000 [0m

                       Computation: 2925 steps/s (collection: 0.646s, learning 2.154s)
               Value function loss: 1401.2414
                    Surrogate loss: 0.0118
             Mean action noise std: 0.97
                       Mean reward: 531.27
               Mean episode length: 251.66
                 Mean success rate: 5.50
                  Mean reward/step: 2.03
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3563520
                    Iteration time: 2.80s
                        Total time: 1146.36s
                               ETA: 525916.8s

################################################################################
                    [1m Learning iteration 435/200000 [0m

                       Computation: 2937 steps/s (collection: 0.637s, learning 2.152s)
               Value function loss: 1238.1105
                    Surrogate loss: 0.0129
             Mean action noise std: 0.97
                       Mean reward: 511.52
               Mean episode length: 249.55
                 Mean success rate: 4.50
                  Mean reward/step: 1.83
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 3571712
                    Iteration time: 2.79s
                        Total time: 1149.15s
                               ETA: 525984.3s

################################################################################
                    [1m Learning iteration 436/200000 [0m

                       Computation: 2954 steps/s (collection: 0.618s, learning 2.155s)
               Value function loss: 1117.8669
                    Surrogate loss: 0.0149
             Mean action noise std: 0.97
                       Mean reward: 493.08
               Mean episode length: 248.64
                 Mean success rate: 3.50
                  Mean reward/step: 1.97
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 3579904
                    Iteration time: 2.77s
                        Total time: 1151.92s
                               ETA: 526044.2s

################################################################################
                    [1m Learning iteration 437/200000 [0m

                       Computation: 2974 steps/s (collection: 0.627s, learning 2.127s)
               Value function loss: 1513.4183
                    Surrogate loss: 0.0111
             Mean action noise std: 0.97
                       Mean reward: 477.80
               Mean episode length: 249.44
                 Mean success rate: 3.00
                  Mean reward/step: 1.87
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3588096
                    Iteration time: 2.75s
                        Total time: 1154.67s
                               ETA: 526095.2s

################################################################################
                    [1m Learning iteration 438/200000 [0m

                       Computation: 2958 steps/s (collection: 0.586s, learning 2.183s)
               Value function loss: 1765.0919
                    Surrogate loss: 0.0110
             Mean action noise std: 0.97
                       Mean reward: 485.24
               Mean episode length: 257.34
                 Mean success rate: 3.00
                  Mean reward/step: 1.97
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 3596288
                    Iteration time: 2.77s
                        Total time: 1157.44s
                               ETA: 526152.9s

################################################################################
                    [1m Learning iteration 439/200000 [0m

                       Computation: 2983 steps/s (collection: 0.628s, learning 2.119s)
               Value function loss: 1471.4816
                    Surrogate loss: 0.0123
             Mean action noise std: 0.97
                       Mean reward: 496.17
               Mean episode length: 258.89
                 Mean success rate: 3.50
                  Mean reward/step: 2.12
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 3604480
                    Iteration time: 2.75s
                        Total time: 1160.19s
                               ETA: 526200.0s

################################################################################
                    [1m Learning iteration 440/200000 [0m

                       Computation: 3027 steps/s (collection: 0.573s, learning 2.133s)
               Value function loss: 2600.9888
                    Surrogate loss: 0.0097
             Mean action noise std: 0.97
                       Mean reward: 544.02
               Mean episode length: 270.37
                 Mean success rate: 5.00
                  Mean reward/step: 2.13
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 3612672
                    Iteration time: 2.71s
                        Total time: 1162.89s
                               ETA: 526228.6s

################################################################################
                    [1m Learning iteration 441/200000 [0m

                       Computation: 2964 steps/s (collection: 0.613s, learning 2.150s)
               Value function loss: 1678.2346
                    Surrogate loss: 0.0123
             Mean action noise std: 0.97
                       Mean reward: 544.24
               Mean episode length: 276.90
                 Mean success rate: 5.00
                  Mean reward/step: 2.04
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3620864
                    Iteration time: 2.76s
                        Total time: 1165.66s
                               ETA: 526282.9s

################################################################################
                    [1m Learning iteration 442/200000 [0m

                       Computation: 2992 steps/s (collection: 0.616s, learning 2.121s)
               Value function loss: 2468.9723
                    Surrogate loss: 0.0109
             Mean action noise std: 0.97
                       Mean reward: 577.02
               Mean episode length: 285.20
                 Mean success rate: 5.50
                  Mean reward/step: 2.18
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 3629056
                    Iteration time: 2.74s
                        Total time: 1168.39s
                               ETA: 526325.4s

################################################################################
                    [1m Learning iteration 443/200000 [0m

                       Computation: 3051 steps/s (collection: 0.598s, learning 2.087s)
               Value function loss: 1910.0003
                    Surrogate loss: 0.0116
             Mean action noise std: 0.97
                       Mean reward: 585.26
               Mean episode length: 294.08
                 Mean success rate: 5.50
                  Mean reward/step: 2.08
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 2.68s
                        Total time: 1171.08s
                               ETA: 526343.9s

################################################################################
                    [1m Learning iteration 444/200000 [0m

                       Computation: 3077 steps/s (collection: 0.589s, learning 2.072s)
               Value function loss: 2530.8548
                    Surrogate loss: 0.0124
             Mean action noise std: 0.97
                       Mean reward: 608.14
               Mean episode length: 297.11
                 Mean success rate: 6.00
                  Mean reward/step: 2.27
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 3645440
                    Iteration time: 2.66s
                        Total time: 1173.74s
                               ETA: 526352.1s

################################################################################
                    [1m Learning iteration 445/200000 [0m

                       Computation: 3045 steps/s (collection: 0.570s, learning 2.120s)
               Value function loss: 2824.7094
                    Surrogate loss: 0.0129
             Mean action noise std: 0.97
                       Mean reward: 631.87
               Mean episode length: 304.19
                 Mean success rate: 7.50
                  Mean reward/step: 2.25
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3653632
                    Iteration time: 2.69s
                        Total time: 1176.43s
                               ETA: 526373.1s

################################################################################
                    [1m Learning iteration 446/200000 [0m

                       Computation: 2983 steps/s (collection: 0.598s, learning 2.148s)
               Value function loss: 2667.4084
                    Surrogate loss: 0.0118
             Mean action noise std: 0.97
                       Mean reward: 648.57
               Mean episode length: 309.16
                 Mean success rate: 9.00
                  Mean reward/step: 2.51
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3661824
                    Iteration time: 2.75s
                        Total time: 1179.18s
                               ETA: 526418.7s

################################################################################
                    [1m Learning iteration 447/200000 [0m

                       Computation: 3032 steps/s (collection: 0.576s, learning 2.126s)
               Value function loss: 3054.2303
                    Surrogate loss: 0.0094
             Mean action noise std: 0.97
                       Mean reward: 622.84
               Mean episode length: 293.45
                 Mean success rate: 9.50
                  Mean reward/step: 2.44
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 3670016
                    Iteration time: 2.70s
                        Total time: 1181.88s
                               ETA: 526444.2s

################################################################################
                    [1m Learning iteration 448/200000 [0m

                       Computation: 3016 steps/s (collection: 0.600s, learning 2.116s)
               Value function loss: 3078.2439
                    Surrogate loss: 0.0136
             Mean action noise std: 0.97
                       Mean reward: 599.47
               Mean episode length: 282.19
                 Mean success rate: 9.50
                  Mean reward/step: 2.52
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 3678208
                    Iteration time: 2.72s
                        Total time: 1184.59s
                               ETA: 526476.2s

################################################################################
                    [1m Learning iteration 449/200000 [0m

                       Computation: 2960 steps/s (collection: 0.616s, learning 2.151s)
               Value function loss: 3505.8475
                    Surrogate loss: 0.0141
             Mean action noise std: 0.97
                       Mean reward: 563.60
               Mean episode length: 260.87
                 Mean success rate: 10.50
                  Mean reward/step: 2.39
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 3686400
                    Iteration time: 2.77s
                        Total time: 1187.36s
                               ETA: 526530.8s

################################################################################
                    [1m Learning iteration 450/200000 [0m

                       Computation: 2990 steps/s (collection: 0.638s, learning 2.101s)
               Value function loss: 3329.9186
                    Surrogate loss: 0.0094
             Mean action noise std: 0.97
                       Mean reward: 586.12
               Mean episode length: 263.31
                 Mean success rate: 11.50
                  Mean reward/step: 2.22
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 3694592
                    Iteration time: 2.74s
                        Total time: 1190.10s
                               ETA: 526572.9s

################################################################################
                    [1m Learning iteration 451/200000 [0m

                       Computation: 3052 steps/s (collection: 0.572s, learning 2.112s)
               Value function loss: 3097.9372
                    Surrogate loss: 0.0109
             Mean action noise std: 0.97
                       Mean reward: 530.07
               Mean episode length: 242.87
                 Mean success rate: 9.50
                  Mean reward/step: 2.03
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 3702784
                    Iteration time: 2.68s
                        Total time: 1192.78s
                               ETA: 526590.1s

################################################################################
                    [1m Learning iteration 452/200000 [0m

                       Computation: 3022 steps/s (collection: 0.571s, learning 2.140s)
               Value function loss: 2071.7214
                    Surrogate loss: 0.0137
             Mean action noise std: 0.97
                       Mean reward: 493.74
               Mean episode length: 236.36
                 Mean success rate: 7.50
                  Mean reward/step: 2.19
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 3710976
                    Iteration time: 2.71s
                        Total time: 1195.49s
                               ETA: 526619.0s

################################################################################
                    [1m Learning iteration 453/200000 [0m

                       Computation: 2991 steps/s (collection: 0.626s, learning 2.113s)
               Value function loss: 4031.2518
                    Surrogate loss: 0.0105
             Mean action noise std: 0.97
                       Mean reward: 533.19
               Mean episode length: 238.95
                 Mean success rate: 9.00
                  Mean reward/step: 2.35
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3719168
                    Iteration time: 2.74s
                        Total time: 1198.23s
                               ETA: 526660.0s

################################################################################
                    [1m Learning iteration 454/200000 [0m

                       Computation: 3010 steps/s (collection: 0.595s, learning 2.127s)
               Value function loss: 3726.0318
                    Surrogate loss: 0.0142
             Mean action noise std: 0.97
                       Mean reward: 545.82
               Mean episode length: 240.40
                 Mean success rate: 9.00
                  Mean reward/step: 2.53
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3727360
                    Iteration time: 2.72s
                        Total time: 1200.95s
                               ETA: 526693.3s

################################################################################
                    [1m Learning iteration 455/200000 [0m

                       Computation: 3035 steps/s (collection: 0.592s, learning 2.106s)
               Value function loss: 3967.5420
                    Surrogate loss: 0.0120
             Mean action noise std: 0.97
                       Mean reward: 521.35
               Mean episode length: 234.27
                 Mean success rate: 7.00
                  Mean reward/step: 2.56
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 2.70s
                        Total time: 1203.65s
                               ETA: 526716.5s

################################################################################
                    [1m Learning iteration 456/200000 [0m

                       Computation: 2954 steps/s (collection: 0.633s, learning 2.140s)
               Value function loss: 4963.9698
                    Surrogate loss: 0.0103
             Mean action noise std: 0.97
                       Mean reward: 541.50
               Mean episode length: 235.56
                 Mean success rate: 8.00
                  Mean reward/step: 2.53
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 3743744
                    Iteration time: 2.77s
                        Total time: 1206.42s
                               ETA: 526771.8s

################################################################################
                    [1m Learning iteration 457/200000 [0m

                       Computation: 2895 steps/s (collection: 0.670s, learning 2.159s)
               Value function loss: 4622.6441
                    Surrogate loss: 0.0081
             Mean action noise std: 0.97
                       Mean reward: 516.65
               Mean episode length: 216.01
                 Mean success rate: 8.50
                  Mean reward/step: 2.46
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 3751936
                    Iteration time: 2.83s
                        Total time: 1209.25s
                               ETA: 526851.6s

################################################################################
                    [1m Learning iteration 458/200000 [0m

                       Computation: 3001 steps/s (collection: 0.619s, learning 2.110s)
               Value function loss: 2586.3711
                    Surrogate loss: 0.0124
             Mean action noise std: 0.97
                       Mean reward: 465.60
               Mean episode length: 209.01
                 Mean success rate: 6.50
                  Mean reward/step: 2.37
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 3760128
                    Iteration time: 2.73s
                        Total time: 1211.98s
                               ETA: 526887.7s

################################################################################
                    [1m Learning iteration 459/200000 [0m

                       Computation: 2988 steps/s (collection: 0.611s, learning 2.130s)
               Value function loss: 3898.3412
                    Surrogate loss: 0.0104
             Mean action noise std: 0.97
                       Mean reward: 473.65
               Mean episode length: 211.94
                 Mean success rate: 8.00
                  Mean reward/step: 2.38
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 3768320
                    Iteration time: 2.74s
                        Total time: 1214.72s
                               ETA: 526928.6s

################################################################################
                    [1m Learning iteration 460/200000 [0m

                       Computation: 2982 steps/s (collection: 0.598s, learning 2.149s)
               Value function loss: 2624.1176
                    Surrogate loss: 0.0131
             Mean action noise std: 0.97
                       Mean reward: 456.47
               Mean episode length: 210.88
                 Mean success rate: 8.50
                  Mean reward/step: 2.47
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 3776512
                    Iteration time: 2.75s
                        Total time: 1217.47s
                               ETA: 526971.8s

################################################################################
                    [1m Learning iteration 461/200000 [0m

                       Computation: 2987 steps/s (collection: 0.608s, learning 2.134s)
               Value function loss: 4054.0915
                    Surrogate loss: 0.0109
             Mean action noise std: 0.97
                       Mean reward: 436.49
               Mean episode length: 207.18
                 Mean success rate: 7.50
                  Mean reward/step: 2.41
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3784704
                    Iteration time: 2.74s
                        Total time: 1220.21s
                               ETA: 527012.8s

################################################################################
                    [1m Learning iteration 462/200000 [0m

                       Computation: 3056 steps/s (collection: 0.582s, learning 2.098s)
               Value function loss: 4262.7269
                    Surrogate loss: 0.0152
             Mean action noise std: 0.97
                       Mean reward: 507.96
               Mean episode length: 217.56
                 Mean success rate: 9.50
                  Mean reward/step: 2.31
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 3792896
                    Iteration time: 2.68s
                        Total time: 1222.89s
                               ETA: 527026.9s

################################################################################
                    [1m Learning iteration 463/200000 [0m

                       Computation: 3023 steps/s (collection: 0.562s, learning 2.147s)
               Value function loss: 4287.7308
                    Surrogate loss: 0.0134
             Mean action noise std: 0.97
                       Mean reward: 551.65
               Mean episode length: 231.38
                 Mean success rate: 10.50
                  Mean reward/step: 2.50
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 3801088
                    Iteration time: 2.71s
                        Total time: 1225.60s
                               ETA: 527053.4s

################################################################################
                    [1m Learning iteration 464/200000 [0m

                       Computation: 2978 steps/s (collection: 0.629s, learning 2.122s)
               Value function loss: 3282.7356
                    Surrogate loss: 0.0126
             Mean action noise std: 0.97
                       Mean reward: 564.08
               Mean episode length: 229.02
                 Mean success rate: 12.00
                  Mean reward/step: 2.63
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 3809280
                    Iteration time: 2.75s
                        Total time: 1228.35s
                               ETA: 527097.6s

################################################################################
                    [1m Learning iteration 465/200000 [0m

                       Computation: 2982 steps/s (collection: 0.625s, learning 2.123s)
               Value function loss: 4328.9290
                    Surrogate loss: 0.0114
             Mean action noise std: 0.97
                       Mean reward: 562.39
               Mean episode length: 222.87
                 Mean success rate: 10.50
                  Mean reward/step: 2.90
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 3817472
                    Iteration time: 2.75s
                        Total time: 1231.10s
                               ETA: 527140.2s

################################################################################
                    [1m Learning iteration 466/200000 [0m

                       Computation: 3042 steps/s (collection: 0.576s, learning 2.116s)
               Value function loss: 3985.1680
                    Surrogate loss: 0.0149
             Mean action noise std: 0.97
                       Mean reward: 574.73
               Mean episode length: 219.91
                 Mean success rate: 11.50
                  Mean reward/step: 2.58
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 3825664
                    Iteration time: 2.69s
                        Total time: 1233.79s
                               ETA: 527159.0s

################################################################################
                    [1m Learning iteration 467/200000 [0m

                       Computation: 2993 steps/s (collection: 0.592s, learning 2.145s)
               Value function loss: 4076.2175
                    Surrogate loss: 0.0147
             Mean action noise std: 0.97
                       Mean reward: 527.55
               Mean episode length: 210.19
                 Mean success rate: 11.00
                  Mean reward/step: 2.74
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 2.74s
                        Total time: 1236.53s
                               ETA: 527196.9s

################################################################################
                    [1m Learning iteration 468/200000 [0m

                       Computation: 2915 steps/s (collection: 0.634s, learning 2.175s)
               Value function loss: 5846.0684
                    Surrogate loss: 0.0111
             Mean action noise std: 0.97
                       Mean reward: 505.21
               Mean episode length: 198.97
                 Mean success rate: 11.50
                  Mean reward/step: 2.85
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 3842048
                    Iteration time: 2.81s
                        Total time: 1239.34s
                               ETA: 527265.4s

################################################################################
                    [1m Learning iteration 469/200000 [0m

                       Computation: 2945 steps/s (collection: 0.662s, learning 2.119s)
               Value function loss: 5028.6790
                    Surrogate loss: 0.0132
             Mean action noise std: 0.97
                       Mean reward: 507.86
               Mean episode length: 189.73
                 Mean success rate: 12.00
                  Mean reward/step: 2.77
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 3850240
                    Iteration time: 2.78s
                        Total time: 1242.12s
                               ETA: 527321.8s

################################################################################
                    [1m Learning iteration 470/200000 [0m

                       Computation: 2966 steps/s (collection: 0.628s, learning 2.133s)
               Value function loss: 3454.4663
                    Surrogate loss: 0.0124
             Mean action noise std: 0.97
                       Mean reward: 467.22
               Mean episode length: 178.84
                 Mean success rate: 11.00
                  Mean reward/step: 2.51
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 3858432
                    Iteration time: 2.76s
                        Total time: 1244.88s
                               ETA: 527369.4s

################################################################################
                    [1m Learning iteration 471/200000 [0m

                       Computation: 2995 steps/s (collection: 0.579s, learning 2.156s)
               Value function loss: 3580.5507
                    Surrogate loss: 0.0113
             Mean action noise std: 0.97
                       Mean reward: 517.34
               Mean episode length: 189.25
                 Mean success rate: 12.50
                  Mean reward/step: 2.37
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 3866624
                    Iteration time: 2.73s
                        Total time: 1247.62s
                               ETA: 527405.5s

################################################################################
                    [1m Learning iteration 472/200000 [0m

                       Computation: 2983 steps/s (collection: 0.620s, learning 2.126s)
               Value function loss: 2407.7516
                    Surrogate loss: 0.0130
             Mean action noise std: 0.97
                       Mean reward: 532.02
               Mean episode length: 200.54
                 Mean success rate: 11.50
                  Mean reward/step: 2.23
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 3874816
                    Iteration time: 2.75s
                        Total time: 1250.36s
                               ETA: 527446.0s

################################################################################
                    [1m Learning iteration 473/200000 [0m

                       Computation: 2999 steps/s (collection: 0.578s, learning 2.153s)
               Value function loss: 2424.1900
                    Surrogate loss: 0.0158
             Mean action noise std: 0.97
                       Mean reward: 536.84
               Mean episode length: 202.50
                 Mean success rate: 10.50
                  Mean reward/step: 2.23
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 3883008
                    Iteration time: 2.73s
                        Total time: 1253.09s
                               ETA: 527480.3s

################################################################################
                    [1m Learning iteration 474/200000 [0m

                       Computation: 3016 steps/s (collection: 0.590s, learning 2.126s)
               Value function loss: 3220.4991
                    Surrogate loss: 0.0142
             Mean action noise std: 0.97
                       Mean reward: 540.79
               Mean episode length: 206.91
                 Mean success rate: 10.50
                  Mean reward/step: 2.61
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 3891200
                    Iteration time: 2.72s
                        Total time: 1255.81s
                               ETA: 527507.9s

################################################################################
                    [1m Learning iteration 475/200000 [0m

                       Computation: 2939 steps/s (collection: 0.645s, learning 2.142s)
               Value function loss: 3251.4451
                    Surrogate loss: 0.0153
             Mean action noise std: 0.97
                       Mean reward: 547.33
               Mean episode length: 216.69
                 Mean success rate: 10.00
                  Mean reward/step: 2.41
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 3899392
                    Iteration time: 2.79s
                        Total time: 1258.59s
                               ETA: 527565.4s

################################################################################
                    [1m Learning iteration 476/200000 [0m

                       Computation: 2967 steps/s (collection: 0.630s, learning 2.130s)
               Value function loss: 2940.5852
                    Surrogate loss: 0.0170
             Mean action noise std: 0.97
                       Mean reward: 540.30
               Mean episode length: 213.62
                 Mean success rate: 10.00
                  Mean reward/step: 2.46
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3907584
                    Iteration time: 2.76s
                        Total time: 1261.36s
                               ETA: 527611.4s

################################################################################
                    [1m Learning iteration 477/200000 [0m

                       Computation: 3051 steps/s (collection: 0.564s, learning 2.121s)
               Value function loss: 2539.8599
                    Surrogate loss: 0.0173
             Mean action noise std: 0.96
                       Mean reward: 483.41
               Mean episode length: 204.15
                 Mean success rate: 8.00
                  Mean reward/step: 2.42
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 3915776
                    Iteration time: 2.68s
                        Total time: 1264.04s
                               ETA: 527625.5s

################################################################################
                    [1m Learning iteration 478/200000 [0m

                       Computation: 3003 steps/s (collection: 0.562s, learning 2.165s)
               Value function loss: 2565.2196
                    Surrogate loss: 0.0152
             Mean action noise std: 0.96
                       Mean reward: 482.94
               Mean episode length: 200.25
                 Mean success rate: 8.00
                  Mean reward/step: 2.36
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 3923968
                    Iteration time: 2.73s
                        Total time: 1266.77s
                               ETA: 527657.4s

################################################################################
                    [1m Learning iteration 479/200000 [0m

                       Computation: 2958 steps/s (collection: 0.618s, learning 2.151s)
               Value function loss: 3153.9807
                    Surrogate loss: 0.0147
             Mean action noise std: 0.96
                       Mean reward: 531.62
               Mean episode length: 216.25
                 Mean success rate: 9.50
                  Mean reward/step: 2.50
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 2.77s
                        Total time: 1269.54s
                               ETA: 527706.6s

################################################################################
                    [1m Learning iteration 480/200000 [0m

                       Computation: 2990 steps/s (collection: 0.607s, learning 2.133s)
               Value function loss: 4096.1322
                    Surrogate loss: 0.0140
             Mean action noise std: 0.96
                       Mean reward: 495.47
               Mean episode length: 212.29
                 Mean success rate: 8.50
                  Mean reward/step: 2.62
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 3940352
                    Iteration time: 2.74s
                        Total time: 1272.28s
                               ETA: 527743.2s

################################################################################
                    [1m Learning iteration 481/200000 [0m

                       Computation: 2988 steps/s (collection: 0.618s, learning 2.123s)
               Value function loss: 3580.5223
                    Surrogate loss: 0.0148
             Mean action noise std: 0.96
                       Mean reward: 482.99
               Mean episode length: 212.81
                 Mean success rate: 8.50
                  Mean reward/step: 2.39
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 3948544
                    Iteration time: 2.74s
                        Total time: 1275.02s
                               ETA: 527780.3s

################################################################################
                    [1m Learning iteration 482/200000 [0m

                       Computation: 2951 steps/s (collection: 0.623s, learning 2.153s)
               Value function loss: 4762.8790
                    Surrogate loss: 0.0134
             Mean action noise std: 0.96
                       Mean reward: 485.17
               Mean episode length: 219.72
                 Mean success rate: 8.00
                  Mean reward/step: 2.85
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 3956736
                    Iteration time: 2.78s
                        Total time: 1277.79s
                               ETA: 527831.4s

################################################################################
                    [1m Learning iteration 483/200000 [0m

                       Computation: 2992 steps/s (collection: 0.596s, learning 2.142s)
               Value function loss: 4522.6199
                    Surrogate loss: 0.0150
             Mean action noise std: 0.96
                       Mean reward: 523.10
               Mean episode length: 223.34
                 Mean success rate: 10.00
                  Mean reward/step: 2.93
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3964928
                    Iteration time: 2.74s
                        Total time: 1280.53s
                               ETA: 527866.7s

################################################################################
                    [1m Learning iteration 484/200000 [0m

                       Computation: 2989 steps/s (collection: 0.599s, learning 2.141s)
               Value function loss: 4244.1422
                    Surrogate loss: 0.0123
             Mean action noise std: 0.96
                       Mean reward: 540.98
               Mean episode length: 230.78
                 Mean success rate: 11.50
                  Mean reward/step: 2.75
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 3973120
                    Iteration time: 2.74s
                        Total time: 1283.27s
                               ETA: 527902.8s

################################################################################
                    [1m Learning iteration 485/200000 [0m

                       Computation: 3045 steps/s (collection: 0.577s, learning 2.113s)
               Value function loss: 3412.6751
                    Surrogate loss: 0.0137
             Mean action noise std: 0.96
                       Mean reward: 530.33
               Mean episode length: 232.46
                 Mean success rate: 11.50
                  Mean reward/step: 2.68
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 3981312
                    Iteration time: 2.69s
                        Total time: 1285.96s
                               ETA: 527918.1s

################################################################################
                    [1m Learning iteration 486/200000 [0m

                       Computation: 3039 steps/s (collection: 0.605s, learning 2.090s)
               Value function loss: 3699.7913
                    Surrogate loss: 0.0121
             Mean action noise std: 0.96
                       Mean reward: 581.75
               Mean episode length: 234.41
                 Mean success rate: 13.50
                  Mean reward/step: 2.61
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 3989504
                    Iteration time: 2.70s
                        Total time: 1288.65s
                               ETA: 527935.6s

################################################################################
                    [1m Learning iteration 487/200000 [0m

                       Computation: 3077 steps/s (collection: 0.569s, learning 2.092s)
               Value function loss: 3991.1456
                    Surrogate loss: 0.0130
             Mean action noise std: 0.96
                       Mean reward: 596.03
               Mean episode length: 238.38
                 Mean success rate: 13.00
                  Mean reward/step: 2.71
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 3997696
                    Iteration time: 2.66s
                        Total time: 1291.32s
                               ETA: 527939.4s

################################################################################
                    [1m Learning iteration 488/200000 [0m

                       Computation: 3120 steps/s (collection: 0.548s, learning 2.077s)
               Value function loss: 3830.0022
                    Surrogate loss: 0.0164
             Mean action noise std: 0.96
                       Mean reward: 634.62
               Mean episode length: 240.37
                 Mean success rate: 14.50
                  Mean reward/step: 2.73
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 4005888
                    Iteration time: 2.63s
                        Total time: 1293.94s
                               ETA: 527928.2s

################################################################################
                    [1m Learning iteration 489/200000 [0m

                       Computation: 3044 steps/s (collection: 0.594s, learning 2.097s)
               Value function loss: 3015.1715
                    Surrogate loss: 0.0134
             Mean action noise std: 0.96
                       Mean reward: 651.58
               Mean episode length: 248.48
                 Mean success rate: 16.00
                  Mean reward/step: 2.61
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 4014080
                    Iteration time: 2.69s
                        Total time: 1296.63s
                               ETA: 527943.8s

################################################################################
                    [1m Learning iteration 490/200000 [0m

                       Computation: 3057 steps/s (collection: 0.622s, learning 2.057s)
               Value function loss: 4464.9657
                    Surrogate loss: 0.0159
             Mean action noise std: 0.96
                       Mean reward: 600.42
               Mean episode length: 228.03
                 Mean success rate: 12.50
                  Mean reward/step: 2.73
       Mean episode length/episode: 26.43
--------------------------------------------------------------------------------
                   Total timesteps: 4022272
                    Iteration time: 2.68s
                        Total time: 1299.31s
                               ETA: 527954.7s

################################################################################
                    [1m Learning iteration 491/200000 [0m

                       Computation: 3238 steps/s (collection: 0.508s, learning 2.021s)
               Value function loss: 4099.8274
                    Surrogate loss: 0.0135
             Mean action noise std: 0.96
                       Mean reward: 611.53
               Mean episode length: 227.56
                 Mean success rate: 13.50
                  Mean reward/step: 2.83
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 2.53s
                        Total time: 1301.84s
                               ETA: 527904.6s

################################################################################
                    [1m Learning iteration 492/200000 [0m

                       Computation: 3278 steps/s (collection: 0.488s, learning 2.011s)
               Value function loss: 3827.7935
                    Surrogate loss: 0.0140
             Mean action noise std: 0.96
                       Mean reward: 597.45
               Mean episode length: 220.18
                 Mean success rate: 14.00
                  Mean reward/step: 2.92
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 4038656
                    Iteration time: 2.50s
                        Total time: 1304.34s
                               ETA: 527842.2s

################################################################################
                    [1m Learning iteration 493/200000 [0m

                       Computation: 3191 steps/s (collection: 0.533s, learning 2.034s)
               Value function loss: 3787.6316
                    Surrogate loss: 0.0155
             Mean action noise std: 0.96
                       Mean reward: 516.96
               Mean episode length: 206.31
                 Mean success rate: 11.50
                  Mean reward/step: 2.91
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 4046848
                    Iteration time: 2.57s
                        Total time: 1306.91s
                               ETA: 527807.8s

################################################################################
                    [1m Learning iteration 494/200000 [0m

                       Computation: 3069 steps/s (collection: 0.594s, learning 2.075s)
               Value function loss: 4147.4630
                    Surrogate loss: 0.0151
             Mean action noise std: 0.96
                       Mean reward: 569.39
               Mean episode length: 205.81
                 Mean success rate: 14.00
                  Mean reward/step: 2.83
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 4055040
                    Iteration time: 2.67s
                        Total time: 1309.58s
                               ETA: 527814.7s

################################################################################
                    [1m Learning iteration 495/200000 [0m

                       Computation: 3207 steps/s (collection: 0.504s, learning 2.050s)
               Value function loss: 4081.3826
                    Surrogate loss: 0.0155
             Mean action noise std: 0.96
                       Mean reward: 589.00
               Mean episode length: 204.75
                 Mean success rate: 15.00
                  Mean reward/step: 2.84
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 4063232
                    Iteration time: 2.55s
                        Total time: 1312.13s
                               ETA: 527775.3s

################################################################################
                    [1m Learning iteration 496/200000 [0m

                       Computation: 3050 steps/s (collection: 0.567s, learning 2.118s)
               Value function loss: 4635.4642
                    Surrogate loss: 0.0148
             Mean action noise std: 0.96
                       Mean reward: 568.83
               Mean episode length: 210.66
                 Mean success rate: 13.00
                  Mean reward/step: 2.85
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 4071424
                    Iteration time: 2.69s
                        Total time: 1314.82s
                               ETA: 527788.8s

################################################################################
                    [1m Learning iteration 497/200000 [0m

                       Computation: 2993 steps/s (collection: 0.636s, learning 2.101s)
               Value function loss: 4805.4853
                    Surrogate loss: 0.0173
             Mean action noise std: 0.96
                       Mean reward: 576.13
               Mean episode length: 215.16
                 Mean success rate: 13.00
                  Mean reward/step: 3.35
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 4079616
                    Iteration time: 2.74s
                        Total time: 1317.55s
                               ETA: 527822.8s

################################################################################
                    [1m Learning iteration 498/200000 [0m

                       Computation: 3128 steps/s (collection: 0.550s, learning 2.068s)
               Value function loss: 5739.8731
                    Surrogate loss: 0.0123
             Mean action noise std: 0.96
                       Mean reward: 622.82
               Mean episode length: 223.23
                 Mean success rate: 14.00
                  Mean reward/step: 3.67
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 4087808
                    Iteration time: 2.62s
                        Total time: 1320.17s
                               ETA: 527809.2s

################################################################################
                    [1m Learning iteration 499/200000 [0m

                       Computation: 3206 steps/s (collection: 0.498s, learning 2.058s)
               Value function loss: 6154.6407
                    Surrogate loss: 0.0136
             Mean action noise std: 0.96
                       Mean reward: 618.73
               Mean episode length: 213.27
                 Mean success rate: 14.00
                  Mean reward/step: 3.78
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 4096000
                    Iteration time: 2.56s
                        Total time: 1322.73s
                               ETA: 527770.5s

################################################################################
                    [1m Learning iteration 500/200000 [0m

                       Computation: 3174 steps/s (collection: 0.540s, learning 2.040s)
               Value function loss: 7291.0874
                    Surrogate loss: 0.0152
             Mean action noise std: 0.96
                       Mean reward: 654.81
               Mean episode length: 220.03
                 Mean success rate: 14.50
                  Mean reward/step: 3.63
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 4104192
                    Iteration time: 2.58s
                        Total time: 1325.31s
                               ETA: 527742.1s

################################################################################
                    [1m Learning iteration 501/200000 [0m

                       Computation: 3186 steps/s (collection: 0.542s, learning 2.029s)
               Value function loss: 6207.7381
                    Surrogate loss: 0.0126
             Mean action noise std: 0.96
                       Mean reward: 624.84
               Mean episode length: 207.97
                 Mean success rate: 14.00
                  Mean reward/step: 3.76
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 4112384
                    Iteration time: 2.57s
                        Total time: 1327.88s
                               ETA: 527709.9s

################################################################################
                    [1m Learning iteration 502/200000 [0m

                       Computation: 3154 steps/s (collection: 0.546s, learning 2.051s)
               Value function loss: 8699.3090
                    Surrogate loss: 0.0140
             Mean action noise std: 0.96
                       Mean reward: 664.78
               Mean episode length: 203.38
                 Mean success rate: 15.00
                  Mean reward/step: 3.93
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 4120576
                    Iteration time: 2.60s
                        Total time: 1330.48s
                               ETA: 527688.1s

################################################################################
                    [1m Learning iteration 503/200000 [0m

                       Computation: 3194 steps/s (collection: 0.512s, learning 2.052s)
               Value function loss: 8054.2257
                    Surrogate loss: 0.0124
             Mean action noise std: 0.96
                       Mean reward: 668.56
               Mean episode length: 203.00
                 Mean success rate: 14.50
                  Mean reward/step: 3.61
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 2.56s
                        Total time: 1333.04s
                               ETA: 527653.6s

################################################################################
                    [1m Learning iteration 504/200000 [0m

                       Computation: 3157 steps/s (collection: 0.519s, learning 2.076s)
               Value function loss: 5283.8144
                    Surrogate loss: 0.0127
             Mean action noise std: 0.96
                       Mean reward: 691.18
               Mean episode length: 205.20
                 Mean success rate: 14.50
                  Mean reward/step: 3.00
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 4136960
                    Iteration time: 2.59s
                        Total time: 1335.63s
                               ETA: 527631.1s

################################################################################
                    [1m Learning iteration 505/200000 [0m

                       Computation: 3179 steps/s (collection: 0.532s, learning 2.045s)
               Value function loss: 4087.9400
                    Surrogate loss: 0.0111
             Mean action noise std: 0.96
                       Mean reward: 711.14
               Mean episode length: 200.50
                 Mean success rate: 15.00
                  Mean reward/step: 2.54
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 4145152
                    Iteration time: 2.58s
                        Total time: 1338.21s
                               ETA: 527601.5s

################################################################################
                    [1m Learning iteration 506/200000 [0m

                       Computation: 3163 steps/s (collection: 0.534s, learning 2.056s)
               Value function loss: 3942.4831
                    Surrogate loss: 0.0147
             Mean action noise std: 0.96
                       Mean reward: 761.79
               Mean episode length: 207.19
                 Mean success rate: 15.50
                  Mean reward/step: 2.71
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 4153344
                    Iteration time: 2.59s
                        Total time: 1340.80s
                               ETA: 527577.2s

################################################################################
                    [1m Learning iteration 507/200000 [0m

                       Computation: 3119 steps/s (collection: 0.557s, learning 2.069s)
               Value function loss: 4880.1598
                    Surrogate loss: 0.0138
             Mean action noise std: 0.96
                       Mean reward: 783.49
               Mean episode length: 209.50
                 Mean success rate: 18.00
                  Mean reward/step: 2.78
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 4161536
                    Iteration time: 2.63s
                        Total time: 1343.43s
                               ETA: 527567.3s

################################################################################
                    [1m Learning iteration 508/200000 [0m

                       Computation: 3172 steps/s (collection: 0.500s, learning 2.082s)
               Value function loss: 4873.4947
                    Surrogate loss: 0.0155
             Mean action noise std: 0.96
                       Mean reward: 739.79
               Mean episode length: 205.07
                 Mean success rate: 17.00
                  Mean reward/step: 3.08
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 4169728
                    Iteration time: 2.58s
                        Total time: 1346.01s
                               ETA: 527540.2s

################################################################################
                    [1m Learning iteration 509/200000 [0m

                       Computation: 3164 steps/s (collection: 0.550s, learning 2.038s)
               Value function loss: 6464.0622
                    Surrogate loss: 0.0161
             Mean action noise std: 0.96
                       Mean reward: 684.50
               Mean episode length: 203.61
                 Mean success rate: 14.00
                  Mean reward/step: 3.65
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 4177920
                    Iteration time: 2.59s
                        Total time: 1348.60s
                               ETA: 527515.8s

################################################################################
                    [1m Learning iteration 510/200000 [0m

                       Computation: 3194 steps/s (collection: 0.545s, learning 2.020s)
               Value function loss: 4405.9715
                    Surrogate loss: 0.0133
             Mean action noise std: 0.96
                       Mean reward: 589.24
               Mean episode length: 200.34
                 Mean success rate: 11.50
                  Mean reward/step: 3.45
       Mean episode length/episode: 25.84
--------------------------------------------------------------------------------
                   Total timesteps: 4186112
                    Iteration time: 2.56s
                        Total time: 1351.16s
                               ETA: 527482.0s

################################################################################
                    [1m Learning iteration 511/200000 [0m

                       Computation: 3231 steps/s (collection: 0.512s, learning 2.022s)
               Value function loss: 7438.1415
                    Surrogate loss: 0.0110
             Mean action noise std: 0.96
                       Mean reward: 609.71
               Mean episode length: 197.63
                 Mean success rate: 12.50
                  Mean reward/step: 3.66
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 4194304
                    Iteration time: 2.53s
                        Total time: 1353.70s
                               ETA: 527436.7s

################################################################################
                    [1m Learning iteration 512/200000 [0m

                       Computation: 3204 steps/s (collection: 0.544s, learning 2.013s)
               Value function loss: 7539.3796
                    Surrogate loss: 0.0114
             Mean action noise std: 0.96
                       Mean reward: 572.90
               Mean episode length: 187.62
                 Mean success rate: 10.50
                  Mean reward/step: 3.63
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 4202496
                    Iteration time: 2.56s
                        Total time: 1356.25s
                               ETA: 527400.0s

################################################################################
                    [1m Learning iteration 513/200000 [0m

                       Computation: 3178 steps/s (collection: 0.554s, learning 2.024s)
               Value function loss: 4754.5782
                    Surrogate loss: 0.0155
             Mean action noise std: 0.96
                       Mean reward: 569.81
               Mean episode length: 188.50
                 Mean success rate: 10.00
                  Mean reward/step: 3.42
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4210688
                    Iteration time: 2.58s
                        Total time: 1358.83s
                               ETA: 527371.6s

################################################################################
                    [1m Learning iteration 514/200000 [0m

                       Computation: 3252 steps/s (collection: 0.497s, learning 2.021s)
               Value function loss: 7944.2589
                    Surrogate loss: 0.0121
             Mean action noise std: 0.96
                       Mean reward: 572.02
               Mean episode length: 190.16
                 Mean success rate: 11.00
                  Mean reward/step: 3.55
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 4218880
                    Iteration time: 2.52s
                        Total time: 1361.35s
                               ETA: 527320.4s

################################################################################
                    [1m Learning iteration 515/200000 [0m

                       Computation: 3258 steps/s (collection: 0.494s, learning 2.020s)
               Value function loss: 5380.4939
                    Surrogate loss: 0.0112
             Mean action noise std: 0.96
                       Mean reward: 622.69
               Mean episode length: 196.25
                 Mean success rate: 12.50
                  Mean reward/step: 3.76
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 2.51s
                        Total time: 1363.86s
                               ETA: 527267.8s

################################################################################
                    [1m Learning iteration 516/200000 [0m

                       Computation: 3253 steps/s (collection: 0.497s, learning 2.022s)
               Value function loss: 7399.2799
                    Surrogate loss: 0.0118
             Mean action noise std: 0.96
                       Mean reward: 600.23
               Mean episode length: 196.78
                 Mean success rate: 11.50
                  Mean reward/step: 3.65
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 4235264
                    Iteration time: 2.52s
                        Total time: 1366.38s
                               ETA: 527216.9s

################################################################################
                    [1m Learning iteration 517/200000 [0m

                       Computation: 3195 steps/s (collection: 0.522s, learning 2.042s)
               Value function loss: 7155.3022
                    Surrogate loss: 0.0132
             Mean action noise std: 0.96
                       Mean reward: 566.91
               Mean episode length: 201.68
                 Mean success rate: 10.50
                  Mean reward/step: 3.93
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 4243456
                    Iteration time: 2.56s
                        Total time: 1368.94s
                               ETA: 527183.7s

################################################################################
                    [1m Learning iteration 518/200000 [0m

                       Computation: 3150 steps/s (collection: 0.566s, learning 2.035s)
               Value function loss: 7385.3038
                    Surrogate loss: 0.0137
             Mean action noise std: 0.96
                       Mean reward: 676.14
               Mean episode length: 207.38
                 Mean success rate: 13.50
                  Mean reward/step: 3.66
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 4251648
                    Iteration time: 2.60s
                        Total time: 1371.54s
                               ETA: 527164.7s

################################################################################
                    [1m Learning iteration 519/200000 [0m

                       Computation: 3260 steps/s (collection: 0.503s, learning 2.009s)
               Value function loss: 5695.9627
                    Surrogate loss: 0.0133
             Mean action noise std: 0.96
                       Mean reward: 685.37
               Mean episode length: 209.70
                 Mean success rate: 14.00
                  Mean reward/step: 3.41
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 4259840
                    Iteration time: 2.51s
                        Total time: 1374.06s
                               ETA: 527112.0s

################################################################################
                    [1m Learning iteration 520/200000 [0m

                       Computation: 3231 steps/s (collection: 0.493s, learning 2.042s)
               Value function loss: 9269.1716
                    Surrogate loss: 0.0139
             Mean action noise std: 0.96
                       Mean reward: 796.44
               Mean episode length: 208.49
                 Mean success rate: 17.50
                  Mean reward/step: 3.55
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 4268032
                    Iteration time: 2.54s
                        Total time: 1376.59s
                               ETA: 527068.2s

################################################################################
                    [1m Learning iteration 521/200000 [0m

                       Computation: 3274 steps/s (collection: 0.472s, learning 2.030s)
               Value function loss: 6228.4650
                    Surrogate loss: 0.0160
             Mean action noise std: 0.96
                       Mean reward: 879.40
               Mean episode length: 212.77
                 Mean success rate: 19.50
                  Mean reward/step: 3.65
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 4276224
                    Iteration time: 2.50s
                        Total time: 1379.09s
                               ETA: 527012.0s

################################################################################
                    [1m Learning iteration 522/200000 [0m

                       Computation: 3236 steps/s (collection: 0.479s, learning 2.052s)
               Value function loss: 5899.2738
                    Surrogate loss: 0.0162
             Mean action noise std: 0.96
                       Mean reward: 868.97
               Mean episode length: 218.34
                 Mean success rate: 20.00
                  Mean reward/step: 3.53
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 4284416
                    Iteration time: 2.53s
                        Total time: 1381.63s
                               ETA: 526967.2s

################################################################################
                    [1m Learning iteration 523/200000 [0m

                       Computation: 3238 steps/s (collection: 0.478s, learning 2.052s)
               Value function loss: 7486.8971
                    Surrogate loss: 0.0151
             Mean action noise std: 0.96
                       Mean reward: 880.98
               Mean episode length: 211.94
                 Mean success rate: 20.50
                  Mean reward/step: 3.55
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 4292608
                    Iteration time: 2.53s
                        Total time: 1384.15s
                               ETA: 526921.8s

################################################################################
                    [1m Learning iteration 524/200000 [0m

                       Computation: 3150 steps/s (collection: 0.555s, learning 2.045s)
               Value function loss: 7775.6505
                    Surrogate loss: 0.0131
             Mean action noise std: 0.96
                       Mean reward: 794.51
               Mean episode length: 207.44
                 Mean success rate: 17.50
                  Mean reward/step: 3.82
       Mean episode length/episode: 26.43
--------------------------------------------------------------------------------
                   Total timesteps: 4300800
                    Iteration time: 2.60s
                        Total time: 1386.75s
                               ETA: 526903.3s

################################################################################
                    [1m Learning iteration 525/200000 [0m

                       Computation: 3193 steps/s (collection: 0.543s, learning 2.022s)
               Value function loss: 8443.4361
                    Surrogate loss: 0.0134
             Mean action noise std: 0.96
                       Mean reward: 752.00
               Mean episode length: 208.69
                 Mean success rate: 17.00
                  Mean reward/step: 4.02
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4308992
                    Iteration time: 2.57s
                        Total time: 1389.32s
                               ETA: 526871.8s

################################################################################
                    [1m Learning iteration 526/200000 [0m

                       Computation: 3220 steps/s (collection: 0.520s, learning 2.024s)
               Value function loss: 13518.6934
                    Surrogate loss: 0.0134
             Mean action noise std: 0.96
                       Mean reward: 761.65
               Mean episode length: 212.69
                 Mean success rate: 16.50
                  Mean reward/step: 4.38
       Mean episode length/episode: 26.51
--------------------------------------------------------------------------------
                   Total timesteps: 4317184
                    Iteration time: 2.54s
                        Total time: 1391.86s
                               ETA: 526832.2s

################################################################################
                    [1m Learning iteration 527/200000 [0m

                       Computation: 3186 steps/s (collection: 0.518s, learning 2.052s)
               Value function loss: 8398.5722
                    Surrogate loss: 0.0155
             Mean action noise std: 0.96
                       Mean reward: 839.22
               Mean episode length: 216.76
                 Mean success rate: 18.50
                  Mean reward/step: 3.91
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 2.57s
                        Total time: 1394.43s
                               ETA: 526803.0s

################################################################################
                    [1m Learning iteration 528/200000 [0m

                       Computation: 3205 steps/s (collection: 0.484s, learning 2.072s)
               Value function loss: 6284.3520
                    Surrogate loss: 0.0154
             Mean action noise std: 0.96
                       Mean reward: 797.49
               Mean episode length: 212.10
                 Mean success rate: 17.50
                  Mean reward/step: 4.25
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 4333568
                    Iteration time: 2.56s
                        Total time: 1396.99s
                               ETA: 526768.2s

################################################################################
                    [1m Learning iteration 529/200000 [0m

                       Computation: 3179 steps/s (collection: 0.514s, learning 2.063s)
               Value function loss: 8840.4698
                    Surrogate loss: 0.0157
             Mean action noise std: 0.96
                       Mean reward: 844.23
               Mean episode length: 209.50
                 Mean success rate: 19.50
                  Mean reward/step: 4.34
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 4341760
                    Iteration time: 2.58s
                        Total time: 1399.57s
                               ETA: 526741.2s

################################################################################
                    [1m Learning iteration 530/200000 [0m

                       Computation: 3230 steps/s (collection: 0.498s, learning 2.037s)
               Value function loss: 9436.6176
                    Surrogate loss: 0.0113
             Mean action noise std: 0.96
                       Mean reward: 771.30
               Mean episode length: 193.46
                 Mean success rate: 17.50
                  Mean reward/step: 4.43
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 4349952
                    Iteration time: 2.54s
                        Total time: 1402.10s
                               ETA: 526699.1s

################################################################################
                    [1m Learning iteration 531/200000 [0m

                       Computation: 3211 steps/s (collection: 0.495s, learning 2.056s)
               Value function loss: 9609.2612
                    Surrogate loss: 0.0135
             Mean action noise std: 0.96
                       Mean reward: 744.48
               Mean episode length: 197.67
                 Mean success rate: 16.00
                  Mean reward/step: 4.46
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 4358144
                    Iteration time: 2.55s
                        Total time: 1404.65s
                               ETA: 526663.0s

################################################################################
                    [1m Learning iteration 532/200000 [0m

                       Computation: 3200 steps/s (collection: 0.527s, learning 2.032s)
               Value function loss: 8029.5680
                    Surrogate loss: 0.0152
             Mean action noise std: 0.96
                       Mean reward: 639.60
               Mean episode length: 186.69
                 Mean success rate: 14.00
                  Mean reward/step: 4.44
       Mean episode length/episode: 26.09
--------------------------------------------------------------------------------
                   Total timesteps: 4366336
                    Iteration time: 2.56s
                        Total time: 1407.21s
                               ETA: 526630.1s

################################################################################
                    [1m Learning iteration 533/200000 [0m

                       Computation: 3178 steps/s (collection: 0.512s, learning 2.066s)
               Value function loss: 10020.4142
                    Surrogate loss: 0.0150
             Mean action noise std: 0.96
                       Mean reward: 676.98
               Mean episode length: 198.56
                 Mean success rate: 14.50
                  Mean reward/step: 4.12
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 4374528
                    Iteration time: 2.58s
                        Total time: 1409.79s
                               ETA: 526604.0s

################################################################################
                    [1m Learning iteration 534/200000 [0m

                       Computation: 3210 steps/s (collection: 0.506s, learning 2.045s)
               Value function loss: 10047.5955
                    Surrogate loss: 0.0139
             Mean action noise std: 0.96
                       Mean reward: 789.21
               Mean episode length: 206.09
                 Mean success rate: 15.00
                  Mean reward/step: 4.01
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 4382720
                    Iteration time: 2.55s
                        Total time: 1412.34s
                               ETA: 526568.3s

################################################################################
                    [1m Learning iteration 535/200000 [0m

                       Computation: 3226 steps/s (collection: 0.513s, learning 2.027s)
               Value function loss: 8648.7514
                    Surrogate loss: 0.0165
             Mean action noise std: 0.96
                       Mean reward: 818.44
               Mean episode length: 193.18
                 Mean success rate: 17.00
                  Mean reward/step: 4.03
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 4390912
                    Iteration time: 2.54s
                        Total time: 1414.88s
                               ETA: 526528.1s

################################################################################
                    [1m Learning iteration 536/200000 [0m

                       Computation: 3216 steps/s (collection: 0.497s, learning 2.050s)
               Value function loss: 8292.3085
                    Surrogate loss: 0.0150
             Mean action noise std: 0.96
                       Mean reward: 879.94
               Mean episode length: 192.49
                 Mean success rate: 18.00
                  Mean reward/step: 4.10
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 4399104
                    Iteration time: 2.55s
                        Total time: 1417.43s
                               ETA: 526491.1s

################################################################################
                    [1m Learning iteration 537/200000 [0m

                       Computation: 3225 steps/s (collection: 0.509s, learning 2.030s)
               Value function loss: 10112.8943
                    Surrogate loss: 0.0176
             Mean action noise std: 0.96
                       Mean reward: 860.32
               Mean episode length: 193.10
                 Mean success rate: 18.00
                  Mean reward/step: 4.84
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4407296
                    Iteration time: 2.54s
                        Total time: 1419.97s
                               ETA: 526451.4s

################################################################################
                    [1m Learning iteration 538/200000 [0m

                       Computation: 3178 steps/s (collection: 0.528s, learning 2.049s)
               Value function loss: 9615.1365
                    Surrogate loss: 0.0147
             Mean action noise std: 0.96
                       Mean reward: 915.69
               Mean episode length: 184.98
                 Mean success rate: 20.00
                  Mean reward/step: 4.93
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 4415488
                    Iteration time: 2.58s
                        Total time: 1422.54s
                               ETA: 526425.7s

################################################################################
                    [1m Learning iteration 539/200000 [0m

                       Computation: 3255 steps/s (collection: 0.494s, learning 2.023s)
               Value function loss: 11961.3298
                    Surrogate loss: 0.0113
             Mean action noise std: 0.96
                       Mean reward: 828.06
               Mean episode length: 182.93
                 Mean success rate: 19.00
                  Mean reward/step: 4.69
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 2.52s
                        Total time: 1425.06s
                               ETA: 526377.8s

################################################################################
                    [1m Learning iteration 540/200000 [0m

                       Computation: 3187 steps/s (collection: 0.544s, learning 2.026s)
               Value function loss: 10973.2733
                    Surrogate loss: 0.0131
             Mean action noise std: 0.96
                       Mean reward: 867.15
               Mean episode length: 179.71
                 Mean success rate: 20.00
                  Mean reward/step: 4.54
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 4431872
                    Iteration time: 2.57s
                        Total time: 1427.63s
                               ETA: 526349.6s

################################################################################
                    [1m Learning iteration 541/200000 [0m

                       Computation: 3207 steps/s (collection: 0.516s, learning 2.038s)
               Value function loss: 14596.4654
                    Surrogate loss: 0.0118
             Mean action noise std: 0.96
                       Mean reward: 924.25
               Mean episode length: 191.09
                 Mean success rate: 22.50
                  Mean reward/step: 4.33
       Mean episode length/episode: 25.92
--------------------------------------------------------------------------------
                   Total timesteps: 4440064
                    Iteration time: 2.55s
                        Total time: 1430.18s
                               ETA: 526315.6s

################################################################################
                    [1m Learning iteration 542/200000 [0m

                       Computation: 3182 steps/s (collection: 0.540s, learning 2.034s)
               Value function loss: 10051.9840
                    Surrogate loss: 0.0118
             Mean action noise std: 0.96
                       Mean reward: 871.08
               Mean episode length: 188.42
                 Mean success rate: 21.50
                  Mean reward/step: 3.81
       Mean episode length/episode: 25.92
--------------------------------------------------------------------------------
                   Total timesteps: 4448256
                    Iteration time: 2.57s
                        Total time: 1432.76s
                               ETA: 526289.2s

################################################################################
                    [1m Learning iteration 543/200000 [0m

                       Computation: 3201 steps/s (collection: 0.509s, learning 2.050s)
               Value function loss: 7897.2153
                    Surrogate loss: 0.0120
             Mean action noise std: 0.96
                       Mean reward: 870.42
               Mean episode length: 193.11
                 Mean success rate: 21.00
                  Mean reward/step: 3.77
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 4456448
                    Iteration time: 2.56s
                        Total time: 1435.32s
                               ETA: 526257.4s

################################################################################
                    [1m Learning iteration 544/200000 [0m

                       Computation: 3209 steps/s (collection: 0.535s, learning 2.018s)
               Value function loss: 8849.3482
                    Surrogate loss: 0.0135
             Mean action noise std: 0.96
                       Mean reward: 752.46
               Mean episode length: 196.79
                 Mean success rate: 17.50
                  Mean reward/step: 3.58
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 4464640
                    Iteration time: 2.55s
                        Total time: 1437.87s
                               ETA: 526223.2s

################################################################################
                    [1m Learning iteration 545/200000 [0m

                       Computation: 3186 steps/s (collection: 0.530s, learning 2.040s)
               Value function loss: 8022.9993
                    Surrogate loss: 0.0160
             Mean action noise std: 0.96
                       Mean reward: 715.44
               Mean episode length: 194.09
                 Mean success rate: 15.00
                  Mean reward/step: 3.73
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 4472832
                    Iteration time: 2.57s
                        Total time: 1440.44s
                               ETA: 526195.9s

################################################################################
                    [1m Learning iteration 546/200000 [0m

                       Computation: 3226 steps/s (collection: 0.512s, learning 2.027s)
               Value function loss: 8340.5432
                    Surrogate loss: 0.0150
             Mean action noise std: 0.96
                       Mean reward: 706.11
               Mean episode length: 188.96
                 Mean success rate: 15.00
                  Mean reward/step: 4.01
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 4481024
                    Iteration time: 2.54s
                        Total time: 1442.98s
                               ETA: 526157.0s

################################################################################
                    [1m Learning iteration 547/200000 [0m

                       Computation: 3235 steps/s (collection: 0.495s, learning 2.038s)
               Value function loss: 9621.0935
                    Surrogate loss: 0.0149
             Mean action noise std: 0.96
                       Mean reward: 733.07
               Mean episode length: 195.54
                 Mean success rate: 15.00
                  Mean reward/step: 4.38
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 4489216
                    Iteration time: 2.53s
                        Total time: 1445.51s
                               ETA: 526115.9s

################################################################################
                    [1m Learning iteration 548/200000 [0m

                       Computation: 3221 steps/s (collection: 0.485s, learning 2.058s)
               Value function loss: 11144.9302
                    Surrogate loss: 0.0148
             Mean action noise std: 0.95
                       Mean reward: 769.26
               Mean episode length: 198.00
                 Mean success rate: 15.00
                  Mean reward/step: 4.49
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 4497408
                    Iteration time: 2.54s
                        Total time: 1448.05s
                               ETA: 526078.9s

################################################################################
                    [1m Learning iteration 549/200000 [0m

                       Computation: 3236 steps/s (collection: 0.503s, learning 2.028s)
               Value function loss: 11748.8118
                    Surrogate loss: 0.0143
             Mean action noise std: 0.95
                       Mean reward: 783.68
               Mean episode length: 185.38
                 Mean success rate: 16.50
                  Mean reward/step: 4.73
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 4505600
                    Iteration time: 2.53s
                        Total time: 1450.59s
                               ETA: 526037.6s

################################################################################
                    [1m Learning iteration 550/200000 [0m

                       Computation: 3207 steps/s (collection: 0.531s, learning 2.023s)
               Value function loss: 9347.5160
                    Surrogate loss: 0.0143
             Mean action noise std: 0.95
                       Mean reward: 750.15
               Mean episode length: 182.72
                 Mean success rate: 16.50
                  Mean reward/step: 4.89
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 4513792
                    Iteration time: 2.55s
                        Total time: 1453.14s
                               ETA: 526004.7s

################################################################################
                    [1m Learning iteration 551/200000 [0m

                       Computation: 3199 steps/s (collection: 0.537s, learning 2.023s)
               Value function loss: 11194.3627
                    Surrogate loss: 0.0138
             Mean action noise std: 0.95
                       Mean reward: 753.21
               Mean episode length: 183.31
                 Mean success rate: 15.50
                  Mean reward/step: 5.05
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 2.56s
                        Total time: 1455.70s
                               ETA: 525974.4s

################################################################################
                    [1m Learning iteration 552/200000 [0m

                       Computation: 3188 steps/s (collection: 0.509s, learning 2.060s)
               Value function loss: 8761.3300
                    Surrogate loss: 0.0143
             Mean action noise std: 0.95
                       Mean reward: 860.94
               Mean episode length: 195.49
                 Mean success rate: 18.00
                  Mean reward/step: 5.05
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 4530176
                    Iteration time: 2.57s
                        Total time: 1458.27s
                               ETA: 525947.2s

################################################################################
                    [1m Learning iteration 553/200000 [0m

                       Computation: 3165 steps/s (collection: 0.537s, learning 2.050s)
               Value function loss: 12458.0103
                    Surrogate loss: 0.0138
             Mean action noise std: 0.95
                       Mean reward: 815.61
               Mean episode length: 185.47
                 Mean success rate: 16.00
                  Mean reward/step: 5.10
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 4538368
                    Iteration time: 2.59s
                        Total time: 1460.86s
                               ETA: 525926.7s

################################################################################
                    [1m Learning iteration 554/200000 [0m

                       Computation: 3210 steps/s (collection: 0.532s, learning 2.019s)
               Value function loss: 12243.2834
                    Surrogate loss: 0.0153
             Mean action noise std: 0.95
                       Mean reward: 870.11
               Mean episode length: 199.38
                 Mean success rate: 16.50
                  Mean reward/step: 5.24
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 4546560
                    Iteration time: 2.55s
                        Total time: 1463.41s
                               ETA: 525893.4s

################################################################################
                    [1m Learning iteration 555/200000 [0m

                       Computation: 3195 steps/s (collection: 0.514s, learning 2.049s)
               Value function loss: 10893.8411
                    Surrogate loss: 0.0182
             Mean action noise std: 0.95
                       Mean reward: 959.68
               Mean episode length: 201.42
                 Mean success rate: 19.00
                  Mean reward/step: 5.38
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 4554752
                    Iteration time: 2.56s
                        Total time: 1465.97s
                               ETA: 525864.5s

################################################################################
                    [1m Learning iteration 556/200000 [0m

                       Computation: 3228 steps/s (collection: 0.496s, learning 2.042s)
               Value function loss: 18291.5470
                    Surrogate loss: 0.0145
             Mean action noise std: 0.95
                       Mean reward: 952.00
               Mean episode length: 199.22
                 Mean success rate: 20.00
                  Mean reward/step: 5.41
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 4562944
                    Iteration time: 2.54s
                        Total time: 1468.51s
                               ETA: 525826.2s

################################################################################
                    [1m Learning iteration 557/200000 [0m

                       Computation: 3179 steps/s (collection: 0.562s, learning 2.015s)
               Value function loss: 11460.1933
                    Surrogate loss: 0.0153
             Mean action noise std: 0.95
                       Mean reward: 983.74
               Mean episode length: 194.59
                 Mean success rate: 22.00
                  Mean reward/step: 4.80
       Mean episode length/episode: 26.60
--------------------------------------------------------------------------------
                   Total timesteps: 4571136
                    Iteration time: 2.58s
                        Total time: 1471.09s
                               ETA: 525802.2s

################################################################################
                    [1m Learning iteration 558/200000 [0m

                       Computation: 3176 steps/s (collection: 0.560s, learning 2.019s)
               Value function loss: 14383.7521
                    Surrogate loss: 0.0168
             Mean action noise std: 0.95
                       Mean reward: 1008.02
               Mean episode length: 204.99
                 Mean success rate: 21.50
                  Mean reward/step: 5.34
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 4579328
                    Iteration time: 2.58s
                        Total time: 1473.66s
                               ETA: 525779.2s

################################################################################
                    [1m Learning iteration 559/200000 [0m

                       Computation: 3067 steps/s (collection: 0.566s, learning 2.105s)
               Value function loss: 12018.8381
                    Surrogate loss: 0.0167
             Mean action noise std: 0.95
                       Mean reward: 939.68
               Mean episode length: 196.64
                 Mean success rate: 20.50
                  Mean reward/step: 5.00
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 4587520
                    Iteration time: 2.67s
                        Total time: 1476.34s
                               ETA: 525788.9s

################################################################################
                    [1m Learning iteration 560/200000 [0m

                       Computation: 2903 steps/s (collection: 0.631s, learning 2.190s)
               Value function loss: 10650.4967
                    Surrogate loss: 0.0157
             Mean action noise std: 0.95
                       Mean reward: 840.35
               Mean episode length: 188.50
                 Mean success rate: 18.00
                  Mean reward/step: 5.33
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 4595712
                    Iteration time: 2.82s
                        Total time: 1479.16s
                               ETA: 525852.0s

################################################################################
                    [1m Learning iteration 561/200000 [0m

                       Computation: 2891 steps/s (collection: 0.689s, learning 2.144s)
               Value function loss: 18559.6605
                    Surrogate loss: 0.0132
             Mean action noise std: 0.95
                       Mean reward: 944.26
               Mean episode length: 186.76
                 Mean success rate: 19.00
                  Mean reward/step: 5.30
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 4603904
                    Iteration time: 2.83s
                        Total time: 1481.99s
                               ETA: 525919.1s

################################################################################
                    [1m Learning iteration 562/200000 [0m

                       Computation: 2967 steps/s (collection: 0.594s, learning 2.166s)
               Value function loss: 16611.0717
                    Surrogate loss: 0.0152
             Mean action noise std: 0.95
                       Mean reward: 1058.75
               Mean episode length: 197.28
                 Mean success rate: 21.00
                  Mean reward/step: 4.94
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 4612096
                    Iteration time: 2.76s
                        Total time: 1484.75s
                               ETA: 525960.0s

################################################################################
                    [1m Learning iteration 563/200000 [0m

                       Computation: 2945 steps/s (collection: 0.632s, learning 2.150s)
               Value function loss: 13806.4716
                    Surrogate loss: 0.0156
             Mean action noise std: 0.95
                       Mean reward: 1093.34
               Mean episode length: 197.04
                 Mean success rate: 23.00
                  Mean reward/step: 5.04
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 2.78s
                        Total time: 1487.53s
                               ETA: 526008.4s

################################################################################
                    [1m Learning iteration 564/200000 [0m

                       Computation: 2936 steps/s (collection: 0.637s, learning 2.153s)
               Value function loss: 11298.0517
                    Surrogate loss: 0.0168
             Mean action noise std: 0.95
                       Mean reward: 1094.50
               Mean episode length: 193.97
                 Mean success rate: 21.00
                  Mean reward/step: 5.27
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 4628480
                    Iteration time: 2.79s
                        Total time: 1490.32s
                               ETA: 526059.4s

################################################################################
                    [1m Learning iteration 565/200000 [0m

                       Computation: 3102 steps/s (collection: 0.560s, learning 2.081s)
               Value function loss: 14361.7422
                    Surrogate loss: 0.0142
             Mean action noise std: 0.95
                       Mean reward: 1213.79
               Mean episode length: 193.97
                 Mean success rate: 23.50
                  Mean reward/step: 4.94
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 4636672
                    Iteration time: 2.64s
                        Total time: 1492.96s
                               ETA: 526057.7s

################################################################################
                    [1m Learning iteration 566/200000 [0m

                       Computation: 3130 steps/s (collection: 0.541s, learning 2.076s)
               Value function loss: 12445.6325
                    Surrogate loss: 0.0183
             Mean action noise std: 0.95
                       Mean reward: 1277.59
               Mean episode length: 212.07
                 Mean success rate: 25.50
                  Mean reward/step: 4.66
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 4644864
                    Iteration time: 2.62s
                        Total time: 1495.58s
                               ETA: 526047.8s

################################################################################
                    [1m Learning iteration 567/200000 [0m

                       Computation: 2985 steps/s (collection: 0.621s, learning 2.123s)
               Value function loss: 12288.0673
                    Surrogate loss: 0.0133
             Mean action noise std: 0.95
                       Mean reward: 1091.40
               Mean episode length: 211.14
                 Mean success rate: 22.00
                  Mean reward/step: 4.71
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 4653056
                    Iteration time: 2.74s
                        Total time: 1498.32s
                               ETA: 526082.4s

################################################################################
                    [1m Learning iteration 568/200000 [0m

                       Computation: 3012 steps/s (collection: 0.572s, learning 2.147s)
               Value function loss: 14002.3120
                    Surrogate loss: 0.0161
             Mean action noise std: 0.95
                       Mean reward: 1108.41
               Mean episode length: 216.87
                 Mean success rate: 22.50
                  Mean reward/step: 4.96
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 4661248
                    Iteration time: 2.72s
                        Total time: 1501.04s
                               ETA: 526108.4s

################################################################################
                    [1m Learning iteration 569/200000 [0m

                       Computation: 2937 steps/s (collection: 0.609s, learning 2.179s)
               Value function loss: 15928.2713
                    Surrogate loss: 0.0139
             Mean action noise std: 0.95
                       Mean reward: 1067.87
               Mean episode length: 214.50
                 Mean success rate: 21.00
                  Mean reward/step: 5.59
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 4669440
                    Iteration time: 2.79s
                        Total time: 1503.83s
                               ETA: 526158.4s

################################################################################
                    [1m Learning iteration 570/200000 [0m

                       Computation: 2982 steps/s (collection: 0.606s, learning 2.141s)
               Value function loss: 10528.3620
                    Surrogate loss: 0.0172
             Mean action noise std: 0.95
                       Mean reward: 1058.35
               Mean episode length: 217.88
                 Mean success rate: 21.50
                  Mean reward/step: 6.08
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 4677632
                    Iteration time: 2.75s
                        Total time: 1506.58s
                               ETA: 526193.7s

################################################################################
                    [1m Learning iteration 571/200000 [0m

                       Computation: 3055 steps/s (collection: 0.567s, learning 2.115s)
               Value function loss: 16809.2280
                    Surrogate loss: 0.0214
             Mean action noise std: 0.95
                       Mean reward: 1032.31
               Mean episode length: 228.78
                 Mean success rate: 22.00
                  Mean reward/step: 6.08
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 4685824
                    Iteration time: 2.68s
                        Total time: 1509.26s
                               ETA: 526206.1s

################################################################################
                    [1m Learning iteration 572/200000 [0m

                       Computation: 2993 steps/s (collection: 0.593s, learning 2.144s)
               Value function loss: 16691.9173
                    Surrogate loss: 0.0152
             Mean action noise std: 0.95
                       Mean reward: 1039.78
               Mean episode length: 236.64
                 Mean success rate: 22.50
                  Mean reward/step: 5.88
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 4694016
                    Iteration time: 2.74s
                        Total time: 1512.00s
                               ETA: 526237.7s

################################################################################
                    [1m Learning iteration 573/200000 [0m

                       Computation: 2990 steps/s (collection: 0.605s, learning 2.134s)
               Value function loss: 14437.7262
                    Surrogate loss: 0.0159
             Mean action noise std: 0.95
                       Mean reward: 1110.86
               Mean episode length: 237.67
                 Mean success rate: 23.50
                  Mean reward/step: 5.78
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 4702208
                    Iteration time: 2.74s
                        Total time: 1514.73s
                               ETA: 526270.1s

################################################################################
                    [1m Learning iteration 574/200000 [0m

                       Computation: 2918 steps/s (collection: 0.593s, learning 2.214s)
               Value function loss: 14038.2721
                    Surrogate loss: 0.0134
             Mean action noise std: 0.95
                       Mean reward: 1137.63
               Mean episode length: 243.86
                 Mean success rate: 25.00
                  Mean reward/step: 5.53
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 4710400
                    Iteration time: 2.81s
                        Total time: 1517.54s
                               ETA: 526325.8s

################################################################################
                    [1m Learning iteration 575/200000 [0m

                       Computation: 2855 steps/s (collection: 0.696s, learning 2.172s)
               Value function loss: 20065.0019
                    Surrogate loss: 0.0122
             Mean action noise std: 0.95
                       Mean reward: 1313.13
               Mean episode length: 260.36
                 Mean success rate: 28.50
                  Mean reward/step: 5.67
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 2.87s
                        Total time: 1520.41s
                               ETA: 526402.5s

################################################################################
                    [1m Learning iteration 576/200000 [0m

                       Computation: 2947 steps/s (collection: 0.622s, learning 2.158s)
               Value function loss: 12294.4997
                    Surrogate loss: 0.0156
             Mean action noise std: 0.95
                       Mean reward: 1350.06
               Mean episode length: 260.62
                 Mean success rate: 29.50
                  Mean reward/step: 5.19
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 4726784
                    Iteration time: 2.78s
                        Total time: 1523.19s
                               ETA: 526448.2s

################################################################################
                    [1m Learning iteration 577/200000 [0m

                       Computation: 2992 steps/s (collection: 0.612s, learning 2.126s)
               Value function loss: 17696.2126
                    Surrogate loss: 0.0146
             Mean action noise std: 0.95
                       Mean reward: 1450.36
               Mean episode length: 257.24
                 Mean success rate: 29.50
                  Mean reward/step: 5.68
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 4734976
                    Iteration time: 2.74s
                        Total time: 1525.93s
                               ETA: 526479.2s

################################################################################
                    [1m Learning iteration 578/200000 [0m

                       Computation: 3001 steps/s (collection: 0.604s, learning 2.126s)
               Value function loss: 17077.2417
                    Surrogate loss: 0.0141
             Mean action noise std: 0.95
                       Mean reward: 1460.45
               Mean episode length: 261.13
                 Mean success rate: 30.00
                  Mean reward/step: 5.31
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 4743168
                    Iteration time: 2.73s
                        Total time: 1528.66s
                               ETA: 526507.3s

################################################################################
                    [1m Learning iteration 579/200000 [0m

                       Computation: 2966 steps/s (collection: 0.607s, learning 2.155s)
               Value function loss: 13403.0682
                    Surrogate loss: 0.0134
             Mean action noise std: 0.95
                       Mean reward: 1493.27
               Mean episode length: 263.96
                 Mean success rate: 31.50
                  Mean reward/step: 5.50
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4751360
                    Iteration time: 2.76s
                        Total time: 1531.42s
                               ETA: 526546.5s

################################################################################
                    [1m Learning iteration 580/200000 [0m

                       Computation: 3000 steps/s (collection: 0.597s, learning 2.133s)
               Value function loss: 11619.9143
                    Surrogate loss: 0.0142
             Mean action noise std: 0.95
                       Mean reward: 1438.69
               Mean episode length: 262.25
                 Mean success rate: 30.00
                  Mean reward/step: 5.42
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 4759552
                    Iteration time: 2.73s
                        Total time: 1534.15s
                               ETA: 526574.7s

################################################################################
                    [1m Learning iteration 581/200000 [0m

                       Computation: 3022 steps/s (collection: 0.585s, learning 2.125s)
               Value function loss: 11704.0411
                    Surrogate loss: 0.0228
             Mean action noise std: 0.95
                       Mean reward: 1356.43
               Mean episode length: 256.11
                 Mean success rate: 28.00
                  Mean reward/step: 5.38
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4767744
                    Iteration time: 2.71s
                        Total time: 1536.86s
                               ETA: 526595.9s

################################################################################
                    [1m Learning iteration 582/200000 [0m

                       Computation: 2919 steps/s (collection: 0.659s, learning 2.147s)
               Value function loss: 16103.0849
                    Surrogate loss: 0.0202
             Mean action noise std: 0.95
                       Mean reward: 1364.24
               Mean episode length: 253.62
                 Mean success rate: 27.50
                  Mean reward/step: 5.60
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 4775936
                    Iteration time: 2.81s
                        Total time: 1539.66s
                               ETA: 526649.7s

################################################################################
                    [1m Learning iteration 583/200000 [0m

                       Computation: 3008 steps/s (collection: 0.603s, learning 2.120s)
               Value function loss: 20969.2187
                    Surrogate loss: 0.0132
             Mean action noise std: 0.95
                       Mean reward: 1413.07
               Mean episode length: 263.59
                 Mean success rate: 27.50
                  Mean reward/step: 5.23
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4784128
                    Iteration time: 2.72s
                        Total time: 1542.39s
                               ETA: 526675.2s

################################################################################
                    [1m Learning iteration 584/200000 [0m

                       Computation: 2979 steps/s (collection: 0.614s, learning 2.135s)
               Value function loss: 15401.3172
                    Surrogate loss: 0.0140
             Mean action noise std: 0.95
                       Mean reward: 1346.77
               Mean episode length: 256.44
                 Mean success rate: 25.00
                  Mean reward/step: 5.48
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 4792320
                    Iteration time: 2.75s
                        Total time: 1545.14s
                               ETA: 526709.6s

################################################################################
                    [1m Learning iteration 585/200000 [0m

                       Computation: 2981 steps/s (collection: 0.610s, learning 2.138s)
               Value function loss: 23018.4834
                    Surrogate loss: 0.0102
             Mean action noise std: 0.95
                       Mean reward: 1352.75
               Mean episode length: 248.95
                 Mean success rate: 23.50
                  Mean reward/step: 5.63
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 4800512
                    Iteration time: 2.75s
                        Total time: 1547.89s
                               ETA: 526743.3s

################################################################################
                    [1m Learning iteration 586/200000 [0m

                       Computation: 2904 steps/s (collection: 0.653s, learning 2.168s)
               Value function loss: 16240.3002
                    Surrogate loss: 0.0126
             Mean action noise std: 0.95
                       Mean reward: 1394.74
               Mean episode length: 247.66
                 Mean success rate: 22.50
                  Mean reward/step: 5.92
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 4808704
                    Iteration time: 2.82s
                        Total time: 1550.71s
                               ETA: 526801.5s

################################################################################
                    [1m Learning iteration 587/200000 [0m

                       Computation: 2985 steps/s (collection: 0.612s, learning 2.131s)
               Value function loss: 17327.4662
                    Surrogate loss: 0.0132
             Mean action noise std: 0.95
                       Mean reward: 1516.68
               Mean episode length: 244.85
                 Mean success rate: 25.00
                  Mean reward/step: 5.80
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 2.74s
                        Total time: 1553.45s
                               ETA: 526833.3s

################################################################################
                    [1m Learning iteration 588/200000 [0m

                       Computation: 2959 steps/s (collection: 0.635s, learning 2.133s)
               Value function loss: 17776.5931
                    Surrogate loss: 0.0110
             Mean action noise std: 0.95
                       Mean reward: 1560.84
               Mean episode length: 251.32
                 Mean success rate: 25.00
                  Mean reward/step: 5.24
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 4825088
                    Iteration time: 2.77s
                        Total time: 1556.22s
                               ETA: 526873.2s

################################################################################
                    [1m Learning iteration 589/200000 [0m

                       Computation: 2989 steps/s (collection: 0.599s, learning 2.141s)
               Value function loss: 12503.9935
                    Surrogate loss: 0.0142
             Mean action noise std: 0.95
                       Mean reward: 1536.73
               Mean episode length: 254.94
                 Mean success rate: 24.00
                  Mean reward/step: 5.26
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 4833280
                    Iteration time: 2.74s
                        Total time: 1558.96s
                               ETA: 526903.7s

################################################################################
                    [1m Learning iteration 590/200000 [0m

                       Computation: 2964 steps/s (collection: 0.624s, learning 2.139s)
               Value function loss: 14585.8157
                    Surrogate loss: 0.0216
             Mean action noise std: 0.95
                       Mean reward: 1567.96
               Mean episode length: 262.65
                 Mean success rate: 24.50
                  Mean reward/step: 5.45
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 4841472
                    Iteration time: 2.76s
                        Total time: 1561.72s
                               ETA: 526941.9s

################################################################################
                    [1m Learning iteration 591/200000 [0m

                       Computation: 3001 steps/s (collection: 0.603s, learning 2.126s)
               Value function loss: 18125.6767
                    Surrogate loss: 0.0137
             Mean action noise std: 0.95
                       Mean reward: 1564.33
               Mean episode length: 275.62
                 Mean success rate: 26.00
                  Mean reward/step: 5.41
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 4849664
                    Iteration time: 2.73s
                        Total time: 1564.45s
                               ETA: 526968.5s

################################################################################
                    [1m Learning iteration 592/200000 [0m

                       Computation: 2941 steps/s (collection: 0.618s, learning 2.167s)
               Value function loss: 26279.1457
                    Surrogate loss: 0.0109
             Mean action noise std: 0.95
                       Mean reward: 1600.41
               Mean episode length: 287.14
                 Mean success rate: 27.50
                  Mean reward/step: 5.73
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 4857856
                    Iteration time: 2.79s
                        Total time: 1567.23s
                               ETA: 527013.8s

################################################################################
                    [1m Learning iteration 593/200000 [0m

                       Computation: 2922 steps/s (collection: 0.633s, learning 2.170s)
               Value function loss: 15860.7104
                    Surrogate loss: 0.0125
             Mean action noise std: 0.95
                       Mean reward: 1700.28
               Mean episode length: 296.83
                 Mean success rate: 29.00
                  Mean reward/step: 5.55
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 4866048
                    Iteration time: 2.80s
                        Total time: 1570.04s
                               ETA: 527064.9s

################################################################################
                    [1m Learning iteration 594/200000 [0m

                       Computation: 3120 steps/s (collection: 0.571s, learning 2.054s)
               Value function loss: 18639.6058
                    Surrogate loss: 0.0149
             Mean action noise std: 0.95
                       Mean reward: 1649.38
               Mean episode length: 294.94
                 Mean success rate: 29.50
                  Mean reward/step: 5.10
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 4874240
                    Iteration time: 2.63s
                        Total time: 1572.66s
                               ETA: 527056.2s

################################################################################
                    [1m Learning iteration 595/200000 [0m

                       Computation: 3195 steps/s (collection: 0.512s, learning 2.051s)
               Value function loss: 13070.1632
                    Surrogate loss: 0.0127
             Mean action noise std: 0.95
                       Mean reward: 1582.82
               Mean episode length: 290.21
                 Mean success rate: 29.00
                  Mean reward/step: 4.47
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 4882432
                    Iteration time: 2.56s
                        Total time: 1575.23s
                               ETA: 527026.9s

################################################################################
                    [1m Learning iteration 596/200000 [0m

                       Computation: 3088 steps/s (collection: 0.539s, learning 2.114s)
               Value function loss: 17691.7312
                    Surrogate loss: 0.0103
             Mean action noise std: 0.95
                       Mean reward: 1612.97
               Mean episode length: 294.86
                 Mean success rate: 30.00
                  Mean reward/step: 5.27
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 4890624
                    Iteration time: 2.65s
                        Total time: 1577.88s
                               ETA: 527027.4s

################################################################################
                    [1m Learning iteration 597/200000 [0m

                       Computation: 3074 steps/s (collection: 0.579s, learning 2.086s)
               Value function loss: 19043.4820
                    Surrogate loss: 0.0134
             Mean action noise std: 0.95
                       Mean reward: 1572.88
               Mean episode length: 297.81
                 Mean success rate: 30.00
                  Mean reward/step: 5.78
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 4898816
                    Iteration time: 2.66s
                        Total time: 1580.54s
                               ETA: 527032.0s

################################################################################
                    [1m Learning iteration 598/200000 [0m

                       Computation: 3076 steps/s (collection: 0.594s, learning 2.068s)
               Value function loss: 18212.3121
                    Surrogate loss: 0.0177
             Mean action noise std: 0.95
                       Mean reward: 1495.03
               Mean episode length: 281.24
                 Mean success rate: 27.00
                  Mean reward/step: 5.81
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 4907008
                    Iteration time: 2.66s
                        Total time: 1583.21s
                               ETA: 527035.9s

################################################################################
                    [1m Learning iteration 599/200000 [0m

                       Computation: 3071 steps/s (collection: 0.522s, learning 2.145s)
               Value function loss: 20617.1855
                    Surrogate loss: 0.0126
             Mean action noise std: 0.95
                       Mean reward: 1531.63
               Mean episode length: 283.43
                 Mean success rate: 28.00
                  Mean reward/step: 6.01
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 2.67s
                        Total time: 1585.87s
                               ETA: 527041.3s

################################################################################
                    [1m Learning iteration 600/200000 [0m

                       Computation: 2996 steps/s (collection: 0.596s, learning 2.137s)
               Value function loss: 15012.1612
                    Surrogate loss: 0.0148
             Mean action noise std: 0.95
                       Mean reward: 1315.01
               Mean episode length: 263.88
                 Mean success rate: 22.50
                  Mean reward/step: 6.59
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 4923392
                    Iteration time: 2.73s
                        Total time: 1588.61s
                               ETA: 527068.7s

################################################################################
                    [1m Learning iteration 601/200000 [0m

                       Computation: 2985 steps/s (collection: 0.618s, learning 2.126s)
               Value function loss: 18528.3517
                    Surrogate loss: 0.0160
             Mean action noise std: 0.95
                       Mean reward: 1221.30
               Mean episode length: 269.66
                 Mean success rate: 21.00
                  Mean reward/step: 6.19
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4931584
                    Iteration time: 2.74s
                        Total time: 1591.35s
                               ETA: 527099.3s

################################################################################
                    [1m Learning iteration 602/200000 [0m

                       Computation: 2988 steps/s (collection: 0.617s, learning 2.125s)
               Value function loss: 23004.0687
                    Surrogate loss: 0.0135
             Mean action noise std: 0.95
                       Mean reward: 1283.56
               Mean episode length: 271.41
                 Mean success rate: 21.00
                  Mean reward/step: 6.06
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 4939776
                    Iteration time: 2.74s
                        Total time: 1594.09s
                               ETA: 527129.1s

################################################################################
                    [1m Learning iteration 603/200000 [0m

                       Computation: 3023 steps/s (collection: 0.576s, learning 2.133s)
               Value function loss: 19504.7111
                    Surrogate loss: 0.0154
             Mean action noise std: 0.95
                       Mean reward: 1352.30
               Mean episode length: 266.35
                 Mean success rate: 22.50
                  Mean reward/step: 6.31
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 4947968
                    Iteration time: 2.71s
                        Total time: 1596.80s
                               ETA: 527148.1s

################################################################################
                    [1m Learning iteration 604/200000 [0m

                       Computation: 2841 steps/s (collection: 0.659s, learning 2.224s)
               Value function loss: 23853.9389
                    Surrogate loss: 0.0135
             Mean action noise std: 0.95
                       Mean reward: 1646.72
               Mean episode length: 281.08
                 Mean success rate: 28.00
                  Mean reward/step: 6.50
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 4956160
                    Iteration time: 2.88s
                        Total time: 1599.68s
                               ETA: 527224.4s

################################################################################
                    [1m Learning iteration 605/200000 [0m

                       Computation: 2994 steps/s (collection: 0.614s, learning 2.122s)
               Value function loss: 16094.2416
                    Surrogate loss: 0.0185
             Mean action noise std: 0.95
                       Mean reward: 1450.37
               Mean episode length: 268.26
                 Mean success rate: 27.00
                  Mean reward/step: 6.66
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 4964352
                    Iteration time: 2.74s
                        Total time: 1602.42s
                               ETA: 527252.0s

################################################################################
                    [1m Learning iteration 606/200000 [0m

                       Computation: 2973 steps/s (collection: 0.615s, learning 2.140s)
               Value function loss: 23706.9469
                    Surrogate loss: 0.0136
             Mean action noise std: 0.95
                       Mean reward: 1659.08
               Mean episode length: 276.94
                 Mean success rate: 32.50
                  Mean reward/step: 6.94
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 4972544
                    Iteration time: 2.76s
                        Total time: 1605.18s
                               ETA: 527285.8s

################################################################################
                    [1m Learning iteration 607/200000 [0m

                       Computation: 2992 steps/s (collection: 0.612s, learning 2.125s)
               Value function loss: 14500.5370
                    Surrogate loss: 0.0175
             Mean action noise std: 0.95
                       Mean reward: 1583.47
               Mean episode length: 261.78
                 Mean success rate: 30.50
                  Mean reward/step: 6.79
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 4980736
                    Iteration time: 2.74s
                        Total time: 1607.91s
                               ETA: 527313.6s

################################################################################
                    [1m Learning iteration 608/200000 [0m

                       Computation: 2883 steps/s (collection: 0.659s, learning 2.182s)
               Value function loss: 24476.2735
                    Surrogate loss: 0.0157
             Mean action noise std: 0.95
                       Mean reward: 1614.92
               Mean episode length: 260.02
                 Mean success rate: 32.50
                  Mean reward/step: 6.93
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 4988928
                    Iteration time: 2.84s
                        Total time: 1610.75s
                               ETA: 527375.2s

################################################################################
                    [1m Learning iteration 609/200000 [0m

                       Computation: 2942 steps/s (collection: 0.665s, learning 2.119s)
               Value function loss: 23909.4357
                    Surrogate loss: 0.0130
             Mean action noise std: 0.95
                       Mean reward: 1599.12
               Mean episode length: 250.65
                 Mean success rate: 33.00
                  Mean reward/step: 6.61
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 4997120
                    Iteration time: 2.78s
                        Total time: 1613.54s
                               ETA: 527418.2s

################################################################################
                    [1m Learning iteration 610/200000 [0m

                       Computation: 2982 steps/s (collection: 0.598s, learning 2.149s)
               Value function loss: 19777.3543
                    Surrogate loss: 0.0133
             Mean action noise std: 0.95
                       Mean reward: 1552.67
               Mean episode length: 245.93
                 Mean success rate: 31.00
                  Mean reward/step: 6.71
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5005312
                    Iteration time: 2.75s
                        Total time: 1616.29s
                               ETA: 527448.6s

################################################################################
                    [1m Learning iteration 611/200000 [0m

                       Computation: 2949 steps/s (collection: 0.605s, learning 2.172s)
               Value function loss: 22318.4808
                    Surrogate loss: 0.0122
             Mean action noise std: 0.95
                       Mean reward: 1615.03
               Mean episode length: 252.91
                 Mean success rate: 30.50
                  Mean reward/step: 7.03
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 2.78s
                        Total time: 1619.06s
                               ETA: 527488.9s

################################################################################
                    [1m Learning iteration 612/200000 [0m

                       Computation: 2999 steps/s (collection: 0.595s, learning 2.136s)
               Value function loss: 22567.9633
                    Surrogate loss: 0.0134
             Mean action noise std: 0.95
                       Mean reward: 1663.32
               Mean episode length: 258.07
                 Mean success rate: 30.00
                  Mean reward/step: 6.80
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 5021696
                    Iteration time: 2.73s
                        Total time: 1621.79s
                               ETA: 527514.2s

################################################################################
                    [1m Learning iteration 613/200000 [0m

                       Computation: 2918 steps/s (collection: 0.633s, learning 2.173s)
               Value function loss: 29138.9264
                    Surrogate loss: 0.0122
             Mean action noise std: 0.95
                       Mean reward: 1734.98
               Mean episode length: 263.55
                 Mean success rate: 30.00
                  Mean reward/step: 7.85
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5029888
                    Iteration time: 2.81s
                        Total time: 1624.60s
                               ETA: 527563.8s

################################################################################
                    [1m Learning iteration 614/200000 [0m

                       Computation: 2950 steps/s (collection: 0.642s, learning 2.135s)
               Value function loss: 20085.0217
                    Surrogate loss: 0.0140
             Mean action noise std: 0.95
                       Mean reward: 1808.75
               Mean episode length: 268.28
                 Mean success rate: 30.50
                  Mean reward/step: 7.94
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5038080
                    Iteration time: 2.78s
                        Total time: 1627.38s
                               ETA: 527603.5s

################################################################################
                    [1m Learning iteration 615/200000 [0m

                       Computation: 2961 steps/s (collection: 0.611s, learning 2.155s)
               Value function loss: 17886.4196
                    Surrogate loss: 0.0156
             Mean action noise std: 0.95
                       Mean reward: 1732.85
               Mean episode length: 266.75
                 Mean success rate: 29.00
                  Mean reward/step: 8.07
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5046272
                    Iteration time: 2.77s
                        Total time: 1630.14s
                               ETA: 527639.6s

################################################################################
                    [1m Learning iteration 616/200000 [0m

                       Computation: 2946 steps/s (collection: 0.644s, learning 2.137s)
               Value function loss: 29700.4162
                    Surrogate loss: 0.0126
             Mean action noise std: 0.95
                       Mean reward: 1884.33
               Mean episode length: 282.75
                 Mean success rate: 30.50
                  Mean reward/step: 7.14
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5054464
                    Iteration time: 2.78s
                        Total time: 1632.92s
                               ETA: 527680.3s

################################################################################
                    [1m Learning iteration 617/200000 [0m

                       Computation: 2955 steps/s (collection: 0.637s, learning 2.134s)
               Value function loss: 19080.4070
                    Surrogate loss: 0.0119
             Mean action noise std: 0.95
                       Mean reward: 1821.01
               Mean episode length: 279.80
                 Mean success rate: 28.00
                  Mean reward/step: 7.34
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 5062656
                    Iteration time: 2.77s
                        Total time: 1635.69s
                               ETA: 527717.9s

################################################################################
                    [1m Learning iteration 618/200000 [0m

                       Computation: 2914 steps/s (collection: 0.621s, learning 2.189s)
               Value function loss: 29594.5932
                    Surrogate loss: 0.0151
             Mean action noise std: 0.95
                       Mean reward: 1924.36
               Mean episode length: 280.02
                 Mean success rate: 30.00
                  Mean reward/step: 7.40
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 5070848
                    Iteration time: 2.81s
                        Total time: 1638.51s
                               ETA: 527768.1s

################################################################################
                    [1m Learning iteration 619/200000 [0m

                       Computation: 2902 steps/s (collection: 0.637s, learning 2.186s)
               Value function loss: 31176.0273
                    Surrogate loss: 0.0158
             Mean action noise std: 0.95
                       Mean reward: 1920.48
               Mean episode length: 277.19
                 Mean success rate: 30.50
                  Mean reward/step: 7.43
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 5079040
                    Iteration time: 2.82s
                        Total time: 1641.33s
                               ETA: 527822.0s

################################################################################
                    [1m Learning iteration 620/200000 [0m

                       Computation: 2943 steps/s (collection: 0.632s, learning 2.152s)
               Value function loss: 23660.0350
                    Surrogate loss: 0.0137
             Mean action noise std: 0.95
                       Mean reward: 1919.15
               Mean episode length: 271.93
                 Mean success rate: 31.50
                  Mean reward/step: 7.62
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5087232
                    Iteration time: 2.78s
                        Total time: 1644.11s
                               ETA: 527863.0s

################################################################################
                    [1m Learning iteration 621/200000 [0m

                       Computation: 2755 steps/s (collection: 0.739s, learning 2.234s)
               Value function loss: 29833.4952
                    Surrogate loss: 0.0155
             Mean action noise std: 0.95
                       Mean reward: 1986.70
               Mean episode length: 274.96
                 Mean success rate: 33.00
                  Mean reward/step: 7.64
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5095424
                    Iteration time: 2.97s
                        Total time: 1647.08s
                               ETA: 527964.9s

################################################################################
                    [1m Learning iteration 622/200000 [0m

                       Computation: 3076 steps/s (collection: 0.573s, learning 2.090s)
               Value function loss: 33727.3936
                    Surrogate loss: 0.0160
             Mean action noise std: 0.95
                       Mean reward: 1999.30
               Mean episode length: 262.52
                 Mean success rate: 33.50
                  Mean reward/step: 8.23
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5103616
                    Iteration time: 2.66s
                        Total time: 1649.75s
                               ETA: 527966.8s

################################################################################
                    [1m Learning iteration 623/200000 [0m

                       Computation: 3122 steps/s (collection: 0.516s, learning 2.107s)
               Value function loss: 25252.9643
                    Surrogate loss: 0.0141
             Mean action noise std: 0.95
                       Mean reward: 2050.35
               Mean episode length: 261.76
                 Mean success rate: 34.50
                  Mean reward/step: 8.56
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 2.62s
                        Total time: 1652.37s
                               ETA: 527956.2s

################################################################################
                    [1m Learning iteration 624/200000 [0m

                       Computation: 3221 steps/s (collection: 0.503s, learning 2.040s)
               Value function loss: 29429.2678
                    Surrogate loss: 0.0147
             Mean action noise std: 0.95
                       Mean reward: 2136.15
               Mean episode length: 260.59
                 Mean success rate: 35.50
                  Mean reward/step: 8.26
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5120000
                    Iteration time: 2.54s
                        Total time: 1654.91s
                               ETA: 527920.1s

################################################################################
                    [1m Learning iteration 625/200000 [0m

                       Computation: 3136 steps/s (collection: 0.518s, learning 2.094s)
               Value function loss: 26656.4370
                    Surrogate loss: 0.0190
             Mean action noise std: 0.95
                       Mean reward: 2118.61
               Mean episode length: 263.94
                 Mean success rate: 34.50
                  Mean reward/step: 8.61
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 5128192
                    Iteration time: 2.61s
                        Total time: 1657.53s
                               ETA: 527905.9s

################################################################################
                    [1m Learning iteration 626/200000 [0m

                       Computation: 3033 steps/s (collection: 0.556s, learning 2.144s)
               Value function loss: 28541.8103
                    Surrogate loss: 0.0151
             Mean action noise std: 0.95
                       Mean reward: 2086.69
               Mean episode length: 257.74
                 Mean success rate: 34.50
                  Mean reward/step: 8.27
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 5136384
                    Iteration time: 2.70s
                        Total time: 1660.23s
                               ETA: 527919.9s

################################################################################
                    [1m Learning iteration 627/200000 [0m

                       Computation: 3106 steps/s (collection: 0.565s, learning 2.072s)
               Value function loss: 25523.7091
                    Surrogate loss: 0.0155
             Mean action noise std: 0.95
                       Mean reward: 2247.78
               Mean episode length: 273.75
                 Mean success rate: 37.00
                  Mean reward/step: 8.43
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5144576
                    Iteration time: 2.64s
                        Total time: 1662.86s
                               ETA: 527913.9s

################################################################################
                    [1m Learning iteration 628/200000 [0m

                       Computation: 3123 steps/s (collection: 0.554s, learning 2.068s)
               Value function loss: 28427.3730
                    Surrogate loss: 0.0145
             Mean action noise std: 0.95
                       Mean reward: 2192.23
               Mean episode length: 264.75
                 Mean success rate: 33.00
                  Mean reward/step: 8.44
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5152768
                    Iteration time: 2.62s
                        Total time: 1665.49s
                               ETA: 527903.2s

################################################################################
                    [1m Learning iteration 629/200000 [0m

                       Computation: 3164 steps/s (collection: 0.521s, learning 2.068s)
               Value function loss: 28117.8041
                    Surrogate loss: 0.0164
             Mean action noise std: 0.95
                       Mean reward: 2152.09
               Mean episode length: 272.92
                 Mean success rate: 33.00
                  Mean reward/step: 7.96
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 5160960
                    Iteration time: 2.59s
                        Total time: 1668.07s
                               ETA: 527881.8s

################################################################################
                    [1m Learning iteration 630/200000 [0m

                       Computation: 3119 steps/s (collection: 0.538s, learning 2.088s)
               Value function loss: 37558.6782
                    Surrogate loss: 0.0180
             Mean action noise std: 0.95
                       Mean reward: 2052.77
               Mean episode length: 266.67
                 Mean success rate: 32.50
                  Mean reward/step: 7.60
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 5169152
                    Iteration time: 2.63s
                        Total time: 1670.70s
                               ETA: 527872.3s

################################################################################
                    [1m Learning iteration 631/200000 [0m

                       Computation: 3026 steps/s (collection: 0.564s, learning 2.143s)
               Value function loss: 24526.1376
                    Surrogate loss: 0.0148
             Mean action noise std: 0.95
                       Mean reward: 1976.58
               Mean episode length: 260.46
                 Mean success rate: 33.00
                  Mean reward/step: 7.95
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5177344
                    Iteration time: 2.71s
                        Total time: 1673.41s
                               ETA: 527888.2s

################################################################################
                    [1m Learning iteration 632/200000 [0m

                       Computation: 2998 steps/s (collection: 0.594s, learning 2.138s)
               Value function loss: 31703.5414
                    Surrogate loss: 0.0152
             Mean action noise std: 0.95
                       Mean reward: 2042.03
               Mean episode length: 270.54
                 Mean success rate: 33.00
                  Mean reward/step: 8.06
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 5185536
                    Iteration time: 2.73s
                        Total time: 1676.14s
                               ETA: 527912.1s

################################################################################
                    [1m Learning iteration 633/200000 [0m

                       Computation: 2958 steps/s (collection: 0.593s, learning 2.176s)
               Value function loss: 32096.6300
                    Surrogate loss: 0.0153
             Mean action noise std: 0.95
                       Mean reward: 2060.70
               Mean episode length: 259.89
                 Mean success rate: 31.50
                  Mean reward/step: 7.90
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 5193728
                    Iteration time: 2.77s
                        Total time: 1678.91s
                               ETA: 527947.6s

################################################################################
                    [1m Learning iteration 634/200000 [0m

                       Computation: 3021 steps/s (collection: 0.607s, learning 2.104s)
               Value function loss: 28814.6140
                    Surrogate loss: 0.0111
             Mean action noise std: 0.95
                       Mean reward: 2113.03
               Mean episode length: 257.24
                 Mean success rate: 33.00
                  Mean reward/step: 7.45
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 5201920
                    Iteration time: 2.71s
                        Total time: 1681.62s
                               ETA: 527964.8s

################################################################################
                    [1m Learning iteration 635/200000 [0m

                       Computation: 3017 steps/s (collection: 0.603s, learning 2.113s)
               Value function loss: 33689.1019
                    Surrogate loss: 0.0130
             Mean action noise std: 0.95
                       Mean reward: 1991.47
               Mean episode length: 242.41
                 Mean success rate: 31.00
                  Mean reward/step: 7.43
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 2.72s
                        Total time: 1684.33s
                               ETA: 527983.1s

################################################################################
                    [1m Learning iteration 636/200000 [0m

                       Computation: 2993 steps/s (collection: 0.566s, learning 2.170s)
               Value function loss: 18772.3906
                    Surrogate loss: 0.0150
             Mean action noise std: 0.95
                       Mean reward: 1943.82
               Mean episode length: 233.09
                 Mean success rate: 29.50
                  Mean reward/step: 7.84
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5218304
                    Iteration time: 2.74s
                        Total time: 1687.07s
                               ETA: 528008.1s

################################################################################
                    [1m Learning iteration 637/200000 [0m

                       Computation: 2987 steps/s (collection: 0.587s, learning 2.155s)
               Value function loss: 34595.0910
                    Surrogate loss: 0.0118
             Mean action noise std: 0.95
                       Mean reward: 1956.70
               Mean episode length: 235.38
                 Mean success rate: 28.50
                  Mean reward/step: 8.25
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 5226496
                    Iteration time: 2.74s
                        Total time: 1689.81s
                               ETA: 528034.6s

################################################################################
                    [1m Learning iteration 638/200000 [0m

                       Computation: 3000 steps/s (collection: 0.603s, learning 2.127s)
               Value function loss: 36322.7452
                    Surrogate loss: 0.0129
             Mean action noise std: 0.95
                       Mean reward: 1944.95
               Mean episode length: 230.18
                 Mean success rate: 28.00
                  Mean reward/step: 8.23
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5234688
                    Iteration time: 2.73s
                        Total time: 1692.54s
                               ETA: 528057.5s

################################################################################
                    [1m Learning iteration 639/200000 [0m

                       Computation: 2997 steps/s (collection: 0.610s, learning 2.123s)
               Value function loss: 26220.7659
                    Surrogate loss: 0.0137
             Mean action noise std: 0.95
                       Mean reward: 1678.23
               Mean episode length: 215.09
                 Mean success rate: 23.50
                  Mean reward/step: 8.18
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 5242880
                    Iteration time: 2.73s
                        Total time: 1695.28s
                               ETA: 528081.1s

################################################################################
                    [1m Learning iteration 640/200000 [0m

                       Computation: 2979 steps/s (collection: 0.627s, learning 2.122s)
               Value function loss: 22251.5533
                    Surrogate loss: 0.0122
             Mean action noise std: 0.95
                       Mean reward: 1547.62
               Mean episode length: 221.62
                 Mean success rate: 22.00
                  Mean reward/step: 8.69
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 5251072
                    Iteration time: 2.75s
                        Total time: 1698.03s
                               ETA: 528109.8s

################################################################################
                    [1m Learning iteration 641/200000 [0m

                       Computation: 2962 steps/s (collection: 0.608s, learning 2.158s)
               Value function loss: 35863.3557
                    Surrogate loss: 0.0141
             Mean action noise std: 0.95
                       Mean reward: 1566.74
               Mean episode length: 226.75
                 Mean success rate: 22.00
                  Mean reward/step: 8.81
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 5259264
                    Iteration time: 2.77s
                        Total time: 1700.79s
                               ETA: 528143.4s

################################################################################
                    [1m Learning iteration 642/200000 [0m

                       Computation: 3085 steps/s (collection: 0.553s, learning 2.102s)
               Value function loss: 24947.3178
                    Surrogate loss: 0.0151
             Mean action noise std: 0.95
                       Mean reward: 1488.65
               Mean episode length: 217.13
                 Mean success rate: 20.50
                  Mean reward/step: 9.09
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 5267456
                    Iteration time: 2.66s
                        Total time: 1703.45s
                               ETA: 528142.6s

################################################################################
                    [1m Learning iteration 643/200000 [0m

                       Computation: 3009 steps/s (collection: 0.618s, learning 2.104s)
               Value function loss: 37909.8163
                    Surrogate loss: 0.0152
             Mean action noise std: 0.95
                       Mean reward: 1501.18
               Mean episode length: 218.08
                 Mean success rate: 20.50
                  Mean reward/step: 9.50
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 5275648
                    Iteration time: 2.72s
                        Total time: 1706.17s
                               ETA: 528162.4s

################################################################################
                    [1m Learning iteration 644/200000 [0m

                       Computation: 2956 steps/s (collection: 0.650s, learning 2.121s)
               Value function loss: 39161.5342
                    Surrogate loss: 0.0155
             Mean action noise std: 0.95
                       Mean reward: 1565.48
               Mean episode length: 224.16
                 Mean success rate: 21.50
                  Mean reward/step: 9.67
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5283840
                    Iteration time: 2.77s
                        Total time: 1708.94s
                               ETA: 528197.4s

################################################################################
                    [1m Learning iteration 645/200000 [0m

                       Computation: 3003 steps/s (collection: 0.602s, learning 2.125s)
               Value function loss: 28221.6771
                    Surrogate loss: 0.0152
             Mean action noise std: 0.95
                       Mean reward: 1608.34
               Mean episode length: 221.12
                 Mean success rate: 22.00
                  Mean reward/step: 10.11
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5292032
                    Iteration time: 2.73s
                        Total time: 1711.67s
                               ETA: 528218.9s

################################################################################
                    [1m Learning iteration 646/200000 [0m

                       Computation: 3023 steps/s (collection: 0.595s, learning 2.115s)
               Value function loss: 40450.2810
                    Surrogate loss: 0.0151
             Mean action noise std: 0.95
                       Mean reward: 1842.72
               Mean episode length: 227.54
                 Mean success rate: 25.00
                  Mean reward/step: 10.38
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 5300224
                    Iteration time: 2.71s
                        Total time: 1714.38s
                               ETA: 528234.6s

################################################################################
                    [1m Learning iteration 647/200000 [0m

                       Computation: 2972 steps/s (collection: 0.636s, learning 2.120s)
               Value function loss: 33380.9804
                    Surrogate loss: 0.0165
             Mean action noise std: 0.95
                       Mean reward: 1899.56
               Mean episode length: 225.59
                 Mean success rate: 25.50
                  Mean reward/step: 10.29
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 2.76s
                        Total time: 1717.13s
                               ETA: 528264.7s

################################################################################
                    [1m Learning iteration 648/200000 [0m

                       Computation: 2902 steps/s (collection: 0.660s, learning 2.162s)
               Value function loss: 33511.3579
                    Surrogate loss: 0.0173
             Mean action noise std: 0.95
                       Mean reward: 2163.63
               Mean episode length: 236.56
                 Mean success rate: 28.50
                  Mean reward/step: 10.29
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 5316608
                    Iteration time: 2.82s
                        Total time: 1719.95s
                               ETA: 528315.0s

################################################################################
                    [1m Learning iteration 649/200000 [0m

                       Computation: 3000 steps/s (collection: 0.626s, learning 2.104s)
               Value function loss: 40291.1369
                    Surrogate loss: 0.0145
             Mean action noise std: 0.95
                       Mean reward: 2115.81
               Mean episode length: 228.34
                 Mean success rate: 28.00
                  Mean reward/step: 10.24
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 5324800
                    Iteration time: 2.73s
                        Total time: 1722.69s
                               ETA: 528336.9s

################################################################################
                    [1m Learning iteration 650/200000 [0m

                       Computation: 3050 steps/s (collection: 0.568s, learning 2.118s)
               Value function loss: 46948.6811
                    Surrogate loss: 0.0107
             Mean action noise std: 0.95
                       Mean reward: 2219.99
               Mean episode length: 235.13
                 Mean success rate: 29.50
                  Mean reward/step: 10.02
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 5332992
                    Iteration time: 2.69s
                        Total time: 1725.37s
                               ETA: 528345.2s

################################################################################
                    [1m Learning iteration 651/200000 [0m

                       Computation: 3008 steps/s (collection: 0.568s, learning 2.156s)
               Value function loss: 35655.2719
                    Surrogate loss: 0.0149
             Mean action noise std: 0.95
                       Mean reward: 2281.85
               Mean episode length: 236.16
                 Mean success rate: 29.00
                  Mean reward/step: 9.33
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5341184
                    Iteration time: 2.72s
                        Total time: 1728.09s
                               ETA: 528364.9s

################################################################################
                    [1m Learning iteration 652/200000 [0m

                       Computation: 2902 steps/s (collection: 0.641s, learning 2.182s)
               Value function loss: 29229.8591
                    Surrogate loss: 0.0157
             Mean action noise std: 0.95
                       Mean reward: 2220.74
               Mean episode length: 231.16
                 Mean success rate: 28.00
                  Mean reward/step: 9.17
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 5349376
                    Iteration time: 2.82s
                        Total time: 1730.92s
                               ETA: 528414.8s

################################################################################
                    [1m Learning iteration 653/200000 [0m

                       Computation: 2978 steps/s (collection: 0.622s, learning 2.128s)
               Value function loss: 28902.8536
                    Surrogate loss: 0.0170
             Mean action noise std: 0.95
                       Mean reward: 2151.11
               Mean episode length: 230.94
                 Mean success rate: 27.50
                  Mean reward/step: 9.00
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 5357568
                    Iteration time: 2.75s
                        Total time: 1733.67s
                               ETA: 528442.6s

################################################################################
                    [1m Learning iteration 654/200000 [0m

                       Computation: 3196 steps/s (collection: 0.509s, learning 2.054s)
               Value function loss: 33782.6872
                    Surrogate loss: 0.0163
             Mean action noise std: 0.94
                       Mean reward: 2392.03
               Mean episode length: 240.39
                 Mean success rate: 31.00
                  Mean reward/step: 8.42
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5365760
                    Iteration time: 2.56s
                        Total time: 1736.23s
                               ETA: 528413.3s

################################################################################
                    [1m Learning iteration 655/200000 [0m

                       Computation: 3146 steps/s (collection: 0.525s, learning 2.078s)
               Value function loss: 41849.6191
                    Surrogate loss: 0.0137
             Mean action noise std: 0.94
                       Mean reward: 2533.67
               Mean episode length: 247.19
                 Mean success rate: 32.00
                  Mean reward/step: 8.62
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 5373952
                    Iteration time: 2.60s
                        Total time: 1738.83s
                               ETA: 528396.2s

################################################################################
                    [1m Learning iteration 656/200000 [0m

                       Computation: 3155 steps/s (collection: 0.539s, learning 2.057s)
               Value function loss: 47129.6179
                    Surrogate loss: 0.0157
             Mean action noise std: 0.94
                       Mean reward: 2616.54
               Mean episode length: 245.68
                 Mean success rate: 33.50
                  Mean reward/step: 8.64
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 5382144
                    Iteration time: 2.60s
                        Total time: 1741.43s
                               ETA: 528377.0s

################################################################################
                    [1m Learning iteration 657/200000 [0m

                       Computation: 3147 steps/s (collection: 0.540s, learning 2.063s)
               Value function loss: 42460.6255
                    Surrogate loss: 0.0156
             Mean action noise std: 0.94
                       Mean reward: 2645.39
               Mean episode length: 244.56
                 Mean success rate: 33.50
                  Mean reward/step: 8.32
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 5390336
                    Iteration time: 2.60s
                        Total time: 1744.03s
                               ETA: 528359.9s

################################################################################
                    [1m Learning iteration 658/200000 [0m

                       Computation: 3181 steps/s (collection: 0.523s, learning 2.052s)
               Value function loss: 25796.7510
                    Surrogate loss: 0.0161
             Mean action noise std: 0.94
                       Mean reward: 2655.79
               Mean episode length: 247.51
                 Mean success rate: 33.00
                  Mean reward/step: 8.01
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5398528
                    Iteration time: 2.57s
                        Total time: 1746.61s
                               ETA: 528334.3s

################################################################################
                    [1m Learning iteration 659/200000 [0m

                       Computation: 3114 steps/s (collection: 0.535s, learning 2.096s)
               Value function loss: 27790.2831
                    Surrogate loss: 0.0149
             Mean action noise std: 0.94
                       Mean reward: 2520.33
               Mean episode length: 248.10
                 Mean success rate: 31.00
                  Mean reward/step: 8.08
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 2.63s
                        Total time: 1749.24s
                               ETA: 528325.7s

################################################################################
                    [1m Learning iteration 660/200000 [0m

                       Computation: 3022 steps/s (collection: 0.616s, learning 2.094s)
               Value function loss: 19256.4224
                    Surrogate loss: 0.0142
             Mean action noise std: 0.94
                       Mean reward: 2067.90
               Mean episode length: 228.68
                 Mean success rate: 25.00
                  Mean reward/step: 8.63
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 5414912
                    Iteration time: 2.71s
                        Total time: 1751.95s
                               ETA: 528341.1s

################################################################################
                    [1m Learning iteration 661/200000 [0m

                       Computation: 2997 steps/s (collection: 0.565s, learning 2.168s)
               Value function loss: 30032.3100
                    Surrogate loss: 0.0153
             Mean action noise std: 0.94
                       Mean reward: 1902.63
               Mean episode length: 222.11
                 Mean success rate: 22.00
                  Mean reward/step: 9.18
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 5423104
                    Iteration time: 2.73s
                        Total time: 1754.68s
                               ETA: 528363.3s

################################################################################
                    [1m Learning iteration 662/200000 [0m

                       Computation: 3065 steps/s (collection: 0.559s, learning 2.114s)
               Value function loss: 29585.1630
                    Surrogate loss: 0.0143
             Mean action noise std: 0.94
                       Mean reward: 1731.00
               Mean episode length: 214.27
                 Mean success rate: 20.50
                  Mean reward/step: 9.13
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 5431296
                    Iteration time: 2.67s
                        Total time: 1757.35s
                               ETA: 528367.3s

################################################################################
                    [1m Learning iteration 663/200000 [0m

                       Computation: 2967 steps/s (collection: 0.584s, learning 2.176s)
               Value function loss: 21651.8976
                    Surrogate loss: 0.0186
             Mean action noise std: 0.94
                       Mean reward: 1375.59
               Mean episode length: 198.54
                 Mean success rate: 17.00
                  Mean reward/step: 9.04
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 5439488
                    Iteration time: 2.76s
                        Total time: 1760.11s
                               ETA: 528397.6s

################################################################################
                    [1m Learning iteration 664/200000 [0m

                       Computation: 2975 steps/s (collection: 0.612s, learning 2.141s)
               Value function loss: 34152.9690
                    Surrogate loss: 0.0174
             Mean action noise std: 0.94
                       Mean reward: 1341.53
               Mean episode length: 188.04
                 Mean success rate: 18.00
                  Mean reward/step: 9.26
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 5447680
                    Iteration time: 2.75s
                        Total time: 1762.87s
                               ETA: 528425.6s

################################################################################
                    [1m Learning iteration 665/200000 [0m

                       Computation: 3068 steps/s (collection: 0.574s, learning 2.096s)
               Value function loss: 42880.9767
                    Surrogate loss: 0.0155
             Mean action noise std: 0.94
                       Mean reward: 1648.93
               Mean episode length: 203.38
                 Mean success rate: 22.00
                  Mean reward/step: 9.37
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 5455872
                    Iteration time: 2.67s
                        Total time: 1765.54s
                               ETA: 528428.5s

################################################################################
                    [1m Learning iteration 666/200000 [0m

                       Computation: 3030 steps/s (collection: 0.581s, learning 2.122s)
               Value function loss: 35010.4014
                    Surrogate loss: 0.0137
             Mean action noise std: 0.94
                       Mean reward: 1957.86
               Mean episode length: 223.54
                 Mean success rate: 26.50
                  Mean reward/step: 8.75
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 5464064
                    Iteration time: 2.70s
                        Total time: 1768.24s
                               ETA: 528441.6s

################################################################################
                    [1m Learning iteration 667/200000 [0m

                       Computation: 2956 steps/s (collection: 0.629s, learning 2.141s)
               Value function loss: 34467.7229
                    Surrogate loss: 0.0153
             Mean action noise std: 0.94
                       Mean reward: 1975.67
               Mean episode length: 227.56
                 Mean success rate: 25.50
                  Mean reward/step: 8.66
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 5472256
                    Iteration time: 2.77s
                        Total time: 1771.01s
                               ETA: 528474.7s

################################################################################
                    [1m Learning iteration 668/200000 [0m

                       Computation: 3036 steps/s (collection: 0.599s, learning 2.098s)
               Value function loss: 35426.7560
                    Surrogate loss: 0.0152
             Mean action noise std: 0.94
                       Mean reward: 2139.56
               Mean episode length: 237.94
                 Mean success rate: 27.50
                  Mean reward/step: 9.20
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5480448
                    Iteration time: 2.70s
                        Total time: 1773.71s
                               ETA: 528485.8s

################################################################################
                    [1m Learning iteration 669/200000 [0m

                       Computation: 3040 steps/s (collection: 0.588s, learning 2.106s)
               Value function loss: 37828.1063
                    Surrogate loss: 0.0147
             Mean action noise std: 0.94
                       Mean reward: 2398.22
               Mean episode length: 255.85
                 Mean success rate: 29.50
                  Mean reward/step: 9.31
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 5488640
                    Iteration time: 2.69s
                        Total time: 1776.40s
                               ETA: 528495.8s

################################################################################
                    [1m Learning iteration 670/200000 [0m

                       Computation: 2981 steps/s (collection: 0.601s, learning 2.147s)
               Value function loss: 45530.7211
                    Surrogate loss: 0.0143
             Mean action noise std: 0.94
                       Mean reward: 2497.56
               Mean episode length: 260.73
                 Mean success rate: 30.00
                  Mean reward/step: 9.69
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 5496832
                    Iteration time: 2.75s
                        Total time: 1779.15s
                               ETA: 528521.8s

################################################################################
                    [1m Learning iteration 671/200000 [0m

                       Computation: 2970 steps/s (collection: 0.619s, learning 2.139s)
               Value function loss: 28451.7491
                    Surrogate loss: 0.0173
             Mean action noise std: 0.94
                       Mean reward: 2296.57
               Mean episode length: 251.04
                 Mean success rate: 28.50
                  Mean reward/step: 9.21
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 2.76s
                        Total time: 1781.91s
                               ETA: 528550.6s

################################################################################
                    [1m Learning iteration 672/200000 [0m

                       Computation: 3051 steps/s (collection: 0.565s, learning 2.120s)
               Value function loss: 46114.6095
                    Surrogate loss: 0.0161
             Mean action noise std: 0.94
                       Mean reward: 2338.29
               Mean episode length: 247.83
                 Mean success rate: 28.50
                  Mean reward/step: 9.38
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 5513216
                    Iteration time: 2.68s
                        Total time: 1784.59s
                               ETA: 528557.8s

################################################################################
                    [1m Learning iteration 673/200000 [0m

                       Computation: 3030 steps/s (collection: 0.579s, learning 2.124s)
               Value function loss: 36484.0094
                    Surrogate loss: 0.0142
             Mean action noise std: 0.94
                       Mean reward: 2326.11
               Mean episode length: 248.14
                 Mean success rate: 28.00
                  Mean reward/step: 9.09
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 5521408
                    Iteration time: 2.70s
                        Total time: 1787.30s
                               ETA: 528570.4s

################################################################################
                    [1m Learning iteration 674/200000 [0m

                       Computation: 2959 steps/s (collection: 0.590s, learning 2.178s)
               Value function loss: 37562.7032
                    Surrogate loss: 0.0161
             Mean action noise std: 0.94
                       Mean reward: 2401.63
               Mean episode length: 254.34
                 Mean success rate: 28.00
                  Mean reward/step: 9.07
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 5529600
                    Iteration time: 2.77s
                        Total time: 1790.06s
                               ETA: 528602.0s

################################################################################
                    [1m Learning iteration 675/200000 [0m

                       Computation: 2959 steps/s (collection: 0.643s, learning 2.126s)
               Value function loss: 32807.6356
                    Surrogate loss: 0.0170
             Mean action noise std: 0.94
                       Mean reward: 2235.39
               Mean episode length: 240.72
                 Mean success rate: 26.50
                  Mean reward/step: 8.63
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 5537792
                    Iteration time: 2.77s
                        Total time: 1792.83s
                               ETA: 528633.7s

################################################################################
                    [1m Learning iteration 676/200000 [0m

                       Computation: 3050 steps/s (collection: 0.583s, learning 2.103s)
               Value function loss: 38280.3752
                    Surrogate loss: 0.0114
             Mean action noise std: 0.94
                       Mean reward: 2355.37
               Mean episode length: 252.02
                 Mean success rate: 28.50
                  Mean reward/step: 9.21
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5545984
                    Iteration time: 2.69s
                        Total time: 1795.52s
                               ETA: 528641.0s

################################################################################
                    [1m Learning iteration 677/200000 [0m

                       Computation: 3039 steps/s (collection: 0.566s, learning 2.130s)
               Value function loss: 46213.8187
                    Surrogate loss: 0.0118
             Mean action noise std: 0.94
                       Mean reward: 2344.91
               Mean episode length: 252.54
                 Mean success rate: 29.00
                  Mean reward/step: 9.32
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5554176
                    Iteration time: 2.70s
                        Total time: 1798.21s
                               ETA: 528651.0s

################################################################################
                    [1m Learning iteration 678/200000 [0m

                       Computation: 2978 steps/s (collection: 0.610s, learning 2.141s)
               Value function loss: 30622.7783
                    Surrogate loss: 0.0157
             Mean action noise std: 0.94
                       Mean reward: 2261.96
               Mean episode length: 252.63
                 Mean success rate: 27.50
                  Mean reward/step: 10.12
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5562368
                    Iteration time: 2.75s
                        Total time: 1800.96s
                               ETA: 528677.1s

################################################################################
                    [1m Learning iteration 679/200000 [0m

                       Computation: 3034 steps/s (collection: 0.583s, learning 2.117s)
               Value function loss: 42400.0263
                    Surrogate loss: 0.0151
             Mean action noise std: 0.94
                       Mean reward: 2121.06
               Mean episode length: 248.25
                 Mean success rate: 27.00
                  Mean reward/step: 10.92
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5570560
                    Iteration time: 2.70s
                        Total time: 1803.66s
                               ETA: 528688.2s

################################################################################
                    [1m Learning iteration 680/200000 [0m

                       Computation: 3024 steps/s (collection: 0.606s, learning 2.102s)
               Value function loss: 46404.3928
                    Surrogate loss: 0.0132
             Mean action noise std: 0.94
                       Mean reward: 2156.95
               Mean episode length: 251.52
                 Mean success rate: 27.50
                  Mean reward/step: 10.32
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 5578752
                    Iteration time: 2.71s
                        Total time: 1806.37s
                               ETA: 528701.9s

################################################################################
                    [1m Learning iteration 681/200000 [0m

                       Computation: 2954 steps/s (collection: 0.639s, learning 2.134s)
               Value function loss: 46489.1256
                    Surrogate loss: 0.0149
             Mean action noise std: 0.94
                       Mean reward: 2487.40
               Mean episode length: 264.13
                 Mean success rate: 31.00
                  Mean reward/step: 10.08
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 5586944
                    Iteration time: 2.77s
                        Total time: 1809.14s
                               ETA: 528734.3s

################################################################################
                    [1m Learning iteration 682/200000 [0m

                       Computation: 3044 steps/s (collection: 0.614s, learning 2.077s)
               Value function loss: 54540.6814
                    Surrogate loss: 0.0139
             Mean action noise std: 0.94
                       Mean reward: 2513.10
               Mean episode length: 266.29
                 Mean success rate: 32.00
                  Mean reward/step: 9.70
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 5595136
                    Iteration time: 2.69s
                        Total time: 1811.83s
                               ETA: 528742.6s

################################################################################
                    [1m Learning iteration 683/200000 [0m

                       Computation: 3076 steps/s (collection: 0.574s, learning 2.089s)
               Value function loss: 31985.9222
                    Surrogate loss: 0.0208
             Mean action noise std: 0.94
                       Mean reward: 2313.68
               Mean episode length: 249.72
                 Mean success rate: 29.50
                  Mean reward/step: 9.57
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 2.66s
                        Total time: 1814.50s
                               ETA: 528742.9s

################################################################################
                    [1m Learning iteration 684/200000 [0m

                       Computation: 3108 steps/s (collection: 0.569s, learning 2.067s)
               Value function loss: 34184.1073
                    Surrogate loss: 0.0200
             Mean action noise std: 0.94
                       Mean reward: 2568.77
               Mean episode length: 261.62
                 Mean success rate: 32.50
                  Mean reward/step: 9.16
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 5611520
                    Iteration time: 2.64s
                        Total time: 1817.13s
                               ETA: 528735.2s

################################################################################
                    [1m Learning iteration 685/200000 [0m

                       Computation: 3098 steps/s (collection: 0.557s, learning 2.087s)
               Value function loss: 34886.6570
                    Surrogate loss: 0.0170
             Mean action noise std: 0.94
                       Mean reward: 2441.95
               Mean episode length: 254.62
                 Mean success rate: 32.50
                  Mean reward/step: 9.49
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5619712
                    Iteration time: 2.64s
                        Total time: 1819.78s
                               ETA: 528730.0s

################################################################################
                    [1m Learning iteration 686/200000 [0m

                       Computation: 3036 steps/s (collection: 0.620s, learning 2.078s)
               Value function loss: 44269.7593
                    Surrogate loss: 0.0144
             Mean action noise std: 0.94
                       Mean reward: 2622.18
               Mean episode length: 266.65
                 Mean success rate: 34.00
                  Mean reward/step: 9.91
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5627904
                    Iteration time: 2.70s
                        Total time: 1822.47s
                               ETA: 528740.5s

################################################################################
                    [1m Learning iteration 687/200000 [0m

                       Computation: 3084 steps/s (collection: 0.548s, learning 2.108s)
               Value function loss: 30385.6865
                    Surrogate loss: 0.0136
             Mean action noise std: 0.94
                       Mean reward: 2468.09
               Mean episode length: 252.45
                 Mean success rate: 33.00
                  Mean reward/step: 9.98
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5636096
                    Iteration time: 2.66s
                        Total time: 1825.13s
                               ETA: 528738.8s

################################################################################
                    [1m Learning iteration 688/200000 [0m

                       Computation: 3110 steps/s (collection: 0.568s, learning 2.066s)
               Value function loss: 61826.0485
                    Surrogate loss: 0.0116
             Mean action noise std: 0.94
                       Mean reward: 2601.83
               Mean episode length: 256.95
                 Mean success rate: 35.00
                  Mean reward/step: 9.52
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 5644288
                    Iteration time: 2.63s
                        Total time: 1827.76s
                               ETA: 528730.6s

################################################################################
                    [1m Learning iteration 689/200000 [0m

                       Computation: 3025 steps/s (collection: 0.573s, learning 2.135s)
               Value function loss: 26746.4189
                    Surrogate loss: 0.0116
             Mean action noise std: 0.94
                       Mean reward: 2559.76
               Mean episode length: 255.74
                 Mean success rate: 33.00
                  Mean reward/step: 9.12
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 5652480
                    Iteration time: 2.71s
                        Total time: 1830.47s
                               ETA: 528743.8s

################################################################################
                    [1m Learning iteration 690/200000 [0m

                       Computation: 3029 steps/s (collection: 0.605s, learning 2.099s)
               Value function loss: 40883.4005
                    Surrogate loss: 0.0137
             Mean action noise std: 0.94
                       Mean reward: 2480.88
               Mean episode length: 258.50
                 Mean success rate: 31.50
                  Mean reward/step: 9.69
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 5660672
                    Iteration time: 2.70s
                        Total time: 1833.18s
                               ETA: 528755.8s

################################################################################
                    [1m Learning iteration 691/200000 [0m

                       Computation: 3052 steps/s (collection: 0.594s, learning 2.090s)
               Value function loss: 36379.1262
                    Surrogate loss: 0.0144
             Mean action noise std: 0.94
                       Mean reward: 2629.50
               Mean episode length: 260.95
                 Mean success rate: 31.50
                  Mean reward/step: 9.77
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5668864
                    Iteration time: 2.68s
                        Total time: 1835.86s
                               ETA: 528762.1s

################################################################################
                    [1m Learning iteration 692/200000 [0m

                       Computation: 3081 steps/s (collection: 0.566s, learning 2.093s)
               Value function loss: 30870.8196
                    Surrogate loss: 0.0140
             Mean action noise std: 0.94
                       Mean reward: 2632.35
               Mean episode length: 257.86
                 Mean success rate: 31.50
                  Mean reward/step: 9.99
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 5677056
                    Iteration time: 2.66s
                        Total time: 1838.52s
                               ETA: 528761.0s

################################################################################
                    [1m Learning iteration 693/200000 [0m

                       Computation: 3037 steps/s (collection: 0.588s, learning 2.110s)
               Value function loss: 31937.2393
                    Surrogate loss: 0.0145
             Mean action noise std: 0.94
                       Mean reward: 2443.61
               Mean episode length: 256.53
                 Mean success rate: 30.50
                  Mean reward/step: 9.66
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 5685248
                    Iteration time: 2.70s
                        Total time: 1841.22s
                               ETA: 528771.0s

################################################################################
                    [1m Learning iteration 694/200000 [0m

                       Computation: 3030 steps/s (collection: 0.622s, learning 2.081s)
               Value function loss: 26088.2645
                    Surrogate loss: 0.0161
             Mean action noise std: 0.94
                       Mean reward: 2208.63
               Mean episode length: 242.98
                 Mean success rate: 27.50
                  Mean reward/step: 9.70
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5693440
                    Iteration time: 2.70s
                        Total time: 1843.92s
                               ETA: 528782.8s

################################################################################
                    [1m Learning iteration 695/200000 [0m

                       Computation: 2984 steps/s (collection: 0.612s, learning 2.133s)
               Value function loss: 47413.6621
                    Surrogate loss: 0.0137
             Mean action noise std: 0.94
                       Mean reward: 2071.81
               Mean episode length: 241.53
                 Mean success rate: 25.00
                  Mean reward/step: 9.69
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 2.74s
                        Total time: 1846.66s
                               ETA: 528806.4s

################################################################################
                    [1m Learning iteration 696/200000 [0m

                       Computation: 3025 steps/s (collection: 0.596s, learning 2.112s)
               Value function loss: 31329.3527
                    Surrogate loss: 0.0132
             Mean action noise std: 0.94
                       Mean reward: 2223.98
               Mean episode length: 244.10
                 Mean success rate: 26.50
                  Mean reward/step: 9.69
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5709824
                    Iteration time: 2.71s
                        Total time: 1849.37s
                               ETA: 528819.3s

################################################################################
                    [1m Learning iteration 697/200000 [0m

                       Computation: 3126 steps/s (collection: 0.564s, learning 2.057s)
               Value function loss: 60391.0871
                    Surrogate loss: 0.0103
             Mean action noise std: 0.94
                       Mean reward: 2517.38
               Mean episode length: 257.88
                 Mean success rate: 30.00
                  Mean reward/step: 10.37
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 5718016
                    Iteration time: 2.62s
                        Total time: 1851.99s
                               ETA: 528807.3s

################################################################################
                    [1m Learning iteration 698/200000 [0m

                       Computation: 3196 steps/s (collection: 0.510s, learning 2.052s)
               Value function loss: 41132.6398
                    Surrogate loss: 0.0133
             Mean action noise std: 0.94
                       Mean reward: 2483.10
               Mean episode length: 258.85
                 Mean success rate: 31.00
                  Mean reward/step: 10.49
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5726208
                    Iteration time: 2.56s
                        Total time: 1854.55s
                               ETA: 528778.8s

################################################################################
                    [1m Learning iteration 699/200000 [0m

                       Computation: 3187 steps/s (collection: 0.512s, learning 2.058s)
               Value function loss: 47079.9714
                    Surrogate loss: 0.0135
             Mean action noise std: 0.94
                       Mean reward: 2610.91
               Mean episode length: 261.61
                 Mean success rate: 32.50
                  Mean reward/step: 10.20
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 5734400
                    Iteration time: 2.57s
                        Total time: 1857.12s
                               ETA: 528752.5s

################################################################################
                    [1m Learning iteration 700/200000 [0m

                       Computation: 3064 steps/s (collection: 0.566s, learning 2.107s)
               Value function loss: 52817.2873
                    Surrogate loss: 0.0119
             Mean action noise std: 0.94
                       Mean reward: 2914.24
               Mean episode length: 275.64
                 Mean success rate: 34.50
                  Mean reward/step: 9.74
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 5742592
                    Iteration time: 2.67s
                        Total time: 1859.80s
                               ETA: 528755.6s

################################################################################
                    [1m Learning iteration 701/200000 [0m

                       Computation: 3061 steps/s (collection: 0.629s, learning 2.047s)
               Value function loss: 49854.3516
                    Surrogate loss: 0.0124
             Mean action noise std: 0.94
                       Mean reward: 2867.45
               Mean episode length: 273.33
                 Mean success rate: 34.50
                  Mean reward/step: 9.60
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 5750784
                    Iteration time: 2.68s
                        Total time: 1862.47s
                               ETA: 528759.4s

################################################################################
                    [1m Learning iteration 702/200000 [0m

                       Computation: 3239 steps/s (collection: 0.505s, learning 2.024s)
               Value function loss: 36339.1131
                    Surrogate loss: 0.0144
             Mean action noise std: 0.94
                       Mean reward: 2914.17
               Mean episode length: 282.15
                 Mean success rate: 35.50
                  Mean reward/step: 9.62
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 5758976
                    Iteration time: 2.53s
                        Total time: 1865.00s
                               ETA: 528721.6s

################################################################################
                    [1m Learning iteration 703/200000 [0m

                       Computation: 3245 steps/s (collection: 0.476s, learning 2.048s)
               Value function loss: 29444.7953
                    Surrogate loss: 0.0163
             Mean action noise std: 0.94
                       Mean reward: 2709.68
               Mean episode length: 275.45
                 Mean success rate: 33.50
                  Mean reward/step: 9.74
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 5767168
                    Iteration time: 2.52s
                        Total time: 1867.53s
                               ETA: 528682.4s

################################################################################
                    [1m Learning iteration 704/200000 [0m

                       Computation: 3137 steps/s (collection: 0.571s, learning 2.040s)
               Value function loss: 44192.0146
                    Surrogate loss: 0.0128
             Mean action noise std: 0.94
                       Mean reward: 2631.88
               Mean episode length: 267.50
                 Mean success rate: 33.50
                  Mean reward/step: 9.82
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5775360
                    Iteration time: 2.61s
                        Total time: 1870.14s
                               ETA: 528668.0s

################################################################################
                    [1m Learning iteration 705/200000 [0m

                       Computation: 3206 steps/s (collection: 0.507s, learning 2.048s)
               Value function loss: 24328.8825
                    Surrogate loss: 0.0146
             Mean action noise std: 0.94
                       Mean reward: 2153.41
               Mean episode length: 246.34
                 Mean success rate: 27.00
                  Mean reward/step: 10.47
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 5783552
                    Iteration time: 2.55s
                        Total time: 1872.69s
                               ETA: 528637.7s

################################################################################
                    [1m Learning iteration 706/200000 [0m

                       Computation: 3258 steps/s (collection: 0.485s, learning 2.029s)
               Value function loss: 54713.7884
                    Surrogate loss: 0.0135
             Mean action noise std: 0.94
                       Mean reward: 2224.56
               Mean episode length: 251.70
                 Mean success rate: 29.50
                  Mean reward/step: 10.84
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 5791744
                    Iteration time: 2.51s
                        Total time: 1875.21s
                               ETA: 528596.0s

################################################################################
                    [1m Learning iteration 707/200000 [0m

                       Computation: 3159 steps/s (collection: 0.543s, learning 2.050s)
               Value function loss: 38536.4550
                    Surrogate loss: 0.0153
             Mean action noise std: 0.94
                       Mean reward: 2116.41
               Mean episode length: 247.13
                 Mean success rate: 28.50
                  Mean reward/step: 10.86
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 2.59s
                        Total time: 1877.80s
                               ETA: 528576.6s

################################################################################
                    [1m Learning iteration 708/200000 [0m

                       Computation: 3062 steps/s (collection: 0.595s, learning 2.079s)
               Value function loss: 51555.4946
                    Surrogate loss: 0.0141
             Mean action noise std: 0.94
                       Mean reward: 2261.54
               Mean episode length: 248.06
                 Mean success rate: 29.00
                  Mean reward/step: 11.23
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 5808128
                    Iteration time: 2.67s
                        Total time: 1880.47s
                               ETA: 528580.2s

################################################################################
                    [1m Learning iteration 709/200000 [0m

                       Computation: 3123 steps/s (collection: 0.559s, learning 2.065s)
               Value function loss: 34048.3353
                    Surrogate loss: 0.0146
             Mean action noise std: 0.94
                       Mean reward: 2305.47
               Mean episode length: 243.34
                 Mean success rate: 29.50
                  Mean reward/step: 11.09
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 5816320
                    Iteration time: 2.62s
                        Total time: 1883.10s
                               ETA: 528569.3s

################################################################################
                    [1m Learning iteration 710/200000 [0m

                       Computation: 3172 steps/s (collection: 0.511s, learning 2.071s)
               Value function loss: 38966.0100
                    Surrogate loss: 0.0123
             Mean action noise std: 0.94
                       Mean reward: 2140.79
               Mean episode length: 232.15
                 Mean success rate: 26.50
                  Mean reward/step: 10.77
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 5824512
                    Iteration time: 2.58s
                        Total time: 1885.68s
                               ETA: 528547.0s

################################################################################
                    [1m Learning iteration 711/200000 [0m

                       Computation: 3208 steps/s (collection: 0.508s, learning 2.045s)
               Value function loss: 50919.1097
                    Surrogate loss: 0.0139
             Mean action noise std: 0.94
                       Mean reward: 2522.75
               Mean episode length: 244.57
                 Mean success rate: 31.00
                  Mean reward/step: 10.49
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 5832704
                    Iteration time: 2.55s
                        Total time: 1888.23s
                               ETA: 528516.6s

################################################################################
                    [1m Learning iteration 712/200000 [0m

                       Computation: 3114 steps/s (collection: 0.541s, learning 2.090s)
               Value function loss: 37737.1517
                    Surrogate loss: 0.0174
             Mean action noise std: 0.94
                       Mean reward: 2532.89
               Mean episode length: 236.81
                 Mean success rate: 30.00
                  Mean reward/step: 10.74
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5840896
                    Iteration time: 2.63s
                        Total time: 1890.86s
                               ETA: 528507.9s

################################################################################
                    [1m Learning iteration 713/200000 [0m

                       Computation: 3160 steps/s (collection: 0.554s, learning 2.038s)
               Value function loss: 46531.3913
                    Surrogate loss: 0.0153
             Mean action noise std: 0.94
                       Mean reward: 2679.24
               Mean episode length: 239.87
                 Mean success rate: 30.00
                  Mean reward/step: 10.56
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 5849088
                    Iteration time: 2.59s
                        Total time: 1893.45s
                               ETA: 528488.5s

################################################################################
                    [1m Learning iteration 714/200000 [0m

                       Computation: 3231 steps/s (collection: 0.496s, learning 2.039s)
               Value function loss: 54475.0075
                    Surrogate loss: 0.0190
             Mean action noise std: 0.94
                       Mean reward: 2726.91
               Mean episode length: 245.63
                 Mean success rate: 32.00
                  Mean reward/step: 10.19
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 5857280
                    Iteration time: 2.54s
                        Total time: 1895.99s
                               ETA: 528453.3s

################################################################################
                    [1m Learning iteration 715/200000 [0m

                       Computation: 3135 steps/s (collection: 0.524s, learning 2.089s)
               Value function loss: 38625.7982
                    Surrogate loss: 0.0108
             Mean action noise std: 0.94
                       Mean reward: 2815.94
               Mean episode length: 254.62
                 Mean success rate: 32.50
                  Mean reward/step: 9.81
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 5865472
                    Iteration time: 2.61s
                        Total time: 1898.60s
                               ETA: 528439.8s

################################################################################
                    [1m Learning iteration 716/200000 [0m

                       Computation: 3199 steps/s (collection: 0.505s, learning 2.055s)
               Value function loss: 34159.6484
                    Surrogate loss: 0.0181
             Mean action noise std: 0.94
                       Mean reward: 3000.08
               Mean episode length: 267.36
                 Mean success rate: 34.00
                  Mean reward/step: 9.99
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 5873664
                    Iteration time: 2.56s
                        Total time: 1901.16s
                               ETA: 528411.8s

################################################################################
                    [1m Learning iteration 717/200000 [0m

                       Computation: 3167 steps/s (collection: 0.522s, learning 2.064s)
               Value function loss: 41342.7652
                    Surrogate loss: 0.0109
             Mean action noise std: 0.94
                       Mean reward: 2839.14
               Mean episode length: 263.93
                 Mean success rate: 32.50
                  Mean reward/step: 9.35
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 5881856
                    Iteration time: 2.59s
                        Total time: 1903.75s
                               ETA: 528390.9s

################################################################################
                    [1m Learning iteration 718/200000 [0m

                       Computation: 3234 steps/s (collection: 0.499s, learning 2.033s)
               Value function loss: 27466.4293
                    Surrogate loss: 0.0143
             Mean action noise std: 0.93
                       Mean reward: 2678.91
               Mean episode length: 258.16
                 Mean success rate: 30.50
                  Mean reward/step: 9.71
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 5890048
                    Iteration time: 2.53s
                        Total time: 1906.28s
                               ETA: 528355.2s

################################################################################
                    [1m Learning iteration 719/200000 [0m

                       Computation: 3275 steps/s (collection: 0.464s, learning 2.037s)
               Value function loss: 36652.9377
                    Surrogate loss: 0.0184
             Mean action noise std: 0.93
                       Mean reward: 2954.23
               Mean episode length: 269.36
                 Mean success rate: 34.00
                  Mean reward/step: 10.18
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 2.50s
                        Total time: 1908.78s
                               ETA: 528311.0s

################################################################################
                    [1m Learning iteration 720/200000 [0m

                       Computation: 3153 steps/s (collection: 0.529s, learning 2.068s)
               Value function loss: 40571.0662
                    Surrogate loss: 0.0148
             Mean action noise std: 0.93
                       Mean reward: 2882.20
               Mean episode length: 267.57
                 Mean success rate: 34.00
                  Mean reward/step: 10.06
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 5906432
                    Iteration time: 2.60s
                        Total time: 1911.38s
                               ETA: 528293.6s

################################################################################
                    [1m Learning iteration 721/200000 [0m

                       Computation: 3172 steps/s (collection: 0.521s, learning 2.061s)
               Value function loss: 41560.9194
                    Surrogate loss: 0.0164
             Mean action noise std: 0.93
                       Mean reward: 2705.29
               Mean episode length: 260.40
                 Mean success rate: 30.50
                  Mean reward/step: 9.92
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 5914624
                    Iteration time: 2.58s
                        Total time: 1913.96s
                               ETA: 528272.0s

################################################################################
                    [1m Learning iteration 722/200000 [0m

                       Computation: 3200 steps/s (collection: 0.526s, learning 2.033s)
               Value function loss: 30788.2206
                    Surrogate loss: 0.0202
             Mean action noise std: 0.93
                       Mean reward: 2464.05
               Mean episode length: 243.04
                 Mean success rate: 28.00
                  Mean reward/step: 9.70
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 5922816
                    Iteration time: 2.56s
                        Total time: 1916.52s
                               ETA: 528244.1s

################################################################################
                    [1m Learning iteration 723/200000 [0m

                       Computation: 3237 steps/s (collection: 0.477s, learning 2.054s)
               Value function loss: 41796.8022
                    Surrogate loss: 0.0161
             Mean action noise std: 0.93
                       Mean reward: 2332.61
               Mean episode length: 237.00
                 Mean success rate: 26.50
                  Mean reward/step: 9.79
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 5931008
                    Iteration time: 2.53s
                        Total time: 1919.05s
                               ETA: 528208.3s

################################################################################
                    [1m Learning iteration 724/200000 [0m

                       Computation: 3046 steps/s (collection: 0.574s, learning 2.115s)
               Value function loss: 33693.0069
                    Surrogate loss: 0.0159
             Mean action noise std: 0.93
                       Mean reward: 2306.87
               Mean episode length: 236.89
                 Mean success rate: 26.50
                  Mean reward/step: 10.00
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5939200
                    Iteration time: 2.69s
                        Total time: 1921.74s
                               ETA: 528216.3s

################################################################################
                    [1m Learning iteration 725/200000 [0m

                       Computation: 3159 steps/s (collection: 0.485s, learning 2.108s)
               Value function loss: 29942.6465
                    Surrogate loss: 0.0181
             Mean action noise std: 0.93
                       Mean reward: 2228.57
               Mean episode length: 234.31
                 Mean success rate: 25.50
                  Mean reward/step: 10.87
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 5947392
                    Iteration time: 2.59s
                        Total time: 1924.33s
                               ETA: 528197.8s

################################################################################
                    [1m Learning iteration 726/200000 [0m

                       Computation: 3280 steps/s (collection: 0.475s, learning 2.022s)
               Value function loss: 55725.3715
                    Surrogate loss: 0.0145
             Mean action noise std: 0.93
                       Mean reward: 2400.94
               Mean episode length: 245.60
                 Mean success rate: 26.00
                  Mean reward/step: 10.92
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 5955584
                    Iteration time: 2.50s
                        Total time: 1926.83s
                               ETA: 528153.1s

################################################################################
                    [1m Learning iteration 727/200000 [0m

                       Computation: 3241 steps/s (collection: 0.484s, learning 2.043s)
               Value function loss: 36765.5273
                    Surrogate loss: 0.0156
             Mean action noise std: 0.93
                       Mean reward: 2400.70
               Mean episode length: 248.83
                 Mean success rate: 26.00
                  Mean reward/step: 10.99
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 5963776
                    Iteration time: 2.53s
                        Total time: 1929.36s
                               ETA: 528116.7s

################################################################################
                    [1m Learning iteration 728/200000 [0m

                       Computation: 3250 steps/s (collection: 0.476s, learning 2.044s)
               Value function loss: 56846.5642
                    Surrogate loss: 0.0124
             Mean action noise std: 0.93
                       Mean reward: 2736.68
               Mean episode length: 271.56
                 Mean success rate: 30.50
                  Mean reward/step: 11.03
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5971968
                    Iteration time: 2.52s
                        Total time: 1931.88s
                               ETA: 528078.4s

################################################################################
                    [1m Learning iteration 729/200000 [0m

                       Computation: 3308 steps/s (collection: 0.461s, learning 2.015s)
               Value function loss: 52396.0054
                    Surrogate loss: 0.0117
             Mean action noise std: 0.93
                       Mean reward: 3016.60
               Mean episode length: 287.56
                 Mean success rate: 33.50
                  Mean reward/step: 10.75
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 5980160
                    Iteration time: 2.48s
                        Total time: 1934.35s
                               ETA: 528028.4s

################################################################################
                    [1m Learning iteration 730/200000 [0m

                       Computation: 3254 steps/s (collection: 0.499s, learning 2.018s)
               Value function loss: 44533.0387
                    Surrogate loss: 0.0115
             Mean action noise std: 0.93
                       Mean reward: 3083.85
               Mean episode length: 288.69
                 Mean success rate: 33.50
                  Mean reward/step: 10.13
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 5988352
                    Iteration time: 2.52s
                        Total time: 1936.87s
                               ETA: 527989.6s

################################################################################
                    [1m Learning iteration 731/200000 [0m

                       Computation: 3175 steps/s (collection: 0.480s, learning 2.099s)
               Value function loss: 41298.0617
                    Surrogate loss: 0.0126
             Mean action noise std: 0.93
                       Mean reward: 3243.23
               Mean episode length: 293.38
                 Mean success rate: 34.50
                  Mean reward/step: 10.03
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 2.58s
                        Total time: 1939.45s
                               ETA: 527967.9s

################################################################################
                    [1m Learning iteration 732/200000 [0m

                       Computation: 3126 steps/s (collection: 0.554s, learning 2.066s)
               Value function loss: 41259.4086
                    Surrogate loss: 0.0134
             Mean action noise std: 0.93
                       Mean reward: 3179.12
               Mean episode length: 288.24
                 Mean success rate: 34.50
                  Mean reward/step: 9.61
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 6004736
                    Iteration time: 2.62s
                        Total time: 1942.07s
                               ETA: 527957.3s

################################################################################
                    [1m Learning iteration 733/200000 [0m

                       Computation: 3183 steps/s (collection: 0.483s, learning 2.090s)
               Value function loss: 36911.8161
                    Surrogate loss: 0.0174
             Mean action noise std: 0.93
                       Mean reward: 3147.72
               Mean episode length: 283.89
                 Mean success rate: 35.50
                  Mean reward/step: 9.56
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6012928
                    Iteration time: 2.57s
                        Total time: 1944.64s
                               ETA: 527933.8s

################################################################################
                    [1m Learning iteration 734/200000 [0m

                       Computation: 3173 steps/s (collection: 0.503s, learning 2.078s)
               Value function loss: 28042.2257
                    Surrogate loss: 0.0185
             Mean action noise std: 0.93
                       Mean reward: 2935.72
               Mean episode length: 277.75
                 Mean success rate: 34.00
                  Mean reward/step: 10.05
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6021120
                    Iteration time: 2.58s
                        Total time: 1947.23s
                               ETA: 527912.7s

################################################################################
                    [1m Learning iteration 735/200000 [0m

                       Computation: 3027 steps/s (collection: 0.554s, learning 2.152s)
               Value function loss: 32446.6276
                    Surrogate loss: 0.0162
             Mean action noise std: 0.93
                       Mean reward: 2742.99
               Mean episode length: 261.69
                 Mean success rate: 31.50
                  Mean reward/step: 10.49
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 6029312
                    Iteration time: 2.71s
                        Total time: 1949.93s
                               ETA: 527925.3s

################################################################################
                    [1m Learning iteration 736/200000 [0m

                       Computation: 3094 steps/s (collection: 0.572s, learning 2.075s)
               Value function loss: 58620.5734
                    Surrogate loss: 0.0141
             Mean action noise std: 0.93
                       Mean reward: 2508.25
               Mean episode length: 249.38
                 Mean success rate: 29.50
                  Mean reward/step: 10.67
       Mean episode length/episode: 26.43
--------------------------------------------------------------------------------
                   Total timesteps: 6037504
                    Iteration time: 2.65s
                        Total time: 1952.58s
                               ETA: 527922.2s

################################################################################
                    [1m Learning iteration 737/200000 [0m

                       Computation: 3109 steps/s (collection: 0.584s, learning 2.051s)
               Value function loss: 38469.2458
                    Surrogate loss: 0.0140
             Mean action noise std: 0.93
                       Mean reward: 2569.49
               Mean episode length: 250.76
                 Mean success rate: 31.00
                  Mean reward/step: 10.08
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6045696
                    Iteration time: 2.63s
                        Total time: 1955.21s
                               ETA: 527915.6s

################################################################################
                    [1m Learning iteration 738/200000 [0m

                       Computation: 3113 steps/s (collection: 0.610s, learning 2.022s)
               Value function loss: 57474.3481
                    Surrogate loss: 0.0119
             Mean action noise std: 0.93
                       Mean reward: 2724.34
               Mean episode length: 259.27
                 Mean success rate: 33.00
                  Mean reward/step: 10.14
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 6053888
                    Iteration time: 2.63s
                        Total time: 1957.84s
                               ETA: 527908.1s

################################################################################
                    [1m Learning iteration 739/200000 [0m

                       Computation: 3148 steps/s (collection: 0.529s, learning 2.073s)
               Value function loss: 38738.7764
                    Surrogate loss: 0.0136
             Mean action noise std: 0.93
                       Mean reward: 2587.23
               Mean episode length: 257.65
                 Mean success rate: 31.50
                  Mean reward/step: 9.20
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6062080
                    Iteration time: 2.60s
                        Total time: 1960.45s
                               ETA: 527892.8s

################################################################################
                    [1m Learning iteration 740/200000 [0m

                       Computation: 3186 steps/s (collection: 0.509s, learning 2.062s)
               Value function loss: 42852.1832
                    Surrogate loss: 0.0132
             Mean action noise std: 0.93
                       Mean reward: 2948.07
               Mean episode length: 269.79
                 Mean success rate: 35.50
                  Mean reward/step: 9.43
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 6070272
                    Iteration time: 2.57s
                        Total time: 1963.02s
                               ETA: 527869.0s

################################################################################
                    [1m Learning iteration 741/200000 [0m

                       Computation: 3200 steps/s (collection: 0.521s, learning 2.039s)
               Value function loss: 34669.3207
                    Surrogate loss: 0.0140
             Mean action noise std: 0.93
                       Mean reward: 2846.39
               Mean episode length: 270.12
                 Mean success rate: 34.00
                  Mean reward/step: 9.81
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6078464
                    Iteration time: 2.56s
                        Total time: 1965.58s
                               ETA: 527842.3s

################################################################################
                    [1m Learning iteration 742/200000 [0m

                       Computation: 3154 steps/s (collection: 0.553s, learning 2.044s)
               Value function loss: 37824.8583
                    Surrogate loss: 0.0149
             Mean action noise std: 0.93
                       Mean reward: 2667.08
               Mean episode length: 265.48
                 Mean success rate: 33.00
                  Mean reward/step: 9.62
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 6086656
                    Iteration time: 2.60s
                        Total time: 1968.17s
                               ETA: 527825.7s

################################################################################
                    [1m Learning iteration 743/200000 [0m

                       Computation: 3100 steps/s (collection: 0.541s, learning 2.101s)
               Value function loss: 41621.2672
                    Surrogate loss: 0.0133
             Mean action noise std: 0.93
                       Mean reward: 2608.04
               Mean episode length: 264.31
                 Mean success rate: 32.50
                  Mean reward/step: 10.32
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 2.64s
                        Total time: 1970.82s
                               ETA: 527821.2s

################################################################################
                    [1m Learning iteration 744/200000 [0m

                       Computation: 3137 steps/s (collection: 0.541s, learning 2.071s)
               Value function loss: 36876.0254
                    Surrogate loss: 0.0143
             Mean action noise std: 0.93
                       Mean reward: 2539.06
               Mean episode length: 258.81
                 Mean success rate: 32.00
                  Mean reward/step: 10.01
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 6103040
                    Iteration time: 2.61s
                        Total time: 1973.43s
                               ETA: 527808.5s

################################################################################
                    [1m Learning iteration 745/200000 [0m

                       Computation: 3077 steps/s (collection: 0.548s, learning 2.114s)
               Value function loss: 35295.9621
                    Surrogate loss: 0.0146
             Mean action noise std: 0.93
                       Mean reward: 2329.94
               Mean episode length: 240.50
                 Mean success rate: 28.50
                  Mean reward/step: 10.58
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 6111232
                    Iteration time: 2.66s
                        Total time: 1976.09s
                               ETA: 527809.2s

################################################################################
                    [1m Learning iteration 746/200000 [0m

                       Computation: 3047 steps/s (collection: 0.574s, learning 2.114s)
               Value function loss: 30261.5012
                    Surrogate loss: 0.0149
             Mean action noise std: 0.93
                       Mean reward: 2144.66
               Mean episode length: 235.69
                 Mean success rate: 26.50
                  Mean reward/step: 9.85
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6119424
                    Iteration time: 2.69s
                        Total time: 1978.78s
                               ETA: 527816.9s

################################################################################
                    [1m Learning iteration 747/200000 [0m

                       Computation: 3138 steps/s (collection: 0.546s, learning 2.064s)
               Value function loss: 45411.8012
                    Surrogate loss: 0.0126
             Mean action noise std: 0.93
                       Mean reward: 2365.18
               Mean episode length: 239.02
                 Mean success rate: 29.00
                  Mean reward/step: 9.39
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 6127616
                    Iteration time: 2.61s
                        Total time: 1981.39s
                               ETA: 527804.0s

################################################################################
                    [1m Learning iteration 748/200000 [0m

                       Computation: 2950 steps/s (collection: 0.631s, learning 2.145s)
               Value function loss: 34735.6497
                    Surrogate loss: 0.0155
             Mean action noise std: 0.93
                       Mean reward: 2408.11
               Mean episode length: 237.88
                 Mean success rate: 29.50
                  Mean reward/step: 9.44
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 6135808
                    Iteration time: 2.78s
                        Total time: 1984.16s
                               ETA: 527835.3s

################################################################################
                    [1m Learning iteration 749/200000 [0m

                       Computation: 3103 steps/s (collection: 0.509s, learning 2.130s)
               Value function loss: 32961.5109
                    Surrogate loss: 0.0186
             Mean action noise std: 0.93
                       Mean reward: 2356.33
               Mean episode length: 234.73
                 Mean success rate: 30.00
                  Mean reward/step: 9.55
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 6144000
                    Iteration time: 2.64s
                        Total time: 1986.80s
                               ETA: 527830.2s

################################################################################
                    [1m Learning iteration 750/200000 [0m

                       Computation: 3130 steps/s (collection: 0.536s, learning 2.081s)
               Value function loss: 16337.0663
                    Surrogate loss: 0.0188
             Mean action noise std: 0.93
                       Mean reward: 2121.69
               Mean episode length: 225.12
                 Mean success rate: 27.00
                  Mean reward/step: 9.97
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6152192
                    Iteration time: 2.62s
                        Total time: 1989.42s
                               ETA: 527819.1s

################################################################################
                    [1m Learning iteration 751/200000 [0m

                       Computation: 3152 steps/s (collection: 0.500s, learning 2.099s)
               Value function loss: 29913.6317
                    Surrogate loss: 0.0129
             Mean action noise std: 0.93
                       Mean reward: 1997.05
               Mean episode length: 226.49
                 Mean success rate: 26.50
                  Mean reward/step: 10.26
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 6160384
                    Iteration time: 2.60s
                        Total time: 1992.02s
                               ETA: 527803.2s

################################################################################
                    [1m Learning iteration 752/200000 [0m

                       Computation: 3153 steps/s (collection: 0.500s, learning 2.097s)
               Value function loss: 49470.6723
                    Surrogate loss: 0.0134
             Mean action noise std: 0.93
                       Mean reward: 1950.98
               Mean episode length: 227.42
                 Mean success rate: 25.50
                  Mean reward/step: 9.96
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 6168576
                    Iteration time: 2.60s
                        Total time: 1994.62s
                               ETA: 527786.9s

################################################################################
                    [1m Learning iteration 753/200000 [0m

                       Computation: 2970 steps/s (collection: 0.579s, learning 2.179s)
               Value function loss: 37197.2800
                    Surrogate loss: 0.0130
             Mean action noise std: 0.93
                       Mean reward: 1982.68
               Mean episode length: 226.28
                 Mean success rate: 25.50
                  Mean reward/step: 9.84
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 6176768
                    Iteration time: 2.76s
                        Total time: 1997.37s
                               ETA: 527812.9s

################################################################################
                    [1m Learning iteration 754/200000 [0m

                       Computation: 3072 steps/s (collection: 0.586s, learning 2.080s)
               Value function loss: 42796.9942
                    Surrogate loss: 0.0113
             Mean action noise std: 0.93
                       Mean reward: 2056.78
               Mean episode length: 222.94
                 Mean success rate: 24.00
                  Mean reward/step: 9.44
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 6184960
                    Iteration time: 2.67s
                        Total time: 2000.04s
                               ETA: 527814.7s

################################################################################
                    [1m Learning iteration 755/200000 [0m

                       Computation: 3114 steps/s (collection: 0.549s, learning 2.081s)
               Value function loss: 34468.4098
                    Surrogate loss: 0.0137
             Mean action noise std: 0.93
                       Mean reward: 2284.95
               Mean episode length: 227.00
                 Mean success rate: 26.00
                  Mean reward/step: 9.57
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 2.63s
                        Total time: 2002.67s
                               ETA: 527807.0s

################################################################################
                    [1m Learning iteration 756/200000 [0m

                       Computation: 3058 steps/s (collection: 0.554s, learning 2.125s)
               Value function loss: 40566.8394
                    Surrogate loss: 0.0109
             Mean action noise std: 0.93
                       Mean reward: 2384.48
               Mean episode length: 226.49
                 Mean success rate: 26.50
                  Mean reward/step: 10.06
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 6201344
                    Iteration time: 2.68s
                        Total time: 2005.35s
                               ETA: 527812.2s

################################################################################
                    [1m Learning iteration 757/200000 [0m

                       Computation: 3156 steps/s (collection: 0.512s, learning 2.084s)
               Value function loss: 39079.4775
                    Surrogate loss: 0.0139
             Mean action noise std: 0.93
                       Mean reward: 2362.84
               Mean episode length: 225.06
                 Mean success rate: 25.50
                  Mean reward/step: 9.23
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 6209536
                    Iteration time: 2.60s
                        Total time: 2007.94s
                               ETA: 527795.4s

################################################################################
                    [1m Learning iteration 758/200000 [0m

                       Computation: 3137 steps/s (collection: 0.542s, learning 2.069s)
               Value function loss: 30730.3039
                    Surrogate loss: 0.0147
             Mean action noise std: 0.93
                       Mean reward: 2355.77
               Mean episode length: 223.44
                 Mean success rate: 25.50
                  Mean reward/step: 9.33
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6217728
                    Iteration time: 2.61s
                        Total time: 2010.56s
                               ETA: 527782.8s

################################################################################
                    [1m Learning iteration 759/200000 [0m

                       Computation: 3115 steps/s (collection: 0.541s, learning 2.089s)
               Value function loss: 40089.9257
                    Surrogate loss: 0.0153
             Mean action noise std: 0.93
                       Mean reward: 2026.07
               Mean episode length: 213.13
                 Mean success rate: 22.50
                  Mean reward/step: 9.20
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 6225920
                    Iteration time: 2.63s
                        Total time: 2013.19s
                               ETA: 527775.0s

################################################################################
                    [1m Learning iteration 760/200000 [0m

                       Computation: 3175 steps/s (collection: 0.522s, learning 2.058s)
               Value function loss: 35102.3393
                    Surrogate loss: 0.0150
             Mean action noise std: 0.93
                       Mean reward: 2091.68
               Mean episode length: 217.22
                 Mean success rate: 22.50
                  Mean reward/step: 9.60
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 6234112
                    Iteration time: 2.58s
                        Total time: 2015.76s
                               ETA: 527754.3s

################################################################################
                    [1m Learning iteration 761/200000 [0m

                       Computation: 3205 steps/s (collection: 0.512s, learning 2.044s)
               Value function loss: 38989.8685
                    Surrogate loss: 0.0130
             Mean action noise std: 0.93
                       Mean reward: 2132.82
               Mean episode length: 221.82
                 Mean success rate: 23.00
                  Mean reward/step: 9.18
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 6242304
                    Iteration time: 2.56s
                        Total time: 2018.32s
                               ETA: 527727.3s

################################################################################
                    [1m Learning iteration 762/200000 [0m

                       Computation: 3186 steps/s (collection: 0.552s, learning 2.019s)
               Value function loss: 42580.9176
                    Surrogate loss: 0.0163
             Mean action noise std: 0.93
                       Mean reward: 1854.93
               Mean episode length: 206.42
                 Mean success rate: 20.00
                  Mean reward/step: 9.87
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 6250496
                    Iteration time: 2.57s
                        Total time: 2020.89s
                               ETA: 527704.2s

################################################################################
                    [1m Learning iteration 763/200000 [0m

                       Computation: 3223 steps/s (collection: 0.503s, learning 2.038s)
               Value function loss: 40123.9054
                    Surrogate loss: 0.0121
             Mean action noise std: 0.93
                       Mean reward: 1993.39
               Mean episode length: 207.77
                 Mean success rate: 21.50
                  Mean reward/step: 10.02
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 6258688
                    Iteration time: 2.54s
                        Total time: 2023.43s
                               ETA: 527673.5s

################################################################################
                    [1m Learning iteration 764/200000 [0m

                       Computation: 3158 steps/s (collection: 0.520s, learning 2.074s)
               Value function loss: 36633.5609
                    Surrogate loss: 0.0178
             Mean action noise std: 0.93
                       Mean reward: 1948.43
               Mean episode length: 204.69
                 Mean success rate: 21.00
                  Mean reward/step: 10.22
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6266880
                    Iteration time: 2.59s
                        Total time: 2026.03s
                               ETA: 527656.7s

################################################################################
                    [1m Learning iteration 765/200000 [0m

                       Computation: 3213 steps/s (collection: 0.502s, learning 2.047s)
               Value function loss: 39706.2908
                    Surrogate loss: 0.0135
             Mean action noise std: 0.93
                       Mean reward: 1960.81
               Mean episode length: 206.34
                 Mean success rate: 21.50
                  Mean reward/step: 9.88
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 6275072
                    Iteration time: 2.55s
                        Total time: 2028.58s
                               ETA: 527628.1s

################################################################################
                    [1m Learning iteration 766/200000 [0m

                       Computation: 3125 steps/s (collection: 0.557s, learning 2.064s)
               Value function loss: 35902.9668
                    Surrogate loss: 0.0127
             Mean action noise std: 0.93
                       Mean reward: 1969.17
               Mean episode length: 207.85
                 Mean success rate: 23.50
                  Mean reward/step: 9.64
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 6283264
                    Iteration time: 2.62s
                        Total time: 2031.20s
                               ETA: 527618.4s

################################################################################
                    [1m Learning iteration 767/200000 [0m

                       Computation: 3199 steps/s (collection: 0.552s, learning 2.009s)
               Value function loss: 57746.6674
                    Surrogate loss: 0.0132
             Mean action noise std: 0.93
                       Mean reward: 2141.31
               Mean episode length: 221.24
                 Mean success rate: 26.50
                  Mean reward/step: 9.61
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 2.56s
                        Total time: 2033.76s
                               ETA: 527592.9s

################################################################################
                    [1m Learning iteration 768/200000 [0m

                       Computation: 3183 steps/s (collection: 0.527s, learning 2.047s)
               Value function loss: 57815.2568
                    Surrogate loss: 0.0136
             Mean action noise std: 0.93
                       Mean reward: 2204.48
               Mean episode length: 227.93
                 Mean success rate: 26.50
                  Mean reward/step: 8.66
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 6299648
                    Iteration time: 2.57s
                        Total time: 2036.33s
                               ETA: 527570.9s

################################################################################
                    [1m Learning iteration 769/200000 [0m

                       Computation: 3204 steps/s (collection: 0.499s, learning 2.057s)
               Value function loss: 36194.5112
                    Surrogate loss: 0.0169
             Mean action noise std: 0.93
                       Mean reward: 2144.13
               Mean episode length: 226.62
                 Mean success rate: 27.00
                  Mean reward/step: 8.78
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 6307840
                    Iteration time: 2.56s
                        Total time: 2038.89s
                               ETA: 527544.5s

################################################################################
                    [1m Learning iteration 770/200000 [0m

                       Computation: 3061 steps/s (collection: 0.561s, learning 2.114s)
               Value function loss: 38406.6303
                    Surrogate loss: 0.0179
             Mean action noise std: 0.93
                       Mean reward: 2211.82
               Mean episode length: 225.06
                 Mean success rate: 28.50
                  Mean reward/step: 9.41
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 6316032
                    Iteration time: 2.68s
                        Total time: 2041.56s
                               ETA: 527549.0s

################################################################################
                    [1m Learning iteration 771/200000 [0m

                       Computation: 3158 steps/s (collection: 0.513s, learning 2.081s)
               Value function loss: 40822.7058
                    Surrogate loss: 0.0137
             Mean action noise std: 0.93
                       Mean reward: 2050.48
               Mean episode length: 216.91
                 Mean success rate: 27.00
                  Mean reward/step: 9.78
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 6324224
                    Iteration time: 2.59s
                        Total time: 2044.16s
                               ETA: 527532.4s

################################################################################
                    [1m Learning iteration 772/200000 [0m

                       Computation: 3152 steps/s (collection: 0.523s, learning 2.076s)
               Value function loss: 43798.6969
                    Surrogate loss: 0.0138
             Mean action noise std: 0.93
                       Mean reward: 2194.63
               Mean episode length: 216.22
                 Mean success rate: 27.50
                  Mean reward/step: 9.77
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 6332416
                    Iteration time: 2.60s
                        Total time: 2046.75s
                               ETA: 527517.1s

################################################################################
                    [1m Learning iteration 773/200000 [0m

                       Computation: 3186 steps/s (collection: 0.482s, learning 2.088s)
               Value function loss: 37412.1902
                    Surrogate loss: 0.0187
             Mean action noise std: 0.93
                       Mean reward: 2005.85
               Mean episode length: 203.33
                 Mean success rate: 25.00
                  Mean reward/step: 10.25
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6340608
                    Iteration time: 2.57s
                        Total time: 2049.32s
                               ETA: 527494.6s

################################################################################
                    [1m Learning iteration 774/200000 [0m

                       Computation: 3180 steps/s (collection: 0.493s, learning 2.083s)
               Value function loss: 35131.8023
                    Surrogate loss: 0.0156
             Mean action noise std: 0.93
                       Mean reward: 2076.73
               Mean episode length: 207.96
                 Mean success rate: 25.00
                  Mean reward/step: 10.59
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 6348800
                    Iteration time: 2.58s
                        Total time: 2051.90s
                               ETA: 527473.5s

################################################################################
                    [1m Learning iteration 775/200000 [0m

                       Computation: 3135 steps/s (collection: 0.517s, learning 2.096s)
               Value function loss: 31344.1584
                    Surrogate loss: 0.0145
             Mean action noise std: 0.93
                       Mean reward: 1989.49
               Mean episode length: 204.79
                 Mean success rate: 24.50
                  Mean reward/step: 10.74
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 6356992
                    Iteration time: 2.61s
                        Total time: 2054.51s
                               ETA: 527462.0s

################################################################################
                    [1m Learning iteration 776/200000 [0m

                       Computation: 3092 steps/s (collection: 0.525s, learning 2.123s)
               Value function loss: 42087.3094
                    Surrogate loss: 0.0137
             Mean action noise std: 0.93
                       Mean reward: 1951.95
               Mean episode length: 198.91
                 Mean success rate: 23.00
                  Mean reward/step: 11.25
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6365184
                    Iteration time: 2.65s
                        Total time: 2057.16s
                               ETA: 527459.6s

################################################################################
                    [1m Learning iteration 777/200000 [0m

                       Computation: 3151 steps/s (collection: 0.521s, learning 2.079s)
               Value function loss: 41630.3505
                    Surrogate loss: 0.0144
             Mean action noise std: 0.93
                       Mean reward: 2019.19
               Mean episode length: 207.09
                 Mean success rate: 23.50
                  Mean reward/step: 11.81
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6373376
                    Iteration time: 2.60s
                        Total time: 2059.76s
                               ETA: 527444.7s

################################################################################
                    [1m Learning iteration 778/200000 [0m

                       Computation: 3124 steps/s (collection: 0.556s, learning 2.066s)
               Value function loss: 43808.8134
                    Surrogate loss: 0.0155
             Mean action noise std: 0.93
                       Mean reward: 1985.79
               Mean episode length: 214.70
                 Mean success rate: 24.00
                  Mean reward/step: 12.00
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6381568
                    Iteration time: 2.62s
                        Total time: 2062.38s
                               ETA: 527435.5s

################################################################################
                    [1m Learning iteration 779/200000 [0m

                       Computation: 3208 steps/s (collection: 0.519s, learning 2.034s)
               Value function loss: 35679.9112
                    Surrogate loss: 0.0136
             Mean action noise std: 0.93
                       Mean reward: 1927.70
               Mean episode length: 215.00
                 Mean success rate: 25.50
                  Mean reward/step: 11.59
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 2.55s
                        Total time: 2064.94s
                               ETA: 527408.8s

################################################################################
                    [1m Learning iteration 780/200000 [0m

                       Computation: 3246 steps/s (collection: 0.502s, learning 2.022s)
               Value function loss: 44645.4592
                    Surrogate loss: 0.0125
             Mean action noise std: 0.92
                       Mean reward: 1876.79
               Mean episode length: 208.70
                 Mean success rate: 24.50
                  Mean reward/step: 12.13
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6397952
                    Iteration time: 2.52s
                        Total time: 2067.46s
                               ETA: 527374.5s

################################################################################
                    [1m Learning iteration 781/200000 [0m

                       Computation: 3217 steps/s (collection: 0.503s, learning 2.043s)
               Value function loss: 44500.5390
                    Surrogate loss: 0.0141
             Mean action noise std: 0.92
                       Mean reward: 2027.89
               Mean episode length: 217.60
                 Mean success rate: 26.50
                  Mean reward/step: 11.94
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 6406144
                    Iteration time: 2.55s
                        Total time: 2070.01s
                               ETA: 527346.1s

################################################################################
                    [1m Learning iteration 782/200000 [0m

                       Computation: 3149 steps/s (collection: 0.544s, learning 2.057s)
               Value function loss: 70787.4817
                    Surrogate loss: 0.0128
             Mean action noise std: 0.92
                       Mean reward: 2340.93
               Mean episode length: 234.45
                 Mean success rate: 29.50
                  Mean reward/step: 11.92
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 6414336
                    Iteration time: 2.60s
                        Total time: 2072.61s
                               ETA: 527331.8s

################################################################################
                    [1m Learning iteration 783/200000 [0m

                       Computation: 3272 steps/s (collection: 0.472s, learning 2.032s)
               Value function loss: 66829.0659
                    Surrogate loss: 0.0134
             Mean action noise std: 0.92
                       Mean reward: 2631.86
               Mean episode length: 241.84
                 Mean success rate: 32.00
                  Mean reward/step: 10.79
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 6422528
                    Iteration time: 2.50s
                        Total time: 2075.11s
                               ETA: 527292.6s

################################################################################
                    [1m Learning iteration 784/200000 [0m

                       Computation: 3227 steps/s (collection: 0.474s, learning 2.064s)
               Value function loss: 35332.9619
                    Surrogate loss: 0.0156
             Mean action noise std: 0.92
                       Mean reward: 2808.25
               Mean episode length: 244.49
                 Mean success rate: 34.50
                  Mean reward/step: 10.32
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 6430720
                    Iteration time: 2.54s
                        Total time: 2077.65s
                               ETA: 527262.3s

################################################################################
                    [1m Learning iteration 785/200000 [0m

                       Computation: 3180 steps/s (collection: 0.528s, learning 2.048s)
               Value function loss: 56480.2937
                    Surrogate loss: 0.0125
             Mean action noise std: 0.92
                       Mean reward: 3011.93
               Mean episode length: 247.74
                 Mean success rate: 34.50
                  Mean reward/step: 10.87
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 6438912
                    Iteration time: 2.58s
                        Total time: 2080.22s
                               ETA: 527241.7s

################################################################################
                    [1m Learning iteration 786/200000 [0m

                       Computation: 3241 steps/s (collection: 0.480s, learning 2.047s)
               Value function loss: 51691.9806
                    Surrogate loss: 0.0133
             Mean action noise std: 0.92
                       Mean reward: 3148.27
               Mean episode length: 255.38
                 Mean success rate: 36.50
                  Mean reward/step: 11.42
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 6447104
                    Iteration time: 2.53s
                        Total time: 2082.75s
                               ETA: 527208.9s

################################################################################
                    [1m Learning iteration 787/200000 [0m

                       Computation: 3180 steps/s (collection: 0.513s, learning 2.062s)
               Value function loss: 51385.3867
                    Surrogate loss: 0.0125
             Mean action noise std: 0.92
                       Mean reward: 3459.31
               Mean episode length: 277.06
                 Mean success rate: 40.50
                  Mean reward/step: 11.86
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 6455296
                    Iteration time: 2.58s
                        Total time: 2085.33s
                               ETA: 527188.3s

################################################################################
                    [1m Learning iteration 788/200000 [0m

                       Computation: 3179 steps/s (collection: 0.521s, learning 2.056s)
               Value function loss: 46652.6267
                    Surrogate loss: 0.0120
             Mean action noise std: 0.92
                       Mean reward: 3018.19
               Mean episode length: 256.15
                 Mean success rate: 36.50
                  Mean reward/step: 11.64
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 6463488
                    Iteration time: 2.58s
                        Total time: 2087.90s
                               ETA: 527168.1s

################################################################################
                    [1m Learning iteration 789/200000 [0m

                       Computation: 3140 steps/s (collection: 0.528s, learning 2.080s)
               Value function loss: 47734.3299
                    Surrogate loss: 0.0126
             Mean action noise std: 0.92
                       Mean reward: 3009.54
               Mean episode length: 249.75
                 Mean success rate: 35.50
                  Mean reward/step: 11.77
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6471680
                    Iteration time: 2.61s
                        Total time: 2090.51s
                               ETA: 527155.9s

################################################################################
                    [1m Learning iteration 790/200000 [0m

                       Computation: 3125 steps/s (collection: 0.524s, learning 2.097s)
               Value function loss: 51942.9829
                    Surrogate loss: 0.0123
             Mean action noise std: 0.92
                       Mean reward: 2641.17
               Mean episode length: 234.33
                 Mean success rate: 31.00
                  Mean reward/step: 11.84
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 6479872
                    Iteration time: 2.62s
                        Total time: 2093.13s
                               ETA: 527146.8s

################################################################################
                    [1m Learning iteration 791/200000 [0m

                       Computation: 3192 steps/s (collection: 0.507s, learning 2.059s)
               Value function loss: 35103.4796
                    Surrogate loss: 0.0129
             Mean action noise std: 0.92
                       Mean reward: 2538.63
               Mean episode length: 231.31
                 Mean success rate: 30.50
                  Mean reward/step: 10.91
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 2.57s
                        Total time: 2095.70s
                               ETA: 527123.9s

################################################################################
                    [1m Learning iteration 792/200000 [0m

                       Computation: 3239 steps/s (collection: 0.488s, learning 2.041s)
               Value function loss: 37778.9076
                    Surrogate loss: 0.0182
             Mean action noise std: 0.92
                       Mean reward: 2406.22
               Mean episode length: 219.47
                 Mean success rate: 28.50
                  Mean reward/step: 11.12
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 6496256
                    Iteration time: 2.53s
                        Total time: 2098.23s
                               ETA: 527091.7s

################################################################################
                    [1m Learning iteration 793/200000 [0m

                       Computation: 3139 steps/s (collection: 0.500s, learning 2.109s)
               Value function loss: 41528.0898
                    Surrogate loss: 0.0167
             Mean action noise std: 0.92
                       Mean reward: 2193.85
               Mean episode length: 210.07
                 Mean success rate: 27.50
                  Mean reward/step: 10.85
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 6504448
                    Iteration time: 2.61s
                        Total time: 2100.84s
                               ETA: 527079.9s

################################################################################
                    [1m Learning iteration 794/200000 [0m

                       Computation: 3114 steps/s (collection: 0.542s, learning 2.088s)
               Value function loss: 47953.8070
                    Surrogate loss: 0.0134
             Mean action noise std: 0.92
                       Mean reward: 2253.89
               Mean episode length: 212.81
                 Mean success rate: 28.50
                  Mean reward/step: 10.26
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 6512640
                    Iteration time: 2.63s
                        Total time: 2103.47s
                               ETA: 527073.3s

################################################################################
                    [1m Learning iteration 795/200000 [0m

                       Computation: 3238 steps/s (collection: 0.476s, learning 2.053s)
               Value function loss: 39430.2484
                    Surrogate loss: 0.0119
             Mean action noise std: 0.92
                       Mean reward: 2050.76
               Mean episode length: 204.04
                 Mean success rate: 26.50
                  Mean reward/step: 10.05
       Mean episode length/episode: 26.09
--------------------------------------------------------------------------------
                   Total timesteps: 6520832
                    Iteration time: 2.53s
                        Total time: 2106.00s
                               ETA: 527041.5s

################################################################################
                    [1m Learning iteration 796/200000 [0m

                       Computation: 3255 steps/s (collection: 0.478s, learning 2.038s)
               Value function loss: 37664.1995
                    Surrogate loss: 0.0149
             Mean action noise std: 0.92
                       Mean reward: 1843.13
               Mean episode length: 186.12
                 Mean success rate: 24.50
                  Mean reward/step: 10.13
       Mean episode length/episode: 25.76
--------------------------------------------------------------------------------
                   Total timesteps: 6529024
                    Iteration time: 2.52s
                        Total time: 2108.51s
                               ETA: 527006.5s

################################################################################
                    [1m Learning iteration 797/200000 [0m

                       Computation: 3259 steps/s (collection: 0.498s, learning 2.016s)
               Value function loss: 28499.3702
                    Surrogate loss: 0.0160
             Mean action noise std: 0.92
                       Mean reward: 1635.97
               Mean episode length: 175.90
                 Mean success rate: 20.50
                  Mean reward/step: 10.07
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 6537216
                    Iteration time: 2.51s
                        Total time: 2111.03s
                               ETA: 526970.9s

################################################################################
                    [1m Learning iteration 798/200000 [0m

                       Computation: 3165 steps/s (collection: 0.521s, learning 2.067s)
               Value function loss: 61233.8824
                    Surrogate loss: 0.0151
             Mean action noise std: 0.92
                       Mean reward: 1683.54
               Mean episode length: 169.15
                 Mean success rate: 20.00
                  Mean reward/step: 10.34
       Mean episode length/episode: 26.09
--------------------------------------------------------------------------------
                   Total timesteps: 6545408
                    Iteration time: 2.59s
                        Total time: 2113.61s
                               ETA: 526953.9s

################################################################################
                    [1m Learning iteration 799/200000 [0m

                       Computation: 3162 steps/s (collection: 0.494s, learning 2.096s)
               Value function loss: 49592.3910
                    Surrogate loss: 0.0156
             Mean action noise std: 0.92
                       Mean reward: 2198.00
               Mean episode length: 192.26
                 Mean success rate: 26.50
                  Mean reward/step: 9.60
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 6553600
                    Iteration time: 2.59s
                        Total time: 2116.20s
                               ETA: 526937.5s

################################################################################
                    [1m Learning iteration 800/200000 [0m

                       Computation: 3188 steps/s (collection: 0.495s, learning 2.074s)
               Value function loss: 30042.3609
                    Surrogate loss: 0.0136
             Mean action noise std: 0.92
                       Mean reward: 2305.55
               Mean episode length: 192.94
                 Mean success rate: 26.00
                  Mean reward/step: 9.24
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6561792
                    Iteration time: 2.57s
                        Total time: 2118.77s
                               ETA: 526916.0s

################################################################################
                    [1m Learning iteration 801/200000 [0m

                       Computation: 3092 steps/s (collection: 0.568s, learning 2.081s)
               Value function loss: 49057.2838
                    Surrogate loss: 0.0118
             Mean action noise std: 0.92
                       Mean reward: 2277.25
               Mean episode length: 186.75
                 Mean success rate: 25.00
                  Mean reward/step: 9.31
       Mean episode length/episode: 24.82
--------------------------------------------------------------------------------
                   Total timesteps: 6569984
                    Iteration time: 2.65s
                        Total time: 2121.42s
                               ETA: 526914.2s

################################################################################
                    [1m Learning iteration 802/200000 [0m

                       Computation: 3136 steps/s (collection: 0.552s, learning 2.059s)
               Value function loss: 43930.1033
                    Surrogate loss: 0.0116
             Mean action noise std: 0.92
                       Mean reward: 2149.45
               Mean episode length: 181.68
                 Mean success rate: 23.50
                  Mean reward/step: 9.26
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 6578176
                    Iteration time: 2.61s
                        Total time: 2124.03s
                               ETA: 526903.2s

################################################################################
                    [1m Learning iteration 803/200000 [0m

                       Computation: 3218 steps/s (collection: 0.481s, learning 2.064s)
               Value function loss: 38006.4895
                    Surrogate loss: 0.0161
             Mean action noise std: 0.92
                       Mean reward: 1582.49
               Mean episode length: 160.49
                 Mean success rate: 17.50
                  Mean reward/step: 8.98
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 2.55s
                        Total time: 2126.58s
                               ETA: 526875.8s

################################################################################
                    [1m Learning iteration 804/200000 [0m

                       Computation: 3242 steps/s (collection: 0.484s, learning 2.043s)
               Value function loss: 31069.4771
                    Surrogate loss: 0.0160
             Mean action noise std: 0.92
                       Mean reward: 1598.82
               Mean episode length: 164.24
                 Mean success rate: 19.00
                  Mean reward/step: 9.18
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 6594560
                    Iteration time: 2.53s
                        Total time: 2129.11s
                               ETA: 526843.9s

################################################################################
                    [1m Learning iteration 805/200000 [0m

                       Computation: 3114 steps/s (collection: 0.563s, learning 2.068s)
               Value function loss: 31570.2895
                    Surrogate loss: 0.0178
             Mean action noise std: 0.92
                       Mean reward: 1746.08
               Mean episode length: 170.93
                 Mean success rate: 21.00
                  Mean reward/step: 9.88
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 6602752
                    Iteration time: 2.63s
                        Total time: 2131.74s
                               ETA: 526837.6s

################################################################################
                    [1m Learning iteration 806/200000 [0m

                       Computation: 3133 steps/s (collection: 0.552s, learning 2.062s)
               Value function loss: 49290.8606
                    Surrogate loss: 0.0153
             Mean action noise std: 0.92
                       Mean reward: 2007.79
               Mean episode length: 181.47
                 Mean success rate: 24.00
                  Mean reward/step: 9.95
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6610944
                    Iteration time: 2.61s
                        Total time: 2134.35s
                               ETA: 526827.4s

################################################################################
                    [1m Learning iteration 807/200000 [0m

                       Computation: 3249 steps/s (collection: 0.492s, learning 2.029s)
               Value function loss: 22277.4530
                    Surrogate loss: 0.0215
             Mean action noise std: 0.92
                       Mean reward: 1783.34
               Mean episode length: 179.01
                 Mean success rate: 22.50
                  Mean reward/step: 9.32
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 6619136
                    Iteration time: 2.52s
                        Total time: 2136.87s
                               ETA: 526794.3s

################################################################################
                    [1m Learning iteration 808/200000 [0m

                       Computation: 3203 steps/s (collection: 0.508s, learning 2.049s)
               Value function loss: 38234.1644
                    Surrogate loss: 0.0280
             Mean action noise std: 0.92
                       Mean reward: 1790.44
               Mean episode length: 180.67
                 Mean success rate: 21.50
                  Mean reward/step: 9.18
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 6627328
                    Iteration time: 2.56s
                        Total time: 2139.43s
                               ETA: 526770.1s

################################################################################
                    [1m Learning iteration 809/200000 [0m

                       Computation: 3176 steps/s (collection: 0.533s, learning 2.045s)
               Value function loss: 36315.9235
                    Surrogate loss: 0.0130
             Mean action noise std: 0.92
                       Mean reward: 1842.89
               Mean episode length: 181.28
                 Mean success rate: 21.50
                  Mean reward/step: 9.26
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6635520
                    Iteration time: 2.58s
                        Total time: 2142.01s
                               ETA: 526751.3s

################################################################################
                    [1m Learning iteration 810/200000 [0m

                       Computation: 3239 steps/s (collection: 0.498s, learning 2.031s)
               Value function loss: 42083.1912
                    Surrogate loss: 0.0112
             Mean action noise std: 0.92
                       Mean reward: 2075.45
               Mean episode length: 192.47
                 Mean success rate: 24.00
                  Mean reward/step: 9.27
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 6643712
                    Iteration time: 2.53s
                        Total time: 2144.54s
                               ETA: 526720.2s

################################################################################
                    [1m Learning iteration 811/200000 [0m

                       Computation: 3209 steps/s (collection: 0.531s, learning 2.022s)
               Value function loss: 35487.3534
                    Surrogate loss: 0.0153
             Mean action noise std: 0.92
                       Mean reward: 1775.41
               Mean episode length: 190.95
                 Mean success rate: 22.00
                  Mean reward/step: 9.30
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 6651904
                    Iteration time: 2.55s
                        Total time: 2147.09s
                               ETA: 526695.0s

################################################################################
                    [1m Learning iteration 812/200000 [0m

                       Computation: 3179 steps/s (collection: 0.542s, learning 2.035s)
               Value function loss: 30400.8251
                    Surrogate loss: 0.0241
             Mean action noise std: 0.92
                       Mean reward: 1848.60
               Mean episode length: 201.88
                 Mean success rate: 23.00
                  Mean reward/step: 9.78
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 6660096
                    Iteration time: 2.58s
                        Total time: 2149.66s
                               ETA: 526675.8s

################################################################################
                    [1m Learning iteration 813/200000 [0m

                       Computation: 3091 steps/s (collection: 0.567s, learning 2.083s)
               Value function loss: 28469.0850
                    Surrogate loss: 0.0147
             Mean action noise std: 0.92
                       Mean reward: 1987.65
               Mean episode length: 203.38
                 Mean success rate: 25.00
                  Mean reward/step: 9.98
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 6668288
                    Iteration time: 2.65s
                        Total time: 2152.31s
                               ETA: 526674.6s

################################################################################
                    [1m Learning iteration 814/200000 [0m

                       Computation: 3214 steps/s (collection: 0.500s, learning 2.049s)
               Value function loss: 46015.5510
                    Surrogate loss: 0.0140
             Mean action noise std: 0.92
                       Mean reward: 1883.03
               Mean episode length: 204.68
                 Mean success rate: 24.00
                  Mean reward/step: 9.78
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 6676480
                    Iteration time: 2.55s
                        Total time: 2154.86s
                               ETA: 526648.6s

################################################################################
                    [1m Learning iteration 815/200000 [0m

                       Computation: 3237 steps/s (collection: 0.509s, learning 2.022s)
               Value function loss: 32132.2275
                    Surrogate loss: 0.0145
             Mean action noise std: 0.92
                       Mean reward: 1723.49
               Mean episode length: 200.49
                 Mean success rate: 21.50
                  Mean reward/step: 9.46
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 2.53s
                        Total time: 2157.39s
                               ETA: 526618.2s

################################################################################
                    [1m Learning iteration 816/200000 [0m

                       Computation: 3150 steps/s (collection: 0.533s, learning 2.068s)
               Value function loss: 57550.3064
                    Surrogate loss: 0.0120
             Mean action noise std: 0.92
                       Mean reward: 1831.71
               Mean episode length: 196.58
                 Mean success rate: 23.50
                  Mean reward/step: 9.51
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 6692864
                    Iteration time: 2.60s
                        Total time: 2159.99s
                               ETA: 526604.9s

################################################################################
                    [1m Learning iteration 817/200000 [0m

                       Computation: 3052 steps/s (collection: 0.595s, learning 2.088s)
               Value function loss: 35315.1229
                    Surrogate loss: 0.0132
             Mean action noise std: 0.92
                       Mean reward: 2082.82
               Mean episode length: 209.57
                 Mean success rate: 25.00
                  Mean reward/step: 8.77
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 6701056
                    Iteration time: 2.68s
                        Total time: 2162.68s
                               ETA: 526611.9s

################################################################################
                    [1m Learning iteration 818/200000 [0m

                       Computation: 3271 steps/s (collection: 0.488s, learning 2.016s)
               Value function loss: 29632.9507
                    Surrogate loss: 0.0159
             Mean action noise std: 0.92
                       Mean reward: 2115.30
               Mean episode length: 219.85
                 Mean success rate: 26.50
                  Mean reward/step: 8.45
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 6709248
                    Iteration time: 2.50s
                        Total time: 2165.18s
                               ETA: 526575.2s

################################################################################
                    [1m Learning iteration 819/200000 [0m

                       Computation: 3201 steps/s (collection: 0.533s, learning 2.026s)
               Value function loss: 37886.7216
                    Surrogate loss: 0.0164
             Mean action noise std: 0.92
                       Mean reward: 2155.72
               Mean episode length: 220.53
                 Mean success rate: 27.50
                  Mean reward/step: 9.00
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 6717440
                    Iteration time: 2.56s
                        Total time: 2167.74s
                               ETA: 526552.0s

################################################################################
                    [1m Learning iteration 820/200000 [0m

                       Computation: 3176 steps/s (collection: 0.520s, learning 2.059s)
               Value function loss: 30800.8389
                    Surrogate loss: 0.0147
             Mean action noise std: 0.92
                       Mean reward: 2038.59
               Mean episode length: 220.66
                 Mean success rate: 26.50
                  Mean reward/step: 8.99
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 6725632
                    Iteration time: 2.58s
                        Total time: 2170.32s
                               ETA: 526533.7s

################################################################################
                    [1m Learning iteration 821/200000 [0m

                       Computation: 3201 steps/s (collection: 0.519s, learning 2.040s)
               Value function loss: 41898.4767
                    Surrogate loss: 0.0150
             Mean action noise std: 0.92
                       Mean reward: 2212.55
               Mean episode length: 225.72
                 Mean success rate: 29.00
                  Mean reward/step: 9.02
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 6733824
                    Iteration time: 2.56s
                        Total time: 2172.88s
                               ETA: 526510.5s

################################################################################
                    [1m Learning iteration 822/200000 [0m

                       Computation: 3188 steps/s (collection: 0.526s, learning 2.043s)
               Value function loss: 31331.4227
                    Surrogate loss: 0.0160
             Mean action noise std: 0.92
                       Mean reward: 1809.91
               Mean episode length: 201.51
                 Mean success rate: 22.50
                  Mean reward/step: 9.61
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 6742016
                    Iteration time: 2.57s
                        Total time: 2175.45s
                               ETA: 526489.8s

################################################################################
                    [1m Learning iteration 823/200000 [0m

                       Computation: 3262 steps/s (collection: 0.476s, learning 2.034s)
               Value function loss: 32610.1903
                    Surrogate loss: 0.0176
             Mean action noise std: 0.92
                       Mean reward: 1918.96
               Mean episode length: 206.22
                 Mean success rate: 24.00
                  Mean reward/step: 10.07
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 6750208
                    Iteration time: 2.51s
                        Total time: 2177.96s
                               ETA: 526455.2s

################################################################################
                    [1m Learning iteration 824/200000 [0m

                       Computation: 3091 steps/s (collection: 0.557s, learning 2.093s)
               Value function loss: 41059.1679
                    Surrogate loss: 0.0121
             Mean action noise std: 0.92
                       Mean reward: 1823.65
               Mean episode length: 206.68
                 Mean success rate: 21.00
                  Mean reward/step: 10.76
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 6758400
                    Iteration time: 2.65s
                        Total time: 2180.61s
                               ETA: 526454.1s

################################################################################
                    [1m Learning iteration 825/200000 [0m

                       Computation: 3103 steps/s (collection: 0.565s, learning 2.075s)
               Value function loss: 50071.7720
                    Surrogate loss: 0.0154
             Mean action noise std: 0.92
                       Mean reward: 2029.06
               Mean episode length: 218.53
                 Mean success rate: 23.50
                  Mean reward/step: 11.66
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 6766592
                    Iteration time: 2.64s
                        Total time: 2183.25s
                               ETA: 526450.6s

################################################################################
                    [1m Learning iteration 826/200000 [0m

                       Computation: 3228 steps/s (collection: 0.497s, learning 2.040s)
               Value function loss: 39954.3112
                    Surrogate loss: 0.0137
             Mean action noise std: 0.92
                       Mean reward: 2014.69
               Mean episode length: 217.06
                 Mean success rate: 22.50
                  Mean reward/step: 11.70
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 6774784
                    Iteration time: 2.54s
                        Total time: 2185.78s
                               ETA: 526422.4s

################################################################################
                    [1m Learning iteration 827/200000 [0m

                       Computation: 3220 steps/s (collection: 0.513s, learning 2.030s)
               Value function loss: 43756.8268
                    Surrogate loss: 0.0154
             Mean action noise std: 0.92
                       Mean reward: 2032.37
               Mean episode length: 217.71
                 Mean success rate: 24.00
                  Mean reward/step: 11.47
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 2.54s
                        Total time: 2188.33s
                               ETA: 526395.8s

################################################################################
                    [1m Learning iteration 828/200000 [0m

                       Computation: 3073 steps/s (collection: 0.591s, learning 2.075s)
               Value function loss: 41309.5400
                    Surrogate loss: 0.0147
             Mean action noise std: 0.92
                       Mean reward: 2124.37
               Mean episode length: 224.79
                 Mean success rate: 27.00
                  Mean reward/step: 11.52
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 6791168
                    Iteration time: 2.67s
                        Total time: 2190.99s
                               ETA: 526398.5s

################################################################################
                    [1m Learning iteration 829/200000 [0m

                       Computation: 3115 steps/s (collection: 0.548s, learning 2.082s)
               Value function loss: 41536.5517
                    Surrogate loss: 0.0135
             Mean action noise std: 0.92
                       Mean reward: 2121.89
               Mean episode length: 221.97
                 Mean success rate: 27.00
                  Mean reward/step: 11.63
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 6799360
                    Iteration time: 2.63s
                        Total time: 2193.62s
                               ETA: 526392.7s

################################################################################
                    [1m Learning iteration 830/200000 [0m

                       Computation: 3158 steps/s (collection: 0.552s, learning 2.041s)
               Value function loss: 57944.7657
                    Surrogate loss: 0.0096
             Mean action noise std: 0.92
                       Mean reward: 2305.43
               Mean episode length: 223.69
                 Mean success rate: 29.50
                  Mean reward/step: 11.89
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 6807552
                    Iteration time: 2.59s
                        Total time: 2196.22s
                               ETA: 526378.2s

################################################################################
                    [1m Learning iteration 831/200000 [0m

                       Computation: 3122 steps/s (collection: 0.570s, learning 2.053s)
               Value function loss: 41080.2400
                    Surrogate loss: 0.0146
             Mean action noise std: 0.92
                       Mean reward: 2108.82
               Mean episode length: 210.72
                 Mean success rate: 28.50
                  Mean reward/step: 12.01
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 6815744
                    Iteration time: 2.62s
                        Total time: 2198.84s
                               ETA: 526371.0s

################################################################################
                    [1m Learning iteration 832/200000 [0m

                       Computation: 3107 steps/s (collection: 0.544s, learning 2.092s)
               Value function loss: 55489.8950
                    Surrogate loss: 0.0157
             Mean action noise std: 0.92
                       Mean reward: 2224.76
               Mean episode length: 215.75
                 Mean success rate: 30.00
                  Mean reward/step: 12.68
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 6823936
                    Iteration time: 2.64s
                        Total time: 2201.48s
                               ETA: 526366.7s

################################################################################
                    [1m Learning iteration 833/200000 [0m

                       Computation: 3126 steps/s (collection: 0.562s, learning 2.058s)
               Value function loss: 43449.1310
                    Surrogate loss: 0.0135
             Mean action noise std: 0.92
                       Mean reward: 2222.67
               Mean episode length: 209.38
                 Mean success rate: 28.00
                  Mean reward/step: 13.08
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6832128
                    Iteration time: 2.62s
                        Total time: 2204.10s
                               ETA: 526358.5s

################################################################################
                    [1m Learning iteration 834/200000 [0m

                       Computation: 3145 steps/s (collection: 0.547s, learning 2.057s)
               Value function loss: 55119.1247
                    Surrogate loss: 0.0129
             Mean action noise std: 0.92
                       Mean reward: 2388.39
               Mean episode length: 210.13
                 Mean success rate: 28.50
                  Mean reward/step: 13.15
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 6840320
                    Iteration time: 2.60s
                        Total time: 2206.70s
                               ETA: 526346.7s

################################################################################
                    [1m Learning iteration 835/200000 [0m

                       Computation: 3162 steps/s (collection: 0.566s, learning 2.024s)
               Value function loss: 59563.2387
                    Surrogate loss: 0.0128
             Mean action noise std: 0.92
                       Mean reward: 2601.76
               Mean episode length: 221.55
                 Mean success rate: 31.50
                  Mean reward/step: 12.30
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 6848512
                    Iteration time: 2.59s
                        Total time: 2209.29s
                               ETA: 526331.6s

################################################################################
                    [1m Learning iteration 836/200000 [0m

                       Computation: 3044 steps/s (collection: 0.605s, learning 2.086s)
               Value function loss: 71770.8606
                    Surrogate loss: 0.0119
             Mean action noise std: 0.92
                       Mean reward: 2815.29
               Mean episode length: 232.06
                 Mean success rate: 34.00
                  Mean reward/step: 12.60
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 6856704
                    Iteration time: 2.69s
                        Total time: 2211.98s
                               ETA: 526340.4s

################################################################################
                    [1m Learning iteration 837/200000 [0m

                       Computation: 3117 steps/s (collection: 0.576s, learning 2.051s)
               Value function loss: 56166.6227
                    Surrogate loss: 0.0127
             Mean action noise std: 0.92
                       Mean reward: 3258.24
               Mean episode length: 254.21
                 Mean success rate: 38.00
                  Mean reward/step: 12.04
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 6864896
                    Iteration time: 2.63s
                        Total time: 2214.61s
                               ETA: 526334.1s

################################################################################
                    [1m Learning iteration 838/200000 [0m

                       Computation: 3196 steps/s (collection: 0.497s, learning 2.065s)
               Value function loss: 42716.6616
                    Surrogate loss: 0.0128
             Mean action noise std: 0.92
                       Mean reward: 3118.65
               Mean episode length: 246.10
                 Mean success rate: 37.50
                  Mean reward/step: 11.97
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 6873088
                    Iteration time: 2.56s
                        Total time: 2217.17s
                               ETA: 526312.4s

################################################################################
                    [1m Learning iteration 839/200000 [0m

                       Computation: 3107 steps/s (collection: 0.553s, learning 2.083s)
               Value function loss: 44835.4496
                    Surrogate loss: 0.0142
             Mean action noise std: 0.92
                       Mean reward: 3039.32
               Mean episode length: 247.28
                 Mean success rate: 36.50
                  Mean reward/step: 12.74
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 2.64s
                        Total time: 2219.81s
                               ETA: 526308.2s

################################################################################
                    [1m Learning iteration 840/200000 [0m

                       Computation: 3056 steps/s (collection: 0.588s, learning 2.092s)
               Value function loss: 49179.7829
                    Surrogate loss: 0.0128
             Mean action noise std: 0.92
                       Mean reward: 3211.74
               Mean episode length: 259.15
                 Mean success rate: 39.50
                  Mean reward/step: 12.30
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 6889472
                    Iteration time: 2.68s
                        Total time: 2222.49s
                               ETA: 526314.4s

################################################################################
                    [1m Learning iteration 841/200000 [0m

                       Computation: 3169 steps/s (collection: 0.551s, learning 2.034s)
               Value function loss: 45064.6675
                    Surrogate loss: 0.0129
             Mean action noise std: 0.92
                       Mean reward: 2990.79
               Mean episode length: 246.83
                 Mean success rate: 38.00
                  Mean reward/step: 12.02
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 6897664
                    Iteration time: 2.58s
                        Total time: 2225.07s
                               ETA: 526298.1s

################################################################################
                    [1m Learning iteration 842/200000 [0m

                       Computation: 3159 steps/s (collection: 0.542s, learning 2.050s)
               Value function loss: 51123.1966
                    Surrogate loss: 0.0138
             Mean action noise std: 0.92
                       Mean reward: 3180.59
               Mean episode length: 259.77
                 Mean success rate: 40.00
                  Mean reward/step: 11.83
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 6905856
                    Iteration time: 2.59s
                        Total time: 2227.66s
                               ETA: 526283.6s

################################################################################
                    [1m Learning iteration 843/200000 [0m

                       Computation: 3220 steps/s (collection: 0.514s, learning 2.029s)
               Value function loss: 66146.5064
                    Surrogate loss: 0.0136
             Mean action noise std: 0.92
                       Mean reward: 3148.03
               Mean episode length: 254.24
                 Mean success rate: 39.50
                  Mean reward/step: 11.15
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 6914048
                    Iteration time: 2.54s
                        Total time: 2230.21s
                               ETA: 526257.7s

################################################################################
                    [1m Learning iteration 844/200000 [0m

                       Computation: 3156 steps/s (collection: 0.542s, learning 2.053s)
               Value function loss: 43606.4219
                    Surrogate loss: 0.0127
             Mean action noise std: 0.92
                       Mean reward: 3206.37
               Mean episode length: 249.10
                 Mean success rate: 38.50
                  Mean reward/step: 10.26
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 6922240
                    Iteration time: 2.60s
                        Total time: 2232.80s
                               ETA: 526243.9s

################################################################################
                    [1m Learning iteration 845/200000 [0m

                       Computation: 3161 steps/s (collection: 0.544s, learning 2.047s)
               Value function loss: 35262.7561
                    Surrogate loss: 0.0183
             Mean action noise std: 0.92
                       Mean reward: 3122.42
               Mean episode length: 242.28
                 Mean success rate: 37.00
                  Mean reward/step: 10.03
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 6930432
                    Iteration time: 2.59s
                        Total time: 2235.39s
                               ETA: 526229.2s

################################################################################
                    [1m Learning iteration 846/200000 [0m

                       Computation: 3195 steps/s (collection: 0.524s, learning 2.040s)
               Value function loss: 45617.4999
                    Surrogate loss: 0.0225
             Mean action noise std: 0.92
                       Mean reward: 3110.47
               Mean episode length: 242.46
                 Mean success rate: 36.50
                  Mean reward/step: 9.69
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 6938624
                    Iteration time: 2.56s
                        Total time: 2237.96s
                               ETA: 526208.1s

################################################################################
                    [1m Learning iteration 847/200000 [0m

                       Computation: 3122 steps/s (collection: 0.554s, learning 2.070s)
               Value function loss: 48583.0824
                    Surrogate loss: 0.0074
             Mean action noise std: 0.92
                       Mean reward: 2631.07
               Mean episode length: 222.66
                 Mean success rate: 30.50
                  Mean reward/step: 9.32
       Mean episode length/episode: 26.34
--------------------------------------------------------------------------------
                   Total timesteps: 6946816
                    Iteration time: 2.62s
                        Total time: 2240.58s
                               ETA: 526201.2s

################################################################################
                    [1m Learning iteration 848/200000 [0m

                       Computation: 3058 steps/s (collection: 0.605s, learning 2.073s)
               Value function loss: 34974.2146
                    Surrogate loss: 0.0148
             Mean action noise std: 0.92
                       Mean reward: 1987.55
               Mean episode length: 198.05
                 Mean success rate: 24.00
                  Mean reward/step: 9.42
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 6955008
                    Iteration time: 2.68s
                        Total time: 2243.26s
                               ETA: 526207.0s

################################################################################
                    [1m Learning iteration 849/200000 [0m

                       Computation: 3179 steps/s (collection: 0.528s, learning 2.049s)
               Value function loss: 42290.6378
                    Surrogate loss: 0.0151
             Mean action noise std: 0.92
                       Mean reward: 1977.39
               Mean episode length: 196.08
                 Mean success rate: 23.50
                  Mean reward/step: 10.30
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 6963200
                    Iteration time: 2.58s
                        Total time: 2245.84s
                               ETA: 526189.0s

################################################################################
                    [1m Learning iteration 850/200000 [0m

                       Computation: 3196 steps/s (collection: 0.515s, learning 2.048s)
               Value function loss: 52264.1952
                    Surrogate loss: 0.0128
             Mean action noise std: 0.92
                       Mean reward: 1961.73
               Mean episode length: 193.28
                 Mean success rate: 21.50
                  Mean reward/step: 10.13
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 6971392
                    Iteration time: 2.56s
                        Total time: 2248.40s
                               ETA: 526167.8s

################################################################################
                    [1m Learning iteration 851/200000 [0m

                       Computation: 3107 steps/s (collection: 0.573s, learning 2.064s)
               Value function loss: 43046.9388
                    Surrogate loss: 0.0123
             Mean action noise std: 0.92
                       Mean reward: 1897.68
               Mean episode length: 192.81
                 Mean success rate: 21.50
                  Mean reward/step: 9.99
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 2.64s
                        Total time: 2251.04s
                               ETA: 526163.8s

################################################################################
                    [1m Learning iteration 852/200000 [0m

                       Computation: 3105 steps/s (collection: 0.557s, learning 2.081s)
               Value function loss: 47228.5063
                    Surrogate loss: 0.0144
             Mean action noise std: 0.92
                       Mean reward: 2171.84
               Mean episode length: 204.10
                 Mean success rate: 24.00
                  Mean reward/step: 9.99
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 6987776
                    Iteration time: 2.64s
                        Total time: 2253.67s
                               ETA: 526160.1s

################################################################################
                    [1m Learning iteration 853/200000 [0m

                       Computation: 3197 steps/s (collection: 0.507s, learning 2.055s)
               Value function loss: 45656.4122
                    Surrogate loss: 0.0198
             Mean action noise std: 0.92
                       Mean reward: 2318.45
               Mean episode length: 212.44
                 Mean success rate: 26.50
                  Mean reward/step: 8.99
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 6995968
                    Iteration time: 2.56s
                        Total time: 2256.24s
                               ETA: 526138.8s

################################################################################
                    [1m Learning iteration 854/200000 [0m

                       Computation: 3215 steps/s (collection: 0.486s, learning 2.062s)
               Value function loss: 42378.8248
                    Surrogate loss: 0.0129
             Mean action noise std: 0.92
                       Mean reward: 2336.64
               Mean episode length: 213.94
                 Mean success rate: 27.50
                  Mean reward/step: 8.72
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 7004160
                    Iteration time: 2.55s
                        Total time: 2258.78s
                               ETA: 526114.2s

################################################################################
                    [1m Learning iteration 855/200000 [0m

                       Computation: 3152 steps/s (collection: 0.546s, learning 2.052s)
               Value function loss: 28096.2797
                    Surrogate loss: 0.0125
             Mean action noise std: 0.92
                       Mean reward: 2029.83
               Mean episode length: 196.12
                 Mean success rate: 24.50
                  Mean reward/step: 8.09
       Mean episode length/episode: 26.43
--------------------------------------------------------------------------------
                   Total timesteps: 7012352
                    Iteration time: 2.60s
                        Total time: 2261.38s
                               ETA: 526101.5s

################################################################################
                    [1m Learning iteration 856/200000 [0m

                       Computation: 3173 steps/s (collection: 0.542s, learning 2.039s)
               Value function loss: 35577.6417
                    Surrogate loss: 0.0117
             Mean action noise std: 0.92
                       Mean reward: 1890.73
               Mean episode length: 188.59
                 Mean success rate: 23.00
                  Mean reward/step: 8.29
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 7020544
                    Iteration time: 2.58s
                        Total time: 2263.96s
                               ETA: 526084.9s

################################################################################
                    [1m Learning iteration 857/200000 [0m

                       Computation: 3203 steps/s (collection: 0.517s, learning 2.040s)
               Value function loss: 24464.3075
                    Surrogate loss: 0.0151
             Mean action noise std: 0.92
                       Mean reward: 1540.09
               Mean episode length: 171.09
                 Mean success rate: 18.50
                  Mean reward/step: 8.29
       Mean episode length/episode: 26.86
--------------------------------------------------------------------------------
                   Total timesteps: 7028736
                    Iteration time: 2.56s
                        Total time: 2266.52s
                               ETA: 526062.6s

################################################################################
                    [1m Learning iteration 858/200000 [0m

                       Computation: 3131 steps/s (collection: 0.530s, learning 2.086s)
               Value function loss: 35885.6274
                    Surrogate loss: 0.0140
             Mean action noise std: 0.92
                       Mean reward: 1308.86
               Mean episode length: 155.01
                 Mean success rate: 15.50
                  Mean reward/step: 8.47
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 7036928
                    Iteration time: 2.62s
                        Total time: 2269.14s
                               ETA: 526054.0s

################################################################################
                    [1m Learning iteration 859/200000 [0m

                       Computation: 3023 steps/s (collection: 0.574s, learning 2.136s)
               Value function loss: 36040.1990
                    Surrogate loss: 0.0130
             Mean action noise std: 0.92
                       Mean reward: 1553.19
               Mean episode length: 165.88
                 Mean success rate: 18.50
                  Mean reward/step: 8.28
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 7045120
                    Iteration time: 2.71s
                        Total time: 2271.85s
                               ETA: 526067.1s

################################################################################
                    [1m Learning iteration 860/200000 [0m

                       Computation: 3059 steps/s (collection: 0.618s, learning 2.060s)
               Value function loss: 25202.8187
                    Surrogate loss: 0.0122
             Mean action noise std: 0.92
                       Mean reward: 1416.16
               Mean episode length: 154.65
                 Mean success rate: 16.50
                  Mean reward/step: 8.55
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 7053312
                    Iteration time: 2.68s
                        Total time: 2274.52s
                               ETA: 526072.8s

################################################################################
                    [1m Learning iteration 861/200000 [0m

                       Computation: 3176 steps/s (collection: 0.497s, learning 2.082s)
               Value function loss: 24886.6678
                    Surrogate loss: 0.0136
             Mean action noise std: 0.92
                       Mean reward: 1386.62
               Mean episode length: 160.47
                 Mean success rate: 17.00
                  Mean reward/step: 8.59
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 7061504
                    Iteration time: 2.58s
                        Total time: 2277.10s
                               ETA: 526055.6s

################################################################################
                    [1m Learning iteration 862/200000 [0m

                       Computation: 3022 steps/s (collection: 0.583s, learning 2.127s)
               Value function loss: 50906.3809
                    Surrogate loss: 0.0141
             Mean action noise std: 0.92
                       Mean reward: 1719.34
               Mean episode length: 182.19
                 Mean success rate: 22.00
                  Mean reward/step: 8.35
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 7069696
                    Iteration time: 2.71s
                        Total time: 2279.81s
                               ETA: 526068.8s

################################################################################
                    [1m Learning iteration 863/200000 [0m

                       Computation: 2800 steps/s (collection: 0.724s, learning 2.201s)
               Value function loss: 26688.8754
                    Surrogate loss: 0.0158
             Mean action noise std: 0.92
                       Mean reward: 1709.99
               Mean episode length: 190.94
                 Mean success rate: 23.50
                  Mean reward/step: 7.94
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 2.93s
                        Total time: 2282.74s
                               ETA: 526131.5s

################################################################################
                    [1m Learning iteration 864/200000 [0m

                       Computation: 2941 steps/s (collection: 0.614s, learning 2.171s)
               Value function loss: 30813.2261
                    Surrogate loss: 0.0166
             Mean action noise std: 0.92
                       Mean reward: 1741.48
               Mean episode length: 197.10
                 Mean success rate: 24.00
                  Mean reward/step: 8.50
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 7086080
                    Iteration time: 2.78s
                        Total time: 2285.52s
                               ETA: 526161.7s

################################################################################
                    [1m Learning iteration 865/200000 [0m

                       Computation: 2970 steps/s (collection: 0.626s, learning 2.132s)
               Value function loss: 29060.6945
                    Surrogate loss: 0.0145
             Mean action noise std: 0.92
                       Mean reward: 1936.54
               Mean episode length: 210.09
                 Mean success rate: 26.00
                  Mean reward/step: 8.71
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 7094272
                    Iteration time: 2.76s
                        Total time: 2288.28s
                               ETA: 526185.7s

################################################################################
                    [1m Learning iteration 866/200000 [0m

                       Computation: 2914 steps/s (collection: 0.647s, learning 2.164s)
               Value function loss: 26325.6459
                    Surrogate loss: 0.0131
             Mean action noise std: 0.92
                       Mean reward: 1985.68
               Mean episode length: 211.72
                 Mean success rate: 26.50
                  Mean reward/step: 9.48
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 7102464
                    Iteration time: 2.81s
                        Total time: 2291.09s
                               ETA: 526221.7s

################################################################################
                    [1m Learning iteration 867/200000 [0m

                       Computation: 2939 steps/s (collection: 0.644s, learning 2.143s)
               Value function loss: 40114.9749
                    Surrogate loss: 0.0108
             Mean action noise std: 0.92
                       Mean reward: 2027.51
               Mean episode length: 217.22
                 Mean success rate: 27.50
                  Mean reward/step: 9.03
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 7110656
                    Iteration time: 2.79s
                        Total time: 2293.88s
                               ETA: 526252.2s

################################################################################
                    [1m Learning iteration 868/200000 [0m

                       Computation: 2923 steps/s (collection: 0.650s, learning 2.152s)
               Value function loss: 27363.2928
                    Surrogate loss: 0.0149
             Mean action noise std: 0.92
                       Mean reward: 1688.34
               Mean episode length: 198.78
                 Mean success rate: 22.00
                  Mean reward/step: 8.61
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 7118848
                    Iteration time: 2.80s
                        Total time: 2296.68s
                               ETA: 526286.0s

################################################################################
                    [1m Learning iteration 869/200000 [0m

                       Computation: 2951 steps/s (collection: 0.631s, learning 2.144s)
               Value function loss: 23865.2719
                    Surrogate loss: 0.0177
             Mean action noise std: 0.92
                       Mean reward: 1577.60
               Mean episode length: 193.78
                 Mean success rate: 21.00
                  Mean reward/step: 9.23
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 7127040
                    Iteration time: 2.78s
                        Total time: 2299.46s
                               ETA: 526313.7s

################################################################################
                    [1m Learning iteration 870/200000 [0m

                       Computation: 2845 steps/s (collection: 0.678s, learning 2.201s)
               Value function loss: 27849.2426
                    Surrogate loss: 0.0151
             Mean action noise std: 0.92
                       Mean reward: 1508.48
               Mean episode length: 194.88
                 Mean success rate: 20.00
                  Mean reward/step: 9.50
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 7135232
                    Iteration time: 2.88s
                        Total time: 2302.33s
                               ETA: 526364.9s

################################################################################
                    [1m Learning iteration 871/200000 [0m

                       Computation: 2946 steps/s (collection: 0.631s, learning 2.149s)
               Value function loss: 30697.8989
                    Surrogate loss: 0.0164
             Mean action noise std: 0.92
                       Mean reward: 1585.92
               Mean episode length: 194.01
                 Mean success rate: 20.50
                  Mean reward/step: 9.56
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7143424
                    Iteration time: 2.78s
                        Total time: 2305.11s
                               ETA: 526393.4s

################################################################################
                    [1m Learning iteration 872/200000 [0m

                       Computation: 2999 steps/s (collection: 0.591s, learning 2.140s)
               Value function loss: 40572.9326
                    Surrogate loss: 0.0105
             Mean action noise std: 0.92
                       Mean reward: 1656.16
               Mean episode length: 204.40
                 Mean success rate: 21.00
                  Mean reward/step: 9.86
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 7151616
                    Iteration time: 2.73s
                        Total time: 2307.84s
                               ETA: 526410.7s

################################################################################
                    [1m Learning iteration 873/200000 [0m

                       Computation: 2547 steps/s (collection: 0.899s, learning 2.317s)
               Value function loss: 30790.5419
                    Surrogate loss: 0.0190
             Mean action noise std: 0.92
                       Mean reward: 1715.76
               Mean episode length: 204.58
                 Mean success rate: 21.50
                  Mean reward/step: 10.05
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 7159808
                    Iteration time: 3.22s
                        Total time: 2311.06s
                               ETA: 526538.5s

################################################################################
                    [1m Learning iteration 874/200000 [0m

                       Computation: 2665 steps/s (collection: 0.726s, learning 2.348s)
               Value function loss: 39725.5168
                    Surrogate loss: 0.0182
             Mean action noise std: 0.92
                       Mean reward: 1857.50
               Mean episode length: 213.64
                 Mean success rate: 23.00
                  Mean reward/step: 10.45
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 7168000
                    Iteration time: 3.07s
                        Total time: 2314.13s
                               ETA: 526633.6s

################################################################################
                    [1m Learning iteration 875/200000 [0m

                       Computation: 2737 steps/s (collection: 0.820s, learning 2.173s)
               Value function loss: 43076.7408
                    Surrogate loss: 0.0100
             Mean action noise std: 0.92
                       Mean reward: 2201.57
               Mean episode length: 228.22
                 Mean success rate: 28.00
                  Mean reward/step: 10.18
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 2.99s
                        Total time: 2317.13s
                               ETA: 526710.1s

################################################################################
                    [1m Learning iteration 876/200000 [0m

                       Computation: 2948 steps/s (collection: 0.641s, learning 2.138s)
               Value function loss: 46461.9035
                    Surrogate loss: 0.0124
             Mean action noise std: 0.92
                       Mean reward: 2381.74
               Mean episode length: 236.19
                 Mean success rate: 31.00
                  Mean reward/step: 10.52
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 7184384
                    Iteration time: 2.78s
                        Total time: 2319.91s
                               ETA: 526737.7s

################################################################################
                    [1m Learning iteration 877/200000 [0m

                       Computation: 2953 steps/s (collection: 0.645s, learning 2.128s)
               Value function loss: 40130.4781
                    Surrogate loss: 0.0153
             Mean action noise std: 0.92
                       Mean reward: 2275.20
               Mean episode length: 231.88
                 Mean success rate: 30.00
                  Mean reward/step: 10.42
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 7192576
                    Iteration time: 2.77s
                        Total time: 2322.68s
                               ETA: 526764.2s

################################################################################
                    [1m Learning iteration 878/200000 [0m

                       Computation: 2949 steps/s (collection: 0.583s, learning 2.195s)
               Value function loss: 41826.8791
                    Surrogate loss: 0.0175
             Mean action noise std: 0.92
                       Mean reward: 2246.51
               Mean episode length: 232.09
                 Mean success rate: 29.50
                  Mean reward/step: 10.28
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 7200768
                    Iteration time: 2.78s
                        Total time: 2325.46s
                               ETA: 526791.4s

################################################################################
                    [1m Learning iteration 879/200000 [0m

                       Computation: 2916 steps/s (collection: 0.679s, learning 2.130s)
               Value function loss: 36423.2658
                    Surrogate loss: 0.0168
             Mean action noise std: 0.92
                       Mean reward: 2223.08
               Mean episode length: 232.78
                 Mean success rate: 30.00
                  Mean reward/step: 10.24
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 7208960
                    Iteration time: 2.81s
                        Total time: 2328.27s
                               ETA: 526825.8s

################################################################################
                    [1m Learning iteration 880/200000 [0m

                       Computation: 2942 steps/s (collection: 0.629s, learning 2.155s)
               Value function loss: 33246.3914
                    Surrogate loss: 0.0168
             Mean action noise std: 0.91
                       Mean reward: 2103.52
               Mean episode length: 226.44
                 Mean success rate: 29.00
                  Mean reward/step: 11.08
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 7217152
                    Iteration time: 2.78s
                        Total time: 2331.05s
                               ETA: 526854.4s

################################################################################
                    [1m Learning iteration 881/200000 [0m

                       Computation: 2941 steps/s (collection: 0.630s, learning 2.155s)
               Value function loss: 33655.1858
                    Surrogate loss: 0.0174
             Mean action noise std: 0.91
                       Mean reward: 2104.91
               Mean episode length: 224.01
                 Mean success rate: 28.50
                  Mean reward/step: 11.50
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 7225344
                    Iteration time: 2.78s
                        Total time: 2333.84s
                               ETA: 526883.1s

################################################################################
                    [1m Learning iteration 882/200000 [0m

                       Computation: 2899 steps/s (collection: 0.688s, learning 2.137s)
               Value function loss: 36833.7718
                    Surrogate loss: 0.0135
             Mean action noise std: 0.91
                       Mean reward: 2174.86
               Mean episode length: 223.32
                 Mean success rate: 29.50
                  Mean reward/step: 11.39
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 7233536
                    Iteration time: 2.82s
                        Total time: 2336.66s
                               ETA: 526920.8s

################################################################################
                    [1m Learning iteration 883/200000 [0m

                       Computation: 2953 steps/s (collection: 0.624s, learning 2.150s)
               Value function loss: 45003.9049
                    Surrogate loss: 0.0105
             Mean action noise std: 0.92
                       Mean reward: 2256.70
               Mean episode length: 222.44
                 Mean success rate: 30.50
                  Mean reward/step: 11.04
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 7241728
                    Iteration time: 2.77s
                        Total time: 2339.43s
                               ETA: 526946.9s

################################################################################
                    [1m Learning iteration 884/200000 [0m

                       Computation: 2837 steps/s (collection: 0.704s, learning 2.184s)
               Value function loss: 54473.9172
                    Surrogate loss: 0.0124
             Mean action noise std: 0.91
                       Mean reward: 2315.75
               Mean episode length: 214.94
                 Mean success rate: 30.50
                  Mean reward/step: 10.70
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 7249920
                    Iteration time: 2.89s
                        Total time: 2342.32s
                               ETA: 526998.5s

################################################################################
                    [1m Learning iteration 885/200000 [0m

                       Computation: 2972 steps/s (collection: 0.627s, learning 2.130s)
               Value function loss: 45477.9993
                    Surrogate loss: 0.0113
             Mean action noise std: 0.91
                       Mean reward: 2357.20
               Mean episode length: 209.85
                 Mean success rate: 30.50
                  Mean reward/step: 10.06
       Mean episode length/episode: 26.77
--------------------------------------------------------------------------------
                   Total timesteps: 7258112
                    Iteration time: 2.76s
                        Total time: 2345.08s
                               ETA: 527020.4s

################################################################################
                    [1m Learning iteration 886/200000 [0m

                       Computation: 3021 steps/s (collection: 0.583s, learning 2.128s)
               Value function loss: 40567.8861
                    Surrogate loss: 0.0107
             Mean action noise std: 0.91
                       Mean reward: 2484.50
               Mean episode length: 219.97
                 Mean success rate: 32.00
                  Mean reward/step: 9.73
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 7266304
                    Iteration time: 2.71s
                        Total time: 2347.79s
                               ETA: 527032.3s

################################################################################
                    [1m Learning iteration 887/200000 [0m

                       Computation: 2948 steps/s (collection: 0.636s, learning 2.142s)
               Value function loss: 33500.6965
                    Surrogate loss: 0.0152
             Mean action noise std: 0.91
                       Mean reward: 2257.14
               Mean episode length: 207.31
                 Mean success rate: 28.50
                  Mean reward/step: 9.68
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 2.78s
                        Total time: 2350.57s
                               ETA: 527059.0s

################################################################################
                    [1m Learning iteration 888/200000 [0m

                       Computation: 2970 steps/s (collection: 0.615s, learning 2.142s)
               Value function loss: 35260.4478
                    Surrogate loss: 0.0101
             Mean action noise std: 0.91
                       Mean reward: 2071.08
               Mean episode length: 196.81
                 Mean success rate: 26.00
                  Mean reward/step: 9.99
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 7282688
                    Iteration time: 2.76s
                        Total time: 2353.32s
                               ETA: 527081.1s

################################################################################
                    [1m Learning iteration 889/200000 [0m

                       Computation: 2997 steps/s (collection: 0.597s, learning 2.137s)
               Value function loss: 30376.5907
                    Surrogate loss: 0.0140
             Mean action noise std: 0.91
                       Mean reward: 2046.04
               Mean episode length: 198.96
                 Mean success rate: 25.00
                  Mean reward/step: 10.65
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 7290880
                    Iteration time: 2.73s
                        Total time: 2356.06s
                               ETA: 527097.7s

################################################################################
                    [1m Learning iteration 890/200000 [0m

                       Computation: 2984 steps/s (collection: 0.581s, learning 2.164s)
               Value function loss: 53027.9066
                    Surrogate loss: 0.0120
             Mean action noise std: 0.92
                       Mean reward: 1963.10
               Mean episode length: 191.97
                 Mean success rate: 22.50
                  Mean reward/step: 10.74
       Mean episode length/episode: 26.09
--------------------------------------------------------------------------------
                   Total timesteps: 7299072
                    Iteration time: 2.74s
                        Total time: 2358.80s
                               ETA: 527116.7s

################################################################################
                    [1m Learning iteration 891/200000 [0m

                       Computation: 2971 steps/s (collection: 0.610s, learning 2.147s)
               Value function loss: 26612.4269
                    Surrogate loss: 0.0179
             Mean action noise std: 0.91
                       Mean reward: 1661.72
               Mean episode length: 176.03
                 Mean success rate: 19.50
                  Mean reward/step: 11.25
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 7307264
                    Iteration time: 2.76s
                        Total time: 2361.56s
                               ETA: 527138.6s

################################################################################
                    [1m Learning iteration 892/200000 [0m

                       Computation: 2988 steps/s (collection: 0.607s, learning 2.135s)
               Value function loss: 45075.5254
                    Surrogate loss: 0.0152
             Mean action noise std: 0.91
                       Mean reward: 1892.56
               Mean episode length: 191.00
                 Mean success rate: 22.00
                  Mean reward/step: 11.51
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 7315456
                    Iteration time: 2.74s
                        Total time: 2364.30s
                               ETA: 527156.9s

################################################################################
                    [1m Learning iteration 893/200000 [0m

                       Computation: 2992 steps/s (collection: 0.620s, learning 2.118s)
               Value function loss: 45541.4602
                    Surrogate loss: 0.0145
             Mean action noise std: 0.91
                       Mean reward: 1959.93
               Mean episode length: 194.08
                 Mean success rate: 24.50
                  Mean reward/step: 11.27
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 7323648
                    Iteration time: 2.74s
                        Total time: 2367.04s
                               ETA: 527174.2s

################################################################################
                    [1m Learning iteration 894/200000 [0m

                       Computation: 3015 steps/s (collection: 0.594s, learning 2.122s)
               Value function loss: 42192.1100
                    Surrogate loss: 0.0138
             Mean action noise std: 0.91
                       Mean reward: 2184.49
               Mean episode length: 208.12
                 Mean success rate: 28.00
                  Mean reward/step: 11.12
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 7331840
                    Iteration time: 2.72s
                        Total time: 2369.75s
                               ETA: 527186.9s

################################################################################
                    [1m Learning iteration 895/200000 [0m

                       Computation: 2905 steps/s (collection: 0.643s, learning 2.177s)
               Value function loss: 47451.4439
                    Surrogate loss: 0.0151
             Mean action noise std: 0.91
                       Mean reward: 2090.33
               Mean episode length: 203.19
                 Mean success rate: 26.00
                  Mean reward/step: 11.25
       Mean episode length/episode: 26.51
--------------------------------------------------------------------------------
                   Total timesteps: 7340032
                    Iteration time: 2.82s
                        Total time: 2372.57s
                               ETA: 527222.3s

################################################################################
                    [1m Learning iteration 896/200000 [0m

                       Computation: 3005 steps/s (collection: 0.625s, learning 2.101s)
               Value function loss: 38400.2401
                    Surrogate loss: 0.0140
             Mean action noise std: 0.91
                       Mean reward: 2262.54
               Mean episode length: 210.75
                 Mean success rate: 27.50
                  Mean reward/step: 11.39
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7348224
                    Iteration time: 2.73s
                        Total time: 2375.30s
                               ETA: 527236.8s

################################################################################
                    [1m Learning iteration 897/200000 [0m

                       Computation: 3017 steps/s (collection: 0.597s, learning 2.118s)
               Value function loss: 46535.0500
                    Surrogate loss: 0.0166
             Mean action noise std: 0.92
                       Mean reward: 2233.75
               Mean episode length: 206.12
                 Mean success rate: 27.50
                  Mean reward/step: 11.50
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 7356416
                    Iteration time: 2.71s
                        Total time: 2378.01s
                               ETA: 527249.0s

################################################################################
                    [1m Learning iteration 898/200000 [0m

                       Computation: 2950 steps/s (collection: 0.634s, learning 2.142s)
               Value function loss: 40612.5812
                    Surrogate loss: 0.0158
             Mean action noise std: 0.92
                       Mean reward: 2202.52
               Mean episode length: 200.10
                 Mean success rate: 26.50
                  Mean reward/step: 12.02
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7364608
                    Iteration time: 2.78s
                        Total time: 2380.79s
                               ETA: 527274.8s

################################################################################
                    [1m Learning iteration 899/200000 [0m

                       Computation: 2973 steps/s (collection: 0.632s, learning 2.123s)
               Value function loss: 43748.0657
                    Surrogate loss: 0.0143
             Mean action noise std: 0.92
                       Mean reward: 2238.95
               Mean episode length: 200.41
                 Mean success rate: 27.00
                  Mean reward/step: 12.32
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 2.76s
                        Total time: 2383.55s
                               ETA: 527295.8s

################################################################################
                    [1m Learning iteration 900/200000 [0m

                       Computation: 3006 steps/s (collection: 0.617s, learning 2.108s)
               Value function loss: 40002.9514
                    Surrogate loss: 0.0150
             Mean action noise std: 0.92
                       Mean reward: 2330.79
               Mean episode length: 201.84
                 Mean success rate: 27.50
                  Mean reward/step: 12.05
       Mean episode length/episode: 27.13
--------------------------------------------------------------------------------
                   Total timesteps: 7380992
                    Iteration time: 2.73s
                        Total time: 2386.27s
                               ETA: 527310.1s

################################################################################
                    [1m Learning iteration 901/200000 [0m

                       Computation: 3030 steps/s (collection: 0.575s, learning 2.128s)
               Value function loss: 38249.2731
                    Surrogate loss: 0.0159
             Mean action noise std: 0.92
                       Mean reward: 2304.18
               Mean episode length: 205.18
                 Mean success rate: 27.50
                  Mean reward/step: 12.14
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 7389184
                    Iteration time: 2.70s
                        Total time: 2388.97s
                               ETA: 527319.5s

################################################################################
                    [1m Learning iteration 902/200000 [0m

                       Computation: 2966 steps/s (collection: 0.625s, learning 2.136s)
               Value function loss: 56553.1426
                    Surrogate loss: 0.0163
             Mean action noise std: 0.92
                       Mean reward: 2183.32
               Mean episode length: 200.06
                 Mean success rate: 26.00
                  Mean reward/step: 12.73
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 7397376
                    Iteration time: 2.76s
                        Total time: 2391.73s
                               ETA: 527341.7s

################################################################################
                    [1m Learning iteration 903/200000 [0m

                       Computation: 2906 steps/s (collection: 0.664s, learning 2.155s)
               Value function loss: 61461.8227
                    Surrogate loss: 0.0137
             Mean action noise std: 0.91
                       Mean reward: 2500.16
               Mean episode length: 213.95
                 Mean success rate: 30.00
                  Mean reward/step: 13.10
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 7405568
                    Iteration time: 2.82s
                        Total time: 2394.55s
                               ETA: 527376.5s

################################################################################
                    [1m Learning iteration 904/200000 [0m

                       Computation: 3001 steps/s (collection: 0.585s, learning 2.144s)
               Value function loss: 40290.0147
                    Surrogate loss: 0.0136
             Mean action noise std: 0.92
                       Mean reward: 2462.89
               Mean episode length: 211.44
                 Mean success rate: 28.50
                  Mean reward/step: 13.25
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 7413760
                    Iteration time: 2.73s
                        Total time: 2397.28s
                               ETA: 527391.5s

################################################################################
                    [1m Learning iteration 905/200000 [0m

                       Computation: 2958 steps/s (collection: 0.605s, learning 2.164s)
               Value function loss: 55145.0514
                    Surrogate loss: 0.0172
             Mean action noise std: 0.92
                       Mean reward: 2555.57
               Mean episode length: 219.91
                 Mean success rate: 29.50
                  Mean reward/step: 13.73
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 7421952
                    Iteration time: 2.77s
                        Total time: 2400.05s
                               ETA: 527415.2s

################################################################################
                    [1m Learning iteration 906/200000 [0m

                       Computation: 2816 steps/s (collection: 0.693s, learning 2.216s)
               Value function loss: 60761.7728
                    Surrogate loss: 0.0138
             Mean action noise std: 0.92
                       Mean reward: 2853.65
               Mean episode length: 231.75
                 Mean success rate: 33.50
                  Mean reward/step: 12.46
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 7430144
                    Iteration time: 2.91s
                        Total time: 2402.96s
                               ETA: 527469.4s

################################################################################
                    [1m Learning iteration 907/200000 [0m

                       Computation: 2815 steps/s (collection: 0.702s, learning 2.208s)
               Value function loss: 53267.9277
                    Surrogate loss: 0.0136
             Mean action noise std: 0.92
                       Mean reward: 3007.49
               Mean episode length: 236.88
                 Mean success rate: 36.00
                  Mean reward/step: 12.36
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 7438336
                    Iteration time: 2.91s
                        Total time: 2405.87s
                               ETA: 527523.9s

################################################################################
                    [1m Learning iteration 908/200000 [0m

                       Computation: 2929 steps/s (collection: 0.611s, learning 2.185s)
               Value function loss: 46261.3006
                    Surrogate loss: 0.0156
             Mean action noise std: 0.91
                       Mean reward: 2941.55
               Mean episode length: 233.13
                 Mean success rate: 34.50
                  Mean reward/step: 12.35
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 7446528
                    Iteration time: 2.80s
                        Total time: 2408.67s
                               ETA: 527553.3s

################################################################################
                    [1m Learning iteration 909/200000 [0m

                       Computation: 2951 steps/s (collection: 0.660s, learning 2.115s)
               Value function loss: 51701.9900
                    Surrogate loss: 0.0141
             Mean action noise std: 0.91
                       Mean reward: 3192.88
               Mean episode length: 247.19
                 Mean success rate: 36.50
                  Mean reward/step: 12.42
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 7454720
                    Iteration time: 2.78s
                        Total time: 2411.44s
                               ETA: 527578.1s

################################################################################
                    [1m Learning iteration 910/200000 [0m

                       Computation: 3003 steps/s (collection: 0.593s, learning 2.135s)
               Value function loss: 37495.2163
                    Surrogate loss: 0.0155
             Mean action noise std: 0.91
                       Mean reward: 3208.44
               Mean episode length: 251.19
                 Mean success rate: 37.00
                  Mean reward/step: 12.89
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 7462912
                    Iteration time: 2.73s
                        Total time: 2414.17s
                               ETA: 527592.5s

################################################################################
                    [1m Learning iteration 911/200000 [0m

                       Computation: 3034 steps/s (collection: 0.581s, learning 2.119s)
               Value function loss: 55452.6319
                    Surrogate loss: 0.0128
             Mean action noise std: 0.91
                       Mean reward: 3046.94
               Mean episode length: 246.53
                 Mean success rate: 36.50
                  Mean reward/step: 13.31
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 2.70s
                        Total time: 2416.87s
                               ETA: 527600.6s

################################################################################
                    [1m Learning iteration 912/200000 [0m

                       Computation: 3044 steps/s (collection: 0.582s, learning 2.108s)
               Value function loss: 45806.6160
                    Surrogate loss: 0.0139
             Mean action noise std: 0.91
                       Mean reward: 2892.85
               Mean episode length: 237.60
                 Mean success rate: 34.00
                  Mean reward/step: 13.48
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7479296
                    Iteration time: 2.69s
                        Total time: 2419.56s
                               ETA: 527606.7s

################################################################################
                    [1m Learning iteration 913/200000 [0m

                       Computation: 2955 steps/s (collection: 0.585s, learning 2.187s)
               Value function loss: 59873.6521
                    Surrogate loss: 0.0148
             Mean action noise std: 0.91
                       Mean reward: 2790.86
               Mean episode length: 234.61
                 Mean success rate: 34.00
                  Mean reward/step: 13.62
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 7487488
                    Iteration time: 2.77s
                        Total time: 2422.33s
                               ETA: 527630.5s

################################################################################
                    [1m Learning iteration 914/200000 [0m

                       Computation: 2865 steps/s (collection: 0.716s, learning 2.143s)
               Value function loss: 49093.2340
                    Surrogate loss: 0.0157
             Mean action noise std: 0.91
                       Mean reward: 2603.90
               Mean episode length: 220.88
                 Mean success rate: 32.50
                  Mean reward/step: 13.61
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 7495680
                    Iteration time: 2.86s
                        Total time: 2425.19s
                               ETA: 527673.2s

################################################################################
                    [1m Learning iteration 915/200000 [0m

                       Computation: 2989 steps/s (collection: 0.607s, learning 2.133s)
               Value function loss: 56419.8208
                    Surrogate loss: 0.0199
             Mean action noise std: 0.91
                       Mean reward: 2658.84
               Mean episode length: 223.39
                 Mean success rate: 34.00
                  Mean reward/step: 12.93
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 7503872
                    Iteration time: 2.74s
                        Total time: 2427.93s
                               ETA: 527690.1s

################################################################################
                    [1m Learning iteration 916/200000 [0m

                       Computation: 2922 steps/s (collection: 0.649s, learning 2.154s)
               Value function loss: 50954.8285
                    Surrogate loss: 0.0150
             Mean action noise std: 0.91
                       Mean reward: 2654.77
               Mean episode length: 213.90
                 Mean success rate: 33.00
                  Mean reward/step: 12.28
       Mean episode length/episode: 26.51
--------------------------------------------------------------------------------
                   Total timesteps: 7512064
                    Iteration time: 2.80s
                        Total time: 2430.73s
                               ETA: 527720.5s

################################################################################
                    [1m Learning iteration 917/200000 [0m

                       Computation: 2931 steps/s (collection: 0.655s, learning 2.139s)
               Value function loss: 53770.5727
                    Surrogate loss: 0.0141
             Mean action noise std: 0.91
                       Mean reward: 2681.01
               Mean episode length: 212.40
                 Mean success rate: 32.50
                  Mean reward/step: 12.34
       Mean episode length/episode: 26.68
--------------------------------------------------------------------------------
                   Total timesteps: 7520256
                    Iteration time: 2.79s
                        Total time: 2433.53s
                               ETA: 527749.0s

################################################################################
                    [1m Learning iteration 918/200000 [0m

                       Computation: 2979 steps/s (collection: 0.626s, learning 2.123s)
               Value function loss: 38508.6788
                    Surrogate loss: 0.0175
             Mean action noise std: 0.91
                       Mean reward: 2444.39
               Mean episode length: 198.15
                 Mean success rate: 29.00
                  Mean reward/step: 12.62
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 7528448
                    Iteration time: 2.75s
                        Total time: 2436.28s
                               ETA: 527767.7s

################################################################################
                    [1m Learning iteration 919/200000 [0m

                       Computation: 2995 steps/s (collection: 0.617s, learning 2.117s)
               Value function loss: 57988.4216
                    Surrogate loss: 0.0125
             Mean action noise std: 0.91
                       Mean reward: 2323.55
               Mean episode length: 185.84
                 Mean success rate: 26.50
                  Mean reward/step: 12.47
       Mean episode length/episode: 26.43
--------------------------------------------------------------------------------
                   Total timesteps: 7536640
                    Iteration time: 2.73s
                        Total time: 2439.01s
                               ETA: 527783.2s

################################################################################
                    [1m Learning iteration 920/200000 [0m

                       Computation: 2985 steps/s (collection: 0.616s, learning 2.128s)
               Value function loss: 37557.9721
                    Surrogate loss: 0.0163
             Mean action noise std: 0.91
                       Mean reward: 2046.45
               Mean episode length: 173.55
                 Mean success rate: 23.50
                  Mean reward/step: 12.41
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 7544832
                    Iteration time: 2.74s
                        Total time: 2441.75s
                               ETA: 527800.6s

################################################################################
                    [1m Learning iteration 921/200000 [0m

                       Computation: 2979 steps/s (collection: 0.625s, learning 2.125s)
               Value function loss: 64304.5384
                    Surrogate loss: 0.0139
             Mean action noise std: 0.91
                       Mean reward: 2155.80
               Mean episode length: 173.94
                 Mean success rate: 25.00
                  Mean reward/step: 12.35
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 7553024
                    Iteration time: 2.75s
                        Total time: 2444.50s
                               ETA: 527819.2s

################################################################################
                    [1m Learning iteration 922/200000 [0m

                       Computation: 2944 steps/s (collection: 0.632s, learning 2.150s)
               Value function loss: 46420.7366
                    Surrogate loss: 0.0117
             Mean action noise std: 0.91
                       Mean reward: 2178.88
               Mean episode length: 173.31
                 Mean success rate: 25.50
                  Mean reward/step: 11.44
       Mean episode length/episode: 26.26
--------------------------------------------------------------------------------
                   Total timesteps: 7561216
                    Iteration time: 2.78s
                        Total time: 2447.29s
                               ETA: 527844.7s

################################################################################
                    [1m Learning iteration 923/200000 [0m

                       Computation: 2896 steps/s (collection: 0.651s, learning 2.177s)
               Value function loss: 45372.3656
                    Surrogate loss: 0.0123
             Mean action noise std: 0.91
                       Mean reward: 2379.08
               Mean episode length: 183.39
                 Mean success rate: 29.00
                  Mean reward/step: 11.26
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 2.83s
                        Total time: 2450.11s
                               ETA: 527880.1s

################################################################################
                    [1m Learning iteration 924/200000 [0m

                       Computation: 2848 steps/s (collection: 0.672s, learning 2.204s)
               Value function loss: 68124.9583
                    Surrogate loss: 0.0117
             Mean action noise std: 0.91
                       Mean reward: 2788.76
               Mean episode length: 203.50
                 Mean success rate: 34.00
                  Mean reward/step: 11.00
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 7577600
                    Iteration time: 2.88s
                        Total time: 2452.99s
                               ETA: 527925.7s

################################################################################
                    [1m Learning iteration 925/200000 [0m

                       Computation: 2868 steps/s (collection: 0.700s, learning 2.156s)
               Value function loss: 40182.5115
                    Surrogate loss: 0.0153
             Mean action noise std: 0.91
                       Mean reward: 2703.60
               Mean episode length: 204.06
                 Mean success rate: 33.50
                  Mean reward/step: 10.83
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 7585792
                    Iteration time: 2.86s
                        Total time: 2455.85s
                               ETA: 527967.0s

################################################################################
                    [1m Learning iteration 926/200000 [0m

                       Computation: 2908 steps/s (collection: 0.644s, learning 2.173s)
               Value function loss: 23692.9971
                    Surrogate loss: 0.0242
             Mean action noise std: 0.91
                       Mean reward: 2209.18
               Mean episode length: 186.19
                 Mean success rate: 27.00
                  Mean reward/step: 11.07
       Mean episode length/episode: 25.76
--------------------------------------------------------------------------------
                   Total timesteps: 7593984
                    Iteration time: 2.82s
                        Total time: 2458.66s
                               ETA: 527999.8s

################################################################################
                    [1m Learning iteration 927/200000 [0m

                       Computation: 2931 steps/s (collection: 0.623s, learning 2.171s)
               Value function loss: 61622.6914
                    Surrogate loss: 0.0153
             Mean action noise std: 0.91
                       Mean reward: 2313.20
               Mean episode length: 189.16
                 Mean success rate: 26.50
                  Mean reward/step: 11.24
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 7602176
                    Iteration time: 2.79s
                        Total time: 2461.46s
                               ETA: 528027.6s

################################################################################
                    [1m Learning iteration 928/200000 [0m

                       Computation: 2891 steps/s (collection: 0.631s, learning 2.202s)
               Value function loss: 36086.5190
                    Surrogate loss: 0.0164
             Mean action noise std: 0.91
                       Mean reward: 2090.98
               Mean episode length: 182.09
                 Mean success rate: 23.50
                  Mean reward/step: 11.46
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 7610368
                    Iteration time: 2.83s
                        Total time: 2464.29s
                               ETA: 528063.6s

################################################################################
                    [1m Learning iteration 929/200000 [0m

                       Computation: 3021 steps/s (collection: 0.591s, learning 2.121s)
               Value function loss: 37975.1676
                    Surrogate loss: 0.0138
             Mean action noise std: 0.91
                       Mean reward: 1924.67
               Mean episode length: 171.94
                 Mean success rate: 21.50
                  Mean reward/step: 12.46
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 7618560
                    Iteration time: 2.71s
                        Total time: 2467.00s
                               ETA: 528073.5s

################################################################################
                    [1m Learning iteration 930/200000 [0m

                       Computation: 2992 steps/s (collection: 0.603s, learning 2.135s)
               Value function loss: 48395.9589
                    Surrogate loss: 0.0145
             Mean action noise std: 0.91
                       Mean reward: 2115.76
               Mean episode length: 177.93
                 Mean success rate: 23.50
                  Mean reward/step: 13.34
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 7626752
                    Iteration time: 2.74s
                        Total time: 2469.74s
                               ETA: 528089.0s

################################################################################
                    [1m Learning iteration 931/200000 [0m

                       Computation: 2902 steps/s (collection: 0.656s, learning 2.166s)
               Value function loss: 52149.7159
                    Surrogate loss: 0.0149
             Mean action noise std: 0.91
                       Mean reward: 2246.74
               Mean episode length: 183.23
                 Mean success rate: 25.00
                  Mean reward/step: 13.90
       Mean episode length/episode: 26.95
--------------------------------------------------------------------------------
                   Total timesteps: 7634944
                    Iteration time: 2.82s
                        Total time: 2472.56s
                               ETA: 528122.5s

################################################################################
                    [1m Learning iteration 932/200000 [0m

                       Computation: 2946 steps/s (collection: 0.632s, learning 2.148s)
               Value function loss: 48463.2904
                    Surrogate loss: 0.0147
             Mean action noise std: 0.91
                       Mean reward: 2115.79
               Mean episode length: 177.43
                 Mean success rate: 26.00
                  Mean reward/step: 14.16
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 7643136
                    Iteration time: 2.78s
                        Total time: 2475.34s
                               ETA: 528146.9s

################################################################################
                    [1m Learning iteration 933/200000 [0m

                       Computation: 3016 steps/s (collection: 0.588s, learning 2.128s)
               Value function loss: 47147.3058
                    Surrogate loss: 0.0158
             Mean action noise std: 0.91
                       Mean reward: 2306.43
               Mean episode length: 185.47
                 Mean success rate: 27.50
                  Mean reward/step: 14.37
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 7651328
                    Iteration time: 2.72s
                        Total time: 2478.06s
                               ETA: 528157.5s

################################################################################
                    [1m Learning iteration 934/200000 [0m

                       Computation: 2965 steps/s (collection: 0.607s, learning 2.156s)
               Value function loss: 62573.4189
                    Surrogate loss: 0.0155
             Mean action noise std: 0.91
                       Mean reward: 2252.55
               Mean episode length: 188.63
                 Mean success rate: 26.50
                  Mean reward/step: 14.65
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 7659520
                    Iteration time: 2.76s
                        Total time: 2480.82s
                               ETA: 528178.2s

################################################################################
                    [1m Learning iteration 935/200000 [0m

                       Computation: 2941 steps/s (collection: 0.629s, learning 2.156s)
               Value function loss: 54045.1679
                    Surrogate loss: 0.0189
             Mean action noise std: 0.91
                       Mean reward: 2418.14
               Mean episode length: 197.81
                 Mean success rate: 29.50
                  Mean reward/step: 15.20
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 2.78s
                        Total time: 2483.60s
                               ETA: 528203.6s

################################################################################
                    [1m Learning iteration 936/200000 [0m

                       Computation: 2942 steps/s (collection: 0.636s, learning 2.148s)
               Value function loss: 42793.3208
                    Surrogate loss: 0.0153
             Mean action noise std: 0.91
                       Mean reward: 2542.06
               Mean episode length: 200.76
                 Mean success rate: 31.00
                  Mean reward/step: 15.48
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7675904
                    Iteration time: 2.78s
                        Total time: 2486.39s
                               ETA: 528228.7s

################################################################################
                    [1m Learning iteration 937/200000 [0m

                       Computation: 3017 steps/s (collection: 0.602s, learning 2.113s)
               Value function loss: 60293.7641
                    Surrogate loss: 0.0130
             Mean action noise std: 0.91
                       Mean reward: 2545.39
               Mean episode length: 203.12
                 Mean success rate: 31.50
                  Mean reward/step: 15.24
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 7684096
                    Iteration time: 2.71s
                        Total time: 2489.10s
                               ETA: 528239.0s

################################################################################
                    [1m Learning iteration 938/200000 [0m

                       Computation: 3053 steps/s (collection: 0.564s, learning 2.119s)
               Value function loss: 31229.1579
                    Surrogate loss: 0.0160
             Mean action noise std: 0.91
                       Mean reward: 2653.35
               Mean episode length: 208.33
                 Mean success rate: 32.50
                  Mean reward/step: 15.00
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 7692288
                    Iteration time: 2.68s
                        Total time: 2491.79s
                               ETA: 528242.6s

################################################################################
                    [1m Learning iteration 939/200000 [0m

                       Computation: 2999 steps/s (collection: 0.592s, learning 2.139s)
               Value function loss: 57490.0606
                    Surrogate loss: 0.0147
             Mean action noise std: 0.91
                       Mean reward: 3144.47
               Mean episode length: 224.36
                 Mean success rate: 37.00
                  Mean reward/step: 14.85
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7700480
                    Iteration time: 2.73s
                        Total time: 2494.52s
                               ETA: 528256.4s

################################################################################
                    [1m Learning iteration 940/200000 [0m

                       Computation: 3035 steps/s (collection: 0.599s, learning 2.100s)
               Value function loss: 62803.4062
                    Surrogate loss: 0.0135
             Mean action noise std: 0.91
                       Mean reward: 3424.81
               Mean episode length: 229.29
                 Mean success rate: 40.00
                  Mean reward/step: 14.36
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 7708672
                    Iteration time: 2.70s
                        Total time: 2497.22s
                               ETA: 528263.3s

################################################################################
                    [1m Learning iteration 941/200000 [0m

                       Computation: 2980 steps/s (collection: 0.600s, learning 2.149s)
               Value function loss: 71107.2327
                    Surrogate loss: 0.0131
             Mean action noise std: 0.91
                       Mean reward: 3433.26
               Mean episode length: 224.30
                 Mean success rate: 38.00
                  Mean reward/step: 13.86
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 7716864
                    Iteration time: 2.75s
                        Total time: 2499.96s
                               ETA: 528280.7s

################################################################################
                    [1m Learning iteration 942/200000 [0m

                       Computation: 2954 steps/s (collection: 0.605s, learning 2.168s)
               Value function loss: 58242.5416
                    Surrogate loss: 0.0135
             Mean action noise std: 0.91
                       Mean reward: 3652.36
               Mean episode length: 238.84
                 Mean success rate: 39.50
                  Mean reward/step: 13.84
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 7725056
                    Iteration time: 2.77s
                        Total time: 2502.74s
                               ETA: 528303.2s

################################################################################
                    [1m Learning iteration 943/200000 [0m

                       Computation: 3020 steps/s (collection: 0.596s, learning 2.117s)
               Value function loss: 55547.3370
                    Surrogate loss: 0.0139
             Mean action noise std: 0.91
                       Mean reward: 3525.02
               Mean episode length: 232.21
                 Mean success rate: 37.00
                  Mean reward/step: 14.64
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 7733248
                    Iteration time: 2.71s
                        Total time: 2505.45s
                               ETA: 528312.8s

################################################################################
                    [1m Learning iteration 944/200000 [0m

                       Computation: 3001 steps/s (collection: 0.604s, learning 2.125s)
               Value function loss: 44945.5826
                    Surrogate loss: 0.0142
             Mean action noise std: 0.91
                       Mean reward: 3551.59
               Mean episode length: 235.94
                 Mean success rate: 37.00
                  Mean reward/step: 15.25
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 7741440
                    Iteration time: 2.73s
                        Total time: 2508.18s
                               ETA: 528325.9s

################################################################################
                    [1m Learning iteration 945/200000 [0m

                       Computation: 2963 steps/s (collection: 0.640s, learning 2.124s)
               Value function loss: 47774.6358
                    Surrogate loss: 0.0195
             Mean action noise std: 0.91
                       Mean reward: 3426.04
               Mean episode length: 233.31
                 Mean success rate: 36.00
                  Mean reward/step: 16.27
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7749632
                    Iteration time: 2.76s
                        Total time: 2510.94s
                               ETA: 528346.5s

################################################################################
                    [1m Learning iteration 946/200000 [0m

                       Computation: 2908 steps/s (collection: 0.636s, learning 2.181s)
               Value function loss: 82055.0002
                    Surrogate loss: 0.0169
             Mean action noise std: 0.91
                       Mean reward: 3516.30
               Mean episode length: 238.91
                 Mean success rate: 36.50
                  Mean reward/step: 15.15
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 7757824
                    Iteration time: 2.82s
                        Total time: 2513.76s
                               ETA: 528378.1s

################################################################################
                    [1m Learning iteration 947/200000 [0m

                       Computation: 2961 steps/s (collection: 0.610s, learning 2.156s)
               Value function loss: 59687.0526
                    Surrogate loss: 0.0100
             Mean action noise std: 0.91
                       Mean reward: 3775.83
               Mean episode length: 250.32
                 Mean success rate: 39.50
                  Mean reward/step: 14.61
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 2.77s
                        Total time: 2516.53s
                               ETA: 528398.9s

################################################################################
                    [1m Learning iteration 948/200000 [0m

                       Computation: 3029 steps/s (collection: 0.598s, learning 2.106s)
               Value function loss: 54128.5428
                    Surrogate loss: 0.0153
             Mean action noise std: 0.91
                       Mean reward: 3608.13
               Mean episode length: 241.11
                 Mean success rate: 38.00
                  Mean reward/step: 14.30
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 7774208
                    Iteration time: 2.70s
                        Total time: 2519.23s
                               ETA: 528406.6s

################################################################################
                    [1m Learning iteration 949/200000 [0m

                       Computation: 2964 steps/s (collection: 0.603s, learning 2.161s)
               Value function loss: 57739.5255
                    Surrogate loss: 0.0132
             Mean action noise std: 0.91
                       Mean reward: 3996.57
               Mean episode length: 253.13
                 Mean success rate: 42.00
                  Mean reward/step: 14.14
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 7782400
                    Iteration time: 2.76s
                        Total time: 2521.99s
                               ETA: 528426.8s

################################################################################
                    [1m Learning iteration 950/200000 [0m

                       Computation: 2882 steps/s (collection: 0.657s, learning 2.185s)
               Value function loss: 56857.8249
                    Surrogate loss: 0.0154
             Mean action noise std: 0.91
                       Mean reward: 4070.27
               Mean episode length: 259.61
                 Mean success rate: 43.00
                  Mean reward/step: 14.76
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 7790592
                    Iteration time: 2.84s
                        Total time: 2524.84s
                               ETA: 528463.3s

################################################################################
                    [1m Learning iteration 951/200000 [0m

                       Computation: 3030 steps/s (collection: 0.601s, learning 2.102s)
               Value function loss: 43435.0760
                    Surrogate loss: 0.0135
             Mean action noise std: 0.91
                       Mean reward: 3905.88
               Mean episode length: 254.88
                 Mean success rate: 42.00
                  Mean reward/step: 14.65
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 7798784
                    Iteration time: 2.70s
                        Total time: 2527.54s
                               ETA: 528470.8s

################################################################################
                    [1m Learning iteration 952/200000 [0m

                       Computation: 3009 steps/s (collection: 0.598s, learning 2.124s)
               Value function loss: 52189.0139
                    Surrogate loss: 0.0134
             Mean action noise std: 0.91
                       Mean reward: 3753.64
               Mean episode length: 248.84
                 Mean success rate: 41.50
                  Mean reward/step: 14.41
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 7806976
                    Iteration time: 2.72s
                        Total time: 2530.26s
                               ETA: 528482.1s

################################################################################
                    [1m Learning iteration 953/200000 [0m

                       Computation: 3009 steps/s (collection: 0.573s, learning 2.149s)
               Value function loss: 67242.7602
                    Surrogate loss: 0.0137
             Mean action noise std: 0.91
                       Mean reward: 3664.55
               Mean episode length: 245.81
                 Mean success rate: 40.50
                  Mean reward/step: 14.40
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7815168
                    Iteration time: 2.72s
                        Total time: 2532.98s
                               ETA: 528493.4s

################################################################################
                    [1m Learning iteration 954/200000 [0m

                       Computation: 3008 steps/s (collection: 0.593s, learning 2.130s)
               Value function loss: 57867.8731
                    Surrogate loss: 0.0124
             Mean action noise std: 0.91
                       Mean reward: 3638.84
               Mean episode length: 248.06
                 Mean success rate: 39.50
                  Mean reward/step: 14.30
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 7823360
                    Iteration time: 2.72s
                        Total time: 2535.71s
                               ETA: 528504.9s

################################################################################
                    [1m Learning iteration 955/200000 [0m

                       Computation: 2953 steps/s (collection: 0.624s, learning 2.150s)
               Value function loss: 56486.1385
                    Surrogate loss: 0.0135
             Mean action noise std: 0.91
                       Mean reward: 3353.47
               Mean episode length: 241.10
                 Mean success rate: 36.50
                  Mean reward/step: 13.63
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 7831552
                    Iteration time: 2.77s
                        Total time: 2538.48s
                               ETA: 528526.9s

################################################################################
                    [1m Learning iteration 956/200000 [0m

                       Computation: 2946 steps/s (collection: 0.618s, learning 2.162s)
               Value function loss: 67294.0671
                    Surrogate loss: 0.0099
             Mean action noise std: 0.91
                       Mean reward: 3639.56
               Mean episode length: 252.01
                 Mean success rate: 39.00
                  Mean reward/step: 13.24
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 7839744
                    Iteration time: 2.78s
                        Total time: 2541.26s
                               ETA: 528550.2s

################################################################################
                    [1m Learning iteration 957/200000 [0m

                       Computation: 2891 steps/s (collection: 0.657s, learning 2.176s)
               Value function loss: 68860.0350
                    Surrogate loss: 0.0133
             Mean action noise std: 0.91
                       Mean reward: 3943.64
               Mean episode length: 263.38
                 Mean success rate: 42.00
                  Mean reward/step: 13.16
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 7847936
                    Iteration time: 2.83s
                        Total time: 2544.09s
                               ETA: 528584.4s

################################################################################
                    [1m Learning iteration 958/200000 [0m

                       Computation: 2981 steps/s (collection: 0.606s, learning 2.141s)
               Value function loss: 53885.8514
                    Surrogate loss: 0.0147
             Mean action noise std: 0.91
                       Mean reward: 4132.65
               Mean episode length: 275.12
                 Mean success rate: 44.50
                  Mean reward/step: 13.47
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7856128
                    Iteration time: 2.75s
                        Total time: 2546.84s
                               ETA: 528600.8s

################################################################################
                    [1m Learning iteration 959/200000 [0m

                       Computation: 2952 steps/s (collection: 0.626s, learning 2.149s)
               Value function loss: 73850.1805
                    Surrogate loss: 0.0124
             Mean action noise std: 0.91
                       Mean reward: 3969.76
               Mean episode length: 276.70
                 Mean success rate: 43.00
                  Mean reward/step: 13.66
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 2.77s
                        Total time: 2549.61s
                               ETA: 528622.8s

################################################################################
                    [1m Learning iteration 960/200000 [0m

                       Computation: 2956 steps/s (collection: 0.624s, learning 2.147s)
               Value function loss: 45792.8056
                    Surrogate loss: 0.0149
             Mean action noise std: 0.91
                       Mean reward: 3783.70
               Mean episode length: 266.58
                 Mean success rate: 42.00
                  Mean reward/step: 13.29
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 7872512
                    Iteration time: 2.77s
                        Total time: 2552.39s
                               ETA: 528643.9s

################################################################################
                    [1m Learning iteration 961/200000 [0m

                       Computation: 2910 steps/s (collection: 0.629s, learning 2.185s)
               Value function loss: 40203.3938
                    Surrogate loss: 0.0172
             Mean action noise std: 0.91
                       Mean reward: 3719.15
               Mean episode length: 262.90
                 Mean success rate: 41.00
                  Mean reward/step: 14.15
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7880704
                    Iteration time: 2.81s
                        Total time: 2555.20s
                               ETA: 528674.1s

################################################################################
                    [1m Learning iteration 962/200000 [0m

                       Computation: 2979 steps/s (collection: 0.627s, learning 2.123s)
               Value function loss: 56847.0118
                    Surrogate loss: 0.0147
             Mean action noise std: 0.91
                       Mean reward: 3536.04
               Mean episode length: 256.09
                 Mean success rate: 39.50
                  Mean reward/step: 13.83
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 7888896
                    Iteration time: 2.75s
                        Total time: 2557.95s
                               ETA: 528690.7s

################################################################################
                    [1m Learning iteration 963/200000 [0m

                       Computation: 2958 steps/s (collection: 0.623s, learning 2.147s)
               Value function loss: 68406.7278
                    Surrogate loss: 0.0147
             Mean action noise std: 0.91
                       Mean reward: 3371.34
               Mean episode length: 250.78
                 Mean success rate: 39.00
                  Mean reward/step: 14.03
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 7897088
                    Iteration time: 2.77s
                        Total time: 2560.72s
                               ETA: 528711.4s

################################################################################
                    [1m Learning iteration 964/200000 [0m

                       Computation: 2878 steps/s (collection: 0.665s, learning 2.181s)
               Value function loss: 40641.2709
                    Surrogate loss: 0.0199
             Mean action noise std: 0.91
                       Mean reward: 3382.75
               Mean episode length: 253.59
                 Mean success rate: 38.00
                  Mean reward/step: 13.92
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7905280
                    Iteration time: 2.85s
                        Total time: 2563.56s
                               ETA: 528747.9s

################################################################################
                    [1m Learning iteration 965/200000 [0m

                       Computation: 3002 steps/s (collection: 0.595s, learning 2.133s)
               Value function loss: 65206.6212
                    Surrogate loss: 0.0168
             Mean action noise std: 0.91
                       Mean reward: 3252.43
               Mean episode length: 252.15
                 Mean success rate: 37.00
                  Mean reward/step: 13.64
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 7913472
                    Iteration time: 2.73s
                        Total time: 2566.29s
                               ETA: 528759.9s

################################################################################
                    [1m Learning iteration 966/200000 [0m

                       Computation: 2986 steps/s (collection: 0.615s, learning 2.129s)
               Value function loss: 58126.0475
                    Surrogate loss: 0.0157
             Mean action noise std: 0.91
                       Mean reward: 3338.20
               Mean episode length: 250.31
                 Mean success rate: 38.00
                  Mean reward/step: 13.42
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 7921664
                    Iteration time: 2.74s
                        Total time: 2569.04s
                               ETA: 528775.1s

################################################################################
                    [1m Learning iteration 967/200000 [0m

                       Computation: 2933 steps/s (collection: 0.653s, learning 2.139s)
               Value function loss: 52450.2380
                    Surrogate loss: 0.0135
             Mean action noise std: 0.91
                       Mean reward: 3501.96
               Mean episode length: 254.54
                 Mean success rate: 40.00
                  Mean reward/step: 13.35
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7929856
                    Iteration time: 2.79s
                        Total time: 2571.83s
                               ETA: 528800.4s

################################################################################
                    [1m Learning iteration 968/200000 [0m

                       Computation: 2912 steps/s (collection: 0.631s, learning 2.181s)
               Value function loss: 43280.7473
                    Surrogate loss: 0.0137
             Mean action noise std: 0.91
                       Mean reward: 3305.36
               Mean episode length: 245.78
                 Mean success rate: 38.50
                  Mean reward/step: 13.51
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 7938048
                    Iteration time: 2.81s
                        Total time: 2574.64s
                               ETA: 528829.7s

################################################################################
                    [1m Learning iteration 969/200000 [0m

                       Computation: 2964 steps/s (collection: 0.599s, learning 2.164s)
               Value function loss: 48730.8792
                    Surrogate loss: 0.0153
             Mean action noise std: 0.91
                       Mean reward: 3000.22
               Mean episode length: 225.85
                 Mean success rate: 34.00
                  Mean reward/step: 14.65
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 7946240
                    Iteration time: 2.76s
                        Total time: 2577.40s
                               ETA: 528848.7s

################################################################################
                    [1m Learning iteration 970/200000 [0m

                       Computation: 3006 steps/s (collection: 0.610s, learning 2.116s)
               Value function loss: 69759.9785
                    Surrogate loss: 0.0130
             Mean action noise std: 0.91
                       Mean reward: 3366.98
               Mean episode length: 232.85
                 Mean success rate: 36.50
                  Mean reward/step: 14.91
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 7954432
                    Iteration time: 2.73s
                        Total time: 2580.13s
                               ETA: 528860.0s

################################################################################
                    [1m Learning iteration 971/200000 [0m

                       Computation: 3031 steps/s (collection: 0.559s, learning 2.143s)
               Value function loss: 36097.9668
                    Surrogate loss: 0.0166
             Mean action noise std: 0.91
                       Mean reward: 3266.26
               Mean episode length: 229.64
                 Mean success rate: 36.00
                  Mean reward/step: 14.31
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 2.70s
                        Total time: 2582.83s
                               ETA: 528866.6s

################################################################################
                    [1m Learning iteration 972/200000 [0m

                       Computation: 2991 steps/s (collection: 0.613s, learning 2.126s)
               Value function loss: 72633.9281
                    Surrogate loss: 0.0139
             Mean action noise std: 0.91
                       Mean reward: 3377.77
               Mean episode length: 235.03
                 Mean success rate: 37.00
                  Mean reward/step: 14.45
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 7970816
                    Iteration time: 2.74s
                        Total time: 2585.57s
                               ETA: 528880.5s

################################################################################
                    [1m Learning iteration 973/200000 [0m

                       Computation: 2994 steps/s (collection: 0.610s, learning 2.125s)
               Value function loss: 56763.3056
                    Surrogate loss: 0.0147
             Mean action noise std: 0.91
                       Mean reward: 3256.46
               Mean episode length: 234.12
                 Mean success rate: 35.50
                  Mean reward/step: 13.82
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 7979008
                    Iteration time: 2.74s
                        Total time: 2588.31s
                               ETA: 528893.9s

################################################################################
                    [1m Learning iteration 974/200000 [0m

                       Computation: 3052 steps/s (collection: 0.599s, learning 2.085s)
               Value function loss: 68118.6622
                    Surrogate loss: 0.0131
             Mean action noise std: 0.91
                       Mean reward: 3512.16
               Mean episode length: 246.00
                 Mean success rate: 39.00
                  Mean reward/step: 13.41
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 7987200
                    Iteration time: 2.68s
                        Total time: 2590.99s
                               ETA: 528896.7s

################################################################################
                    [1m Learning iteration 975/200000 [0m

                       Computation: 2973 steps/s (collection: 0.620s, learning 2.135s)
               Value function loss: 59833.9858
                    Surrogate loss: 0.0158
             Mean action noise std: 0.91
                       Mean reward: 3844.32
               Mean episode length: 261.48
                 Mean success rate: 43.50
                  Mean reward/step: 13.31
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 7995392
                    Iteration time: 2.75s
                        Total time: 2593.74s
                               ETA: 528913.9s

################################################################################
                    [1m Learning iteration 976/200000 [0m

                       Computation: 2968 steps/s (collection: 0.624s, learning 2.136s)
               Value function loss: 62129.5302
                    Surrogate loss: 0.0138
             Mean action noise std: 0.91
                       Mean reward: 3642.71
               Mean episode length: 258.93
                 Mean success rate: 43.00
                  Mean reward/step: 12.87
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 8003584
                    Iteration time: 2.76s
                        Total time: 2596.50s
                               ETA: 528932.0s

################################################################################
                    [1m Learning iteration 977/200000 [0m

                       Computation: 3023 steps/s (collection: 0.602s, learning 2.107s)
               Value function loss: 47887.4986
                    Surrogate loss: 0.0162
             Mean action noise std: 0.91
                       Mean reward: 3695.86
               Mean episode length: 256.55
                 Mean success rate: 42.00
                  Mean reward/step: 13.30
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 8011776
                    Iteration time: 2.71s
                        Total time: 2599.21s
                               ETA: 528939.9s

################################################################################
                    [1m Learning iteration 978/200000 [0m

                       Computation: 3005 steps/s (collection: 0.580s, learning 2.145s)
               Value function loss: 27491.2652
                    Surrogate loss: 0.0197
             Mean action noise std: 0.91
                       Mean reward: 3346.42
               Mean episode length: 242.94
                 Mean success rate: 38.50
                  Mean reward/step: 13.09
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8019968
                    Iteration time: 2.73s
                        Total time: 2601.94s
                               ETA: 528951.1s

################################################################################
                    [1m Learning iteration 979/200000 [0m

                       Computation: 2937 steps/s (collection: 0.645s, learning 2.144s)
               Value function loss: 40738.1208
                    Surrogate loss: 0.0184
             Mean action noise std: 0.91
                       Mean reward: 3186.67
               Mean episode length: 237.41
                 Mean success rate: 37.00
                  Mean reward/step: 13.83
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 8028160
                    Iteration time: 2.79s
                        Total time: 2604.73s
                               ETA: 528975.0s

################################################################################
                    [1m Learning iteration 980/200000 [0m

                       Computation: 3010 steps/s (collection: 0.610s, learning 2.111s)
               Value function loss: 45040.2392
                    Surrogate loss: 0.0179
             Mean action noise std: 0.91
                       Mean reward: 3080.95
               Mean episode length: 231.90
                 Mean success rate: 34.50
                  Mean reward/step: 14.04
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 8036352
                    Iteration time: 2.72s
                        Total time: 2607.45s
                               ETA: 528985.2s

################################################################################
                    [1m Learning iteration 981/200000 [0m

                       Computation: 3012 steps/s (collection: 0.644s, learning 2.076s)
               Value function loss: 62985.5299
                    Surrogate loss: 0.0172
             Mean action noise std: 0.91
                       Mean reward: 2930.53
               Mean episode length: 231.43
                 Mean success rate: 33.50
                  Mean reward/step: 14.27
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 8044544
                    Iteration time: 2.72s
                        Total time: 2610.17s
                               ETA: 528995.1s

################################################################################
                    [1m Learning iteration 982/200000 [0m

                       Computation: 3076 steps/s (collection: 0.559s, learning 2.104s)
               Value function loss: 47297.0966
                    Surrogate loss: 0.0170
             Mean action noise std: 0.91
                       Mean reward: 2888.81
               Mean episode length: 221.49
                 Mean success rate: 31.50
                  Mean reward/step: 14.31
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 8052736
                    Iteration time: 2.66s
                        Total time: 2612.83s
                               ETA: 528993.4s

################################################################################
                    [1m Learning iteration 983/200000 [0m

                       Computation: 3063 steps/s (collection: 0.551s, learning 2.123s)
               Value function loss: 41113.9930
                    Surrogate loss: 0.0165
             Mean action noise std: 0.91
                       Mean reward: 2852.26
               Mean episode length: 221.22
                 Mean success rate: 32.00
                  Mean reward/step: 14.31
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 2.67s
                        Total time: 2615.51s
                               ETA: 528994.0s

################################################################################
                    [1m Learning iteration 984/200000 [0m

                       Computation: 3004 steps/s (collection: 0.600s, learning 2.126s)
               Value function loss: 48128.7499
                    Surrogate loss: 0.0183
             Mean action noise std: 0.91
                       Mean reward: 3183.21
               Mean episode length: 232.01
                 Mean success rate: 36.00
                  Mean reward/step: 14.74
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8069120
                    Iteration time: 2.73s
                        Total time: 2618.23s
                               ETA: 529005.2s

################################################################################
                    [1m Learning iteration 985/200000 [0m

                       Computation: 3090 steps/s (collection: 0.584s, learning 2.067s)
               Value function loss: 56573.5865
                    Surrogate loss: 0.0173
             Mean action noise std: 0.91
                       Mean reward: 3413.15
               Mean episode length: 239.37
                 Mean success rate: 38.50
                  Mean reward/step: 14.84
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 8077312
                    Iteration time: 2.65s
                        Total time: 2620.88s
                               ETA: 529001.0s

################################################################################
                    [1m Learning iteration 986/200000 [0m

                       Computation: 2990 steps/s (collection: 0.629s, learning 2.111s)
               Value function loss: 55824.6991
                    Surrogate loss: 0.0148
             Mean action noise std: 0.91
                       Mean reward: 3585.08
               Mean episode length: 249.90
                 Mean success rate: 40.00
                  Mean reward/step: 14.80
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 8085504
                    Iteration time: 2.74s
                        Total time: 2623.62s
                               ETA: 529014.7s

################################################################################
                    [1m Learning iteration 987/200000 [0m

                       Computation: 3103 steps/s (collection: 0.554s, learning 2.085s)
               Value function loss: 49724.9079
                    Surrogate loss: 0.0171
             Mean action noise std: 0.91
                       Mean reward: 3429.97
               Mean episode length: 240.59
                 Mean success rate: 40.00
                  Mean reward/step: 14.67
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 8093696
                    Iteration time: 2.64s
                        Total time: 2626.26s
                               ETA: 529008.3s

################################################################################
                    [1m Learning iteration 988/200000 [0m

                       Computation: 3092 steps/s (collection: 0.538s, learning 2.111s)
               Value function loss: 55986.5609
                    Surrogate loss: 0.0151
             Mean action noise std: 0.91
                       Mean reward: 3640.09
               Mean episode length: 251.87
                 Mean success rate: 41.50
                  Mean reward/step: 14.86
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8101888
                    Iteration time: 2.65s
                        Total time: 2628.91s
                               ETA: 529003.7s

################################################################################
                    [1m Learning iteration 989/200000 [0m

                       Computation: 3072 steps/s (collection: 0.584s, learning 2.082s)
               Value function loss: 53956.8197
                    Surrogate loss: 0.0179
             Mean action noise std: 0.91
                       Mean reward: 3713.84
               Mean episode length: 257.19
                 Mean success rate: 42.50
                  Mean reward/step: 14.67
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8110080
                    Iteration time: 2.67s
                        Total time: 2631.58s
                               ETA: 529002.6s

################################################################################
                    [1m Learning iteration 990/200000 [0m

                       Computation: 3061 steps/s (collection: 0.591s, learning 2.085s)
               Value function loss: 51045.8877
                    Surrogate loss: 0.0173
             Mean action noise std: 0.91
                       Mean reward: 3945.15
               Mean episode length: 274.33
                 Mean success rate: 44.00
                  Mean reward/step: 14.82
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8118272
                    Iteration time: 2.68s
                        Total time: 2634.25s
                               ETA: 529003.5s

################################################################################
                    [1m Learning iteration 991/200000 [0m

                       Computation: 3058 steps/s (collection: 0.583s, learning 2.095s)
               Value function loss: 70086.4990
                    Surrogate loss: 0.0151
             Mean action noise std: 0.91
                       Mean reward: 3863.31
               Mean episode length: 272.79
                 Mean success rate: 43.50
                  Mean reward/step: 15.03
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 8126464
                    Iteration time: 2.68s
                        Total time: 2636.93s
                               ETA: 529004.8s

################################################################################
                    [1m Learning iteration 992/200000 [0m

                       Computation: 3055 steps/s (collection: 0.610s, learning 2.072s)
               Value function loss: 62454.1844
                    Surrogate loss: 0.0159
             Mean action noise std: 0.91
                       Mean reward: 4021.94
               Mean episode length: 274.88
                 Mean success rate: 46.50
                  Mean reward/step: 14.60
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8134656
                    Iteration time: 2.68s
                        Total time: 2639.61s
                               ETA: 529006.8s

################################################################################
                    [1m Learning iteration 993/200000 [0m

                       Computation: 3010 steps/s (collection: 0.604s, learning 2.117s)
               Value function loss: 69775.2345
                    Surrogate loss: 0.0181
             Mean action noise std: 0.91
                       Mean reward: 4033.95
               Mean episode length: 274.23
                 Mean success rate: 45.00
                  Mean reward/step: 14.69
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 8142848
                    Iteration time: 2.72s
                        Total time: 2642.33s
                               ETA: 529016.7s

################################################################################
                    [1m Learning iteration 994/200000 [0m

                       Computation: 2976 steps/s (collection: 0.649s, learning 2.103s)
               Value function loss: 73282.7248
                    Surrogate loss: 0.0145
             Mean action noise std: 0.91
                       Mean reward: 4167.89
               Mean episode length: 279.14
                 Mean success rate: 45.00
                  Mean reward/step: 14.22
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 8151040
                    Iteration time: 2.75s
                        Total time: 2645.08s
                               ETA: 529032.8s

################################################################################
                    [1m Learning iteration 995/200000 [0m

                       Computation: 3126 steps/s (collection: 0.548s, learning 2.073s)
               Value function loss: 48243.3852
                    Surrogate loss: 0.0164
             Mean action noise std: 0.91
                       Mean reward: 4069.46
               Mean episode length: 272.12
                 Mean success rate: 44.00
                  Mean reward/step: 14.25
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 2.62s
                        Total time: 2647.70s
                               ETA: 529022.6s

################################################################################
                    [1m Learning iteration 996/200000 [0m

                       Computation: 3085 steps/s (collection: 0.556s, learning 2.099s)
               Value function loss: 55425.6020
                    Surrogate loss: 0.0136
             Mean action noise std: 0.91
                       Mean reward: 3759.01
               Mean episode length: 254.50
                 Mean success rate: 40.50
                  Mean reward/step: 15.49
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 8167424
                    Iteration time: 2.65s
                        Total time: 2650.36s
                               ETA: 529019.3s

################################################################################
                    [1m Learning iteration 997/200000 [0m

                       Computation: 3095 steps/s (collection: 0.560s, learning 2.087s)
               Value function loss: 96741.5964
                    Surrogate loss: 0.0125
             Mean action noise std: 0.91
                       Mean reward: 4038.86
               Mean episode length: 264.21
                 Mean success rate: 43.50
                  Mean reward/step: 15.14
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 8175616
                    Iteration time: 2.65s
                        Total time: 2653.01s
                               ETA: 529014.2s

################################################################################
                    [1m Learning iteration 998/200000 [0m

                       Computation: 3080 steps/s (collection: 0.584s, learning 2.076s)
               Value function loss: 46285.0797
                    Surrogate loss: 0.0125
             Mean action noise std: 0.91
                       Mean reward: 4028.75
               Mean episode length: 266.54
                 Mean success rate: 43.50
                  Mean reward/step: 14.25
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8183808
                    Iteration time: 2.66s
                        Total time: 2655.67s
                               ETA: 529011.7s

################################################################################
                    [1m Learning iteration 999/200000 [0m

                       Computation: 3054 steps/s (collection: 0.600s, learning 2.082s)
               Value function loss: 62448.6216
                    Surrogate loss: 0.0125
             Mean action noise std: 0.91
                       Mean reward: 3935.34
               Mean episode length: 263.06
                 Mean success rate: 42.00
                  Mean reward/step: 14.26
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8192000
                    Iteration time: 2.68s
                        Total time: 2658.35s
                               ETA: 529013.8s

################################################################################
                    [1m Learning iteration 1000/200000 [0m

                       Computation: 3071 steps/s (collection: 0.574s, learning 2.093s)
               Value function loss: 77065.9739
                    Surrogate loss: 0.0145
             Mean action noise std: 0.91
                       Mean reward: 3838.65
               Mean episode length: 260.19
                 Mean success rate: 41.50
                  Mean reward/step: 14.13
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 8200192
                    Iteration time: 2.67s
                        Total time: 2661.01s
                               ETA: 529012.8s

################################################################################
                    [1m Learning iteration 1001/200000 [0m

                       Computation: 2942 steps/s (collection: 0.628s, learning 2.155s)
               Value function loss: 60991.3970
                    Surrogate loss: 0.0144
             Mean action noise std: 0.91
                       Mean reward: 3795.67
               Mean episode length: 261.53
                 Mean success rate: 43.50
                  Mean reward/step: 13.65
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 8208384
                    Iteration time: 2.78s
                        Total time: 2663.80s
                               ETA: 529035.0s

################################################################################
                    [1m Learning iteration 1002/200000 [0m

                       Computation: 3040 steps/s (collection: 0.612s, learning 2.082s)
               Value function loss: 56732.3313
                    Surrogate loss: 0.0158
             Mean action noise std: 0.91
                       Mean reward: 3884.57
               Mean episode length: 261.37
                 Mean success rate: 44.50
                  Mean reward/step: 14.10
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 8216576
                    Iteration time: 2.69s
                        Total time: 2666.49s
                               ETA: 529039.5s

################################################################################
                    [1m Learning iteration 1003/200000 [0m

                       Computation: 3043 steps/s (collection: 0.587s, learning 2.105s)
               Value function loss: 74308.1077
                    Surrogate loss: 0.0159
             Mean action noise std: 0.91
                       Mean reward: 3990.06
               Mean episode length: 273.62
                 Mean success rate: 46.50
                  Mean reward/step: 14.07
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8224768
                    Iteration time: 2.69s
                        Total time: 2669.18s
                               ETA: 529043.5s

################################################################################
                    [1m Learning iteration 1004/200000 [0m

                       Computation: 3059 steps/s (collection: 0.553s, learning 2.124s)
               Value function loss: 46640.8698
                    Surrogate loss: 0.0152
             Mean action noise std: 0.91
                       Mean reward: 3709.87
               Mean episode length: 259.61
                 Mean success rate: 42.50
                  Mean reward/step: 14.49
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8232960
                    Iteration time: 2.68s
                        Total time: 2671.86s
                               ETA: 529044.5s

################################################################################
                    [1m Learning iteration 1005/200000 [0m

                       Computation: 2957 steps/s (collection: 0.634s, learning 2.135s)
               Value function loss: 71562.2363
                    Surrogate loss: 0.0132
             Mean action noise std: 0.91
                       Mean reward: 3807.45
               Mean episode length: 264.16
                 Mean success rate: 44.50
                  Mean reward/step: 15.02
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 8241152
                    Iteration time: 2.77s
                        Total time: 2674.63s
                               ETA: 529063.8s

################################################################################
                    [1m Learning iteration 1006/200000 [0m

                       Computation: 3052 steps/s (collection: 0.567s, learning 2.117s)
               Value function loss: 71959.0508
                    Surrogate loss: 0.0117
             Mean action noise std: 0.91
                       Mean reward: 3954.58
               Mean episode length: 268.63
                 Mean success rate: 45.50
                  Mean reward/step: 14.38
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 8249344
                    Iteration time: 2.68s
                        Total time: 2677.32s
                               ETA: 529066.2s

################################################################################
                    [1m Learning iteration 1007/200000 [0m

                       Computation: 3085 steps/s (collection: 0.554s, learning 2.101s)
               Value function loss: 51204.0776
                    Surrogate loss: 0.0145
             Mean action noise std: 0.91
                       Mean reward: 4154.22
               Mean episode length: 279.53
                 Mean success rate: 47.50
                  Mean reward/step: 14.28
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 2.65s
                        Total time: 2679.97s
                               ETA: 529062.7s

################################################################################
                    [1m Learning iteration 1008/200000 [0m

                       Computation: 3022 steps/s (collection: 0.565s, learning 2.145s)
               Value function loss: 57167.1066
                    Surrogate loss: 0.0151
             Mean action noise std: 0.91
                       Mean reward: 4060.93
               Mean episode length: 273.34
                 Mean success rate: 45.50
                  Mean reward/step: 14.66
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 8265728
                    Iteration time: 2.71s
                        Total time: 2682.68s
                               ETA: 529070.3s

################################################################################
                    [1m Learning iteration 1009/200000 [0m

                       Computation: 2903 steps/s (collection: 0.636s, learning 2.185s)
               Value function loss: 52871.1063
                    Surrogate loss: 0.0151
             Mean action noise std: 0.91
                       Mean reward: 3922.78
               Mean episode length: 269.83
                 Mean success rate: 43.50
                  Mean reward/step: 14.93
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 8273920
                    Iteration time: 2.82s
                        Total time: 2685.50s
                               ETA: 529099.6s

################################################################################
                    [1m Learning iteration 1010/200000 [0m

                       Computation: 3028 steps/s (collection: 0.597s, learning 2.108s)
               Value function loss: 55463.7192
                    Surrogate loss: 0.0168
             Mean action noise std: 0.91
                       Mean reward: 3911.96
               Mean episode length: 266.75
                 Mean success rate: 45.00
                  Mean reward/step: 15.35
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 8282112
                    Iteration time: 2.70s
                        Total time: 2688.21s
                               ETA: 529105.9s

################################################################################
                    [1m Learning iteration 1011/200000 [0m

                       Computation: 2985 steps/s (collection: 0.603s, learning 2.140s)
               Value function loss: 38294.6106
                    Surrogate loss: 0.0173
             Mean action noise std: 0.91
                       Mean reward: 3420.51
               Mean episode length: 246.62
                 Mean success rate: 38.50
                  Mean reward/step: 16.08
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 8290304
                    Iteration time: 2.74s
                        Total time: 2690.95s
                               ETA: 529119.9s

################################################################################
                    [1m Learning iteration 1012/200000 [0m

                       Computation: 2971 steps/s (collection: 0.621s, learning 2.135s)
               Value function loss: 64529.1458
                    Surrogate loss: 0.0167
             Mean action noise std: 0.91
                       Mean reward: 3169.28
               Mean episode length: 233.39
                 Mean success rate: 37.00
                  Mean reward/step: 16.81
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 8298496
                    Iteration time: 2.76s
                        Total time: 2693.71s
                               ETA: 529136.4s

################################################################################
                    [1m Learning iteration 1013/200000 [0m

                       Computation: 2952 steps/s (collection: 0.632s, learning 2.143s)
               Value function loss: 54053.0596
                    Surrogate loss: 0.0207
             Mean action noise std: 0.91
                       Mean reward: 2906.40
               Mean episode length: 226.51
                 Mean success rate: 34.00
                  Mean reward/step: 16.65
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 8306688
                    Iteration time: 2.77s
                        Total time: 2696.48s
                               ETA: 529156.3s

################################################################################
                    [1m Learning iteration 1014/200000 [0m

                       Computation: 3028 steps/s (collection: 0.610s, learning 2.094s)
               Value function loss: 35527.4595
                    Surrogate loss: 0.0333
             Mean action noise std: 0.91
                       Mean reward: 2952.15
               Mean episode length: 227.45
                 Mean success rate: 34.50
                  Mean reward/step: 16.37
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 8314880
                    Iteration time: 2.70s
                        Total time: 2699.18s
                               ETA: 529162.5s

################################################################################
                    [1m Learning iteration 1015/200000 [0m

                       Computation: 3060 steps/s (collection: 0.552s, learning 2.125s)
               Value function loss: 73849.0001
                    Surrogate loss: 0.0206
             Mean action noise std: 0.91
                       Mean reward: 3226.17
               Mean episode length: 235.12
                 Mean success rate: 36.00
                  Mean reward/step: 16.45
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 8323072
                    Iteration time: 2.68s
                        Total time: 2701.86s
                               ETA: 529163.2s

################################################################################
                    [1m Learning iteration 1016/200000 [0m

                       Computation: 2995 steps/s (collection: 0.611s, learning 2.124s)
               Value function loss: 76018.7184
                    Surrogate loss: 0.0168
             Mean action noise std: 0.91
                       Mean reward: 3346.80
               Mean episode length: 235.79
                 Mean success rate: 36.00
                  Mean reward/step: 16.26
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 8331264
                    Iteration time: 2.73s
                        Total time: 2704.60s
                               ETA: 529175.3s

################################################################################
                    [1m Learning iteration 1017/200000 [0m

                       Computation: 3067 steps/s (collection: 0.571s, learning 2.099s)
               Value function loss: 68856.6790
                    Surrogate loss: 0.0142
             Mean action noise std: 0.91
                       Mean reward: 3844.62
               Mean episode length: 255.94
                 Mean success rate: 41.00
                  Mean reward/step: 16.12
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8339456
                    Iteration time: 2.67s
                        Total time: 2707.27s
                               ETA: 529174.8s

################################################################################
                    [1m Learning iteration 1018/200000 [0m

                       Computation: 3027 steps/s (collection: 0.582s, learning 2.123s)
               Value function loss: 58472.4316
                    Surrogate loss: 0.0136
             Mean action noise std: 0.91
                       Mean reward: 4320.95
               Mean episode length: 270.23
                 Mean success rate: 45.00
                  Mean reward/step: 16.01
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8347648
                    Iteration time: 2.71s
                        Total time: 2709.97s
                               ETA: 529181.2s

################################################################################
                    [1m Learning iteration 1019/200000 [0m

                       Computation: 2944 steps/s (collection: 0.624s, learning 2.159s)
               Value function loss: 32528.8292
                    Surrogate loss: 0.0142
             Mean action noise std: 0.90
                       Mean reward: 4418.59
               Mean episode length: 275.18
                 Mean success rate: 45.00
                  Mean reward/step: 16.34
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 2.78s
                        Total time: 2712.75s
                               ETA: 529202.5s

################################################################################
                    [1m Learning iteration 1020/200000 [0m

                       Computation: 3040 steps/s (collection: 0.598s, learning 2.096s)
               Value function loss: 68963.7327
                    Surrogate loss: 0.0114
             Mean action noise std: 0.90
                       Mean reward: 4741.31
               Mean episode length: 285.28
                 Mean success rate: 48.00
                  Mean reward/step: 15.83
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 8364032
                    Iteration time: 2.69s
                        Total time: 2715.45s
                               ETA: 529206.6s

################################################################################
                    [1m Learning iteration 1021/200000 [0m

                       Computation: 3044 steps/s (collection: 0.601s, learning 2.090s)
               Value function loss: 67306.8421
                    Surrogate loss: 0.0112
             Mean action noise std: 0.90
                       Mean reward: 4882.99
               Mean episode length: 293.58
                 Mean success rate: 49.00
                  Mean reward/step: 15.09
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 8372224
                    Iteration time: 2.69s
                        Total time: 2718.14s
                               ETA: 529210.0s

################################################################################
                    [1m Learning iteration 1022/200000 [0m

                       Computation: 3102 steps/s (collection: 0.561s, learning 2.080s)
               Value function loss: 83154.1892
                    Surrogate loss: 0.0121
             Mean action noise std: 0.90
                       Mean reward: 4786.31
               Mean episode length: 289.55
                 Mean success rate: 48.00
                  Mean reward/step: 14.75
       Mean episode length/episode: 27.22
--------------------------------------------------------------------------------
                   Total timesteps: 8380416
                    Iteration time: 2.64s
                        Total time: 2720.78s
                               ETA: 529203.7s

################################################################################
                    [1m Learning iteration 1023/200000 [0m

                       Computation: 2981 steps/s (collection: 0.586s, learning 2.162s)
               Value function loss: 49756.1234
                    Surrogate loss: 0.0143
             Mean action noise std: 0.91
                       Mean reward: 4434.69
               Mean episode length: 277.38
                 Mean success rate: 44.00
                  Mean reward/step: 14.86
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8388608
                    Iteration time: 2.75s
                        Total time: 2723.53s
                               ETA: 529218.1s

################################################################################
                    [1m Learning iteration 1024/200000 [0m

                       Computation: 3042 steps/s (collection: 0.579s, learning 2.113s)
               Value function loss: 69846.9495
                    Surrogate loss: 0.0124
             Mean action noise std: 0.91
                       Mean reward: 4410.93
               Mean episode length: 278.13
                 Mean success rate: 43.50
                  Mean reward/step: 15.56
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8396800
                    Iteration time: 2.69s
                        Total time: 2726.22s
                               ETA: 529221.8s

################################################################################
                    [1m Learning iteration 1025/200000 [0m

                       Computation: 3052 steps/s (collection: 0.603s, learning 2.080s)
               Value function loss: 62778.5567
                    Surrogate loss: 0.0127
             Mean action noise std: 0.91
                       Mean reward: 4505.52
               Mean episode length: 284.35
                 Mean success rate: 44.50
                  Mean reward/step: 15.67
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8404992
                    Iteration time: 2.68s
                        Total time: 2728.90s
                               ETA: 529223.7s

################################################################################
                    [1m Learning iteration 1026/200000 [0m

                       Computation: 3060 steps/s (collection: 0.587s, learning 2.089s)
               Value function loss: 42389.6257
                    Surrogate loss: 0.0182
             Mean action noise std: 0.91
                       Mean reward: 4149.18
               Mean episode length: 269.82
                 Mean success rate: 41.00
                  Mean reward/step: 16.44
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 8413184
                    Iteration time: 2.68s
                        Total time: 2731.58s
                               ETA: 529224.4s

################################################################################
                    [1m Learning iteration 1027/200000 [0m

                       Computation: 2981 steps/s (collection: 0.640s, learning 2.107s)
               Value function loss: 63250.0670
                    Surrogate loss: 0.0204
             Mean action noise std: 0.91
                       Mean reward: 4176.70
               Mean episode length: 269.77
                 Mean success rate: 41.00
                  Mean reward/step: 16.44
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8421376
                    Iteration time: 2.75s
                        Total time: 2734.33s
                               ETA: 529238.6s

################################################################################
                    [1m Learning iteration 1028/200000 [0m

                       Computation: 3136 steps/s (collection: 0.550s, learning 2.061s)
               Value function loss: 75591.4293
                    Surrogate loss: 0.0160
             Mean action noise std: 0.91
                       Mean reward: 4394.76
               Mean episode length: 278.50
                 Mean success rate: 42.50
                  Mean reward/step: 16.22
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 8429568
                    Iteration time: 2.61s
                        Total time: 2736.94s
                               ETA: 529226.7s

################################################################################
                    [1m Learning iteration 1029/200000 [0m

                       Computation: 3072 steps/s (collection: 0.564s, learning 2.102s)
               Value function loss: 50696.1259
                    Surrogate loss: 0.0184
             Mean action noise std: 0.91
                       Mean reward: 4345.62
               Mean episode length: 272.54
                 Mean success rate: 42.50
                  Mean reward/step: 15.25
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8437760
                    Iteration time: 2.67s
                        Total time: 2739.61s
                               ETA: 529225.3s

################################################################################
                    [1m Learning iteration 1030/200000 [0m

                       Computation: 3025 steps/s (collection: 0.600s, learning 2.107s)
               Value function loss: 51407.4729
                    Surrogate loss: 0.0196
             Mean action noise std: 0.91
                       Mean reward: 4176.20
               Mean episode length: 265.58
                 Mean success rate: 41.00
                  Mean reward/step: 14.68
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 8445952
                    Iteration time: 2.71s
                        Total time: 2742.31s
                               ETA: 529231.8s

################################################################################
                    [1m Learning iteration 1031/200000 [0m

                       Computation: 3043 steps/s (collection: 0.580s, learning 2.112s)
               Value function loss: 51062.8810
                    Surrogate loss: 0.0164
             Mean action noise std: 0.91
                       Mean reward: 3754.17
               Mean episode length: 245.88
                 Mean success rate: 38.00
                  Mean reward/step: 14.72
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 2.69s
                        Total time: 2745.00s
                               ETA: 529235.3s

################################################################################
                    [1m Learning iteration 1032/200000 [0m

                       Computation: 3039 steps/s (collection: 0.624s, learning 2.071s)
               Value function loss: 76301.3831
                    Surrogate loss: 0.0120
             Mean action noise std: 0.91
                       Mean reward: 4110.73
               Mean episode length: 263.62
                 Mean success rate: 42.00
                  Mean reward/step: 14.20
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 8462336
                    Iteration time: 2.69s
                        Total time: 2747.70s
                               ETA: 529239.4s

################################################################################
                    [1m Learning iteration 1033/200000 [0m

                       Computation: 3088 steps/s (collection: 0.587s, learning 2.066s)
               Value function loss: 44742.7753
                    Surrogate loss: 0.0123
             Mean action noise std: 0.91
                       Mean reward: 3786.61
               Mean episode length: 250.47
                 Mean success rate: 39.50
                  Mean reward/step: 14.33
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 8470528
                    Iteration time: 2.65s
                        Total time: 2750.35s
                               ETA: 529235.4s

################################################################################
                    [1m Learning iteration 1034/200000 [0m

                       Computation: 3065 steps/s (collection: 0.581s, learning 2.092s)
               Value function loss: 58034.6051
                    Surrogate loss: 0.0142
             Mean action noise std: 0.91
                       Mean reward: 3192.65
               Mean episode length: 231.60
                 Mean success rate: 33.50
                  Mean reward/step: 15.14
       Mean episode length/episode: 27.31
--------------------------------------------------------------------------------
                   Total timesteps: 8478720
                    Iteration time: 2.67s
                        Total time: 2753.02s
                               ETA: 529235.1s

################################################################################
                    [1m Learning iteration 1035/200000 [0m

                       Computation: 2982 steps/s (collection: 0.636s, learning 2.111s)
               Value function loss: 61224.5974
                    Surrogate loss: 0.0125
             Mean action noise std: 0.91
                       Mean reward: 3256.81
               Mean episode length: 231.96
                 Mean success rate: 34.00
                  Mean reward/step: 15.59
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 8486912
                    Iteration time: 2.75s
                        Total time: 2755.77s
                               ETA: 529249.1s

################################################################################
                    [1m Learning iteration 1036/200000 [0m

                       Computation: 3061 steps/s (collection: 0.585s, learning 2.090s)
               Value function loss: 72717.1458
                    Surrogate loss: 0.0166
             Mean action noise std: 0.91
                       Mean reward: 3485.02
               Mean episode length: 238.16
                 Mean success rate: 34.50
                  Mean reward/step: 15.45
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 8495104
                    Iteration time: 2.68s
                        Total time: 2758.45s
                               ETA: 529249.4s

################################################################################
                    [1m Learning iteration 1037/200000 [0m

                       Computation: 3068 steps/s (collection: 0.568s, learning 2.102s)
               Value function loss: 64380.0351
                    Surrogate loss: 0.0140
             Mean action noise std: 0.91
                       Mean reward: 3275.74
               Mean episode length: 230.81
                 Mean success rate: 33.00
                  Mean reward/step: 15.13
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 8503296
                    Iteration time: 2.67s
                        Total time: 2761.12s
                               ETA: 529248.6s

################################################################################
                    [1m Learning iteration 1038/200000 [0m

                       Computation: 2982 steps/s (collection: 0.620s, learning 2.127s)
               Value function loss: 76786.4857
                    Surrogate loss: 0.0131
             Mean action noise std: 0.91
                       Mean reward: 3580.27
               Mean episode length: 242.37
                 Mean success rate: 35.50
                  Mean reward/step: 15.03
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8511488
                    Iteration time: 2.75s
                        Total time: 2763.86s
                               ETA: 529262.5s

################################################################################
                    [1m Learning iteration 1039/200000 [0m

                       Computation: 3040 steps/s (collection: 0.596s, learning 2.099s)
               Value function loss: 72200.6965
                    Surrogate loss: 0.0143
             Mean action noise std: 0.91
                       Mean reward: 3857.35
               Mean episode length: 246.87
                 Mean success rate: 38.00
                  Mean reward/step: 15.10
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 8519680
                    Iteration time: 2.69s
                        Total time: 2766.56s
                               ETA: 529266.4s

################################################################################
                    [1m Learning iteration 1040/200000 [0m

                       Computation: 3114 steps/s (collection: 0.538s, learning 2.092s)
               Value function loss: 54970.9235
                    Surrogate loss: 0.0155
             Mean action noise std: 0.91
                       Mean reward: 3905.41
               Mean episode length: 250.00
                 Mean success rate: 38.50
                  Mean reward/step: 15.45
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 8527872
                    Iteration time: 2.63s
                        Total time: 2769.19s
                               ETA: 529258.0s

################################################################################
                    [1m Learning iteration 1041/200000 [0m

                       Computation: 3062 steps/s (collection: 0.582s, learning 2.093s)
               Value function loss: 64854.5495
                    Surrogate loss: 0.0133
             Mean action noise std: 0.91
                       Mean reward: 3864.46
               Mean episode length: 247.24
                 Mean success rate: 37.50
                  Mean reward/step: 16.03
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 8536064
                    Iteration time: 2.67s
                        Total time: 2771.86s
                               ETA: 529258.1s

################################################################################
                    [1m Learning iteration 1042/200000 [0m

                       Computation: 3060 steps/s (collection: 0.577s, learning 2.100s)
               Value function loss: 51016.2749
                    Surrogate loss: 0.0141
             Mean action noise std: 0.91
                       Mean reward: 4011.49
               Mean episode length: 249.95
                 Mean success rate: 39.00
                  Mean reward/step: 16.13
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 8544256
                    Iteration time: 2.68s
                        Total time: 2774.54s
                               ETA: 529258.7s

################################################################################
                    [1m Learning iteration 1043/200000 [0m

                       Computation: 3062 steps/s (collection: 0.559s, learning 2.116s)
               Value function loss: 69628.3209
                    Surrogate loss: 0.0124
             Mean action noise std: 0.91
                       Mean reward: 3925.31
               Mean episode length: 247.99
                 Mean success rate: 37.50
                  Mean reward/step: 16.05
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 2.68s
                        Total time: 2777.21s
                               ETA: 529258.9s

################################################################################
                    [1m Learning iteration 1044/200000 [0m

                       Computation: 3074 steps/s (collection: 0.567s, learning 2.098s)
               Value function loss: 71291.0140
                    Surrogate loss: 0.0146
             Mean action noise std: 0.91
                       Mean reward: 3646.12
               Mean episode length: 233.91
                 Mean success rate: 35.00
                  Mean reward/step: 16.30
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 8560640
                    Iteration time: 2.66s
                        Total time: 2779.88s
                               ETA: 529257.1s

################################################################################
                    [1m Learning iteration 1045/200000 [0m

                       Computation: 3054 steps/s (collection: 0.576s, learning 2.106s)
               Value function loss: 48135.1138
                    Surrogate loss: 0.0186
             Mean action noise std: 0.91
                       Mean reward: 3420.66
               Mean episode length: 227.49
                 Mean success rate: 33.50
                  Mean reward/step: 16.47
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 8568832
                    Iteration time: 2.68s
                        Total time: 2782.56s
                               ETA: 529258.7s

################################################################################
                    [1m Learning iteration 1046/200000 [0m

                       Computation: 2997 steps/s (collection: 0.634s, learning 2.099s)
               Value function loss: 68984.7350
                    Surrogate loss: 0.0159
             Mean action noise std: 0.91
                       Mean reward: 3487.35
               Mean episode length: 226.28
                 Mean success rate: 34.00
                  Mean reward/step: 16.83
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 8577024
                    Iteration time: 2.73s
                        Total time: 2785.29s
                               ETA: 529269.8s

################################################################################
                    [1m Learning iteration 1047/200000 [0m

                       Computation: 3025 steps/s (collection: 0.615s, learning 2.093s)
               Value function loss: 98620.3420
                    Surrogate loss: 0.0141
             Mean action noise std: 0.91
                       Mean reward: 3780.54
               Mean episode length: 239.75
                 Mean success rate: 37.50
                  Mean reward/step: 16.61
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 8585216
                    Iteration time: 2.71s
                        Total time: 2788.00s
                               ETA: 529276.1s

################################################################################
                    [1m Learning iteration 1048/200000 [0m

                       Computation: 3093 steps/s (collection: 0.571s, learning 2.077s)
               Value function loss: 52644.1372
                    Surrogate loss: 0.0164
             Mean action noise std: 0.91
                       Mean reward: 3644.47
               Mean episode length: 233.59
                 Mean success rate: 36.00
                  Mean reward/step: 16.24
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8593408
                    Iteration time: 2.65s
                        Total time: 2790.65s
                               ETA: 529271.1s

################################################################################
                    [1m Learning iteration 1049/200000 [0m

                       Computation: 3082 steps/s (collection: 0.546s, learning 2.111s)
               Value function loss: 63821.6367
                    Surrogate loss: 0.0176
             Mean action noise std: 0.91
                       Mean reward: 3753.51
               Mean episode length: 238.37
                 Mean success rate: 38.00
                  Mean reward/step: 16.80
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8601600
                    Iteration time: 2.66s
                        Total time: 2793.31s
                               ETA: 529267.9s

################################################################################
                    [1m Learning iteration 1050/200000 [0m

                       Computation: 3014 steps/s (collection: 0.600s, learning 2.117s)
               Value function loss: 57257.9101
                    Surrogate loss: 0.0161
             Mean action noise std: 0.91
                       Mean reward: 4116.39
               Mean episode length: 253.63
                 Mean success rate: 42.00
                  Mean reward/step: 16.72
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 8609792
                    Iteration time: 2.72s
                        Total time: 2796.02s
                               ETA: 529276.0s

################################################################################
                    [1m Learning iteration 1051/200000 [0m

                       Computation: 3100 steps/s (collection: 0.559s, learning 2.083s)
               Value function loss: 82896.7711
                    Surrogate loss: 0.0164
             Mean action noise std: 0.91
                       Mean reward: 4316.59
               Mean episode length: 260.90
                 Mean success rate: 44.00
                  Mean reward/step: 16.53
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 8617984
                    Iteration time: 2.64s
                        Total time: 2798.67s
                               ETA: 529269.9s

################################################################################
                    [1m Learning iteration 1052/200000 [0m

                       Computation: 3068 steps/s (collection: 0.578s, learning 2.091s)
               Value function loss: 77717.5730
                    Surrogate loss: 0.0169
             Mean action noise std: 0.91
                       Mean reward: 4783.65
               Mean episode length: 280.72
                 Mean success rate: 47.50
                  Mean reward/step: 15.97
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 8626176
                    Iteration time: 2.67s
                        Total time: 2801.34s
                               ETA: 529269.0s

################################################################################
                    [1m Learning iteration 1053/200000 [0m

                       Computation: 2962 steps/s (collection: 0.645s, learning 2.120s)
               Value function loss: 86163.4596
                    Surrogate loss: 0.0150
             Mean action noise std: 0.90
                       Mean reward: 4910.11
               Mean episode length: 283.56
                 Mean success rate: 48.50
                  Mean reward/step: 15.77
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 8634368
                    Iteration time: 2.77s
                        Total time: 2804.10s
                               ETA: 529286.1s

################################################################################
                    [1m Learning iteration 1054/200000 [0m

                       Computation: 3035 steps/s (collection: 0.612s, learning 2.087s)
               Value function loss: 61152.4851
                    Surrogate loss: 0.0178
             Mean action noise std: 0.90
                       Mean reward: 4565.60
               Mean episode length: 269.27
                 Mean success rate: 45.00
                  Mean reward/step: 15.76
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 8642560
                    Iteration time: 2.70s
                        Total time: 2806.80s
                               ETA: 529290.7s

################################################################################
                    [1m Learning iteration 1055/200000 [0m

                       Computation: 3093 steps/s (collection: 0.565s, learning 2.083s)
               Value function loss: 79115.3396
                    Surrogate loss: 0.0153
             Mean action noise std: 0.90
                       Mean reward: 4572.36
               Mean episode length: 272.88
                 Mean success rate: 44.50
                  Mean reward/step: 15.77
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 2.65s
                        Total time: 2809.45s
                               ETA: 529285.7s

################################################################################
                    [1m Learning iteration 1056/200000 [0m

                       Computation: 3060 steps/s (collection: 0.570s, learning 2.107s)
               Value function loss: 59950.0267
                    Surrogate loss: 0.0161
             Mean action noise std: 0.90
                       Mean reward: 4063.19
               Mean episode length: 252.39
                 Mean success rate: 39.50
                  Mean reward/step: 15.79
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 8658944
                    Iteration time: 2.68s
                        Total time: 2812.13s
                               ETA: 529286.1s

################################################################################
                    [1m Learning iteration 1057/200000 [0m

                       Computation: 3077 steps/s (collection: 0.559s, learning 2.103s)
               Value function loss: 54780.7929
                    Surrogate loss: 0.0144
             Mean action noise std: 0.90
                       Mean reward: 3965.46
               Mean episode length: 251.95
                 Mean success rate: 38.50
                  Mean reward/step: 15.21
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 8667136
                    Iteration time: 2.66s
                        Total time: 2814.79s
                               ETA: 529283.7s

################################################################################
                    [1m Learning iteration 1058/200000 [0m

                       Computation: 3080 steps/s (collection: 0.574s, learning 2.086s)
               Value function loss: 43766.2849
                    Surrogate loss: 0.0172
             Mean action noise std: 0.90
                       Mean reward: 4090.39
               Mean episode length: 254.90
                 Mean success rate: 39.50
                  Mean reward/step: 15.52
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 8675328
                    Iteration time: 2.66s
                        Total time: 2817.45s
                               ETA: 529280.8s

################################################################################
                    [1m Learning iteration 1059/200000 [0m

                       Computation: 3105 steps/s (collection: 0.568s, learning 2.070s)
               Value function loss: 72229.9524
                    Surrogate loss: 0.0199
             Mean action noise std: 0.90
                       Mean reward: 3717.92
               Mean episode length: 248.12
                 Mean success rate: 36.50
                  Mean reward/step: 16.09
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 8683520
                    Iteration time: 2.64s
                        Total time: 2820.08s
                               ETA: 529273.9s

################################################################################
                    [1m Learning iteration 1060/200000 [0m

                       Computation: 3026 steps/s (collection: 0.599s, learning 2.108s)
               Value function loss: 69439.5813
                    Surrogate loss: 0.0162
             Mean action noise std: 0.90
                       Mean reward: 3599.93
               Mean episode length: 246.03
                 Mean success rate: 35.00
                  Mean reward/step: 16.35
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 8691712
                    Iteration time: 2.71s
                        Total time: 2822.79s
                               ETA: 529279.9s

################################################################################
                    [1m Learning iteration 1061/200000 [0m

                       Computation: 2973 steps/s (collection: 0.643s, learning 2.112s)
               Value function loss: 81907.0790
                    Surrogate loss: 0.0164
             Mean action noise std: 0.90
                       Mean reward: 3997.66
               Mean episode length: 257.14
                 Mean success rate: 38.00
                  Mean reward/step: 17.48
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8699904
                    Iteration time: 2.76s
                        Total time: 2825.55s
                               ETA: 529295.0s

################################################################################
                    [1m Learning iteration 1062/200000 [0m

                       Computation: 3101 steps/s (collection: 0.557s, learning 2.085s)
               Value function loss: 57976.4233
                    Surrogate loss: 0.0189
             Mean action noise std: 0.90
                       Mean reward: 4019.15
               Mean episode length: 256.57
                 Mean success rate: 37.50
                  Mean reward/step: 18.13
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8708096
                    Iteration time: 2.64s
                        Total time: 2828.19s
                               ETA: 529288.8s

################################################################################
                    [1m Learning iteration 1063/200000 [0m

                       Computation: 3080 steps/s (collection: 0.605s, learning 2.055s)
               Value function loss: 75607.2500
                    Surrogate loss: 0.0124
             Mean action noise std: 0.90
                       Mean reward: 4335.45
               Mean episode length: 271.79
                 Mean success rate: 41.00
                  Mean reward/step: 18.27
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8716288
                    Iteration time: 2.66s
                        Total time: 2830.85s
                               ETA: 529285.8s

################################################################################
                    [1m Learning iteration 1064/200000 [0m

                       Computation: 3049 steps/s (collection: 0.576s, learning 2.111s)
               Value function loss: 67721.4351
                    Surrogate loss: 0.0132
             Mean action noise std: 0.90
                       Mean reward: 4573.80
               Mean episode length: 279.93
                 Mean success rate: 42.50
                  Mean reward/step: 18.64
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 8724480
                    Iteration time: 2.69s
                        Total time: 2833.53s
                               ETA: 529288.0s

################################################################################
                    [1m Learning iteration 1065/200000 [0m

                       Computation: 3017 steps/s (collection: 0.582s, learning 2.133s)
               Value function loss: 83580.4720
                    Surrogate loss: 0.0160
             Mean action noise std: 0.90
                       Mean reward: 4754.44
               Mean episode length: 285.56
                 Mean success rate: 44.00
                  Mean reward/step: 18.95
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 8732672
                    Iteration time: 2.71s
                        Total time: 2836.25s
                               ETA: 529295.5s

################################################################################
                    [1m Learning iteration 1066/200000 [0m

                       Computation: 3107 steps/s (collection: 0.552s, learning 2.084s)
               Value function loss: 95263.7822
                    Surrogate loss: 0.0163
             Mean action noise std: 0.90
                       Mean reward: 5161.57
               Mean episode length: 294.12
                 Mean success rate: 48.50
                  Mean reward/step: 18.31
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8740864
                    Iteration time: 2.64s
                        Total time: 2838.88s
                               ETA: 529288.3s

################################################################################
                    [1m Learning iteration 1067/200000 [0m

                       Computation: 3055 steps/s (collection: 0.585s, learning 2.096s)
               Value function loss: 91847.5062
                    Surrogate loss: 0.0141
             Mean action noise std: 0.90
                       Mean reward: 5324.19
               Mean episode length: 301.56
                 Mean success rate: 50.50
                  Mean reward/step: 17.75
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 2.68s
                        Total time: 2841.57s
                               ETA: 529289.5s

################################################################################
                    [1m Learning iteration 1068/200000 [0m

                       Computation: 2976 steps/s (collection: 0.656s, learning 2.096s)
               Value function loss: 62033.9231
                    Surrogate loss: 0.0196
             Mean action noise std: 0.90
                       Mean reward: 5346.93
               Mean episode length: 305.67
                 Mean success rate: 51.50
                  Mean reward/step: 17.53
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 8757248
                    Iteration time: 2.75s
                        Total time: 2844.32s
                               ETA: 529304.0s

################################################################################
                    [1m Learning iteration 1069/200000 [0m

                       Computation: 3042 steps/s (collection: 0.583s, learning 2.110s)
               Value function loss: 64494.3974
                    Surrogate loss: 0.0195
             Mean action noise std: 0.90
                       Mean reward: 5274.35
               Mean episode length: 301.45
                 Mean success rate: 51.50
                  Mean reward/step: 17.14
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8765440
                    Iteration time: 2.69s
                        Total time: 2847.01s
                               ETA: 529307.3s

################################################################################
                    [1m Learning iteration 1070/200000 [0m

                       Computation: 3072 steps/s (collection: 0.564s, learning 2.102s)
               Value function loss: 83134.1246
                    Surrogate loss: 0.0134
             Mean action noise std: 0.90
                       Mean reward: 5336.75
               Mean episode length: 305.77
                 Mean success rate: 51.50
                  Mean reward/step: 17.06
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 8773632
                    Iteration time: 2.67s
                        Total time: 2849.68s
                               ETA: 529305.6s

################################################################################
                    [1m Learning iteration 1071/200000 [0m

                       Computation: 3064 steps/s (collection: 0.573s, learning 2.100s)
               Value function loss: 71728.9818
                    Surrogate loss: 0.0155
             Mean action noise std: 0.90
                       Mean reward: 4957.21
               Mean episode length: 288.87
                 Mean success rate: 48.50
                  Mean reward/step: 16.00
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 8781824
                    Iteration time: 2.67s
                        Total time: 2852.35s
                               ETA: 529305.2s

################################################################################
                    [1m Learning iteration 1072/200000 [0m

                       Computation: 2937 steps/s (collection: 0.609s, learning 2.180s)
               Value function loss: 118150.7279
                    Surrogate loss: 0.0155
             Mean action noise std: 0.90
                       Mean reward: 4911.02
               Mean episode length: 291.50
                 Mean success rate: 48.00
                  Mean reward/step: 15.37
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 8790016
                    Iteration time: 2.79s
                        Total time: 2855.14s
                               ETA: 529326.3s

################################################################################
                    [1m Learning iteration 1073/200000 [0m

                       Computation: 3071 steps/s (collection: 0.577s, learning 2.090s)
               Value function loss: 52922.0861
                    Surrogate loss: 0.0166
             Mean action noise std: 0.90
                       Mean reward: 4736.32
               Mean episode length: 283.34
                 Mean success rate: 46.00
                  Mean reward/step: 13.85
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8798208
                    Iteration time: 2.67s
                        Total time: 2857.81s
                               ETA: 529324.8s

################################################################################
                    [1m Learning iteration 1074/200000 [0m

                       Computation: 3063 steps/s (collection: 0.600s, learning 2.074s)
               Value function loss: 80359.1772
                    Surrogate loss: 0.0166
             Mean action noise std: 0.90
                       Mean reward: 4863.28
               Mean episode length: 288.68
                 Mean success rate: 47.00
                  Mean reward/step: 14.26
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 8806400
                    Iteration time: 2.67s
                        Total time: 2860.48s
                               ETA: 529324.7s

################################################################################
                    [1m Learning iteration 1075/200000 [0m

                       Computation: 3000 steps/s (collection: 0.578s, learning 2.153s)
               Value function loss: 88405.1935
                    Surrogate loss: 0.0135
             Mean action noise std: 0.90
                       Mean reward: 5113.37
               Mean episode length: 296.00
                 Mean success rate: 49.50
                  Mean reward/step: 14.33
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 8814592
                    Iteration time: 2.73s
                        Total time: 2863.21s
                               ETA: 529334.9s

################################################################################
                    [1m Learning iteration 1076/200000 [0m

                       Computation: 3037 steps/s (collection: 0.595s, learning 2.101s)
               Value function loss: 78768.5956
                    Surrogate loss: 0.0134
             Mean action noise std: 0.90
                       Mean reward: 5364.79
               Mean episode length: 304.33
                 Mean success rate: 52.50
                  Mean reward/step: 14.64
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8822784
                    Iteration time: 2.70s
                        Total time: 2865.91s
                               ETA: 529338.8s

################################################################################
                    [1m Learning iteration 1077/200000 [0m

                       Computation: 2994 steps/s (collection: 0.639s, learning 2.096s)
               Value function loss: 57877.6135
                    Surrogate loss: 0.0165
             Mean action noise std: 0.90
                       Mean reward: 5039.62
               Mean episode length: 293.20
                 Mean success rate: 49.50
                  Mean reward/step: 14.41
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 8830976
                    Iteration time: 2.74s
                        Total time: 2868.64s
                               ETA: 529349.9s

################################################################################
                    [1m Learning iteration 1078/200000 [0m

                       Computation: 3079 steps/s (collection: 0.579s, learning 2.081s)
               Value function loss: 51818.2051
                    Surrogate loss: 0.0180
             Mean action noise std: 0.90
                       Mean reward: 4847.19
               Mean episode length: 285.39
                 Mean success rate: 48.00
                  Mean reward/step: 15.19
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8839168
                    Iteration time: 2.66s
                        Total time: 2871.30s
                               ETA: 529347.1s

################################################################################
                    [1m Learning iteration 1079/200000 [0m

                       Computation: 3007 steps/s (collection: 0.609s, learning 2.115s)
               Value function loss: 41685.4733
                    Surrogate loss: 0.0125
             Mean action noise std: 0.90
                       Mean reward: 4590.66
               Mean episode length: 274.58
                 Mean success rate: 45.00
                  Mean reward/step: 15.93
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 2.72s
                        Total time: 2874.03s
                               ETA: 529356.0s

################################################################################
                    [1m Learning iteration 1080/200000 [0m

                       Computation: 3002 steps/s (collection: 0.581s, learning 2.147s)
               Value function loss: 44940.6940
                    Surrogate loss: 0.0149
             Mean action noise std: 0.90
                       Mean reward: 4178.33
               Mean episode length: 262.02
                 Mean success rate: 42.50
                  Mean reward/step: 16.99
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 8855552
                    Iteration time: 2.73s
                        Total time: 2876.76s
                               ETA: 529365.7s

################################################################################
                    [1m Learning iteration 1081/200000 [0m

                       Computation: 3062 steps/s (collection: 0.563s, learning 2.112s)
               Value function loss: 60500.5120
                    Surrogate loss: 0.0171
             Mean action noise std: 0.90
                       Mean reward: 4286.57
               Mean episode length: 264.96
                 Mean success rate: 43.50
                  Mean reward/step: 17.44
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 8863744
                    Iteration time: 2.68s
                        Total time: 2879.43s
                               ETA: 529365.6s

################################################################################
                    [1m Learning iteration 1082/200000 [0m

                       Computation: 3050 steps/s (collection: 0.565s, learning 2.121s)
               Value function loss: 49950.6273
                    Surrogate loss: 0.0142
             Mean action noise std: 0.90
                       Mean reward: 3724.42
               Mean episode length: 252.26
                 Mean success rate: 39.50
                  Mean reward/step: 17.99
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 8871936
                    Iteration time: 2.69s
                        Total time: 2882.12s
                               ETA: 529367.4s

################################################################################
                    [1m Learning iteration 1083/200000 [0m

                       Computation: 2941 steps/s (collection: 0.641s, learning 2.144s)
               Value function loss: 53180.4764
                    Surrogate loss: 0.0157
             Mean action noise std: 0.90
                       Mean reward: 3565.99
               Mean episode length: 249.72
                 Mean success rate: 38.00
                  Mean reward/step: 18.69
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 8880128
                    Iteration time: 2.79s
                        Total time: 2884.90s
                               ETA: 529387.5s

################################################################################
                    [1m Learning iteration 1084/200000 [0m

                       Computation: 3039 steps/s (collection: 0.606s, learning 2.089s)
               Value function loss: 49535.0960
                    Surrogate loss: 0.0158
             Mean action noise std: 0.90
                       Mean reward: 3647.89
               Mean episode length: 253.81
                 Mean success rate: 39.00
                  Mean reward/step: 18.69
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 8888320
                    Iteration time: 2.70s
                        Total time: 2887.60s
                               ETA: 529391.1s

################################################################################
                    [1m Learning iteration 1085/200000 [0m

                       Computation: 2987 steps/s (collection: 0.637s, learning 2.105s)
               Value function loss: 50691.7912
                    Surrogate loss: 0.0113
             Mean action noise std: 0.90
                       Mean reward: 3628.04
               Mean episode length: 256.81
                 Mean success rate: 40.00
                  Mean reward/step: 19.30
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 8896512
                    Iteration time: 2.74s
                        Total time: 2890.34s
                               ETA: 529403.2s

################################################################################
                    [1m Learning iteration 1086/200000 [0m

                       Computation: 3062 steps/s (collection: 0.565s, learning 2.110s)
               Value function loss: 97221.1814
                    Surrogate loss: 0.0110
             Mean action noise std: 0.90
                       Mean reward: 4136.55
               Mean episode length: 276.01
                 Mean success rate: 44.50
                  Mean reward/step: 19.28
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 8904704
                    Iteration time: 2.67s
                        Total time: 2893.01s
                               ETA: 529402.9s

################################################################################
                    [1m Learning iteration 1087/200000 [0m

                       Computation: 3070 steps/s (collection: 0.577s, learning 2.091s)
               Value function loss: 68310.5211
                    Surrogate loss: 0.0139
             Mean action noise std: 0.90
                       Mean reward: 4485.64
               Mean episode length: 285.87
                 Mean success rate: 48.00
                  Mean reward/step: 18.14
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8912896
                    Iteration time: 2.67s
                        Total time: 2895.68s
                               ETA: 529401.5s

################################################################################
                    [1m Learning iteration 1088/200000 [0m

                       Computation: 3000 steps/s (collection: 0.624s, learning 2.106s)
               Value function loss: 108086.9154
                    Surrogate loss: 0.0130
             Mean action noise std: 0.90
                       Mean reward: 5009.61
               Mean episode length: 303.77
                 Mean success rate: 52.50
                  Mean reward/step: 17.23
       Mean episode length/episode: 27.40
--------------------------------------------------------------------------------
                   Total timesteps: 8921088
                    Iteration time: 2.73s
                        Total time: 2898.41s
                               ETA: 529411.3s

################################################################################
                    [1m Learning iteration 1089/200000 [0m

                       Computation: 3093 steps/s (collection: 0.571s, learning 2.076s)
               Value function loss: 50472.4859
                    Surrogate loss: 0.0161
             Mean action noise std: 0.90
                       Mean reward: 5116.65
               Mean episode length: 309.81
                 Mean success rate: 53.50
                  Mean reward/step: 17.04
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 8929280
                    Iteration time: 2.65s
                        Total time: 2901.06s
                               ETA: 529406.1s

################################################################################
                    [1m Learning iteration 1090/200000 [0m

                       Computation: 3047 steps/s (collection: 0.569s, learning 2.119s)
               Value function loss: 91212.6347
                    Surrogate loss: 0.0136
             Mean action noise std: 0.90
                       Mean reward: 5306.50
               Mean episode length: 304.95
                 Mean success rate: 54.50
                  Mean reward/step: 16.85
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 8937472
                    Iteration time: 2.69s
                        Total time: 2903.75s
                               ETA: 529408.2s

################################################################################
                    [1m Learning iteration 1091/200000 [0m

                       Computation: 3081 steps/s (collection: 0.584s, learning 2.075s)
               Value function loss: 106393.4771
                    Surrogate loss: 0.0091
             Mean action noise std: 0.90
                       Mean reward: 5719.85
               Mean episode length: 322.73
                 Mean success rate: 59.00
                  Mean reward/step: 17.16
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 2.66s
                        Total time: 2906.41s
                               ETA: 529405.1s

################################################################################
                    [1m Learning iteration 1092/200000 [0m

                       Computation: 3038 steps/s (collection: 0.561s, learning 2.135s)
               Value function loss: 67238.1797
                    Surrogate loss: 0.0124
             Mean action noise std: 0.90
                       Mean reward: 5808.83
               Mean episode length: 321.99
                 Mean success rate: 59.00
                  Mean reward/step: 16.60
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8953856
                    Iteration time: 2.70s
                        Total time: 2909.10s
                               ETA: 529408.7s

################################################################################
                    [1m Learning iteration 1093/200000 [0m

                       Computation: 3028 steps/s (collection: 0.587s, learning 2.118s)
               Value function loss: 58798.6850
                    Surrogate loss: 0.0138
             Mean action noise std: 0.90
                       Mean reward: 5788.89
               Mean episode length: 322.59
                 Mean success rate: 59.00
                  Mean reward/step: 16.08
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 8962048
                    Iteration time: 2.71s
                        Total time: 2911.81s
                               ETA: 529414.0s

################################################################################
                    [1m Learning iteration 1094/200000 [0m

                       Computation: 2983 steps/s (collection: 0.608s, learning 2.137s)
               Value function loss: 58360.3066
                    Surrogate loss: 0.0138
             Mean action noise std: 0.90
                       Mean reward: 5612.75
               Mean episode length: 318.93
                 Mean success rate: 57.50
                  Mean reward/step: 16.35
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 8970240
                    Iteration time: 2.75s
                        Total time: 2914.55s
                               ETA: 529426.5s

################################################################################
                    [1m Learning iteration 1095/200000 [0m

                       Computation: 2903 steps/s (collection: 0.616s, learning 2.206s)
               Value function loss: 46573.6481
                    Surrogate loss: 0.0152
             Mean action noise std: 0.90
                       Mean reward: 5298.26
               Mean episode length: 304.83
                 Mean success rate: 54.50
                  Mean reward/step: 16.93
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 8978432
                    Iteration time: 2.82s
                        Total time: 2917.37s
                               ETA: 529452.9s

################################################################################
                    [1m Learning iteration 1096/200000 [0m

                       Computation: 3085 steps/s (collection: 0.574s, learning 2.082s)
               Value function loss: 73145.7944
                    Surrogate loss: 0.0169
             Mean action noise std: 0.90
                       Mean reward: 5672.30
               Mean episode length: 316.30
                 Mean success rate: 56.50
                  Mean reward/step: 17.31
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 8986624
                    Iteration time: 2.66s
                        Total time: 2920.03s
                               ETA: 529449.1s

################################################################################
                    [1m Learning iteration 1097/200000 [0m

                       Computation: 3094 steps/s (collection: 0.550s, learning 2.097s)
               Value function loss: 39234.0034
                    Surrogate loss: 0.0169
             Mean action noise std: 0.90
                       Mean reward: 5313.42
               Mean episode length: 299.55
                 Mean success rate: 53.50
                  Mean reward/step: 17.61
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 8994816
                    Iteration time: 2.65s
                        Total time: 2922.68s
                               ETA: 529443.7s

################################################################################
                    [1m Learning iteration 1098/200000 [0m

                       Computation: 3023 steps/s (collection: 0.593s, learning 2.116s)
               Value function loss: 61363.3541
                    Surrogate loss: 0.0128
             Mean action noise std: 0.90
                       Mean reward: 5369.84
               Mean episode length: 301.57
                 Mean success rate: 53.00
                  Mean reward/step: 17.92
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 9003008
                    Iteration time: 2.71s
                        Total time: 2925.39s
                               ETA: 529449.6s

################################################################################
                    [1m Learning iteration 1099/200000 [0m

                       Computation: 3111 steps/s (collection: 0.550s, learning 2.083s)
               Value function loss: 75086.8245
                    Surrogate loss: 0.0186
             Mean action noise std: 0.90
                       Mean reward: 5482.07
               Mean episode length: 303.29
                 Mean success rate: 53.50
                  Mean reward/step: 18.28
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9011200
                    Iteration time: 2.63s
                        Total time: 2928.02s
                               ETA: 529441.6s

################################################################################
                    [1m Learning iteration 1100/200000 [0m

                       Computation: 3111 steps/s (collection: 0.535s, learning 2.097s)
               Value function loss: 39583.1646
                    Surrogate loss: 0.0214
             Mean action noise std: 0.90
                       Mean reward: 5232.78
               Mean episode length: 298.09
                 Mean success rate: 51.00
                  Mean reward/step: 18.03
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9019392
                    Iteration time: 2.63s
                        Total time: 2930.65s
                               ETA: 529433.7s

################################################################################
                    [1m Learning iteration 1101/200000 [0m

                       Computation: 2998 steps/s (collection: 0.663s, learning 2.069s)
               Value function loss: 74306.8230
                    Surrogate loss: 0.0136
             Mean action noise std: 0.90
                       Mean reward: 5437.28
               Mean episode length: 309.00
                 Mean success rate: 53.50
                  Mean reward/step: 17.61
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9027584
                    Iteration time: 2.73s
                        Total time: 2933.38s
                               ETA: 529443.7s

################################################################################
                    [1m Learning iteration 1102/200000 [0m

                       Computation: 3101 steps/s (collection: 0.581s, learning 2.061s)
               Value function loss: 70016.2674
                    Surrogate loss: 0.0126
             Mean action noise std: 0.90
                       Mean reward: 5251.88
               Mean episode length: 302.68
                 Mean success rate: 53.00
                  Mean reward/step: 17.67
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 9035776
                    Iteration time: 2.64s
                        Total time: 2936.02s
                               ETA: 529437.4s

################################################################################
                    [1m Learning iteration 1103/200000 [0m

                       Computation: 3167 steps/s (collection: 0.525s, learning 2.062s)
               Value function loss: 91631.4641
                    Surrogate loss: 0.0135
             Mean action noise std: 0.90
                       Mean reward: 5250.55
               Mean episode length: 307.23
                 Mean success rate: 54.00
                  Mean reward/step: 17.81
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 2.59s
                        Total time: 2938.61s
                               ETA: 529421.1s

################################################################################
                    [1m Learning iteration 1104/200000 [0m

                       Computation: 3095 steps/s (collection: 0.546s, learning 2.101s)
               Value function loss: 93477.8468
                    Surrogate loss: 0.0130
             Mean action noise std: 0.90
                       Mean reward: 5467.95
               Mean episode length: 317.14
                 Mean success rate: 56.00
                  Mean reward/step: 17.84
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 9052160
                    Iteration time: 2.65s
                        Total time: 2941.26s
                               ETA: 529415.7s

################################################################################
                    [1m Learning iteration 1105/200000 [0m

                       Computation: 3016 steps/s (collection: 0.556s, learning 2.159s)
               Value function loss: 55952.7248
                    Surrogate loss: 0.0141
             Mean action noise std: 0.90
                       Mean reward: 5577.90
               Mean episode length: 322.00
                 Mean success rate: 57.00
                  Mean reward/step: 17.68
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9060352
                    Iteration time: 2.72s
                        Total time: 2943.97s
                               ETA: 529422.8s

################################################################################
                    [1m Learning iteration 1106/200000 [0m

                       Computation: 2974 steps/s (collection: 0.642s, learning 2.112s)
               Value function loss: 86649.4368
                    Surrogate loss: 0.0134
             Mean action noise std: 0.90
                       Mean reward: 5810.12
               Mean episode length: 331.60
                 Mean success rate: 58.50
                  Mean reward/step: 17.51
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 9068544
                    Iteration time: 2.75s
                        Total time: 2946.73s
                               ETA: 529436.6s

################################################################################
                    [1m Learning iteration 1107/200000 [0m

                       Computation: 3073 steps/s (collection: 0.592s, learning 2.073s)
               Value function loss: 72960.2386
                    Surrogate loss: 0.0180
             Mean action noise std: 0.90
                       Mean reward: 5834.61
               Mean episode length: 327.73
                 Mean success rate: 57.50
                  Mean reward/step: 17.19
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 9076736
                    Iteration time: 2.67s
                        Total time: 2949.39s
                               ETA: 529434.5s

################################################################################
                    [1m Learning iteration 1108/200000 [0m

                       Computation: 3139 steps/s (collection: 0.541s, learning 2.068s)
               Value function loss: 55481.8195
                    Surrogate loss: 0.0170
             Mean action noise std: 0.90
                       Mean reward: 5525.45
               Mean episode length: 319.84
                 Mean success rate: 54.50
                  Mean reward/step: 17.37
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9084928
                    Iteration time: 2.61s
                        Total time: 2952.00s
                               ETA: 529422.4s

################################################################################
                    [1m Learning iteration 1109/200000 [0m

                       Computation: 3014 steps/s (collection: 0.598s, learning 2.119s)
               Value function loss: 58229.2120
                    Surrogate loss: 0.0137
             Mean action noise std: 0.90
                       Mean reward: 5214.71
               Mean episode length: 307.39
                 Mean success rate: 50.00
                  Mean reward/step: 17.90
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 9093120
                    Iteration time: 2.72s
                        Total time: 2954.72s
                               ETA: 529429.7s

################################################################################
                    [1m Learning iteration 1110/200000 [0m

                       Computation: 3040 steps/s (collection: 0.604s, learning 2.091s)
               Value function loss: 80907.0193
                    Surrogate loss: 0.0137
             Mean action noise std: 0.90
                       Mean reward: 5762.97
               Mean episode length: 324.77
                 Mean success rate: 54.50
                  Mean reward/step: 18.83
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9101312
                    Iteration time: 2.69s
                        Total time: 2957.41s
                               ETA: 529432.9s

################################################################################
                    [1m Learning iteration 1111/200000 [0m

                       Computation: 3126 steps/s (collection: 0.540s, learning 2.080s)
               Value function loss: 77655.8072
                    Surrogate loss: 0.0139
             Mean action noise std: 0.90
                       Mean reward: 5431.94
               Mean episode length: 310.58
                 Mean success rate: 51.00
                  Mean reward/step: 18.61
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 9109504
                    Iteration time: 2.62s
                        Total time: 2960.03s
                               ETA: 529422.7s

################################################################################
                    [1m Learning iteration 1112/200000 [0m

                       Computation: 3096 steps/s (collection: 0.567s, learning 2.079s)
               Value function loss: 89398.8824
                    Surrogate loss: 0.0130
             Mean action noise std: 0.90
                       Mean reward: 5416.39
               Mean episode length: 308.45
                 Mean success rate: 50.50
                  Mean reward/step: 18.54
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 9117696
                    Iteration time: 2.65s
                        Total time: 2962.68s
                               ETA: 529417.2s

################################################################################
                    [1m Learning iteration 1113/200000 [0m

                       Computation: 3180 steps/s (collection: 0.520s, learning 2.056s)
               Value function loss: 79273.5618
                    Surrogate loss: 0.0145
             Mean action noise std: 0.90
                       Mean reward: 5337.37
               Mean episode length: 301.01
                 Mean success rate: 51.00
                  Mean reward/step: 18.27
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 9125888
                    Iteration time: 2.58s
                        Total time: 2965.26s
                               ETA: 529399.2s

################################################################################
                    [1m Learning iteration 1114/200000 [0m

                       Computation: 3208 steps/s (collection: 0.519s, learning 2.035s)
               Value function loss: 71221.7495
                    Surrogate loss: 0.0153
             Mean action noise std: 0.90
                       Mean reward: 5068.67
               Mean episode length: 283.46
                 Mean success rate: 49.50
                  Mean reward/step: 17.67
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 9134080
                    Iteration time: 2.55s
                        Total time: 2967.81s
                               ETA: 529377.2s

################################################################################
                    [1m Learning iteration 1115/200000 [0m

                       Computation: 3106 steps/s (collection: 0.565s, learning 2.072s)
               Value function loss: 78087.1050
                    Surrogate loss: 0.0151
             Mean action noise std: 0.90
                       Mean reward: 5248.20
               Mean episode length: 286.82
                 Mean success rate: 51.50
                  Mean reward/step: 17.76
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 2.64s
                        Total time: 2970.45s
                               ETA: 529370.1s

################################################################################
                    [1m Learning iteration 1116/200000 [0m

                       Computation: 3101 steps/s (collection: 0.547s, learning 2.094s)
               Value function loss: 64165.4435
                    Surrogate loss: 0.0136
             Mean action noise std: 0.90
                       Mean reward: 5414.28
               Mean episode length: 284.49
                 Mean success rate: 52.50
                  Mean reward/step: 17.38
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9150464
                    Iteration time: 2.64s
                        Total time: 2973.09s
                               ETA: 529363.8s

################################################################################
                    [1m Learning iteration 1117/200000 [0m

                       Computation: 2985 steps/s (collection: 0.613s, learning 2.131s)
               Value function loss: 82725.4133
                    Surrogate loss: 0.0136
             Mean action noise std: 0.90
                       Mean reward: 4853.90
               Mean episode length: 263.99
                 Mean success rate: 47.50
                  Mean reward/step: 17.45
       Mean episode length/episode: 27.04
--------------------------------------------------------------------------------
                   Total timesteps: 9158656
                    Iteration time: 2.74s
                        Total time: 2975.83s
                               ETA: 529375.8s

################################################################################
                    [1m Learning iteration 1118/200000 [0m

                       Computation: 3159 steps/s (collection: 0.533s, learning 2.060s)
               Value function loss: 90542.1451
                    Surrogate loss: 0.0131
             Mean action noise std: 0.90
                       Mean reward: 4688.27
               Mean episode length: 261.20
                 Mean success rate: 46.50
                  Mean reward/step: 17.00
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 9166848
                    Iteration time: 2.59s
                        Total time: 2978.42s
                               ETA: 529360.9s

################################################################################
                    [1m Learning iteration 1119/200000 [0m

                       Computation: 3154 steps/s (collection: 0.533s, learning 2.064s)
               Value function loss: 66008.5071
                    Surrogate loss: 0.0158
             Mean action noise std: 0.90
                       Mean reward: 4727.97
               Mean episode length: 267.93
                 Mean success rate: 47.00
                  Mean reward/step: 16.99
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 9175040
                    Iteration time: 2.60s
                        Total time: 2981.02s
                               ETA: 529346.8s

################################################################################
                    [1m Learning iteration 1120/200000 [0m

                       Computation: 3055 steps/s (collection: 0.539s, learning 2.142s)
               Value function loss: 59359.6618
                    Surrogate loss: 0.0160
             Mean action noise std: 0.90
                       Mean reward: 4730.98
               Mean episode length: 271.38
                 Mean success rate: 46.50
                  Mean reward/step: 16.78
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9183232
                    Iteration time: 2.68s
                        Total time: 2983.70s
                               ETA: 529347.5s

################################################################################
                    [1m Learning iteration 1121/200000 [0m

                       Computation: 2867 steps/s (collection: 0.678s, learning 2.179s)
               Value function loss: 76762.6438
                    Surrogate loss: 0.0160
             Mean action noise std: 0.90
                       Mean reward: 4361.91
               Mean episode length: 261.63
                 Mean success rate: 42.50
                  Mean reward/step: 17.93
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 9191424
                    Iteration time: 2.86s
                        Total time: 2986.56s
                               ETA: 529379.5s

################################################################################
                    [1m Learning iteration 1122/200000 [0m

                       Computation: 3052 steps/s (collection: 0.594s, learning 2.090s)
               Value function loss: 85628.2278
                    Surrogate loss: 0.0128
             Mean action noise std: 0.90
                       Mean reward: 4550.70
               Mean episode length: 271.11
                 Mean success rate: 44.50
                  Mean reward/step: 18.40
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 9199616
                    Iteration time: 2.68s
                        Total time: 2989.24s
                               ETA: 529380.8s

################################################################################
                    [1m Learning iteration 1123/200000 [0m

                       Computation: 3166 steps/s (collection: 0.529s, learning 2.058s)
               Value function loss: 74429.9943
                    Surrogate loss: 0.0146
             Mean action noise std: 0.90
                       Mean reward: 5097.61
               Mean episode length: 296.00
                 Mean success rate: 50.00
                  Mean reward/step: 17.74
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9207808
                    Iteration time: 2.59s
                        Total time: 2991.83s
                               ETA: 529364.9s

################################################################################
                    [1m Learning iteration 1124/200000 [0m

                       Computation: 3190 steps/s (collection: 0.504s, learning 2.064s)
               Value function loss: 67816.6703
                    Surrogate loss: 0.0176
             Mean action noise std: 0.90
                       Mean reward: 5273.22
               Mean episode length: 301.59
                 Mean success rate: 51.00
                  Mean reward/step: 18.14
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9216000
                    Iteration time: 2.57s
                        Total time: 2994.40s
                               ETA: 529345.6s

################################################################################
                    [1m Learning iteration 1125/200000 [0m

                       Computation: 3138 steps/s (collection: 0.537s, learning 2.073s)
               Value function loss: 47423.3964
                    Surrogate loss: 0.0183
             Mean action noise std: 0.90
                       Mean reward: 5192.44
               Mean episode length: 300.61
                 Mean success rate: 50.00
                  Mean reward/step: 18.70
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9224192
                    Iteration time: 2.61s
                        Total time: 2997.01s
                               ETA: 529333.8s

################################################################################
                    [1m Learning iteration 1126/200000 [0m

                       Computation: 3118 steps/s (collection: 0.553s, learning 2.074s)
               Value function loss: 52711.7493
                    Surrogate loss: 0.0133
             Mean action noise std: 0.90
                       Mean reward: 5035.49
               Mean episode length: 293.26
                 Mean success rate: 48.50
                  Mean reward/step: 19.17
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9232384
                    Iteration time: 2.63s
                        Total time: 2999.63s
                               ETA: 529325.0s

################################################################################
                    [1m Learning iteration 1127/200000 [0m

                       Computation: 3139 steps/s (collection: 0.518s, learning 2.091s)
               Value function loss: 82653.8318
                    Surrogate loss: 0.0109
             Mean action noise std: 0.90
                       Mean reward: 5313.45
               Mean episode length: 300.82
                 Mean success rate: 51.00
                  Mean reward/step: 19.29
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 2.61s
                        Total time: 3002.24s
                               ETA: 529313.2s

################################################################################
                    [1m Learning iteration 1128/200000 [0m

                       Computation: 2975 steps/s (collection: 0.618s, learning 2.135s)
               Value function loss: 53140.8560
                    Surrogate loss: 0.0145
             Mean action noise std: 0.90
                       Mean reward: 5218.67
               Mean episode length: 301.75
                 Mean success rate: 50.00
                  Mean reward/step: 18.50
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9248768
                    Iteration time: 2.75s
                        Total time: 3005.00s
                               ETA: 529326.6s

################################################################################
                    [1m Learning iteration 1129/200000 [0m

                       Computation: 3016 steps/s (collection: 0.649s, learning 2.067s)
               Value function loss: 78451.3566
                    Surrogate loss: 0.0135
             Mean action noise std: 0.90
                       Mean reward: 5382.49
               Mean episode length: 305.00
                 Mean success rate: 51.50
                  Mean reward/step: 18.60
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9256960
                    Iteration time: 2.72s
                        Total time: 3007.71s
                               ETA: 529333.5s

################################################################################
                    [1m Learning iteration 1130/200000 [0m

                       Computation: 3105 steps/s (collection: 0.560s, learning 2.078s)
               Value function loss: 78031.3009
                    Surrogate loss: 0.0128
             Mean action noise std: 0.89
                       Mean reward: 5311.68
               Mean episode length: 304.19
                 Mean success rate: 50.50
                  Mean reward/step: 18.72
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9265152
                    Iteration time: 2.64s
                        Total time: 3010.35s
                               ETA: 529326.7s

################################################################################
                    [1m Learning iteration 1131/200000 [0m

                       Computation: 3120 steps/s (collection: 0.551s, learning 2.074s)
               Value function loss: 53353.9865
                    Surrogate loss: 0.0270
             Mean action noise std: 0.90
                       Mean reward: 5246.45
               Mean episode length: 298.11
                 Mean success rate: 49.50
                  Mean reward/step: 18.71
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9273344
                    Iteration time: 2.62s
                        Total time: 3012.98s
                               ETA: 529317.5s

################################################################################
                    [1m Learning iteration 1132/200000 [0m

                       Computation: 3042 steps/s (collection: 0.580s, learning 2.113s)
               Value function loss: 88776.2379
                    Surrogate loss: 0.0162
             Mean action noise std: 0.90
                       Mean reward: 5332.71
               Mean episode length: 303.23
                 Mean success rate: 50.00
                  Mean reward/step: 18.37
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 9281536
                    Iteration time: 2.69s
                        Total time: 3015.67s
                               ETA: 529320.3s

################################################################################
                    [1m Learning iteration 1133/200000 [0m

                       Computation: 3114 steps/s (collection: 0.565s, learning 2.065s)
               Value function loss: 98203.0016
                    Surrogate loss: 0.0095
             Mean action noise std: 0.90
                       Mean reward: 5799.49
               Mean episode length: 321.36
                 Mean success rate: 54.00
                  Mean reward/step: 17.40
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 9289728
                    Iteration time: 2.63s
                        Total time: 3018.30s
                               ETA: 529312.1s

################################################################################
                    [1m Learning iteration 1134/200000 [0m

                       Computation: 3147 steps/s (collection: 0.533s, learning 2.069s)
               Value function loss: 73437.9325
                    Surrogate loss: 0.0118
             Mean action noise std: 0.90
                       Mean reward: 6051.56
               Mean episode length: 328.07
                 Mean success rate: 55.50
                  Mean reward/step: 16.58
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 9297920
                    Iteration time: 2.60s
                        Total time: 3020.90s
                               ETA: 529299.1s

################################################################################
                    [1m Learning iteration 1135/200000 [0m

                       Computation: 3146 steps/s (collection: 0.522s, learning 2.082s)
               Value function loss: 75048.0391
                    Surrogate loss: 0.0119
             Mean action noise std: 0.90
                       Mean reward: 5928.71
               Mean episode length: 323.15
                 Mean success rate: 54.50
                  Mean reward/step: 16.42
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9306112
                    Iteration time: 2.60s
                        Total time: 3023.50s
                               ETA: 529286.3s

################################################################################
                    [1m Learning iteration 1136/200000 [0m

                       Computation: 3101 steps/s (collection: 0.563s, learning 2.079s)
               Value function loss: 70048.9468
                    Surrogate loss: 0.0098
             Mean action noise std: 0.90
                       Mean reward: 6143.23
               Mean episode length: 323.59
                 Mean success rate: 57.00
                  Mean reward/step: 16.59
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 9314304
                    Iteration time: 2.64s
                        Total time: 3026.15s
                               ETA: 529280.1s

################################################################################
                    [1m Learning iteration 1137/200000 [0m

                       Computation: 3046 steps/s (collection: 0.592s, learning 2.097s)
               Value function loss: 72549.8397
                    Surrogate loss: 0.0135
             Mean action noise std: 0.90
                       Mean reward: 6125.03
               Mean episode length: 319.25
                 Mean success rate: 56.00
                  Mean reward/step: 16.99
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 9322496
                    Iteration time: 2.69s
                        Total time: 3028.83s
                               ETA: 529282.2s

################################################################################
                    [1m Learning iteration 1138/200000 [0m

                       Computation: 3130 steps/s (collection: 0.555s, learning 2.062s)
               Value function loss: 73380.5211
                    Surrogate loss: 0.0127
             Mean action noise std: 0.89
                       Mean reward: 5817.40
               Mean episode length: 311.00
                 Mean success rate: 53.00
                  Mean reward/step: 17.05
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 9330688
                    Iteration time: 2.62s
                        Total time: 3031.45s
                               ETA: 529271.7s

################################################################################
                    [1m Learning iteration 1139/200000 [0m

                       Computation: 3075 steps/s (collection: 0.563s, learning 2.101s)
               Value function loss: 72508.8090
                    Surrogate loss: 0.0135
             Mean action noise std: 0.89
                       Mean reward: 5689.31
               Mean episode length: 302.67
                 Mean success rate: 53.00
                  Mean reward/step: 16.89
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 2.66s
                        Total time: 3034.12s
                               ETA: 529269.5s

################################################################################
                    [1m Learning iteration 1140/200000 [0m

                       Computation: 3056 steps/s (collection: 0.579s, learning 2.102s)
               Value function loss: 75078.6722
                    Surrogate loss: 0.0135
             Mean action noise std: 0.89
                       Mean reward: 5576.72
               Mean episode length: 302.56
                 Mean success rate: 52.50
                  Mean reward/step: 17.07
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 9347072
                    Iteration time: 2.68s
                        Total time: 3036.80s
                               ETA: 529270.1s

################################################################################
                    [1m Learning iteration 1141/200000 [0m

                       Computation: 3126 steps/s (collection: 0.560s, learning 2.061s)
               Value function loss: 61495.1114
                    Surrogate loss: 0.0136
             Mean action noise std: 0.89
                       Mean reward: 5021.42
               Mean episode length: 287.00
                 Mean success rate: 48.00
                  Mean reward/step: 17.08
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9355264
                    Iteration time: 2.62s
                        Total time: 3039.42s
                               ETA: 529260.3s

################################################################################
                    [1m Learning iteration 1142/200000 [0m

                       Computation: 3113 steps/s (collection: 0.537s, learning 2.094s)
               Value function loss: 59660.8977
                    Surrogate loss: 0.0165
             Mean action noise std: 0.89
                       Mean reward: 5028.52
               Mean episode length: 287.94
                 Mean success rate: 48.50
                  Mean reward/step: 17.48
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9363456
                    Iteration time: 2.63s
                        Total time: 3042.05s
                               ETA: 529252.4s

################################################################################
                    [1m Learning iteration 1143/200000 [0m

                       Computation: 3004 steps/s (collection: 0.583s, learning 2.143s)
               Value function loss: 92053.3679
                    Surrogate loss: 0.0168
             Mean action noise std: 0.89
                       Mean reward: 5086.47
               Mean episode length: 295.57
                 Mean success rate: 49.00
                  Mean reward/step: 17.00
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 9371648
                    Iteration time: 2.73s
                        Total time: 3044.77s
                               ETA: 529261.1s

################################################################################
                    [1m Learning iteration 1144/200000 [0m

                       Computation: 3027 steps/s (collection: 0.629s, learning 2.076s)
               Value function loss: 49702.1748
                    Surrogate loss: 0.0140
             Mean action noise std: 0.89
                       Mean reward: 4900.90
               Mean episode length: 297.44
                 Mean success rate: 48.00
                  Mean reward/step: 16.19
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9379840
                    Iteration time: 2.71s
                        Total time: 3047.48s
                               ETA: 529266.1s

################################################################################
                    [1m Learning iteration 1145/200000 [0m

                       Computation: 3119 steps/s (collection: 0.577s, learning 2.049s)
               Value function loss: 65302.7445
                    Surrogate loss: 0.0132
             Mean action noise std: 0.89
                       Mean reward: 4994.52
               Mean episode length: 300.50
                 Mean success rate: 49.50
                  Mean reward/step: 15.97
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9388032
                    Iteration time: 2.63s
                        Total time: 3050.11s
                               ETA: 529257.3s

################################################################################
                    [1m Learning iteration 1146/200000 [0m

                       Computation: 3152 steps/s (collection: 0.519s, learning 2.080s)
               Value function loss: 53536.2985
                    Surrogate loss: 0.0130
             Mean action noise std: 0.89
                       Mean reward: 5305.22
               Mean episode length: 311.90
                 Mean success rate: 52.50
                  Mean reward/step: 16.85
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9396224
                    Iteration time: 2.60s
                        Total time: 3052.71s
                               ETA: 529243.8s

################################################################################
                    [1m Learning iteration 1147/200000 [0m

                       Computation: 3176 steps/s (collection: 0.510s, learning 2.070s)
               Value function loss: 52746.8884
                    Surrogate loss: 0.0140
             Mean action noise std: 0.89
                       Mean reward: 5136.26
               Mean episode length: 310.46
                 Mean success rate: 52.00
                  Mean reward/step: 18.01
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9404416
                    Iteration time: 2.58s
                        Total time: 3055.28s
                               ETA: 529226.9s

################################################################################
                    [1m Learning iteration 1148/200000 [0m

                       Computation: 3126 steps/s (collection: 0.535s, learning 2.085s)
               Value function loss: 66220.1018
                    Surrogate loss: 0.0116
             Mean action noise std: 0.89
                       Mean reward: 5120.60
               Mean episode length: 307.25
                 Mean success rate: 52.00
                  Mean reward/step: 18.38
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9412608
                    Iteration time: 2.62s
                        Total time: 3057.90s
                               ETA: 529217.2s

################################################################################
                    [1m Learning iteration 1149/200000 [0m

                       Computation: 3100 steps/s (collection: 0.575s, learning 2.068s)
               Value function loss: 69921.7547
                    Surrogate loss: 0.0123
             Mean action noise std: 0.89
                       Mean reward: 5517.76
               Mean episode length: 318.94
                 Mean success rate: 54.50
                  Mean reward/step: 17.98
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9420800
                    Iteration time: 2.64s
                        Total time: 3060.55s
                               ETA: 529211.2s

################################################################################
                    [1m Learning iteration 1150/200000 [0m

                       Computation: 3056 steps/s (collection: 0.567s, learning 2.113s)
               Value function loss: 49248.8084
                    Surrogate loss: 0.0154
             Mean action noise std: 0.89
                       Mean reward: 5274.85
               Mean episode length: 312.68
                 Mean success rate: 51.50
                  Mean reward/step: 18.12
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9428992
                    Iteration time: 2.68s
                        Total time: 3063.23s
                               ETA: 529211.8s

################################################################################
                    [1m Learning iteration 1151/200000 [0m

                       Computation: 2928 steps/s (collection: 0.665s, learning 2.133s)
               Value function loss: 79637.6595
                    Surrogate loss: 0.0130
             Mean action noise std: 0.89
                       Mean reward: 5458.35
               Mean episode length: 316.79
                 Mean success rate: 52.50
                  Mean reward/step: 18.25
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 2.80s
                        Total time: 3066.02s
                               ETA: 529232.6s

################################################################################
                    [1m Learning iteration 1152/200000 [0m

                       Computation: 3055 steps/s (collection: 0.560s, learning 2.121s)
               Value function loss: 91379.1142
                    Surrogate loss: 0.0130
             Mean action noise std: 0.89
                       Mean reward: 5728.27
               Mean episode length: 329.13
                 Mean success rate: 55.50
                  Mean reward/step: 17.87
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9445376
                    Iteration time: 2.68s
                        Total time: 3068.71s
                               ETA: 529233.3s

################################################################################
                    [1m Learning iteration 1153/200000 [0m

                       Computation: 3153 steps/s (collection: 0.518s, learning 2.081s)
               Value function loss: 80368.8567
                    Surrogate loss: 0.0152
             Mean action noise std: 0.89
                       Mean reward: 6038.60
               Mean episode length: 339.57
                 Mean success rate: 59.50
                  Mean reward/step: 16.43
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 9453568
                    Iteration time: 2.60s
                        Total time: 3071.30s
                               ETA: 529219.7s

################################################################################
                    [1m Learning iteration 1154/200000 [0m

                       Computation: 3141 steps/s (collection: 0.517s, learning 2.091s)
               Value function loss: 64970.9954
                    Surrogate loss: 0.0203
             Mean action noise std: 0.89
                       Mean reward: 5836.34
               Mean episode length: 336.86
                 Mean success rate: 58.50
                  Mean reward/step: 16.45
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9461760
                    Iteration time: 2.61s
                        Total time: 3073.91s
                               ETA: 529207.8s

################################################################################
                    [1m Learning iteration 1155/200000 [0m

                       Computation: 3023 steps/s (collection: 0.595s, learning 2.115s)
               Value function loss: 71881.9882
                    Surrogate loss: 0.0136
             Mean action noise std: 0.89
                       Mean reward: 6240.17
               Mean episode length: 350.33
                 Mean success rate: 60.50
                  Mean reward/step: 16.36
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9469952
                    Iteration time: 2.71s
                        Total time: 3076.62s
                               ETA: 529213.4s

################################################################################
                    [1m Learning iteration 1156/200000 [0m

                       Computation: 3102 steps/s (collection: 0.527s, learning 2.114s)
               Value function loss: 64080.3003
                    Surrogate loss: 0.0137
             Mean action noise std: 0.89
                       Mean reward: 6202.02
               Mean episode length: 349.90
                 Mean success rate: 60.50
                  Mean reward/step: 16.55
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 9478144
                    Iteration time: 2.64s
                        Total time: 3079.26s
                               ETA: 529207.2s

################################################################################
                    [1m Learning iteration 1157/200000 [0m

                       Computation: 3121 steps/s (collection: 0.559s, learning 2.065s)
               Value function loss: 75211.9413
                    Surrogate loss: 0.0138
             Mean action noise std: 0.89
                       Mean reward: 6078.22
               Mean episode length: 341.49
                 Mean success rate: 61.00
                  Mean reward/step: 16.60
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 9486336
                    Iteration time: 2.62s
                        Total time: 3081.89s
                               ETA: 529198.2s

################################################################################
                    [1m Learning iteration 1158/200000 [0m

                       Computation: 3062 steps/s (collection: 0.577s, learning 2.098s)
               Value function loss: 52662.3523
                    Surrogate loss: 0.0148
             Mean action noise std: 0.89
                       Mean reward: 5964.51
               Mean episode length: 343.04
                 Mean success rate: 61.50
                  Mean reward/step: 16.87
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9494528
                    Iteration time: 2.67s
                        Total time: 3084.56s
                               ETA: 529197.8s

################################################################################
                    [1m Learning iteration 1159/200000 [0m

                       Computation: 3157 steps/s (collection: 0.528s, learning 2.067s)
               Value function loss: 85033.3833
                    Surrogate loss: 0.0140
             Mean action noise std: 0.89
                       Mean reward: 5913.33
               Mean episode length: 342.02
                 Mean success rate: 61.00
                  Mean reward/step: 16.47
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 9502720
                    Iteration time: 2.59s
                        Total time: 3087.16s
                               ETA: 529183.7s

################################################################################
                    [1m Learning iteration 1160/200000 [0m

                       Computation: 3059 steps/s (collection: 0.600s, learning 2.078s)
               Value function loss: 71027.3262
                    Surrogate loss: 0.0154
             Mean action noise std: 0.89
                       Mean reward: 5370.59
               Mean episode length: 321.64
                 Mean success rate: 55.50
                  Mean reward/step: 16.19
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 9510912
                    Iteration time: 2.68s
                        Total time: 3089.83s
                               ETA: 529183.8s

################################################################################
                    [1m Learning iteration 1161/200000 [0m

                       Computation: 3075 steps/s (collection: 0.607s, learning 2.056s)
               Value function loss: 50972.3378
                    Surrogate loss: 0.0153
             Mean action noise std: 0.89
                       Mean reward: 5262.33
               Mean episode length: 313.29
                 Mean success rate: 53.50
                  Mean reward/step: 16.01
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9519104
                    Iteration time: 2.66s
                        Total time: 3092.50s
                               ETA: 529181.5s

################################################################################
                    [1m Learning iteration 1162/200000 [0m

                       Computation: 3055 steps/s (collection: 0.570s, learning 2.111s)
               Value function loss: 46180.9748
                    Surrogate loss: 0.0153
             Mean action noise std: 0.89
                       Mean reward: 4900.20
               Mean episode length: 296.95
                 Mean success rate: 50.50
                  Mean reward/step: 16.64
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9527296
                    Iteration time: 2.68s
                        Total time: 3095.18s
                               ETA: 529182.2s

################################################################################
                    [1m Learning iteration 1163/200000 [0m

                       Computation: 3085 steps/s (collection: 0.576s, learning 2.079s)
               Value function loss: 74156.5794
                    Surrogate loss: 0.0125
             Mean action noise std: 0.89
                       Mean reward: 4697.22
               Mean episode length: 287.31
                 Mean success rate: 48.50
                  Mean reward/step: 17.82
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 2.66s
                        Total time: 3097.83s
                               ETA: 529178.5s

################################################################################
                    [1m Learning iteration 1164/200000 [0m

                       Computation: 3098 steps/s (collection: 0.587s, learning 2.057s)
               Value function loss: 76730.0869
                    Surrogate loss: 0.0133
             Mean action noise std: 0.89
                       Mean reward: 4843.73
               Mean episode length: 294.22
                 Mean success rate: 48.00
                  Mean reward/step: 18.17
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 9543680
                    Iteration time: 2.64s
                        Total time: 3100.48s
                               ETA: 529172.9s

################################################################################
                    [1m Learning iteration 1165/200000 [0m

                       Computation: 3080 steps/s (collection: 0.552s, learning 2.108s)
               Value function loss: 50580.7137
                    Surrogate loss: 0.0125
             Mean action noise std: 0.89
                       Mean reward: 4617.61
               Mean episode length: 282.89
                 Mean success rate: 46.50
                  Mean reward/step: 18.32
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9551872
                    Iteration time: 2.66s
                        Total time: 3103.14s
                               ETA: 529169.9s

################################################################################
                    [1m Learning iteration 1166/200000 [0m

                       Computation: 3042 steps/s (collection: 0.583s, learning 2.109s)
               Value function loss: 54283.4488
                    Surrogate loss: 0.0127
             Mean action noise std: 0.89
                       Mean reward: 4557.14
               Mean episode length: 279.77
                 Mean success rate: 46.00
                  Mean reward/step: 19.15
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9560064
                    Iteration time: 2.69s
                        Total time: 3105.83s
                               ETA: 529172.6s

################################################################################
                    [1m Learning iteration 1167/200000 [0m

                       Computation: 3094 steps/s (collection: 0.580s, learning 2.068s)
               Value function loss: 64859.2036
                    Surrogate loss: 0.0132
             Mean action noise std: 0.89
                       Mean reward: 4753.06
               Mean episode length: 290.42
                 Mean success rate: 47.50
                  Mean reward/step: 20.00
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9568256
                    Iteration time: 2.65s
                        Total time: 3108.48s
                               ETA: 529167.5s

################################################################################
                    [1m Learning iteration 1168/200000 [0m

                       Computation: 3102 steps/s (collection: 0.579s, learning 2.061s)
               Value function loss: 91906.1927
                    Surrogate loss: 0.0134
             Mean action noise std: 0.89
                       Mean reward: 5190.78
               Mean episode length: 303.24
                 Mean success rate: 50.50
                  Mean reward/step: 19.69
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 9576448
                    Iteration time: 2.64s
                        Total time: 3111.12s
                               ETA: 529161.3s

################################################################################
                    [1m Learning iteration 1169/200000 [0m

                       Computation: 3077 steps/s (collection: 0.579s, learning 2.083s)
               Value function loss: 75621.6789
                    Surrogate loss: 0.0115
             Mean action noise std: 0.89
                       Mean reward: 5581.05
               Mean episode length: 316.33
                 Mean success rate: 53.00
                  Mean reward/step: 18.96
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9584640
                    Iteration time: 2.66s
                        Total time: 3113.78s
                               ETA: 529158.7s

################################################################################
                    [1m Learning iteration 1170/200000 [0m

                       Computation: 3143 steps/s (collection: 0.532s, learning 2.074s)
               Value function loss: 76069.2417
                    Surrogate loss: 0.0129
             Mean action noise std: 0.89
                       Mean reward: 5561.48
               Mean episode length: 313.30
                 Mean success rate: 52.50
                  Mean reward/step: 18.33
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 9592832
                    Iteration time: 2.61s
                        Total time: 3116.38s
                               ETA: 529146.7s

################################################################################
                    [1m Learning iteration 1171/200000 [0m

                       Computation: 3139 steps/s (collection: 0.550s, learning 2.059s)
               Value function loss: 98190.3473
                    Surrogate loss: 0.0132
             Mean action noise std: 0.89
                       Mean reward: 5998.43
               Mean episode length: 332.74
                 Mean success rate: 57.00
                  Mean reward/step: 17.96
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9601024
                    Iteration time: 2.61s
                        Total time: 3118.99s
                               ETA: 529135.1s

################################################################################
                    [1m Learning iteration 1172/200000 [0m

                       Computation: 3136 steps/s (collection: 0.532s, learning 2.080s)
               Value function loss: 61347.1139
                    Surrogate loss: 0.0163
             Mean action noise std: 0.89
                       Mean reward: 6116.05
               Mean episode length: 337.72
                 Mean success rate: 58.50
                  Mean reward/step: 17.56
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9609216
                    Iteration time: 2.61s
                        Total time: 3121.61s
                               ETA: 529124.1s

################################################################################
                    [1m Learning iteration 1173/200000 [0m

                       Computation: 3117 steps/s (collection: 0.504s, learning 2.123s)
               Value function loss: 78126.4628
                    Surrogate loss: 0.0137
             Mean action noise std: 0.89
                       Mean reward: 6334.01
               Mean episode length: 341.61
                 Mean success rate: 60.50
                  Mean reward/step: 17.31
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9617408
                    Iteration time: 2.63s
                        Total time: 3124.23s
                               ETA: 529115.7s

################################################################################
                    [1m Learning iteration 1174/200000 [0m

                       Computation: 3053 steps/s (collection: 0.571s, learning 2.112s)
               Value function loss: 93810.2703
                    Surrogate loss: 0.0132
             Mean action noise std: 0.89
                       Mean reward: 6550.72
               Mean episode length: 353.13
                 Mean success rate: 62.50
                  Mean reward/step: 17.60
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 9625600
                    Iteration time: 2.68s
                        Total time: 3126.92s
                               ETA: 529116.7s

################################################################################
                    [1m Learning iteration 1175/200000 [0m

                       Computation: 3073 steps/s (collection: 0.583s, learning 2.082s)
               Value function loss: 75343.0996
                    Surrogate loss: 0.0139
             Mean action noise std: 0.89
                       Mean reward: 6198.99
               Mean episode length: 336.34
                 Mean success rate: 59.50
                  Mean reward/step: 17.38
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 2.67s
                        Total time: 3129.58s
                               ETA: 529114.7s

################################################################################
                    [1m Learning iteration 1176/200000 [0m

                       Computation: 3069 steps/s (collection: 0.577s, learning 2.091s)
               Value function loss: 80484.9998
                    Surrogate loss: 0.0138
             Mean action noise std: 0.89
                       Mean reward: 6087.43
               Mean episode length: 339.62
                 Mean success rate: 59.50
                  Mean reward/step: 16.79
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 9641984
                    Iteration time: 2.67s
                        Total time: 3132.25s
                               ETA: 529113.3s

################################################################################
                    [1m Learning iteration 1177/200000 [0m

                       Computation: 3024 steps/s (collection: 0.578s, learning 2.131s)
               Value function loss: 53941.9351
                    Surrogate loss: 0.0136
             Mean action noise std: 0.89
                       Mean reward: 6131.81
               Mean episode length: 341.14
                 Mean success rate: 61.00
                  Mean reward/step: 16.87
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9650176
                    Iteration time: 2.71s
                        Total time: 3134.96s
                               ETA: 529118.6s

################################################################################
                    [1m Learning iteration 1178/200000 [0m

                       Computation: 3079 steps/s (collection: 0.575s, learning 2.086s)
               Value function loss: 48202.3282
                    Surrogate loss: 0.0157
             Mean action noise std: 0.89
                       Mean reward: 6245.70
               Mean episode length: 346.81
                 Mean success rate: 62.50
                  Mean reward/step: 17.31
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9658368
                    Iteration time: 2.66s
                        Total time: 3137.62s
                               ETA: 529115.8s

################################################################################
                    [1m Learning iteration 1179/200000 [0m

                       Computation: 3146 steps/s (collection: 0.544s, learning 2.059s)
               Value function loss: 81532.3031
                    Surrogate loss: 0.0143
             Mean action noise std: 0.89
                       Mean reward: 5755.89
               Mean episode length: 326.78
                 Mean success rate: 57.00
                  Mean reward/step: 17.54
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 9666560
                    Iteration time: 2.60s
                        Total time: 3140.22s
                               ETA: 529103.4s

################################################################################
                    [1m Learning iteration 1180/200000 [0m

                       Computation: 3119 steps/s (collection: 0.556s, learning 2.070s)
               Value function loss: 78752.9610
                    Surrogate loss: 0.0155
             Mean action noise std: 0.89
                       Mean reward: 5821.43
               Mean episode length: 328.01
                 Mean success rate: 58.00
                  Mean reward/step: 16.95
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9674752
                    Iteration time: 2.63s
                        Total time: 3142.85s
                               ETA: 529094.8s

################################################################################
                    [1m Learning iteration 1181/200000 [0m

                       Computation: 3067 steps/s (collection: 0.610s, learning 2.061s)
               Value function loss: 51568.8234
                    Surrogate loss: 0.0131
             Mean action noise std: 0.89
                       Mean reward: 5637.89
               Mean episode length: 320.31
                 Mean success rate: 55.00
                  Mean reward/step: 16.75
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 9682944
                    Iteration time: 2.67s
                        Total time: 3145.52s
                               ETA: 529093.8s

################################################################################
                    [1m Learning iteration 1182/200000 [0m

                       Computation: 3132 steps/s (collection: 0.555s, learning 2.060s)
               Value function loss: 65614.9365
                    Surrogate loss: 0.0157
             Mean action noise std: 0.89
                       Mean reward: 5549.91
               Mean episode length: 314.48
                 Mean success rate: 53.00
                  Mean reward/step: 17.44
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 9691136
                    Iteration time: 2.62s
                        Total time: 3148.13s
                               ETA: 529083.4s

################################################################################
                    [1m Learning iteration 1183/200000 [0m

                       Computation: 3140 steps/s (collection: 0.530s, learning 2.078s)
               Value function loss: 39265.4451
                    Surrogate loss: 0.0131
             Mean action noise std: 0.89
                       Mean reward: 5309.29
               Mean episode length: 305.26
                 Mean success rate: 50.50
                  Mean reward/step: 18.09
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 9699328
                    Iteration time: 2.61s
                        Total time: 3150.74s
                               ETA: 529071.9s

################################################################################
                    [1m Learning iteration 1184/200000 [0m

                       Computation: 3093 steps/s (collection: 0.555s, learning 2.094s)
               Value function loss: 89217.1868
                    Surrogate loss: 0.0107
             Mean action noise std: 0.89
                       Mean reward: 4973.51
               Mean episode length: 291.35
                 Mean success rate: 46.50
                  Mean reward/step: 18.57
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 9707520
                    Iteration time: 2.65s
                        Total time: 3153.39s
                               ETA: 529067.1s

################################################################################
                    [1m Learning iteration 1185/200000 [0m

                       Computation: 3025 steps/s (collection: 0.593s, learning 2.114s)
               Value function loss: 70542.0142
                    Surrogate loss: 0.0122
             Mean action noise std: 0.89
                       Mean reward: 5132.86
               Mean episode length: 294.90
                 Mean success rate: 47.00
                  Mean reward/step: 18.47
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9715712
                    Iteration time: 2.71s
                        Total time: 3156.10s
                               ETA: 529072.2s

################################################################################
                    [1m Learning iteration 1186/200000 [0m

                       Computation: 3115 steps/s (collection: 0.544s, learning 2.086s)
               Value function loss: 78197.6096
                    Surrogate loss: 0.0140
             Mean action noise std: 0.89
                       Mean reward: 5146.15
               Mean episode length: 296.36
                 Mean success rate: 46.50
                  Mean reward/step: 17.83
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9723904
                    Iteration time: 2.63s
                        Total time: 3158.73s
                               ETA: 529064.3s

################################################################################
                    [1m Learning iteration 1187/200000 [0m

                       Computation: 3124 steps/s (collection: 0.541s, learning 2.081s)
               Value function loss: 75254.8084
                    Surrogate loss: 0.0128
             Mean action noise std: 0.89
                       Mean reward: 5577.10
               Mean episode length: 309.86
                 Mean success rate: 50.50
                  Mean reward/step: 17.98
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 2.62s
                        Total time: 3161.35s
                               ETA: 529055.0s

################################################################################
                    [1m Learning iteration 1188/200000 [0m

                       Computation: 3055 steps/s (collection: 0.536s, learning 2.145s)
               Value function loss: 64063.8602
                    Surrogate loss: 0.0119
             Mean action noise std: 0.89
                       Mean reward: 5017.59
               Mean episode length: 289.76
                 Mean success rate: 47.00
                  Mean reward/step: 18.05
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 9740288
                    Iteration time: 2.68s
                        Total time: 3164.03s
                               ETA: 529055.7s

################################################################################
                    [1m Learning iteration 1189/200000 [0m

                       Computation: 3037 steps/s (collection: 0.574s, learning 2.123s)
               Value function loss: 46086.0740
                    Surrogate loss: 0.0156
             Mean action noise std: 0.89
                       Mean reward: 5151.75
               Mean episode length: 297.32
                 Mean success rate: 49.00
                  Mean reward/step: 18.01
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 9748480
                    Iteration time: 2.70s
                        Total time: 3166.73s
                               ETA: 529059.1s

################################################################################
                    [1m Learning iteration 1190/200000 [0m

                       Computation: 3059 steps/s (collection: 0.612s, learning 2.066s)
               Value function loss: 71035.4070
                    Surrogate loss: 0.0145
             Mean action noise std: 0.89
                       Mean reward: 5308.43
               Mean episode length: 303.83
                 Mean success rate: 50.50
                  Mean reward/step: 18.13
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 9756672
                    Iteration time: 2.68s
                        Total time: 3169.41s
                               ETA: 529059.1s

################################################################################
                    [1m Learning iteration 1191/200000 [0m

                       Computation: 3073 steps/s (collection: 0.584s, learning 2.081s)
               Value function loss: 110437.1197
                    Surrogate loss: 0.0121
             Mean action noise std: 0.89
                       Mean reward: 5730.86
               Mean episode length: 316.04
                 Mean success rate: 55.00
                  Mean reward/step: 18.43
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 9764864
                    Iteration time: 2.67s
                        Total time: 3172.07s
                               ETA: 529057.2s

################################################################################
                    [1m Learning iteration 1192/200000 [0m

                       Computation: 3111 steps/s (collection: 0.570s, learning 2.063s)
               Value function loss: 62726.0012
                    Surrogate loss: 0.0160
             Mean action noise std: 0.89
                       Mean reward: 5495.46
               Mean episode length: 311.75
                 Mean success rate: 54.00
                  Mean reward/step: 17.70
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 9773056
                    Iteration time: 2.63s
                        Total time: 3174.70s
                               ETA: 529049.9s

################################################################################
                    [1m Learning iteration 1193/200000 [0m

                       Computation: 3108 steps/s (collection: 0.556s, learning 2.079s)
               Value function loss: 73834.5655
                    Surrogate loss: 0.0176
             Mean action noise std: 0.89
                       Mean reward: 5103.72
               Mean episode length: 297.26
                 Mean success rate: 51.00
                  Mean reward/step: 17.59
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 9781248
                    Iteration time: 2.64s
                        Total time: 3177.34s
                               ETA: 529042.9s

################################################################################
                    [1m Learning iteration 1194/200000 [0m

                       Computation: 3150 steps/s (collection: 0.526s, learning 2.073s)
               Value function loss: 55254.6775
                    Surrogate loss: 0.0152
             Mean action noise std: 0.89
                       Mean reward: 5217.44
               Mean episode length: 302.00
                 Mean success rate: 51.50
                  Mean reward/step: 17.01
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 9789440
                    Iteration time: 2.60s
                        Total time: 3179.94s
                               ETA: 529030.1s

################################################################################
                    [1m Learning iteration 1195/200000 [0m

                       Computation: 3153 steps/s (collection: 0.546s, learning 2.052s)
               Value function loss: 66511.9791
                    Surrogate loss: 0.0153
             Mean action noise std: 0.89
                       Mean reward: 5420.21
               Mean episode length: 310.01
                 Mean success rate: 54.00
                  Mean reward/step: 17.64
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 9797632
                    Iteration time: 2.60s
                        Total time: 3182.54s
                               ETA: 529016.9s

################################################################################
                    [1m Learning iteration 1196/200000 [0m

                       Computation: 3038 steps/s (collection: 0.550s, learning 2.146s)
               Value function loss: 95909.9578
                    Surrogate loss: 0.0141
             Mean action noise std: 0.89
                       Mean reward: 5394.99
               Mean episode length: 302.22
                 Mean success rate: 52.50
                  Mean reward/step: 18.52
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 9805824
                    Iteration time: 2.70s
                        Total time: 3185.23s
                               ETA: 529020.1s

################################################################################
                    [1m Learning iteration 1197/200000 [0m

                       Computation: 2995 steps/s (collection: 0.638s, learning 2.097s)
               Value function loss: 69916.7397
                    Surrogate loss: 0.0144
             Mean action noise std: 0.89
                       Mean reward: 5731.40
               Mean episode length: 318.00
                 Mean success rate: 55.50
                  Mean reward/step: 18.13
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9814016
                    Iteration time: 2.73s
                        Total time: 3187.97s
                               ETA: 529029.7s

################################################################################
                    [1m Learning iteration 1198/200000 [0m

                       Computation: 3099 steps/s (collection: 0.593s, learning 2.050s)
               Value function loss: 72085.5425
                    Surrogate loss: 0.0128
             Mean action noise std: 0.89
                       Mean reward: 5783.98
               Mean episode length: 316.58
                 Mean success rate: 55.50
                  Mean reward/step: 18.05
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9822208
                    Iteration time: 2.64s
                        Total time: 3190.61s
                               ETA: 529024.0s

################################################################################
                    [1m Learning iteration 1199/200000 [0m

                       Computation: 3181 steps/s (collection: 0.507s, learning 2.068s)
               Value function loss: 69542.8724
                    Surrogate loss: 0.0157
             Mean action noise std: 0.89
                       Mean reward: 5914.79
               Mean episode length: 319.04
                 Mean success rate: 56.00
                  Mean reward/step: 18.08
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 2.58s
                        Total time: 3193.19s
                               ETA: 529007.1s

################################################################################
                    [1m Learning iteration 1200/200000 [0m

                       Computation: 3033 steps/s (collection: 0.581s, learning 2.119s)
               Value function loss: 49816.8426
                    Surrogate loss: 0.0149
             Mean action noise std: 0.89
                       Mean reward: 5664.03
               Mean episode length: 310.16
                 Mean success rate: 54.00
                  Mean reward/step: 18.11
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 9838592
                    Iteration time: 2.70s
                        Total time: 3195.89s
                               ETA: 529010.9s

################################################################################
                    [1m Learning iteration 1201/200000 [0m

                       Computation: 3085 steps/s (collection: 0.568s, learning 2.087s)
               Value function loss: 54691.5384
                    Surrogate loss: 0.0197
             Mean action noise std: 0.89
                       Mean reward: 5805.28
               Mean episode length: 316.23
                 Mean success rate: 55.00
                  Mean reward/step: 18.62
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9846784
                    Iteration time: 2.66s
                        Total time: 3198.54s
                               ETA: 529007.3s

################################################################################
                    [1m Learning iteration 1202/200000 [0m

                       Computation: 3162 steps/s (collection: 0.540s, learning 2.050s)
               Value function loss: 72216.0836
                    Surrogate loss: 0.0207
             Mean action noise std: 0.89
                       Mean reward: 5871.55
               Mean episode length: 320.57
                 Mean success rate: 56.00
                  Mean reward/step: 18.76
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 9854976
                    Iteration time: 2.59s
                        Total time: 3201.13s
                               ETA: 528993.0s

################################################################################
                    [1m Learning iteration 1203/200000 [0m

                       Computation: 3099 steps/s (collection: 0.523s, learning 2.120s)
               Value function loss: 67914.1653
                    Surrogate loss: 0.0142
             Mean action noise std: 0.89
                       Mean reward: 5688.85
               Mean episode length: 315.43
                 Mean success rate: 53.00
                  Mean reward/step: 19.01
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9863168
                    Iteration time: 2.64s
                        Total time: 3203.77s
                               ETA: 528987.3s

################################################################################
                    [1m Learning iteration 1204/200000 [0m

                       Computation: 3163 steps/s (collection: 0.505s, learning 2.084s)
               Value function loss: 83449.0825
                    Surrogate loss: 0.0122
             Mean action noise std: 0.89
                       Mean reward: 5972.54
               Mean episode length: 323.83
                 Mean success rate: 56.00
                  Mean reward/step: 17.72
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 9871360
                    Iteration time: 2.59s
                        Total time: 3206.36s
                               ETA: 528972.8s

################################################################################
                    [1m Learning iteration 1205/200000 [0m

                       Computation: 3080 steps/s (collection: 0.587s, learning 2.073s)
               Value function loss: 40436.3785
                    Surrogate loss: 0.0152
             Mean action noise std: 0.89
                       Mean reward: 5519.90
               Mean episode length: 311.22
                 Mean success rate: 52.50
                  Mean reward/step: 17.69
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9879552
                    Iteration time: 2.66s
                        Total time: 3209.02s
                               ETA: 528970.0s

################################################################################
                    [1m Learning iteration 1206/200000 [0m

                       Computation: 3104 steps/s (collection: 0.572s, learning 2.066s)
               Value function loss: 113858.4911
                    Surrogate loss: 0.0157
             Mean action noise std: 0.89
                       Mean reward: 5892.79
               Mean episode length: 326.85
                 Mean success rate: 56.50
                  Mean reward/step: 17.69
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 9887744
                    Iteration time: 2.64s
                        Total time: 3211.66s
                               ETA: 528963.6s

################################################################################
                    [1m Learning iteration 1207/200000 [0m

                       Computation: 3116 steps/s (collection: 0.537s, learning 2.091s)
               Value function loss: 87705.4548
                    Surrogate loss: 0.0137
             Mean action noise std: 0.89
                       Mean reward: 6031.60
               Mean episode length: 329.27
                 Mean success rate: 57.50
                  Mean reward/step: 17.40
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9895936
                    Iteration time: 2.63s
                        Total time: 3214.29s
                               ETA: 528955.7s

################################################################################
                    [1m Learning iteration 1208/200000 [0m

                       Computation: 3080 steps/s (collection: 0.566s, learning 2.093s)
               Value function loss: 50321.3884
                    Surrogate loss: 0.0158
             Mean action noise std: 0.89
                       Mean reward: 6093.77
               Mean episode length: 330.99
                 Mean success rate: 58.00
                  Mean reward/step: 17.09
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 9904128
                    Iteration time: 2.66s
                        Total time: 3216.95s
                               ETA: 528952.8s

################################################################################
                    [1m Learning iteration 1209/200000 [0m

                       Computation: 3026 steps/s (collection: 0.554s, learning 2.153s)
               Value function loss: 72992.9481
                    Surrogate loss: 0.0145
             Mean action noise std: 0.89
                       Mean reward: 6157.70
               Mean episode length: 336.49
                 Mean success rate: 59.50
                  Mean reward/step: 17.81
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 9912320
                    Iteration time: 2.71s
                        Total time: 3219.66s
                               ETA: 528957.7s

################################################################################
                    [1m Learning iteration 1210/200000 [0m

                       Computation: 3131 steps/s (collection: 0.526s, learning 2.090s)
               Value function loss: 73854.0075
                    Surrogate loss: 0.0128
             Mean action noise std: 0.89
                       Mean reward: 6169.94
               Mean episode length: 332.82
                 Mean success rate: 59.00
                  Mean reward/step: 17.86
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9920512
                    Iteration time: 2.62s
                        Total time: 3222.27s
                               ETA: 528947.7s

################################################################################
                    [1m Learning iteration 1211/200000 [0m

                       Computation: 3171 steps/s (collection: 0.496s, learning 2.087s)
               Value function loss: 72261.5434
                    Surrogate loss: 0.0142
             Mean action noise std: 0.89
                       Mean reward: 6143.81
               Mean episode length: 337.75
                 Mean success rate: 60.00
                  Mean reward/step: 16.78
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 2.58s
                        Total time: 3224.86s
                               ETA: 528932.3s

################################################################################
                    [1m Learning iteration 1212/200000 [0m

                       Computation: 3074 steps/s (collection: 0.602s, learning 2.063s)
               Value function loss: 77377.9248
                    Surrogate loss: 0.0140
             Mean action noise std: 0.89
                       Mean reward: 6281.38
               Mean episode length: 346.32
                 Mean success rate: 61.00
                  Mean reward/step: 16.94
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 9936896
                    Iteration time: 2.66s
                        Total time: 3227.52s
                               ETA: 528930.3s

################################################################################
                    [1m Learning iteration 1213/200000 [0m

                       Computation: 3161 steps/s (collection: 0.526s, learning 2.065s)
               Value function loss: 52007.1626
                    Surrogate loss: 0.0129
             Mean action noise std: 0.89
                       Mean reward: 6341.98
               Mean episode length: 347.22
                 Mean success rate: 61.00
                  Mean reward/step: 17.62
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 9945088
                    Iteration time: 2.59s
                        Total time: 3230.11s
                               ETA: 528916.2s

################################################################################
                    [1m Learning iteration 1214/200000 [0m

                       Computation: 3185 steps/s (collection: 0.521s, learning 2.051s)
               Value function loss: 56355.0022
                    Surrogate loss: 0.0158
             Mean action noise std: 0.89
                       Mean reward: 5961.17
               Mean episode length: 333.39
                 Mean success rate: 57.50
                  Mean reward/step: 17.31
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 9953280
                    Iteration time: 2.57s
                        Total time: 3232.68s
                               ETA: 528898.9s

################################################################################
                    [1m Learning iteration 1215/200000 [0m

                       Computation: 3029 steps/s (collection: 0.564s, learning 2.140s)
               Value function loss: 78252.4396
                    Surrogate loss: 0.0144
             Mean action noise std: 0.89
                       Mean reward: 5825.32
               Mean episode length: 326.37
                 Mean success rate: 55.50
                  Mean reward/step: 17.34
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 9961472
                    Iteration time: 2.70s
                        Total time: 3235.39s
                               ETA: 528903.4s

################################################################################
                    [1m Learning iteration 1216/200000 [0m

                       Computation: 3032 steps/s (collection: 0.619s, learning 2.082s)
               Value function loss: 45211.9300
                    Surrogate loss: 0.0139
             Mean action noise std: 0.89
                       Mean reward: 5536.96
               Mean episode length: 320.83
                 Mean success rate: 53.50
                  Mean reward/step: 17.95
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 9969664
                    Iteration time: 2.70s
                        Total time: 3238.09s
                               ETA: 528907.4s

################################################################################
                    [1m Learning iteration 1217/200000 [0m

                       Computation: 3121 steps/s (collection: 0.565s, learning 2.059s)
               Value function loss: 60412.1183
                    Surrogate loss: 0.0133
             Mean action noise std: 0.89
                       Mean reward: 5339.27
               Mean episode length: 314.01
                 Mean success rate: 51.00
                  Mean reward/step: 18.33
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 9977856
                    Iteration time: 2.62s
                        Total time: 3240.71s
                               ETA: 528898.8s

################################################################################
                    [1m Learning iteration 1218/200000 [0m

                       Computation: 3165 steps/s (collection: 0.533s, learning 2.055s)
               Value function loss: 67696.5173
                    Surrogate loss: 0.0178
             Mean action noise std: 0.89
                       Mean reward: 5166.04
               Mean episode length: 308.76
                 Mean success rate: 50.00
                  Mean reward/step: 18.09
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 9986048
                    Iteration time: 2.59s
                        Total time: 3243.30s
                               ETA: 528884.2s

################################################################################
                    [1m Learning iteration 1219/200000 [0m

                       Computation: 2969 steps/s (collection: 0.640s, learning 2.118s)
               Value function loss: 101034.4250
                    Surrogate loss: 0.0141
             Mean action noise std: 0.89
                       Mean reward: 5245.03
               Mean episode length: 309.58
                 Mean success rate: 50.50
                  Mean reward/step: 18.28
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 9994240
                    Iteration time: 2.76s
                        Total time: 3246.06s
                               ETA: 528897.5s

################################################################################
                    [1m Learning iteration 1220/200000 [0m

                       Computation: 3001 steps/s (collection: 0.642s, learning 2.087s)
               Value function loss: 65228.4453
                    Surrogate loss: 0.0224
             Mean action noise std: 0.89
                       Mean reward: 5024.36
               Mean episode length: 299.98
                 Mean success rate: 49.50
                  Mean reward/step: 18.33
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10002432
                    Iteration time: 2.73s
                        Total time: 3248.79s
                               ETA: 528905.9s

################################################################################
                    [1m Learning iteration 1221/200000 [0m

                       Computation: 3062 steps/s (collection: 0.613s, learning 2.062s)
               Value function loss: 73401.4309
                    Surrogate loss: 0.0173
             Mean action noise std: 0.89
                       Mean reward: 5316.61
               Mean episode length: 306.65
                 Mean success rate: 51.50
                  Mean reward/step: 18.21
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10010624
                    Iteration time: 2.68s
                        Total time: 3251.46s
                               ETA: 528905.7s

################################################################################
                    [1m Learning iteration 1222/200000 [0m

                       Computation: 3128 steps/s (collection: 0.545s, learning 2.074s)
               Value function loss: 73851.5546
                    Surrogate loss: 0.0102
             Mean action noise std: 0.89
                       Mean reward: 5636.69
               Mean episode length: 317.19
                 Mean success rate: 54.50
                  Mean reward/step: 18.30
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 10018816
                    Iteration time: 2.62s
                        Total time: 3254.08s
                               ETA: 528896.2s

################################################################################
                    [1m Learning iteration 1223/200000 [0m

                       Computation: 3001 steps/s (collection: 0.591s, learning 2.138s)
               Value function loss: 71061.7745
                    Surrogate loss: 0.0145
             Mean action noise std: 0.89
                       Mean reward: 5602.75
               Mean episode length: 314.73
                 Mean success rate: 54.00
                  Mean reward/step: 18.00
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 2.73s
                        Total time: 3256.81s
                               ETA: 528904.6s

################################################################################
                    [1m Learning iteration 1224/200000 [0m

                       Computation: 3175 steps/s (collection: 0.509s, learning 2.071s)
               Value function loss: 47224.7381
                    Surrogate loss: 0.0149
             Mean action noise std: 0.89
                       Mean reward: 5693.63
               Mean episode length: 311.50
                 Mean success rate: 54.50
                  Mean reward/step: 17.95
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10035200
                    Iteration time: 2.58s
                        Total time: 3259.39s
                               ETA: 528888.9s

################################################################################
                    [1m Learning iteration 1225/200000 [0m

                       Computation: 3191 steps/s (collection: 0.512s, learning 2.055s)
               Value function loss: 63335.3733
                    Surrogate loss: 0.0131
             Mean action noise std: 0.89
                       Mean reward: 6095.88
               Mean episode length: 322.63
                 Mean success rate: 58.50
                  Mean reward/step: 17.89
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10043392
                    Iteration time: 2.57s
                        Total time: 3261.96s
                               ETA: 528871.0s

################################################################################
                    [1m Learning iteration 1226/200000 [0m

                       Computation: 3071 steps/s (collection: 0.572s, learning 2.095s)
               Value function loss: 85370.6228
                    Surrogate loss: 0.0128
             Mean action noise std: 0.89
                       Mean reward: 6117.73
               Mean episode length: 325.79
                 Mean success rate: 59.50
                  Mean reward/step: 17.64
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 10051584
                    Iteration time: 2.67s
                        Total time: 3264.63s
                               ETA: 528869.4s

################################################################################
                    [1m Learning iteration 1227/200000 [0m

                       Computation: 3073 steps/s (collection: 0.613s, learning 2.053s)
               Value function loss: 72384.3930
                    Surrogate loss: 0.0114
             Mean action noise std: 0.89
                       Mean reward: 5999.07
               Mean episode length: 323.71
                 Mean success rate: 58.50
                  Mean reward/step: 17.11
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 10059776
                    Iteration time: 2.67s
                        Total time: 3267.29s
                               ETA: 528867.5s

################################################################################
                    [1m Learning iteration 1228/200000 [0m

                       Computation: 3082 steps/s (collection: 0.575s, learning 2.083s)
               Value function loss: 54904.4440
                    Surrogate loss: 0.0123
             Mean action noise std: 0.89
                       Mean reward: 5645.54
               Mean episode length: 312.04
                 Mean success rate: 55.50
                  Mean reward/step: 16.97
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10067968
                    Iteration time: 2.66s
                        Total time: 3269.95s
                               ETA: 528864.4s

################################################################################
                    [1m Learning iteration 1229/200000 [0m

                       Computation: 3090 steps/s (collection: 0.561s, learning 2.090s)
               Value function loss: 77038.8165
                    Surrogate loss: 0.0128
             Mean action noise std: 0.89
                       Mean reward: 5093.66
               Mean episode length: 288.14
                 Mean success rate: 51.00
                  Mean reward/step: 16.76
       Mean episode length/episode: 27.49
--------------------------------------------------------------------------------
                   Total timesteps: 10076160
                    Iteration time: 2.65s
                        Total time: 3272.60s
                               ETA: 528860.0s

################################################################################
                    [1m Learning iteration 1230/200000 [0m

                       Computation: 3032 steps/s (collection: 0.567s, learning 2.134s)
               Value function loss: 53454.7972
                    Surrogate loss: 0.0122
             Mean action noise std: 0.89
                       Mean reward: 5019.18
               Mean episode length: 285.89
                 Mean success rate: 49.50
                  Mean reward/step: 16.10
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10084352
                    Iteration time: 2.70s
                        Total time: 3275.30s
                               ETA: 528864.0s

################################################################################
                    [1m Learning iteration 1231/200000 [0m

                       Computation: 2970 steps/s (collection: 0.625s, learning 2.133s)
               Value function loss: 49430.6091
                    Surrogate loss: 0.0117
             Mean action noise std: 0.89
                       Mean reward: 5194.43
               Mean episode length: 294.82
                 Mean success rate: 52.00
                  Mean reward/step: 16.15
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10092544
                    Iteration time: 2.76s
                        Total time: 3278.06s
                               ETA: 528877.0s

################################################################################
                    [1m Learning iteration 1232/200000 [0m

                       Computation: 3156 steps/s (collection: 0.530s, learning 2.065s)
               Value function loss: 58770.8423
                    Surrogate loss: 0.0098
             Mean action noise std: 0.89
                       Mean reward: 4887.89
               Mean episode length: 289.29
                 Mean success rate: 49.50
                  Mean reward/step: 16.76
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10100736
                    Iteration time: 2.60s
                        Total time: 3280.65s
                               ETA: 528863.7s

################################################################################
                    [1m Learning iteration 1233/200000 [0m

                       Computation: 3157 steps/s (collection: 0.520s, learning 2.074s)
               Value function loss: 73897.3785
                    Surrogate loss: 0.0145
             Mean action noise std: 0.89
                       Mean reward: 5029.04
               Mean episode length: 288.83
                 Mean success rate: 50.50
                  Mean reward/step: 17.08
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 10108928
                    Iteration time: 2.59s
                        Total time: 3283.25s
                               ETA: 528850.4s

################################################################################
                    [1m Learning iteration 1234/200000 [0m

                       Computation: 2942 steps/s (collection: 0.637s, learning 2.147s)
               Value function loss: 60152.9235
                    Surrogate loss: 0.0138
             Mean action noise std: 0.89
                       Mean reward: 5037.53
               Mean episode length: 288.24
                 Mean success rate: 50.50
                  Mean reward/step: 17.75
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10117120
                    Iteration time: 2.78s
                        Total time: 3286.03s
                               ETA: 528867.6s

################################################################################
                    [1m Learning iteration 1235/200000 [0m

                       Computation: 2964 steps/s (collection: 0.663s, learning 2.100s)
               Value function loss: 104443.2701
                    Surrogate loss: 0.0139
             Mean action noise std: 0.89
                       Mean reward: 5419.71
               Mean episode length: 303.11
                 Mean success rate: 54.00
                  Mean reward/step: 17.56
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 2.76s
                        Total time: 3288.80s
                               ETA: 528881.4s

################################################################################
                    [1m Learning iteration 1236/200000 [0m

                       Computation: 3101 steps/s (collection: 0.582s, learning 2.059s)
               Value function loss: 41648.2128
                    Surrogate loss: 0.0129
             Mean action noise std: 0.89
                       Mean reward: 5440.15
               Mean episode length: 304.46
                 Mean success rate: 55.50
                  Mean reward/step: 16.53
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 10133504
                    Iteration time: 2.64s
                        Total time: 3291.44s
                               ETA: 528875.6s

################################################################################
                    [1m Learning iteration 1237/200000 [0m

                       Computation: 3157 steps/s (collection: 0.536s, learning 2.058s)
               Value function loss: 76013.3042
                    Surrogate loss: 0.0115
             Mean action noise std: 0.89
                       Mean reward: 5355.81
               Mean episode length: 308.26
                 Mean success rate: 55.00
                  Mean reward/step: 17.10
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 10141696
                    Iteration time: 2.59s
                        Total time: 3294.03s
                               ETA: 528862.2s

################################################################################
                    [1m Learning iteration 1238/200000 [0m

                       Computation: 3144 steps/s (collection: 0.546s, learning 2.059s)
               Value function loss: 59709.2405
                    Surrogate loss: 0.0151
             Mean action noise std: 0.89
                       Mean reward: 5069.73
               Mean episode length: 297.12
                 Mean success rate: 53.00
                  Mean reward/step: 17.03
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 10149888
                    Iteration time: 2.61s
                        Total time: 3296.64s
                               ETA: 528850.7s

################################################################################
                    [1m Learning iteration 1239/200000 [0m

                       Computation: 3145 steps/s (collection: 0.546s, learning 2.059s)
               Value function loss: 53244.5155
                    Surrogate loss: 0.0192
             Mean action noise std: 0.89
                       Mean reward: 5211.75
               Mean episode length: 299.94
                 Mean success rate: 54.00
                  Mean reward/step: 17.16
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 10158080
                    Iteration time: 2.60s
                        Total time: 3299.24s
                               ETA: 528838.9s

################################################################################
                    [1m Learning iteration 1240/200000 [0m

                       Computation: 3195 steps/s (collection: 0.510s, learning 2.053s)
               Value function loss: 67501.4157
                    Surrogate loss: 0.0154
             Mean action noise std: 0.89
                       Mean reward: 5231.29
               Mean episode length: 301.08
                 Mean success rate: 54.50
                  Mean reward/step: 17.73
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10166272
                    Iteration time: 2.56s
                        Total time: 3301.80s
                               ETA: 528820.7s

################################################################################
                    [1m Learning iteration 1241/200000 [0m

                       Computation: 3118 steps/s (collection: 0.494s, learning 2.132s)
               Value function loss: 66136.5730
                    Surrogate loss: 0.0155
             Mean action noise std: 0.89
                       Mean reward: 5009.93
               Mean episode length: 291.32
                 Mean success rate: 52.00
                  Mean reward/step: 18.06
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10174464
                    Iteration time: 2.63s
                        Total time: 3304.43s
                               ETA: 528812.6s

################################################################################
                    [1m Learning iteration 1242/200000 [0m

                       Computation: 2929 steps/s (collection: 0.677s, learning 2.119s)
               Value function loss: 75159.4061
                    Surrogate loss: 0.0112
             Mean action noise std: 0.89
                       Mean reward: 4983.15
               Mean episode length: 292.62
                 Mean success rate: 52.00
                  Mean reward/step: 17.87
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 10182656
                    Iteration time: 2.80s
                        Total time: 3307.23s
                               ETA: 528831.6s

################################################################################
                    [1m Learning iteration 1243/200000 [0m

                       Computation: 3176 steps/s (collection: 0.531s, learning 2.048s)
               Value function loss: 79477.0824
                    Surrogate loss: 0.0168
             Mean action noise std: 0.89
                       Mean reward: 5112.50
               Mean episode length: 291.75
                 Mean success rate: 51.50
                  Mean reward/step: 18.73
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10190848
                    Iteration time: 2.58s
                        Total time: 3309.81s
                               ETA: 528815.9s

################################################################################
                    [1m Learning iteration 1244/200000 [0m

                       Computation: 3209 steps/s (collection: 0.500s, learning 2.052s)
               Value function loss: 76069.6654
                    Surrogate loss: 0.0140
             Mean action noise std: 0.89
                       Mean reward: 5418.42
               Mean episode length: 313.30
                 Mean success rate: 55.00
                  Mean reward/step: 19.03
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10199040
                    Iteration time: 2.55s
                        Total time: 3312.36s
                               ETA: 528796.0s

################################################################################
                    [1m Learning iteration 1245/200000 [0m

                       Computation: 3109 steps/s (collection: 0.532s, learning 2.103s)
               Value function loss: 70053.2191
                    Surrogate loss: 0.0141
             Mean action noise std: 0.89
                       Mean reward: 5449.11
               Mean episode length: 314.16
                 Mean success rate: 55.50
                  Mean reward/step: 18.36
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 10207232
                    Iteration time: 2.63s
                        Total time: 3314.99s
                               ETA: 528789.2s

################################################################################
                    [1m Learning iteration 1246/200000 [0m

                       Computation: 3055 steps/s (collection: 0.603s, learning 2.079s)
               Value function loss: 57572.2052
                    Surrogate loss: 0.0170
             Mean action noise std: 0.89
                       Mean reward: 5579.96
               Mean episode length: 319.89
                 Mean success rate: 56.50
                  Mean reward/step: 18.30
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10215424
                    Iteration time: 2.68s
                        Total time: 3317.67s
                               ETA: 528789.9s

################################################################################
                    [1m Learning iteration 1247/200000 [0m

                       Computation: 3145 steps/s (collection: 0.531s, learning 2.074s)
               Value function loss: 46483.5854
                    Surrogate loss: 0.0146
             Mean action noise std: 0.89
                       Mean reward: 5600.16
               Mean episode length: 321.49
                 Mean success rate: 57.00
                  Mean reward/step: 19.15
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 2.60s
                        Total time: 3320.28s
                               ETA: 528778.3s

################################################################################
                    [1m Learning iteration 1248/200000 [0m

                       Computation: 3120 steps/s (collection: 0.554s, learning 2.071s)
               Value function loss: 85819.5342
                    Surrogate loss: 0.0132
             Mean action noise std: 0.89
                       Mean reward: 5695.82
               Mean episode length: 326.56
                 Mean success rate: 57.00
                  Mean reward/step: 19.29
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10231808
                    Iteration time: 2.62s
                        Total time: 3322.90s
                               ETA: 528770.0s

################################################################################
                    [1m Learning iteration 1249/200000 [0m

                       Computation: 3105 steps/s (collection: 0.549s, learning 2.090s)
               Value function loss: 88122.3433
                    Surrogate loss: 0.0111
             Mean action noise std: 0.89
                       Mean reward: 5958.80
               Mean episode length: 337.57
                 Mean success rate: 59.00
                  Mean reward/step: 18.00
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10240000
                    Iteration time: 2.64s
                        Total time: 3325.54s
                               ETA: 528763.8s

################################################################################
                    [1m Learning iteration 1250/200000 [0m

                       Computation: 3124 steps/s (collection: 0.571s, learning 2.050s)
               Value function loss: 64006.8938
                    Surrogate loss: 0.0164
             Mean action noise std: 0.89
                       Mean reward: 6075.58
               Mean episode length: 339.01
                 Mean success rate: 60.50
                  Mean reward/step: 18.04
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 10248192
                    Iteration time: 2.62s
                        Total time: 3328.16s
                               ETA: 528755.0s

################################################################################
                    [1m Learning iteration 1251/200000 [0m

                       Computation: 3102 steps/s (collection: 0.595s, learning 2.046s)
               Value function loss: 83781.6643
                    Surrogate loss: 0.0139
             Mean action noise std: 0.89
                       Mean reward: 6063.56
               Mean episode length: 340.94
                 Mean success rate: 60.50
                  Mean reward/step: 17.44
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 10256384
                    Iteration time: 2.64s
                        Total time: 3330.80s
                               ETA: 528749.2s

################################################################################
                    [1m Learning iteration 1252/200000 [0m

                       Computation: 3139 steps/s (collection: 0.528s, learning 2.082s)
               Value function loss: 57941.5306
                    Surrogate loss: 0.0120
             Mean action noise std: 0.89
                       Mean reward: 6135.53
               Mean episode length: 342.80
                 Mean success rate: 61.50
                  Mean reward/step: 18.22
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10264576
                    Iteration time: 2.61s
                        Total time: 3333.41s
                               ETA: 528738.5s

################################################################################
                    [1m Learning iteration 1253/200000 [0m

                       Computation: 3021 steps/s (collection: 0.601s, learning 2.110s)
               Value function loss: 100453.9688
                    Surrogate loss: 0.0139
             Mean action noise std: 0.89
                       Mean reward: 6569.75
               Mean episode length: 355.51
                 Mean success rate: 64.50
                  Mean reward/step: 18.44
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10272768
                    Iteration time: 2.71s
                        Total time: 3336.12s
                               ETA: 528743.9s

################################################################################
                    [1m Learning iteration 1254/200000 [0m

                       Computation: 2949 steps/s (collection: 0.665s, learning 2.113s)
               Value function loss: 87716.4481
                    Surrogate loss: 0.0170
             Mean action noise std: 0.89
                       Mean reward: 6057.78
               Mean episode length: 334.95
                 Mean success rate: 59.50
                  Mean reward/step: 18.10
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 10280960
                    Iteration time: 2.78s
                        Total time: 3338.90s
                               ETA: 528759.8s

################################################################################
                    [1m Learning iteration 1255/200000 [0m

                       Computation: 3106 steps/s (collection: 0.559s, learning 2.078s)
               Value function loss: 41700.8072
                    Surrogate loss: 0.0146
             Mean action noise std: 0.89
                       Mean reward: 6101.75
               Mean episode length: 332.85
                 Mean success rate: 59.50
                  Mean reward/step: 18.25
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 10289152
                    Iteration time: 2.64s
                        Total time: 3341.54s
                               ETA: 528753.4s

################################################################################
                    [1m Learning iteration 1256/200000 [0m

                       Computation: 3057 steps/s (collection: 0.570s, learning 2.109s)
               Value function loss: 67620.8739
                    Surrogate loss: 0.0168
             Mean action noise std: 0.89
                       Mean reward: 6128.73
               Mean episode length: 331.44
                 Mean success rate: 59.50
                  Mean reward/step: 19.00
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10297344
                    Iteration time: 2.68s
                        Total time: 3344.22s
                               ETA: 528753.7s

################################################################################
                    [1m Learning iteration 1257/200000 [0m

                       Computation: 3008 steps/s (collection: 0.617s, learning 2.105s)
               Value function loss: 79941.7534
                    Surrogate loss: 0.0179
             Mean action noise std: 0.89
                       Mean reward: 6034.48
               Mean episode length: 330.13
                 Mean success rate: 59.50
                  Mean reward/step: 19.05
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 10305536
                    Iteration time: 2.72s
                        Total time: 3346.94s
                               ETA: 528760.9s

################################################################################
                    [1m Learning iteration 1258/200000 [0m

                       Computation: 3106 steps/s (collection: 0.565s, learning 2.072s)
               Value function loss: 66757.5202
                    Surrogate loss: 0.0191
             Mean action noise std: 0.89
                       Mean reward: 5734.91
               Mean episode length: 317.10
                 Mean success rate: 56.00
                  Mean reward/step: 18.80
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10313728
                    Iteration time: 2.64s
                        Total time: 3349.58s
                               ETA: 528754.6s

################################################################################
                    [1m Learning iteration 1259/200000 [0m

                       Computation: 3132 steps/s (collection: 0.549s, learning 2.067s)
               Value function loss: 81907.1486
                    Surrogate loss: 0.0152
             Mean action noise std: 0.89
                       Mean reward: 5842.86
               Mean episode length: 319.62
                 Mean success rate: 57.00
                  Mean reward/step: 17.81
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 2.62s
                        Total time: 3352.19s
                               ETA: 528744.8s

################################################################################
                    [1m Learning iteration 1260/200000 [0m

                       Computation: 3105 steps/s (collection: 0.581s, learning 2.057s)
               Value function loss: 92701.3158
                    Surrogate loss: 0.0182
             Mean action noise std: 0.89
                       Mean reward: 5564.34
               Mean episode length: 301.12
                 Mean success rate: 54.50
                  Mean reward/step: 17.63
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 10330112
                    Iteration time: 2.64s
                        Total time: 3354.83s
                               ETA: 528738.6s

################################################################################
                    [1m Learning iteration 1261/200000 [0m

                       Computation: 3110 steps/s (collection: 0.558s, learning 2.075s)
               Value function loss: 54304.3218
                    Surrogate loss: 0.0158
             Mean action noise std: 0.89
                       Mean reward: 5562.88
               Mean episode length: 304.49
                 Mean success rate: 54.50
                  Mean reward/step: 17.04
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10338304
                    Iteration time: 2.63s
                        Total time: 3357.47s
                               ETA: 528731.7s

################################################################################
                    [1m Learning iteration 1262/200000 [0m

                       Computation: 3118 steps/s (collection: 0.535s, learning 2.092s)
               Value function loss: 66989.4834
                    Surrogate loss: 0.0129
             Mean action noise std: 0.89
                       Mean reward: 5636.49
               Mean episode length: 307.25
                 Mean success rate: 55.00
                  Mean reward/step: 17.68
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10346496
                    Iteration time: 2.63s
                        Total time: 3360.09s
                               ETA: 528723.8s

################################################################################
                    [1m Learning iteration 1263/200000 [0m

                       Computation: 3147 steps/s (collection: 0.539s, learning 2.064s)
               Value function loss: 44194.7260
                    Surrogate loss: 0.0142
             Mean action noise std: 0.89
                       Mean reward: 5379.95
               Mean episode length: 301.69
                 Mean success rate: 53.50
                  Mean reward/step: 18.05
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10354688
                    Iteration time: 2.60s
                        Total time: 3362.70s
                               ETA: 528712.1s

################################################################################
                    [1m Learning iteration 1264/200000 [0m

                       Computation: 3068 steps/s (collection: 0.530s, learning 2.140s)
               Value function loss: 91867.2770
                    Surrogate loss: 0.0147
             Mean action noise std: 0.89
                       Mean reward: 5488.98
               Mean episode length: 303.65
                 Mean success rate: 53.50
                  Mean reward/step: 18.34
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 10362880
                    Iteration time: 2.67s
                        Total time: 3365.37s
                               ETA: 528711.0s

################################################################################
                    [1m Learning iteration 1265/200000 [0m

                       Computation: 2994 steps/s (collection: 0.620s, learning 2.116s)
               Value function loss: 76946.4704
                    Surrogate loss: 0.0125
             Mean action noise std: 0.89
                       Mean reward: 5546.85
               Mean episode length: 304.56
                 Mean success rate: 53.50
                  Mean reward/step: 17.69
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10371072
                    Iteration time: 2.74s
                        Total time: 3368.10s
                               ETA: 528720.1s

################################################################################
                    [1m Learning iteration 1266/200000 [0m

                       Computation: 3095 steps/s (collection: 0.573s, learning 2.074s)
               Value function loss: 50232.6816
                    Surrogate loss: 0.0161
             Mean action noise std: 0.89
                       Mean reward: 5326.48
               Mean episode length: 295.50
                 Mean success rate: 52.00
                  Mean reward/step: 18.15
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10379264
                    Iteration time: 2.65s
                        Total time: 3370.75s
                               ETA: 528715.3s

################################################################################
                    [1m Learning iteration 1267/200000 [0m

                       Computation: 3149 steps/s (collection: 0.533s, learning 2.068s)
               Value function loss: 75193.0154
                    Surrogate loss: 0.0143
             Mean action noise std: 0.89
                       Mean reward: 5306.11
               Mean episode length: 295.92
                 Mean success rate: 52.50
                  Mean reward/step: 17.90
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 10387456
                    Iteration time: 2.60s
                        Total time: 3373.35s
                               ETA: 528703.3s

################################################################################
                    [1m Learning iteration 1268/200000 [0m

                       Computation: 3095 steps/s (collection: 0.531s, learning 2.115s)
               Value function loss: 70375.6952
                    Surrogate loss: 0.0179
             Mean action noise std: 0.89
                       Mean reward: 4974.25
               Mean episode length: 282.63
                 Mean success rate: 48.50
                  Mean reward/step: 17.72
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10395648
                    Iteration time: 2.65s
                        Total time: 3376.00s
                               ETA: 528698.4s

################################################################################
                    [1m Learning iteration 1269/200000 [0m

                       Computation: 3041 steps/s (collection: 0.594s, learning 2.099s)
               Value function loss: 94383.8785
                    Surrogate loss: 0.0148
             Mean action noise std: 0.89
                       Mean reward: 5328.54
               Mean episode length: 294.94
                 Mean success rate: 51.00
                  Mean reward/step: 17.31
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 10403840
                    Iteration time: 2.69s
                        Total time: 3378.69s
                               ETA: 528700.9s

################################################################################
                    [1m Learning iteration 1270/200000 [0m

                       Computation: 3129 steps/s (collection: 0.531s, learning 2.086s)
               Value function loss: 68780.9416
                    Surrogate loss: 0.0170
             Mean action noise std: 0.89
                       Mean reward: 5082.39
               Mean episode length: 290.84
                 Mean success rate: 49.50
                  Mean reward/step: 17.38
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 10412032
                    Iteration time: 2.62s
                        Total time: 3381.31s
                               ETA: 528691.6s

################################################################################
                    [1m Learning iteration 1271/200000 [0m

                       Computation: 3072 steps/s (collection: 0.566s, learning 2.099s)
               Value function loss: 49757.2598
                    Surrogate loss: 0.0158
             Mean action noise std: 0.89
                       Mean reward: 4539.55
               Mean episode length: 268.69
                 Mean success rate: 43.50
                  Mean reward/step: 18.39
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 2.67s
                        Total time: 3383.97s
                               ETA: 528689.8s

################################################################################
                    [1m Learning iteration 1272/200000 [0m

                       Computation: 3090 steps/s (collection: 0.596s, learning 2.054s)
               Value function loss: 73155.0960
                    Surrogate loss: 0.0143
             Mean action noise std: 0.89
                       Mean reward: 4699.01
               Mean episode length: 268.39
                 Mean success rate: 45.00
                  Mean reward/step: 18.86
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10428416
                    Iteration time: 2.65s
                        Total time: 3386.62s
                               ETA: 528685.6s

################################################################################
                    [1m Learning iteration 1273/200000 [0m

                       Computation: 3058 steps/s (collection: 0.578s, learning 2.101s)
               Value function loss: 83127.3707
                    Surrogate loss: 0.0149
             Mean action noise std: 0.89
                       Mean reward: 4726.31
               Mean episode length: 270.13
                 Mean success rate: 44.00
                  Mean reward/step: 18.87
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 10436608
                    Iteration time: 2.68s
                        Total time: 3389.30s
                               ETA: 528685.7s

################################################################################
                    [1m Learning iteration 1274/200000 [0m

                       Computation: 3094 steps/s (collection: 0.565s, learning 2.082s)
               Value function loss: 69883.3656
                    Surrogate loss: 0.0155
             Mean action noise std: 0.89
                       Mean reward: 4903.65
               Mean episode length: 280.60
                 Mean success rate: 47.00
                  Mean reward/step: 17.88
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 10444800
                    Iteration time: 2.65s
                        Total time: 3391.95s
                               ETA: 528681.0s

################################################################################
                    [1m Learning iteration 1275/200000 [0m

                       Computation: 3130 steps/s (collection: 0.533s, learning 2.084s)
               Value function loss: 69097.6897
                    Surrogate loss: 0.0167
             Mean action noise std: 0.89
                       Mean reward: 5057.80
               Mean episode length: 288.48
                 Mean success rate: 48.00
                  Mean reward/step: 18.02
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10452992
                    Iteration time: 2.62s
                        Total time: 3394.56s
                               ETA: 528671.5s

################################################################################
                    [1m Learning iteration 1276/200000 [0m

                       Computation: 3005 steps/s (collection: 0.574s, learning 2.152s)
               Value function loss: 85145.8554
                    Surrogate loss: 0.0149
             Mean action noise std: 0.89
                       Mean reward: 5163.46
               Mean episode length: 286.81
                 Mean success rate: 48.50
                  Mean reward/step: 17.89
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10461184
                    Iteration time: 2.73s
                        Total time: 3397.29s
                               ETA: 528679.1s

################################################################################
                    [1m Learning iteration 1277/200000 [0m

                       Computation: 3108 steps/s (collection: 0.548s, learning 2.087s)
               Value function loss: 60845.9119
                    Surrogate loss: 0.0123
             Mean action noise std: 0.89
                       Mean reward: 5109.99
               Mean episode length: 278.79
                 Mean success rate: 46.00
                  Mean reward/step: 18.52
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 10469376
                    Iteration time: 2.64s
                        Total time: 3399.93s
                               ETA: 528672.5s

################################################################################
                    [1m Learning iteration 1278/200000 [0m

                       Computation: 3128 steps/s (collection: 0.532s, learning 2.086s)
               Value function loss: 47467.5469
                    Surrogate loss: 0.0151
             Mean action noise std: 0.89
                       Mean reward: 5245.79
               Mean episode length: 283.94
                 Mean success rate: 48.00
                  Mean reward/step: 19.30
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 10477568
                    Iteration time: 2.62s
                        Total time: 3402.54s
                               ETA: 528663.3s

################################################################################
                    [1m Learning iteration 1279/200000 [0m

                       Computation: 3096 steps/s (collection: 0.516s, learning 2.130s)
               Value function loss: 59848.2394
                    Surrogate loss: 0.0169
             Mean action noise std: 0.89
                       Mean reward: 5094.15
               Mean episode length: 282.43
                 Mean success rate: 45.50
                  Mean reward/step: 19.59
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10485760
                    Iteration time: 2.65s
                        Total time: 3405.19s
                               ETA: 528658.4s

################################################################################
                    [1m Learning iteration 1280/200000 [0m

                       Computation: 3005 steps/s (collection: 0.632s, learning 2.093s)
               Value function loss: 88178.3320
                    Surrogate loss: 0.0153
             Mean action noise std: 0.89
                       Mean reward: 4828.56
               Mean episode length: 269.45
                 Mean success rate: 42.50
                  Mean reward/step: 19.68
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 10493952
                    Iteration time: 2.73s
                        Total time: 3407.92s
                               ETA: 528665.9s

################################################################################
                    [1m Learning iteration 1281/200000 [0m

                       Computation: 3053 steps/s (collection: 0.602s, learning 2.081s)
               Value function loss: 68203.4721
                    Surrogate loss: 0.0148
             Mean action noise std: 0.89
                       Mean reward: 4971.26
               Mean episode length: 270.12
                 Mean success rate: 42.00
                  Mean reward/step: 19.72
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 10502144
                    Iteration time: 2.68s
                        Total time: 3410.60s
                               ETA: 528666.8s

################################################################################
                    [1m Learning iteration 1282/200000 [0m

                       Computation: 3137 steps/s (collection: 0.520s, learning 2.091s)
               Value function loss: 77154.1528
                    Surrogate loss: 0.0175
             Mean action noise std: 0.89
                       Mean reward: 5007.29
               Mean episode length: 266.97
                 Mean success rate: 42.50
                  Mean reward/step: 20.11
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10510336
                    Iteration time: 2.61s
                        Total time: 3413.21s
                               ETA: 528656.5s

################################################################################
                    [1m Learning iteration 1283/200000 [0m

                       Computation: 3144 steps/s (collection: 0.542s, learning 2.063s)
               Value function loss: 59100.8545
                    Surrogate loss: 0.0180
             Mean action noise std: 0.89
                       Mean reward: 4614.42
               Mean episode length: 256.74
                 Mean success rate: 40.00
                  Mean reward/step: 19.82
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 2.61s
                        Total time: 3415.82s
                               ETA: 528645.3s

################################################################################
                    [1m Learning iteration 1284/200000 [0m

                       Computation: 3092 steps/s (collection: 0.544s, learning 2.106s)
               Value function loss: 107019.1391
                    Surrogate loss: 0.0155
             Mean action noise std: 0.89
                       Mean reward: 5182.33
               Mean episode length: 278.63
                 Mean success rate: 46.00
                  Mean reward/step: 19.40
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 10526720
                    Iteration time: 2.65s
                        Total time: 3418.46s
                               ETA: 528640.9s

################################################################################
                    [1m Learning iteration 1285/200000 [0m

                       Computation: 3098 steps/s (collection: 0.538s, learning 2.106s)
               Value function loss: 95160.6676
                    Surrogate loss: 0.0123
             Mean action noise std: 0.89
                       Mean reward: 5603.39
               Mean episode length: 294.26
                 Mean success rate: 50.00
                  Mean reward/step: 18.64
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10534912
                    Iteration time: 2.64s
                        Total time: 3421.11s
                               ETA: 528635.8s

################################################################################
                    [1m Learning iteration 1286/200000 [0m

                       Computation: 3104 steps/s (collection: 0.528s, learning 2.110s)
               Value function loss: 66092.6256
                    Surrogate loss: 0.0160
             Mean action noise std: 0.89
                       Mean reward: 5830.60
               Mean episode length: 304.37
                 Mean success rate: 52.50
                  Mean reward/step: 18.75
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10543104
                    Iteration time: 2.64s
                        Total time: 3423.75s
                               ETA: 528629.8s

################################################################################
                    [1m Learning iteration 1287/200000 [0m

                       Computation: 2990 steps/s (collection: 0.625s, learning 2.114s)
               Value function loss: 85329.3018
                    Surrogate loss: 0.0145
             Mean action noise std: 0.89
                       Mean reward: 5994.65
               Mean episode length: 306.90
                 Mean success rate: 55.00
                  Mean reward/step: 19.23
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10551296
                    Iteration time: 2.74s
                        Total time: 3426.49s
                               ETA: 528639.3s

################################################################################
                    [1m Learning iteration 1288/200000 [0m

                       Computation: 2986 steps/s (collection: 0.640s, learning 2.103s)
               Value function loss: 79714.8349
                    Surrogate loss: 0.0121
             Mean action noise std: 0.89
                       Mean reward: 5993.29
               Mean episode length: 306.15
                 Mean success rate: 54.50
                  Mean reward/step: 20.11
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10559488
                    Iteration time: 2.74s
                        Total time: 3429.23s
                               ETA: 528649.4s

################################################################################
                    [1m Learning iteration 1289/200000 [0m

                       Computation: 3094 steps/s (collection: 0.583s, learning 2.065s)
               Value function loss: 98677.7342
                    Surrogate loss: 0.0131
             Mean action noise std: 0.89
                       Mean reward: 6048.37
               Mean episode length: 311.82
                 Mean success rate: 55.50
                  Mean reward/step: 20.45
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 10567680
                    Iteration time: 2.65s
                        Total time: 3431.88s
                               ETA: 528644.8s

################################################################################
                    [1m Learning iteration 1290/200000 [0m

                       Computation: 3117 steps/s (collection: 0.553s, learning 2.075s)
               Value function loss: 86760.3990
                    Surrogate loss: 0.0151
             Mean action noise std: 0.89
                       Mean reward: 6238.94
               Mean episode length: 319.74
                 Mean success rate: 56.50
                  Mean reward/step: 20.99
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10575872
                    Iteration time: 2.63s
                        Total time: 3434.50s
                               ETA: 528637.1s

################################################################################
                    [1m Learning iteration 1291/200000 [0m

                       Computation: 3033 steps/s (collection: 0.578s, learning 2.123s)
               Value function loss: 87488.7215
                    Surrogate loss: 0.0134
             Mean action noise std: 0.89
                       Mean reward: 6572.03
               Mean episode length: 327.43
                 Mean success rate: 59.00
                  Mean reward/step: 20.81
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10584064
                    Iteration time: 2.70s
                        Total time: 3437.21s
                               ETA: 528640.6s

################################################################################
                    [1m Learning iteration 1292/200000 [0m

                       Computation: 3137 steps/s (collection: 0.517s, learning 2.094s)
               Value function loss: 89468.1672
                    Surrogate loss: 0.0144
             Mean action noise std: 0.89
                       Mean reward: 6731.30
               Mean episode length: 329.69
                 Mean success rate: 60.00
                  Mean reward/step: 20.76
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10592256
                    Iteration time: 2.61s
                        Total time: 3439.82s
                               ETA: 528630.4s

################################################################################
                    [1m Learning iteration 1293/200000 [0m

                       Computation: 3187 steps/s (collection: 0.517s, learning 2.053s)
               Value function loss: 98962.8168
                    Surrogate loss: 0.0146
             Mean action noise std: 0.89
                       Mean reward: 6583.86
               Mean episode length: 326.58
                 Mean success rate: 59.50
                  Mean reward/step: 21.13
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10600448
                    Iteration time: 2.57s
                        Total time: 3442.39s
                               ETA: 528613.8s

################################################################################
                    [1m Learning iteration 1294/200000 [0m

                       Computation: 3126 steps/s (collection: 0.526s, learning 2.095s)
               Value function loss: 84926.4488
                    Surrogate loss: 0.0149
             Mean action noise std: 0.89
                       Mean reward: 6501.41
               Mean episode length: 321.69
                 Mean success rate: 58.00
                  Mean reward/step: 20.75
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10608640
                    Iteration time: 2.62s
                        Total time: 3445.01s
                               ETA: 528605.1s

################################################################################
                    [1m Learning iteration 1295/200000 [0m

                       Computation: 3138 steps/s (collection: 0.557s, learning 2.053s)
               Value function loss: 75905.7848
                    Surrogate loss: 0.0164
             Mean action noise std: 0.89
                       Mean reward: 6568.91
               Mean episode length: 323.10
                 Mean success rate: 58.00
                  Mean reward/step: 20.95
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 2.61s
                        Total time: 3447.62s
                               ETA: 528594.7s

################################################################################
                    [1m Learning iteration 1296/200000 [0m

                       Computation: 3079 steps/s (collection: 0.555s, learning 2.105s)
               Value function loss: 83747.7440
                    Surrogate loss: 0.0138
             Mean action noise std: 0.89
                       Mean reward: 6408.16
               Mean episode length: 318.62
                 Mean success rate: 58.00
                  Mean reward/step: 21.02
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 10625024
                    Iteration time: 2.66s
                        Total time: 3450.28s
                               ETA: 528592.1s

################################################################################
                    [1m Learning iteration 1297/200000 [0m

                       Computation: 3136 steps/s (collection: 0.534s, learning 2.078s)
               Value function loss: 86041.3561
                    Surrogate loss: 0.0135
             Mean action noise std: 0.89
                       Mean reward: 6596.47
               Mean episode length: 324.44
                 Mean success rate: 59.00
                  Mean reward/step: 20.70
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10633216
                    Iteration time: 2.61s
                        Total time: 3452.89s
                               ETA: 528582.0s

################################################################################
                    [1m Learning iteration 1298/200000 [0m

                       Computation: 3101 steps/s (collection: 0.527s, learning 2.114s)
               Value function loss: 117139.2530
                    Surrogate loss: 0.0140
             Mean action noise std: 0.89
                       Mean reward: 6480.13
               Mean episode length: 319.50
                 Mean success rate: 58.00
                  Mean reward/step: 20.05
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 10641408
                    Iteration time: 2.64s
                        Total time: 3455.53s
                               ETA: 528576.5s

################################################################################
                    [1m Learning iteration 1299/200000 [0m

                       Computation: 3008 steps/s (collection: 0.602s, learning 2.121s)
               Value function loss: 85499.8264
                    Surrogate loss: 0.0164
             Mean action noise std: 0.89
                       Mean reward: 6230.64
               Mean episode length: 318.12
                 Mean success rate: 57.00
                  Mean reward/step: 19.64
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 10649600
                    Iteration time: 2.72s
                        Total time: 3458.25s
                               ETA: 528583.4s

################################################################################
                    [1m Learning iteration 1300/200000 [0m

                       Computation: 3061 steps/s (collection: 0.594s, learning 2.082s)
               Value function loss: 106807.3859
                    Surrogate loss: 0.0174
             Mean action noise std: 0.89
                       Mean reward: 6162.41
               Mean episode length: 315.31
                 Mean success rate: 55.00
                  Mean reward/step: 19.55
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 10657792
                    Iteration time: 2.68s
                        Total time: 3460.93s
                               ETA: 528583.2s

################################################################################
                    [1m Learning iteration 1301/200000 [0m

                       Computation: 3154 steps/s (collection: 0.518s, learning 2.079s)
               Value function loss: 98097.7761
                    Surrogate loss: 0.0157
             Mean action noise std: 0.89
                       Mean reward: 6413.73
               Mean episode length: 326.81
                 Mean success rate: 58.50
                  Mean reward/step: 18.98
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10665984
                    Iteration time: 2.60s
                        Total time: 3463.53s
                               ETA: 528570.9s

################################################################################
                    [1m Learning iteration 1302/200000 [0m

                       Computation: 3052 steps/s (collection: 0.528s, learning 2.155s)
               Value function loss: 87178.5449
                    Surrogate loss: 0.0166
             Mean action noise std: 0.89
                       Mean reward: 6375.95
               Mean episode length: 319.99
                 Mean success rate: 57.50
                  Mean reward/step: 19.22
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10674176
                    Iteration time: 2.68s
                        Total time: 3466.21s
                               ETA: 528571.8s

################################################################################
                    [1m Learning iteration 1303/200000 [0m

                       Computation: 2989 steps/s (collection: 0.642s, learning 2.098s)
               Value function loss: 72626.8969
                    Surrogate loss: 0.0130
             Mean action noise std: 0.89
                       Mean reward: 6312.25
               Mean episode length: 316.68
                 Mean success rate: 57.00
                  Mean reward/step: 19.28
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10682368
                    Iteration time: 2.74s
                        Total time: 3468.95s
                               ETA: 528581.4s

################################################################################
                    [1m Learning iteration 1304/200000 [0m

                       Computation: 3126 steps/s (collection: 0.560s, learning 2.060s)
               Value function loss: 81859.4662
                    Surrogate loss: 0.0166
             Mean action noise std: 0.89
                       Mean reward: 6328.29
               Mean episode length: 319.65
                 Mean success rate: 56.50
                  Mean reward/step: 19.46
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 10690560
                    Iteration time: 2.62s
                        Total time: 3471.57s
                               ETA: 528572.7s

################################################################################
                    [1m Learning iteration 1305/200000 [0m

                       Computation: 3133 steps/s (collection: 0.540s, learning 2.075s)
               Value function loss: 82051.7605
                    Surrogate loss: 0.0167
             Mean action noise std: 0.89
                       Mean reward: 6368.50
               Mean episode length: 317.95
                 Mean success rate: 56.50
                  Mean reward/step: 19.24
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10698752
                    Iteration time: 2.61s
                        Total time: 3474.19s
                               ETA: 528563.1s

################################################################################
                    [1m Learning iteration 1306/200000 [0m

                       Computation: 3151 steps/s (collection: 0.545s, learning 2.054s)
               Value function loss: 72408.8019
                    Surrogate loss: 0.0179
             Mean action noise std: 0.89
                       Mean reward: 6567.92
               Mean episode length: 325.74
                 Mean success rate: 58.50
                  Mean reward/step: 19.24
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10706944
                    Iteration time: 2.60s
                        Total time: 3476.78s
                               ETA: 528551.1s

################################################################################
                    [1m Learning iteration 1307/200000 [0m

                       Computation: 3119 steps/s (collection: 0.549s, learning 2.077s)
               Value function loss: 62330.4127
                    Surrogate loss: 0.0182
             Mean action noise std: 0.89
                       Mean reward: 6517.72
               Mean episode length: 322.64
                 Mean success rate: 58.50
                  Mean reward/step: 18.65
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 2.63s
                        Total time: 3479.41s
                               ETA: 528543.3s

################################################################################
                    [1m Learning iteration 1308/200000 [0m

                       Computation: 3160 steps/s (collection: 0.535s, learning 2.057s)
               Value function loss: 60897.4964
                    Surrogate loss: 0.0176
             Mean action noise std: 0.89
                       Mean reward: 6309.92
               Mean episode length: 314.56
                 Mean success rate: 56.00
                  Mean reward/step: 19.09
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10723328
                    Iteration time: 2.59s
                        Total time: 3482.00s
                               ETA: 528530.3s

################################################################################
                    [1m Learning iteration 1309/200000 [0m

                       Computation: 3108 steps/s (collection: 0.518s, learning 2.118s)
               Value function loss: 91646.5185
                    Surrogate loss: 0.0133
             Mean action noise std: 0.89
                       Mean reward: 6227.62
               Mean episode length: 307.44
                 Mean success rate: 54.00
                  Mean reward/step: 19.20
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10731520
                    Iteration time: 2.64s
                        Total time: 3484.64s
                               ETA: 528523.9s

################################################################################
                    [1m Learning iteration 1310/200000 [0m

                       Computation: 2959 steps/s (collection: 0.610s, learning 2.159s)
               Value function loss: 67180.9815
                    Surrogate loss: 0.0124
             Mean action noise std: 0.89
                       Mean reward: 6565.68
               Mean episode length: 321.35
                 Mean success rate: 56.50
                  Mean reward/step: 19.73
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10739712
                    Iteration time: 2.77s
                        Total time: 3487.41s
                               ETA: 528537.6s

################################################################################
                    [1m Learning iteration 1311/200000 [0m

                       Computation: 3067 steps/s (collection: 0.597s, learning 2.073s)
               Value function loss: 82862.7608
                    Surrogate loss: 0.0118
             Mean action noise std: 0.89
                       Mean reward: 6013.09
               Mean episode length: 305.20
                 Mean success rate: 54.00
                  Mean reward/step: 19.46
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 10747904
                    Iteration time: 2.67s
                        Total time: 3490.08s
                               ETA: 528536.5s

################################################################################
                    [1m Learning iteration 1312/200000 [0m

                       Computation: 3099 steps/s (collection: 0.519s, learning 2.124s)
               Value function loss: 64273.5677
                    Surrogate loss: 0.0149
             Mean action noise std: 0.89
                       Mean reward: 5966.23
               Mean episode length: 305.57
                 Mean success rate: 53.00
                  Mean reward/step: 18.91
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 10756096
                    Iteration time: 2.64s
                        Total time: 3492.72s
                               ETA: 528531.3s

################################################################################
                    [1m Learning iteration 1313/200000 [0m

                       Computation: 3134 steps/s (collection: 0.530s, learning 2.084s)
               Value function loss: 43103.9297
                    Surrogate loss: 0.0144
             Mean action noise std: 0.89
                       Mean reward: 5689.39
               Mean episode length: 295.82
                 Mean success rate: 51.00
                  Mean reward/step: 19.11
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10764288
                    Iteration time: 2.61s
                        Total time: 3495.33s
                               ETA: 528521.6s

################################################################################
                    [1m Learning iteration 1314/200000 [0m

                       Computation: 3009 steps/s (collection: 0.624s, learning 2.098s)
               Value function loss: 107379.5932
                    Surrogate loss: 0.0137
             Mean action noise std: 0.89
                       Mean reward: 5405.65
               Mean episode length: 286.78
                 Mean success rate: 48.00
                  Mean reward/step: 18.19
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 10772480
                    Iteration time: 2.72s
                        Total time: 3498.06s
                               ETA: 528528.3s

################################################################################
                    [1m Learning iteration 1315/200000 [0m

                       Computation: 3105 steps/s (collection: 0.556s, learning 2.082s)
               Value function loss: 97974.3633
                    Surrogate loss: 0.0122
             Mean action noise std: 0.89
                       Mean reward: 6026.83
               Mean episode length: 307.17
                 Mean success rate: 53.50
                  Mean reward/step: 18.07
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 10780672
                    Iteration time: 2.64s
                        Total time: 3500.69s
                               ETA: 528522.3s

################################################################################
                    [1m Learning iteration 1316/200000 [0m

                       Computation: 3094 steps/s (collection: 0.565s, learning 2.082s)
               Value function loss: 95819.9764
                    Surrogate loss: 0.0152
             Mean action noise std: 0.89
                       Mean reward: 5608.70
               Mean episode length: 296.37
                 Mean success rate: 52.00
                  Mean reward/step: 17.65
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 10788864
                    Iteration time: 2.65s
                        Total time: 3503.34s
                               ETA: 528517.7s

################################################################################
                    [1m Learning iteration 1317/200000 [0m

                       Computation: 3097 steps/s (collection: 0.554s, learning 2.090s)
               Value function loss: 83583.8542
                    Surrogate loss: 0.0159
             Mean action noise std: 0.89
                       Mean reward: 5695.43
               Mean episode length: 300.21
                 Mean success rate: 53.00
                  Mean reward/step: 17.24
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10797056
                    Iteration time: 2.64s
                        Total time: 3505.99s
                               ETA: 528512.7s

################################################################################
                    [1m Learning iteration 1318/200000 [0m

                       Computation: 3036 steps/s (collection: 0.583s, learning 2.115s)
               Value function loss: 103120.9387
                    Surrogate loss: 0.0164
             Mean action noise std: 0.89
                       Mean reward: 5931.53
               Mean episode length: 306.03
                 Mean success rate: 54.00
                  Mean reward/step: 16.87
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 10805248
                    Iteration time: 2.70s
                        Total time: 3508.68s
                               ETA: 528515.7s

################################################################################
                    [1m Learning iteration 1319/200000 [0m

                       Computation: 3110 steps/s (collection: 0.568s, learning 2.065s)
               Value function loss: 45994.3561
                    Surrogate loss: 0.0157
             Mean action noise std: 0.89
                       Mean reward: 5887.70
               Mean episode length: 302.79
                 Mean success rate: 54.00
                  Mean reward/step: 17.27
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 2.63s
                        Total time: 3511.32s
                               ETA: 528509.1s

################################################################################
                    [1m Learning iteration 1320/200000 [0m

                       Computation: 3089 steps/s (collection: 0.559s, learning 2.092s)
               Value function loss: 82441.8990
                    Surrogate loss: 0.0175
             Mean action noise std: 0.89
                       Mean reward: 5415.67
               Mean episode length: 289.11
                 Mean success rate: 50.00
                  Mean reward/step: 17.97
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 10821632
                    Iteration time: 2.65s
                        Total time: 3513.97s
                               ETA: 528505.1s

################################################################################
                    [1m Learning iteration 1321/200000 [0m

                       Computation: 3032 steps/s (collection: 0.571s, learning 2.130s)
               Value function loss: 82999.8494
                    Surrogate loss: 0.0182
             Mean action noise std: 0.89
                       Mean reward: 5614.91
               Mean episode length: 295.94
                 Mean success rate: 52.00
                  Mean reward/step: 18.42
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10829824
                    Iteration time: 2.70s
                        Total time: 3516.67s
                               ETA: 528508.6s

################################################################################
                    [1m Learning iteration 1322/200000 [0m

                       Computation: 2948 steps/s (collection: 0.670s, learning 2.109s)
               Value function loss: 57935.7054
                    Surrogate loss: 0.0194
             Mean action noise std: 0.89
                       Mean reward: 5462.08
               Mean episode length: 288.61
                 Mean success rate: 50.00
                  Mean reward/step: 19.04
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10838016
                    Iteration time: 2.78s
                        Total time: 3519.45s
                               ETA: 528523.7s

################################################################################
                    [1m Learning iteration 1323/200000 [0m

                       Computation: 3103 steps/s (collection: 0.546s, learning 2.093s)
               Value function loss: 49598.1119
                    Surrogate loss: 0.0129
             Mean action noise std: 0.89
                       Mean reward: 5480.41
               Mean episode length: 287.63
                 Mean success rate: 49.50
                  Mean reward/step: 20.15
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10846208
                    Iteration time: 2.64s
                        Total time: 3522.09s
                               ETA: 528517.9s

################################################################################
                    [1m Learning iteration 1324/200000 [0m

                       Computation: 3118 steps/s (collection: 0.541s, learning 2.086s)
               Value function loss: 69598.0343
                    Surrogate loss: 0.0119
             Mean action noise std: 0.89
                       Mean reward: 5261.54
               Mean episode length: 281.95
                 Mean success rate: 48.50
                  Mean reward/step: 20.88
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10854400
                    Iteration time: 2.63s
                        Total time: 3524.71s
                               ETA: 528510.2s

################################################################################
                    [1m Learning iteration 1325/200000 [0m

                       Computation: 2931 steps/s (collection: 0.626s, learning 2.169s)
               Value function loss: 79771.1402
                    Surrogate loss: 0.0116
             Mean action noise std: 0.89
                       Mean reward: 5190.67
               Mean episode length: 287.58
                 Mean success rate: 48.50
                  Mean reward/step: 20.51
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10862592
                    Iteration time: 2.79s
                        Total time: 3527.51s
                               ETA: 528527.7s

################################################################################
                    [1m Learning iteration 1326/200000 [0m

                       Computation: 2978 steps/s (collection: 0.630s, learning 2.120s)
               Value function loss: 83018.5516
                    Surrogate loss: 0.0124
             Mean action noise std: 0.89
                       Mean reward: 5299.13
               Mean episode length: 290.25
                 Mean success rate: 49.50
                  Mean reward/step: 20.24
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 10870784
                    Iteration time: 2.75s
                        Total time: 3530.26s
                               ETA: 528538.5s

################################################################################
                    [1m Learning iteration 1327/200000 [0m

                       Computation: 3087 steps/s (collection: 0.573s, learning 2.081s)
               Value function loss: 85897.4730
                    Surrogate loss: 0.0118
             Mean action noise std: 0.89
                       Mean reward: 5640.12
               Mean episode length: 302.83
                 Mean success rate: 52.50
                  Mean reward/step: 19.80
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10878976
                    Iteration time: 2.65s
                        Total time: 3532.91s
                               ETA: 528534.8s

################################################################################
                    [1m Learning iteration 1328/200000 [0m

                       Computation: 3066 steps/s (collection: 0.582s, learning 2.089s)
               Value function loss: 58243.1478
                    Surrogate loss: 0.0136
             Mean action noise std: 0.89
                       Mean reward: 5772.59
               Mean episode length: 310.85
                 Mean success rate: 53.50
                  Mean reward/step: 20.07
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 10887168
                    Iteration time: 2.67s
                        Total time: 3535.58s
                               ETA: 528533.8s

################################################################################
                    [1m Learning iteration 1329/200000 [0m

                       Computation: 3091 steps/s (collection: 0.559s, learning 2.091s)
               Value function loss: 121770.8168
                    Surrogate loss: 0.0129
             Mean action noise std: 0.89
                       Mean reward: 6468.52
               Mean episode length: 336.92
                 Mean success rate: 58.50
                  Mean reward/step: 20.17
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10895360
                    Iteration time: 2.65s
                        Total time: 3538.23s
                               ETA: 528529.6s

################################################################################
                    [1m Learning iteration 1330/200000 [0m

                       Computation: 3109 steps/s (collection: 0.551s, learning 2.084s)
               Value function loss: 77741.4935
                    Surrogate loss: 0.0118
             Mean action noise std: 0.89
                       Mean reward: 6152.08
               Mean episode length: 328.64
                 Mean success rate: 56.00
                  Mean reward/step: 19.14
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 10903552
                    Iteration time: 2.63s
                        Total time: 3540.87s
                               ETA: 528523.0s

################################################################################
                    [1m Learning iteration 1331/200000 [0m

                       Computation: 3109 steps/s (collection: 0.559s, learning 2.076s)
               Value function loss: 75596.8251
                    Surrogate loss: 0.0122
             Mean action noise std: 0.89
                       Mean reward: 6310.62
               Mean episode length: 338.51
                 Mean success rate: 58.00
                  Mean reward/step: 19.32
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 2.63s
                        Total time: 3543.50s
                               ETA: 528516.6s

################################################################################
                    [1m Learning iteration 1332/200000 [0m

                       Computation: 3064 steps/s (collection: 0.559s, learning 2.114s)
               Value function loss: 104767.3389
                    Surrogate loss: 0.0111
             Mean action noise std: 0.89
                       Mean reward: 6792.85
               Mean episode length: 353.48
                 Mean success rate: 61.00
                  Mean reward/step: 18.90
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 10919936
                    Iteration time: 2.67s
                        Total time: 3546.18s
                               ETA: 528515.8s

################################################################################
                    [1m Learning iteration 1333/200000 [0m

                       Computation: 3010 steps/s (collection: 0.608s, learning 2.113s)
               Value function loss: 87293.9383
                    Surrogate loss: 0.0180
             Mean action noise std: 0.89
                       Mean reward: 6773.40
               Mean episode length: 350.88
                 Mean success rate: 60.00
                  Mean reward/step: 19.37
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 10928128
                    Iteration time: 2.72s
                        Total time: 3548.90s
                               ETA: 528522.2s

################################################################################
                    [1m Learning iteration 1334/200000 [0m

                       Computation: 3016 steps/s (collection: 0.593s, learning 2.122s)
               Value function loss: 86609.0803
                    Surrogate loss: 0.0137
             Mean action noise std: 0.89
                       Mean reward: 6814.99
               Mean episode length: 350.24
                 Mean success rate: 60.50
                  Mean reward/step: 19.10
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10936320
                    Iteration time: 2.72s
                        Total time: 3551.61s
                               ETA: 528527.7s

################################################################################
                    [1m Learning iteration 1335/200000 [0m

                       Computation: 3051 steps/s (collection: 0.577s, learning 2.107s)
               Value function loss: 88753.2849
                    Surrogate loss: 0.0143
             Mean action noise std: 0.89
                       Mean reward: 7026.68
               Mean episode length: 359.73
                 Mean success rate: 62.50
                  Mean reward/step: 19.18
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 10944512
                    Iteration time: 2.68s
                        Total time: 3554.30s
                               ETA: 528528.6s

################################################################################
                    [1m Learning iteration 1336/200000 [0m

                       Computation: 3016 steps/s (collection: 0.581s, learning 2.135s)
               Value function loss: 45668.8903
                    Surrogate loss: 0.0158
             Mean action noise std: 0.89
                       Mean reward: 6592.46
               Mean episode length: 342.86
                 Mean success rate: 60.00
                  Mean reward/step: 19.35
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 10952704
                    Iteration time: 2.72s
                        Total time: 3557.01s
                               ETA: 528534.2s

################################################################################
                    [1m Learning iteration 1337/200000 [0m

                       Computation: 3074 steps/s (collection: 0.593s, learning 2.072s)
               Value function loss: 67728.9212
                    Surrogate loss: 0.0177
             Mean action noise std: 0.89
                       Mean reward: 6599.95
               Mean episode length: 343.07
                 Mean success rate: 60.50
                  Mean reward/step: 20.20
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10960896
                    Iteration time: 2.66s
                        Total time: 3559.68s
                               ETA: 528532.1s

################################################################################
                    [1m Learning iteration 1338/200000 [0m

                       Computation: 3080 steps/s (collection: 0.564s, learning 2.095s)
               Value function loss: 78612.1136
                    Surrogate loss: 0.0163
             Mean action noise std: 0.89
                       Mean reward: 6668.09
               Mean episode length: 342.96
                 Mean success rate: 61.00
                  Mean reward/step: 20.64
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 10969088
                    Iteration time: 2.66s
                        Total time: 3562.34s
                               ETA: 528529.2s

################################################################################
                    [1m Learning iteration 1339/200000 [0m

                       Computation: 3031 steps/s (collection: 0.588s, learning 2.114s)
               Value function loss: 60064.5626
                    Surrogate loss: 0.0142
             Mean action noise std: 0.89
                       Mean reward: 6577.00
               Mean episode length: 337.79
                 Mean success rate: 60.00
                  Mean reward/step: 21.25
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 10977280
                    Iteration time: 2.70s
                        Total time: 3565.04s
                               ETA: 528532.8s

################################################################################
                    [1m Learning iteration 1340/200000 [0m

                       Computation: 3075 steps/s (collection: 0.564s, learning 2.099s)
               Value function loss: 86335.2772
                    Surrogate loss: 0.0144
             Mean action noise std: 0.89
                       Mean reward: 6550.33
               Mean episode length: 334.01
                 Mean success rate: 60.00
                  Mean reward/step: 21.03
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 10985472
                    Iteration time: 2.66s
                        Total time: 3567.70s
                               ETA: 528530.6s

################################################################################
                    [1m Learning iteration 1341/200000 [0m

                       Computation: 3078 steps/s (collection: 0.571s, learning 2.090s)
               Value function loss: 37187.5281
                    Surrogate loss: 0.0148
             Mean action noise std: 0.89
                       Mean reward: 6315.90
               Mean episode length: 325.52
                 Mean success rate: 58.00
                  Mean reward/step: 20.94
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 10993664
                    Iteration time: 2.66s
                        Total time: 3570.36s
                               ETA: 528528.0s

################################################################################
                    [1m Learning iteration 1342/200000 [0m

                       Computation: 3022 steps/s (collection: 0.604s, learning 2.106s)
               Value function loss: 73947.7801
                    Surrogate loss: 0.0154
             Mean action noise std: 0.89
                       Mean reward: 6224.61
               Mean episode length: 323.90
                 Mean success rate: 58.00
                  Mean reward/step: 21.12
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 11001856
                    Iteration time: 2.71s
                        Total time: 3573.07s
                               ETA: 528532.7s

################################################################################
                    [1m Learning iteration 1343/200000 [0m

                       Computation: 3080 steps/s (collection: 0.555s, learning 2.103s)
               Value function loss: 70497.7560
                    Surrogate loss: 0.0129
             Mean action noise std: 0.89
                       Mean reward: 6094.27
               Mean episode length: 316.94
                 Mean success rate: 56.50
                  Mean reward/step: 21.11
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 2.66s
                        Total time: 3575.73s
                               ETA: 528529.8s

################################################################################
                    [1m Learning iteration 1344/200000 [0m

                       Computation: 3048 steps/s (collection: 0.565s, learning 2.122s)
               Value function loss: 58630.9871
                    Surrogate loss: 0.0143
             Mean action noise std: 0.89
                       Mean reward: 6040.59
               Mean episode length: 313.41
                 Mean success rate: 55.50
                  Mean reward/step: 20.66
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11018240
                    Iteration time: 2.69s
                        Total time: 3578.42s
                               ETA: 528531.1s

################################################################################
                    [1m Learning iteration 1345/200000 [0m

                       Computation: 3083 steps/s (collection: 0.559s, learning 2.097s)
               Value function loss: 135694.0047
                    Surrogate loss: 0.0130
             Mean action noise std: 0.89
                       Mean reward: 6622.96
               Mean episode length: 334.13
                 Mean success rate: 60.50
                  Mean reward/step: 20.52
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 11026432
                    Iteration time: 2.66s
                        Total time: 3581.08s
                               ETA: 528527.9s

################################################################################
                    [1m Learning iteration 1346/200000 [0m

                       Computation: 3057 steps/s (collection: 0.576s, learning 2.104s)
               Value function loss: 101661.7547
                    Surrogate loss: 0.0109
             Mean action noise std: 0.89
                       Mean reward: 6746.63
               Mean episode length: 334.21
                 Mean success rate: 61.00
                  Mean reward/step: 19.99
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11034624
                    Iteration time: 2.68s
                        Total time: 3583.75s
                               ETA: 528528.0s

################################################################################
                    [1m Learning iteration 1347/200000 [0m

                       Computation: 3071 steps/s (collection: 0.550s, learning 2.116s)
               Value function loss: 85692.4394
                    Surrogate loss: 0.0142
             Mean action noise std: 0.89
                       Mean reward: 6852.12
               Mean episode length: 340.56
                 Mean success rate: 61.50
                  Mean reward/step: 19.89
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11042816
                    Iteration time: 2.67s
                        Total time: 3586.42s
                               ETA: 528526.3s

################################################################################
                    [1m Learning iteration 1348/200000 [0m

                       Computation: 2987 steps/s (collection: 0.620s, learning 2.122s)
               Value function loss: 98220.2896
                    Surrogate loss: 0.0102
             Mean action noise std: 0.89
                       Mean reward: 6944.26
               Mean episode length: 346.11
                 Mean success rate: 62.50
                  Mean reward/step: 19.14
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11051008
                    Iteration time: 2.74s
                        Total time: 3589.16s
                               ETA: 528535.6s

################################################################################
                    [1m Learning iteration 1349/200000 [0m

                       Computation: 3105 steps/s (collection: 0.567s, learning 2.071s)
               Value function loss: 106228.7096
                    Surrogate loss: 0.0138
             Mean action noise std: 0.89
                       Mean reward: 7299.35
               Mean episode length: 351.88
                 Mean success rate: 65.00
                  Mean reward/step: 18.78
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 11059200
                    Iteration time: 2.64s
                        Total time: 3591.80s
                               ETA: 528529.6s

################################################################################
                    [1m Learning iteration 1350/200000 [0m

                       Computation: 3096 steps/s (collection: 0.576s, learning 2.070s)
               Value function loss: 65561.8182
                    Surrogate loss: 0.0136
             Mean action noise std: 0.89
                       Mean reward: 7563.42
               Mean episode length: 356.56
                 Mean success rate: 66.50
                  Mean reward/step: 19.30
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11067392
                    Iteration time: 2.65s
                        Total time: 3594.45s
                               ETA: 528524.8s

################################################################################
                    [1m Learning iteration 1351/200000 [0m

                       Computation: 3058 steps/s (collection: 0.563s, learning 2.116s)
               Value function loss: 90248.1418
                    Surrogate loss: 0.0128
             Mean action noise std: 0.89
                       Mean reward: 7087.95
               Mean episode length: 342.00
                 Mean success rate: 63.00
                  Mean reward/step: 19.87
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 11075584
                    Iteration time: 2.68s
                        Total time: 3597.13s
                               ETA: 528524.8s

################################################################################
                    [1m Learning iteration 1352/200000 [0m

                       Computation: 3120 steps/s (collection: 0.556s, learning 2.069s)
               Value function loss: 80351.0435
                    Surrogate loss: 0.0130
             Mean action noise std: 0.89
                       Mean reward: 7093.78
               Mean episode length: 338.30
                 Mean success rate: 63.00
                  Mean reward/step: 19.96
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11083776
                    Iteration time: 2.63s
                        Total time: 3599.75s
                               ETA: 528516.9s

################################################################################
                    [1m Learning iteration 1353/200000 [0m

                       Computation: 3054 steps/s (collection: 0.593s, learning 2.089s)
               Value function loss: 83839.8919
                    Surrogate loss: 0.0173
             Mean action noise std: 0.89
                       Mean reward: 6213.05
               Mean episode length: 309.96
                 Mean success rate: 55.00
                  Mean reward/step: 19.38
       Mean episode length/episode: 27.86
--------------------------------------------------------------------------------
                   Total timesteps: 11091968
                    Iteration time: 2.68s
                        Total time: 3602.43s
                               ETA: 528517.4s

################################################################################
                    [1m Learning iteration 1354/200000 [0m

                       Computation: 3023 steps/s (collection: 0.598s, learning 2.111s)
               Value function loss: 73904.7678
                    Surrogate loss: 0.0171
             Mean action noise std: 0.89
                       Mean reward: 6252.50
               Mean episode length: 312.00
                 Mean success rate: 56.50
                  Mean reward/step: 19.00
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11100160
                    Iteration time: 2.71s
                        Total time: 3605.14s
                               ETA: 528521.9s

################################################################################
                    [1m Learning iteration 1355/200000 [0m

                       Computation: 2990 steps/s (collection: 0.573s, learning 2.166s)
               Value function loss: 72582.4321
                    Surrogate loss: 0.0136
             Mean action noise std: 0.89
                       Mean reward: 5823.75
               Mean episode length: 298.77
                 Mean success rate: 53.00
                  Mean reward/step: 19.19
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 2.74s
                        Total time: 3607.88s
                               ETA: 528530.8s

################################################################################
                    [1m Learning iteration 1356/200000 [0m

                       Computation: 3109 steps/s (collection: 0.536s, learning 2.098s)
               Value function loss: 77073.1285
                    Surrogate loss: 0.0130
             Mean action noise std: 0.89
                       Mean reward: 5577.72
               Mean episode length: 290.50
                 Mean success rate: 51.00
                  Mean reward/step: 20.25
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 11116544
                    Iteration time: 2.63s
                        Total time: 3610.52s
                               ETA: 528524.3s

################################################################################
                    [1m Learning iteration 1357/200000 [0m

                       Computation: 3089 steps/s (collection: 0.560s, learning 2.092s)
               Value function loss: 65552.3810
                    Surrogate loss: 0.0156
             Mean action noise std: 0.89
                       Mean reward: 5433.57
               Mean episode length: 288.73
                 Mean success rate: 50.00
                  Mean reward/step: 20.40
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11124736
                    Iteration time: 2.65s
                        Total time: 3613.17s
                               ETA: 528520.4s

################################################################################
                    [1m Learning iteration 1358/200000 [0m

                       Computation: 3039 steps/s (collection: 0.586s, learning 2.110s)
               Value function loss: 93282.9385
                    Surrogate loss: 0.0142
             Mean action noise std: 0.89
                       Mean reward: 5738.53
               Mean episode length: 299.87
                 Mean success rate: 53.00
                  Mean reward/step: 20.27
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11132928
                    Iteration time: 2.70s
                        Total time: 3615.86s
                               ETA: 528522.8s

################################################################################
                    [1m Learning iteration 1359/200000 [0m

                       Computation: 2954 steps/s (collection: 0.633s, learning 2.140s)
               Value function loss: 56704.2585
                    Surrogate loss: 0.0152
             Mean action noise std: 0.89
                       Mean reward: 5475.13
               Mean episode length: 293.24
                 Mean success rate: 50.50
                  Mean reward/step: 20.33
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11141120
                    Iteration time: 2.77s
                        Total time: 3618.64s
                               ETA: 528536.4s

################################################################################
                    [1m Learning iteration 1360/200000 [0m

                       Computation: 3073 steps/s (collection: 0.568s, learning 2.098s)
               Value function loss: 63665.2429
                    Surrogate loss: 0.0157
             Mean action noise std: 0.89
                       Mean reward: 5754.94
               Mean episode length: 301.89
                 Mean success rate: 52.00
                  Mean reward/step: 21.46
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11149312
                    Iteration time: 2.67s
                        Total time: 3621.30s
                               ETA: 528534.5s

################################################################################
                    [1m Learning iteration 1361/200000 [0m

                       Computation: 3092 steps/s (collection: 0.555s, learning 2.094s)
               Value function loss: 98263.6870
                    Surrogate loss: 0.0138
             Mean action noise std: 0.89
                       Mean reward: 5982.77
               Mean episode length: 308.74
                 Mean success rate: 54.00
                  Mean reward/step: 20.70
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 11157504
                    Iteration time: 2.65s
                        Total time: 3623.95s
                               ETA: 528530.1s

################################################################################
                    [1m Learning iteration 1362/200000 [0m

                       Computation: 3077 steps/s (collection: 0.541s, learning 2.122s)
               Value function loss: 51686.7114
                    Surrogate loss: 0.0246
             Mean action noise std: 0.89
                       Mean reward: 5916.04
               Mean episode length: 306.12
                 Mean success rate: 53.50
                  Mean reward/step: 20.94
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11165696
                    Iteration time: 2.66s
                        Total time: 3626.61s
                               ETA: 528527.7s

################################################################################
                    [1m Learning iteration 1363/200000 [0m

                       Computation: 3062 steps/s (collection: 0.594s, learning 2.081s)
               Value function loss: 107045.8160
                    Surrogate loss: 0.0139
             Mean action noise std: 0.89
                       Mean reward: 6169.11
               Mean episode length: 310.98
                 Mean success rate: 55.00
                  Mean reward/step: 20.51
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 11173888
                    Iteration time: 2.68s
                        Total time: 3629.29s
                               ETA: 528527.1s

################################################################################
                    [1m Learning iteration 1364/200000 [0m

                       Computation: 3045 steps/s (collection: 0.582s, learning 2.108s)
               Value function loss: 94644.8709
                    Surrogate loss: 0.0140
             Mean action noise std: 0.89
                       Mean reward: 6305.65
               Mean episode length: 314.06
                 Mean success rate: 56.50
                  Mean reward/step: 20.01
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11182080
                    Iteration time: 2.69s
                        Total time: 3631.98s
                               ETA: 528528.6s

################################################################################
                    [1m Learning iteration 1365/200000 [0m

                       Computation: 3100 steps/s (collection: 0.575s, learning 2.067s)
               Value function loss: 86757.6997
                    Surrogate loss: 0.0110
             Mean action noise std: 0.89
                       Mean reward: 6305.40
               Mean episode length: 310.33
                 Mean success rate: 55.50
                  Mean reward/step: 20.06
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11190272
                    Iteration time: 2.64s
                        Total time: 3634.62s
                               ETA: 528523.3s

################################################################################
                    [1m Learning iteration 1366/200000 [0m

                       Computation: 3031 steps/s (collection: 0.598s, learning 2.105s)
               Value function loss: 47964.3864
                    Surrogate loss: 0.0153
             Mean action noise std: 0.89
                       Mean reward: 5996.67
               Mean episode length: 298.26
                 Mean success rate: 53.00
                  Mean reward/step: 20.32
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 11198464
                    Iteration time: 2.70s
                        Total time: 3637.32s
                               ETA: 528526.7s

################################################################################
                    [1m Learning iteration 1367/200000 [0m

                       Computation: 2944 steps/s (collection: 0.661s, learning 2.121s)
               Value function loss: 77591.7161
                    Surrogate loss: 0.0111
             Mean action noise std: 0.89
                       Mean reward: 6164.67
               Mean episode length: 303.19
                 Mean success rate: 54.00
                  Mean reward/step: 20.40
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 2.78s
                        Total time: 3640.10s
                               ETA: 528541.6s

################################################################################
                    [1m Learning iteration 1368/200000 [0m

                       Computation: 3034 steps/s (collection: 0.611s, learning 2.088s)
               Value function loss: 107633.6676
                    Surrogate loss: 0.0123
             Mean action noise std: 0.89
                       Mean reward: 6380.10
               Mean episode length: 308.85
                 Mean success rate: 56.00
                  Mean reward/step: 20.28
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 11214848
                    Iteration time: 2.70s
                        Total time: 3642.80s
                               ETA: 528544.6s

################################################################################
                    [1m Learning iteration 1369/200000 [0m

                       Computation: 3012 steps/s (collection: 0.585s, learning 2.134s)
               Value function loss: 70763.5818
                    Surrogate loss: 0.0117
             Mean action noise std: 0.89
                       Mean reward: 6464.06
               Mean episode length: 314.56
                 Mean success rate: 57.00
                  Mean reward/step: 19.35
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11223040
                    Iteration time: 2.72s
                        Total time: 3645.52s
                               ETA: 528550.4s

################################################################################
                    [1m Learning iteration 1370/200000 [0m

                       Computation: 3052 steps/s (collection: 0.587s, learning 2.097s)
               Value function loss: 87587.0151
                    Surrogate loss: 0.0134
             Mean action noise std: 0.89
                       Mean reward: 6428.25
               Mean episode length: 313.51
                 Mean success rate: 57.00
                  Mean reward/step: 19.19
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11231232
                    Iteration time: 2.68s
                        Total time: 3648.21s
                               ETA: 528551.0s

################################################################################
                    [1m Learning iteration 1371/200000 [0m

                       Computation: 3045 steps/s (collection: 0.607s, learning 2.082s)
               Value function loss: 94931.2727
                    Surrogate loss: 0.0157
             Mean action noise std: 0.89
                       Mean reward: 6646.67
               Mean episode length: 319.90
                 Mean success rate: 58.50
                  Mean reward/step: 18.90
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 11239424
                    Iteration time: 2.69s
                        Total time: 3650.90s
                               ETA: 528552.6s

################################################################################
                    [1m Learning iteration 1372/200000 [0m

                       Computation: 3083 steps/s (collection: 0.565s, learning 2.091s)
               Value function loss: 49683.9309
                    Surrogate loss: 0.0150
             Mean action noise std: 0.89
                       Mean reward: 6537.04
               Mean episode length: 320.93
                 Mean success rate: 58.50
                  Mean reward/step: 18.95
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11247616
                    Iteration time: 2.66s
                        Total time: 3653.55s
                               ETA: 528549.3s

################################################################################
                    [1m Learning iteration 1373/200000 [0m

                       Computation: 3070 steps/s (collection: 0.572s, learning 2.096s)
               Value function loss: 83306.5689
                    Surrogate loss: 0.0134
             Mean action noise std: 0.89
                       Mean reward: 6907.75
               Mean episode length: 333.48
                 Mean success rate: 62.00
                  Mean reward/step: 20.13
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11255808
                    Iteration time: 2.67s
                        Total time: 3656.22s
                               ETA: 528547.7s

################################################################################
                    [1m Learning iteration 1374/200000 [0m

                       Computation: 3033 steps/s (collection: 0.613s, learning 2.088s)
               Value function loss: 89048.7105
                    Surrogate loss: 0.0139
             Mean action noise std: 0.89
                       Mean reward: 7120.66
               Mean episode length: 342.56
                 Mean success rate: 64.00
                  Mean reward/step: 20.49
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 11264000
                    Iteration time: 2.70s
                        Total time: 3658.92s
                               ETA: 528550.7s

################################################################################
                    [1m Learning iteration 1375/200000 [0m

                       Computation: 3023 steps/s (collection: 0.594s, learning 2.115s)
               Value function loss: 55034.3504
                    Surrogate loss: 0.0117
             Mean action noise std: 0.89
                       Mean reward: 6643.78
               Mean episode length: 324.08
                 Mean success rate: 61.00
                  Mean reward/step: 20.38
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 11272192
                    Iteration time: 2.71s
                        Total time: 3661.63s
                               ETA: 528555.0s

################################################################################
                    [1m Learning iteration 1376/200000 [0m

                       Computation: 3041 steps/s (collection: 0.600s, learning 2.093s)
               Value function loss: 86281.5513
                    Surrogate loss: 0.0123
             Mean action noise std: 0.89
                       Mean reward: 6593.01
               Mean episode length: 322.79
                 Mean success rate: 60.50
                  Mean reward/step: 20.98
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11280384
                    Iteration time: 2.69s
                        Total time: 3664.33s
                               ETA: 528557.0s

################################################################################
                    [1m Learning iteration 1377/200000 [0m

                       Computation: 3043 steps/s (collection: 0.557s, learning 2.135s)
               Value function loss: 84378.1750
                    Surrogate loss: 0.0113
             Mean action noise std: 0.89
                       Mean reward: 6943.34
               Mean episode length: 336.56
                 Mean success rate: 63.50
                  Mean reward/step: 20.11
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11288576
                    Iteration time: 2.69s
                        Total time: 3667.02s
                               ETA: 528558.8s

################################################################################
                    [1m Learning iteration 1378/200000 [0m

                       Computation: 3080 steps/s (collection: 0.579s, learning 2.080s)
               Value function loss: 78014.2309
                    Surrogate loss: 0.0133
             Mean action noise std: 0.89
                       Mean reward: 6883.27
               Mean episode length: 333.44
                 Mean success rate: 63.00
                  Mean reward/step: 20.55
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11296768
                    Iteration time: 2.66s
                        Total time: 3669.68s
                               ETA: 528555.8s

################################################################################
                    [1m Learning iteration 1379/200000 [0m

                       Computation: 3046 steps/s (collection: 0.584s, learning 2.105s)
               Value function loss: 99033.6116
                    Surrogate loss: 0.0134
             Mean action noise std: 0.89
                       Mean reward: 6933.04
               Mean episode length: 336.05
                 Mean success rate: 63.00
                  Mean reward/step: 20.10
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 2.69s
                        Total time: 3672.37s
                               ETA: 528557.1s

################################################################################
                    [1m Learning iteration 1380/200000 [0m

                       Computation: 3063 steps/s (collection: 0.589s, learning 2.086s)
               Value function loss: 98201.6090
                    Surrogate loss: 0.0134
             Mean action noise std: 0.89
                       Mean reward: 6918.76
               Mean episode length: 336.89
                 Mean success rate: 62.50
                  Mean reward/step: 19.91
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 11313152
                    Iteration time: 2.67s
                        Total time: 3675.04s
                               ETA: 528556.4s

################################################################################
                    [1m Learning iteration 1381/200000 [0m

                       Computation: 3015 steps/s (collection: 0.619s, learning 2.097s)
               Value function loss: 45299.8877
                    Surrogate loss: 0.0124
             Mean action noise std: 0.89
                       Mean reward: 6490.74
               Mean episode length: 324.04
                 Mean success rate: 59.00
                  Mean reward/step: 20.08
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11321344
                    Iteration time: 2.72s
                        Total time: 3677.76s
                               ETA: 528561.7s

################################################################################
                    [1m Learning iteration 1382/200000 [0m

                       Computation: 3019 steps/s (collection: 0.608s, learning 2.106s)
               Value function loss: 82928.2810
                    Surrogate loss: 0.0123
             Mean action noise std: 0.89
                       Mean reward: 6548.09
               Mean episode length: 329.37
                 Mean success rate: 60.00
                  Mean reward/step: 20.97
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11329536
                    Iteration time: 2.71s
                        Total time: 3680.47s
                               ETA: 528566.5s

################################################################################
                    [1m Learning iteration 1383/200000 [0m

                       Computation: 3024 steps/s (collection: 0.581s, learning 2.127s)
               Value function loss: 84266.3918
                    Surrogate loss: 0.0102
             Mean action noise std: 0.89
                       Mean reward: 6966.34
               Mean episode length: 345.69
                 Mean success rate: 63.50
                  Mean reward/step: 21.38
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11337728
                    Iteration time: 2.71s
                        Total time: 3683.18s
                               ETA: 528570.6s

################################################################################
                    [1m Learning iteration 1384/200000 [0m

                       Computation: 3067 steps/s (collection: 0.568s, learning 2.102s)
               Value function loss: 103517.1244
                    Surrogate loss: 0.0125
             Mean action noise std: 0.89
                       Mean reward: 7211.28
               Mean episode length: 352.06
                 Mean success rate: 64.50
                  Mean reward/step: 21.56
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11345920
                    Iteration time: 2.67s
                        Total time: 3685.85s
                               ETA: 528569.2s

################################################################################
                    [1m Learning iteration 1385/200000 [0m

                       Computation: 3063 steps/s (collection: 0.575s, learning 2.099s)
               Value function loss: 63541.5424
                    Surrogate loss: 0.0165
             Mean action noise std: 0.89
                       Mean reward: 7075.78
               Mean episode length: 348.41
                 Mean success rate: 63.50
                  Mean reward/step: 21.48
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11354112
                    Iteration time: 2.67s
                        Total time: 3688.52s
                               ETA: 528568.5s

################################################################################
                    [1m Learning iteration 1386/200000 [0m

                       Computation: 3091 steps/s (collection: 0.560s, learning 2.090s)
               Value function loss: 90108.2109
                    Surrogate loss: 0.0127
             Mean action noise std: 0.89
                       Mean reward: 7019.57
               Mean episode length: 352.53
                 Mean success rate: 64.00
                  Mean reward/step: 21.77
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 11362304
                    Iteration time: 2.65s
                        Total time: 3691.17s
                               ETA: 528564.2s

################################################################################
                    [1m Learning iteration 1387/200000 [0m

                       Computation: 3130 steps/s (collection: 0.517s, learning 2.100s)
               Value function loss: 97225.2288
                    Surrogate loss: 0.0116
             Mean action noise std: 0.89
                       Mean reward: 7059.54
               Mean episode length: 350.44
                 Mean success rate: 64.00
                  Mean reward/step: 20.78
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11370496
                    Iteration time: 2.62s
                        Total time: 3693.79s
                               ETA: 528555.1s

################################################################################
                    [1m Learning iteration 1388/200000 [0m

                       Computation: 3085 steps/s (collection: 0.577s, learning 2.078s)
               Value function loss: 54407.0607
                    Surrogate loss: 0.0133
             Mean action noise std: 0.89
                       Mean reward: 6947.92
               Mean episode length: 342.54
                 Mean success rate: 63.50
                  Mean reward/step: 20.93
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11378688
                    Iteration time: 2.65s
                        Total time: 3696.44s
                               ETA: 528551.6s

################################################################################
                    [1m Learning iteration 1389/200000 [0m

                       Computation: 3028 steps/s (collection: 0.589s, learning 2.116s)
               Value function loss: 77089.8271
                    Surrogate loss: 0.0109
             Mean action noise std: 0.89
                       Mean reward: 6947.65
               Mean episode length: 342.04
                 Mean success rate: 63.50
                  Mean reward/step: 21.79
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11386880
                    Iteration time: 2.70s
                        Total time: 3699.15s
                               ETA: 528555.2s

################################################################################
                    [1m Learning iteration 1390/200000 [0m

                       Computation: 3108 steps/s (collection: 0.541s, learning 2.094s)
               Value function loss: 100386.7724
                    Surrogate loss: 0.0126
             Mean action noise std: 0.89
                       Mean reward: 7529.83
               Mean episode length: 360.07
                 Mean success rate: 67.50
                  Mean reward/step: 22.01
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11395072
                    Iteration time: 2.63s
                        Total time: 3701.78s
                               ETA: 528548.8s

################################################################################
                    [1m Learning iteration 1391/200000 [0m

                       Computation: 3180 steps/s (collection: 0.542s, learning 2.034s)
               Value function loss: 73649.0503
                    Surrogate loss: 0.0136
             Mean action noise std: 0.89
                       Mean reward: 7057.69
               Mean episode length: 344.43
                 Mean success rate: 63.50
                  Mean reward/step: 22.07
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 2.58s
                        Total time: 3704.36s
                               ETA: 528533.9s

################################################################################
                    [1m Learning iteration 1392/200000 [0m

                       Computation: 3176 steps/s (collection: 0.506s, learning 2.073s)
               Value function loss: 118927.9071
                    Surrogate loss: 0.0130
             Mean action noise std: 0.89
                       Mean reward: 6919.47
               Mean episode length: 341.15
                 Mean success rate: 63.00
                  Mean reward/step: 21.67
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 11411456
                    Iteration time: 2.58s
                        Total time: 3706.94s
                               ETA: 528519.6s

################################################################################
                    [1m Learning iteration 1393/200000 [0m

                       Computation: 3179 steps/s (collection: 0.531s, learning 2.045s)
               Value function loss: 66953.4705
                    Surrogate loss: 0.0141
             Mean action noise std: 0.89
                       Mean reward: 6907.48
               Mean episode length: 337.10
                 Mean success rate: 62.00
                  Mean reward/step: 20.70
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11419648
                    Iteration time: 2.58s
                        Total time: 3709.52s
                               ETA: 528504.8s

################################################################################
                    [1m Learning iteration 1394/200000 [0m

                       Computation: 3214 steps/s (collection: 0.513s, learning 2.036s)
               Value function loss: 94721.0851
                    Surrogate loss: 0.0146
             Mean action noise std: 0.89
                       Mean reward: 7234.15
               Mean episode length: 343.72
                 Mean success rate: 64.50
                  Mean reward/step: 20.92
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11427840
                    Iteration time: 2.55s
                        Total time: 3712.06s
                               ETA: 528486.2s

################################################################################
                    [1m Learning iteration 1395/200000 [0m

                       Computation: 3243 steps/s (collection: 0.480s, learning 2.046s)
               Value function loss: 85620.0239
                    Surrogate loss: 0.0119
             Mean action noise std: 0.89
                       Mean reward: 6897.10
               Mean episode length: 333.99
                 Mean success rate: 62.00
                  Mean reward/step: 21.52
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 11436032
                    Iteration time: 2.53s
                        Total time: 3714.59s
                               ETA: 528464.3s

################################################################################
                    [1m Learning iteration 1396/200000 [0m

                       Computation: 3245 steps/s (collection: 0.486s, learning 2.039s)
               Value function loss: 88364.1029
                    Surrogate loss: 0.0143
             Mean action noise std: 0.89
                       Mean reward: 7259.75
               Mean episode length: 343.67
                 Mean success rate: 64.00
                  Mean reward/step: 21.39
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11444224
                    Iteration time: 2.52s
                        Total time: 3717.11s
                               ETA: 528442.2s

################################################################################
                    [1m Learning iteration 1397/200000 [0m

                       Computation: 3242 steps/s (collection: 0.499s, learning 2.028s)
               Value function loss: 67995.7494
                    Surrogate loss: 0.0124
             Mean action noise std: 0.89
                       Mean reward: 7230.57
               Mean episode length: 340.34
                 Mean success rate: 63.00
                  Mean reward/step: 21.53
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11452416
                    Iteration time: 2.53s
                        Total time: 3719.64s
                               ETA: 528420.5s

################################################################################
                    [1m Learning iteration 1398/200000 [0m

                       Computation: 3144 steps/s (collection: 0.581s, learning 2.025s)
               Value function loss: 96922.3927
                    Surrogate loss: 0.0138
             Mean action noise std: 0.89
                       Mean reward: 7049.37
               Mean episode length: 334.96
                 Mean success rate: 62.00
                  Mean reward/step: 22.19
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11460608
                    Iteration time: 2.61s
                        Total time: 3722.25s
                               ETA: 528410.0s

################################################################################
                    [1m Learning iteration 1399/200000 [0m

                       Computation: 3186 steps/s (collection: 0.534s, learning 2.037s)
               Value function loss: 77595.7727
                    Surrogate loss: 0.0133
             Mean action noise std: 0.89
                       Mean reward: 7376.46
               Mean episode length: 345.62
                 Mean success rate: 65.00
                  Mean reward/step: 21.84
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11468800
                    Iteration time: 2.57s
                        Total time: 3724.82s
                               ETA: 528394.6s

################################################################################
                    [1m Learning iteration 1400/200000 [0m

                       Computation: 3169 steps/s (collection: 0.517s, learning 2.068s)
               Value function loss: 91143.9467
                    Surrogate loss: 0.0144
             Mean action noise std: 0.89
                       Mean reward: 7514.17
               Mean episode length: 348.25
                 Mean success rate: 65.00
                  Mean reward/step: 21.26
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11476992
                    Iteration time: 2.58s
                        Total time: 3727.40s
                               ETA: 528381.2s

################################################################################
                    [1m Learning iteration 1401/200000 [0m

                       Computation: 3186 steps/s (collection: 0.535s, learning 2.035s)
               Value function loss: 90123.4927
                    Surrogate loss: 0.0138
             Mean action noise std: 0.89
                       Mean reward: 7533.73
               Mean episode length: 345.98
                 Mean success rate: 65.50
                  Mean reward/step: 21.51
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11485184
                    Iteration time: 2.57s
                        Total time: 3729.97s
                               ETA: 528365.9s

################################################################################
                    [1m Learning iteration 1402/200000 [0m

                       Computation: 3229 steps/s (collection: 0.504s, learning 2.032s)
               Value function loss: 80003.4354
                    Surrogate loss: 0.0155
             Mean action noise std: 0.89
                       Mean reward: 7496.15
               Mean episode length: 345.70
                 Mean success rate: 65.50
                  Mean reward/step: 21.76
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11493376
                    Iteration time: 2.54s
                        Total time: 3732.51s
                               ETA: 528345.6s

################################################################################
                    [1m Learning iteration 1403/200000 [0m

                       Computation: 3227 steps/s (collection: 0.501s, learning 2.037s)
               Value function loss: 85309.8082
                    Surrogate loss: 0.0141
             Mean action noise std: 0.89
                       Mean reward: 7671.65
               Mean episode length: 349.81
                 Mean success rate: 66.00
                  Mean reward/step: 21.98
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 2.54s
                        Total time: 3735.05s
                               ETA: 528325.7s

################################################################################
                    [1m Learning iteration 1404/200000 [0m

                       Computation: 3127 steps/s (collection: 0.558s, learning 2.062s)
               Value function loss: 82402.9171
                    Surrogate loss: 0.0134
             Mean action noise std: 0.89
                       Mean reward: 7865.17
               Mean episode length: 355.61
                 Mean success rate: 67.50
                  Mean reward/step: 21.41
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11509760
                    Iteration time: 2.62s
                        Total time: 3737.67s
                               ETA: 528317.2s

################################################################################
                    [1m Learning iteration 1405/200000 [0m

                       Computation: 3162 steps/s (collection: 0.559s, learning 2.032s)
               Value function loss: 72725.8835
                    Surrogate loss: 0.0153
             Mean action noise std: 0.89
                       Mean reward: 7488.94
               Mean episode length: 342.98
                 Mean success rate: 66.00
                  Mean reward/step: 21.45
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11517952
                    Iteration time: 2.59s
                        Total time: 3740.26s
                               ETA: 528304.7s

################################################################################
                    [1m Learning iteration 1406/200000 [0m

                       Computation: 3173 steps/s (collection: 0.555s, learning 2.026s)
               Value function loss: 89717.4491
                    Surrogate loss: 0.0168
             Mean action noise std: 0.89
                       Mean reward: 7322.70
               Mean episode length: 338.19
                 Mean success rate: 65.00
                  Mean reward/step: 20.81
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 11526144
                    Iteration time: 2.58s
                        Total time: 3742.84s
                               ETA: 528290.9s

################################################################################
                    [1m Learning iteration 1407/200000 [0m

                       Computation: 3191 steps/s (collection: 0.525s, learning 2.042s)
               Value function loss: 73071.0080
                    Surrogate loss: 0.0162
             Mean action noise std: 0.89
                       Mean reward: 7289.12
               Mean episode length: 336.93
                 Mean success rate: 64.00
                  Mean reward/step: 20.43
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11534336
                    Iteration time: 2.57s
                        Total time: 3745.41s
                               ETA: 528275.1s

################################################################################
                    [1m Learning iteration 1408/200000 [0m

                       Computation: 3156 steps/s (collection: 0.527s, learning 2.068s)
               Value function loss: 114311.8185
                    Surrogate loss: 0.0161
             Mean action noise std: 0.89
                       Mean reward: 7661.33
               Mean episode length: 348.67
                 Mean success rate: 67.00
                  Mean reward/step: 19.51
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 11542528
                    Iteration time: 2.60s
                        Total time: 3748.00s
                               ETA: 528263.3s

################################################################################
                    [1m Learning iteration 1409/200000 [0m

                       Computation: 3205 steps/s (collection: 0.527s, learning 2.028s)
               Value function loss: 59902.1607
                    Surrogate loss: 0.0133
             Mean action noise std: 0.89
                       Mean reward: 7573.30
               Mean episode length: 346.02
                 Mean success rate: 66.00
                  Mean reward/step: 18.98
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11550720
                    Iteration time: 2.56s
                        Total time: 3750.56s
                               ETA: 528245.9s

################################################################################
                    [1m Learning iteration 1410/200000 [0m

                       Computation: 3193 steps/s (collection: 0.511s, learning 2.054s)
               Value function loss: 115538.3847
                    Surrogate loss: 0.0108
             Mean action noise std: 0.89
                       Mean reward: 7346.89
               Mean episode length: 343.69
                 Mean success rate: 64.50
                  Mean reward/step: 18.96
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 11558912
                    Iteration time: 2.57s
                        Total time: 3753.12s
                               ETA: 528229.9s

################################################################################
                    [1m Learning iteration 1411/200000 [0m

                       Computation: 3168 steps/s (collection: 0.517s, learning 2.069s)
               Value function loss: 92450.6912
                    Surrogate loss: 0.0129
             Mean action noise std: 0.89
                       Mean reward: 7384.42
               Mean episode length: 348.27
                 Mean success rate: 65.50
                  Mean reward/step: 19.39
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11567104
                    Iteration time: 2.59s
                        Total time: 3755.71s
                               ETA: 528216.8s

################################################################################
                    [1m Learning iteration 1412/200000 [0m

                       Computation: 3164 steps/s (collection: 0.530s, learning 2.059s)
               Value function loss: 87630.1428
                    Surrogate loss: 0.0163
             Mean action noise std: 0.89
                       Mean reward: 7009.80
               Mean episode length: 340.88
                 Mean success rate: 65.00
                  Mean reward/step: 19.18
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 11575296
                    Iteration time: 2.59s
                        Total time: 3758.30s
                               ETA: 528204.1s

################################################################################
                    [1m Learning iteration 1413/200000 [0m

                       Computation: 3172 steps/s (collection: 0.529s, learning 2.053s)
               Value function loss: 70081.7448
                    Surrogate loss: 0.0133
             Mean action noise std: 0.89
                       Mean reward: 7159.33
               Mean episode length: 345.27
                 Mean success rate: 65.50
                  Mean reward/step: 19.38
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11583488
                    Iteration time: 2.58s
                        Total time: 3760.88s
                               ETA: 528190.6s

################################################################################
                    [1m Learning iteration 1414/200000 [0m

                       Computation: 3179 steps/s (collection: 0.504s, learning 2.073s)
               Value function loss: 79331.9813
                    Surrogate loss: 0.0144
             Mean action noise std: 0.89
                       Mean reward: 7461.85
               Mean episode length: 356.62
                 Mean success rate: 67.50
                  Mean reward/step: 19.55
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11591680
                    Iteration time: 2.58s
                        Total time: 3763.46s
                               ETA: 528176.3s

################################################################################
                    [1m Learning iteration 1415/200000 [0m

                       Computation: 3197 steps/s (collection: 0.511s, learning 2.051s)
               Value function loss: 84831.1104
                    Surrogate loss: 0.0147
             Mean action noise std: 0.89
                       Mean reward: 7468.78
               Mean episode length: 358.55
                 Mean success rate: 68.00
                  Mean reward/step: 20.06
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 2.56s
                        Total time: 3766.02s
                               ETA: 528160.0s

################################################################################
                    [1m Learning iteration 1416/200000 [0m

                       Computation: 3175 steps/s (collection: 0.530s, learning 2.050s)
               Value function loss: 69554.3619
                    Surrogate loss: 0.0149
             Mean action noise std: 0.89
                       Mean reward: 7240.97
               Mean episode length: 354.10
                 Mean success rate: 67.00
                  Mean reward/step: 20.01
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11608064
                    Iteration time: 2.58s
                        Total time: 3768.60s
                               ETA: 528146.1s

################################################################################
                    [1m Learning iteration 1417/200000 [0m

                       Computation: 3195 steps/s (collection: 0.517s, learning 2.047s)
               Value function loss: 59086.6944
                    Surrogate loss: 0.0140
             Mean action noise std: 0.89
                       Mean reward: 7092.94
               Mean episode length: 350.56
                 Mean success rate: 66.50
                  Mean reward/step: 20.81
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11616256
                    Iteration time: 2.56s
                        Total time: 3771.16s
                               ETA: 528130.1s

################################################################################
                    [1m Learning iteration 1418/200000 [0m

                       Computation: 3182 steps/s (collection: 0.511s, learning 2.063s)
               Value function loss: 79718.6771
                    Surrogate loss: 0.0163
             Mean action noise std: 0.89
                       Mean reward: 7093.36
               Mean episode length: 353.49
                 Mean success rate: 67.00
                  Mean reward/step: 21.37
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11624448
                    Iteration time: 2.57s
                        Total time: 3773.74s
                               ETA: 528115.5s

################################################################################
                    [1m Learning iteration 1419/200000 [0m

                       Computation: 3128 steps/s (collection: 0.512s, learning 2.107s)
               Value function loss: 76400.0541
                    Surrogate loss: 0.0157
             Mean action noise std: 0.89
                       Mean reward: 7484.74
               Mean episode length: 366.06
                 Mean success rate: 70.50
                  Mean reward/step: 21.10
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11632640
                    Iteration time: 2.62s
                        Total time: 3776.35s
                               ETA: 528107.1s

################################################################################
                    [1m Learning iteration 1420/200000 [0m

                       Computation: 3112 steps/s (collection: 0.508s, learning 2.125s)
               Value function loss: 65056.0148
                    Surrogate loss: 0.0149
             Mean action noise std: 0.89
                       Mean reward: 7245.84
               Mean episode length: 358.94
                 Mean success rate: 70.00
                  Mean reward/step: 21.49
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11640832
                    Iteration time: 2.63s
                        Total time: 3778.99s
                               ETA: 528100.6s

################################################################################
                    [1m Learning iteration 1421/200000 [0m

                       Computation: 3036 steps/s (collection: 0.554s, learning 2.144s)
               Value function loss: 73052.4372
                    Surrogate loss: 0.0122
             Mean action noise std: 0.89
                       Mean reward: 7319.73
               Mean episode length: 361.09
                 Mean success rate: 70.50
                  Mean reward/step: 21.36
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11649024
                    Iteration time: 2.70s
                        Total time: 3781.68s
                               ETA: 528103.4s

################################################################################
                    [1m Learning iteration 1422/200000 [0m

                       Computation: 3092 steps/s (collection: 0.529s, learning 2.120s)
               Value function loss: 78902.8618
                    Surrogate loss: 0.0149
             Mean action noise std: 0.89
                       Mean reward: 7509.13
               Mean episode length: 367.55
                 Mean success rate: 72.00
                  Mean reward/step: 20.94
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 11657216
                    Iteration time: 2.65s
                        Total time: 3784.33s
                               ETA: 528099.3s

################################################################################
                    [1m Learning iteration 1423/200000 [0m

                       Computation: 3177 steps/s (collection: 0.523s, learning 2.055s)
               Value function loss: 92974.9303
                    Surrogate loss: 0.0142
             Mean action noise std: 0.89
                       Mean reward: 7237.39
               Mean episode length: 360.25
                 Mean success rate: 68.50
                  Mean reward/step: 20.55
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 11665408
                    Iteration time: 2.58s
                        Total time: 3786.91s
                               ETA: 528085.3s

################################################################################
                    [1m Learning iteration 1424/200000 [0m

                       Computation: 3132 steps/s (collection: 0.509s, learning 2.105s)
               Value function loss: 60534.9965
                    Surrogate loss: 0.0121
             Mean action noise std: 0.89
                       Mean reward: 7657.16
               Mean episode length: 379.02
                 Mean success rate: 72.50
                  Mean reward/step: 19.37
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11673600
                    Iteration time: 2.61s
                        Total time: 3789.53s
                               ETA: 528076.5s

################################################################################
                    [1m Learning iteration 1425/200000 [0m

                       Computation: 3126 steps/s (collection: 0.525s, learning 2.096s)
               Value function loss: 66814.6865
                    Surrogate loss: 0.0142
             Mean action noise std: 0.89
                       Mean reward: 7599.36
               Mean episode length: 380.04
                 Mean success rate: 73.50
                  Mean reward/step: 20.72
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11681792
                    Iteration time: 2.62s
                        Total time: 3792.15s
                               ETA: 528068.4s

################################################################################
                    [1m Learning iteration 1426/200000 [0m

                       Computation: 3106 steps/s (collection: 0.524s, learning 2.113s)
               Value function loss: 113619.3057
                    Surrogate loss: 0.0140
             Mean action noise std: 0.89
                       Mean reward: 7678.74
               Mean episode length: 381.92
                 Mean success rate: 73.50
                  Mean reward/step: 20.93
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 11689984
                    Iteration time: 2.64s
                        Total time: 3794.78s
                               ETA: 528062.7s

################################################################################
                    [1m Learning iteration 1427/200000 [0m

                       Computation: 3120 steps/s (collection: 0.522s, learning 2.103s)
               Value function loss: 63861.3255
                    Surrogate loss: 0.0119
             Mean action noise std: 0.88
                       Mean reward: 7393.81
               Mean episode length: 368.86
                 Mean success rate: 71.00
                  Mean reward/step: 20.77
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 2.62s
                        Total time: 3797.41s
                               ETA: 528055.2s

################################################################################
                    [1m Learning iteration 1428/200000 [0m

                       Computation: 3100 steps/s (collection: 0.526s, learning 2.116s)
               Value function loss: 101157.8714
                    Surrogate loss: 0.0157
             Mean action noise std: 0.88
                       Mean reward: 7273.90
               Mean episode length: 365.96
                 Mean success rate: 69.50
                  Mean reward/step: 20.89
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11706368
                    Iteration time: 2.64s
                        Total time: 3800.05s
                               ETA: 528050.2s

################################################################################
                    [1m Learning iteration 1429/200000 [0m

                       Computation: 3127 steps/s (collection: 0.520s, learning 2.099s)
               Value function loss: 56128.0884
                    Surrogate loss: 0.0135
             Mean action noise std: 0.88
                       Mean reward: 7095.43
               Mean episode length: 361.93
                 Mean success rate: 68.00
                  Mean reward/step: 21.14
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 11714560
                    Iteration time: 2.62s
                        Total time: 3802.67s
                               ETA: 528042.0s

################################################################################
                    [1m Learning iteration 1430/200000 [0m

                       Computation: 3201 steps/s (collection: 0.511s, learning 2.048s)
               Value function loss: 98631.3287
                    Surrogate loss: 0.0148
             Mean action noise std: 0.88
                       Mean reward: 7411.79
               Mean episode length: 368.33
                 Mean success rate: 69.00
                  Mean reward/step: 20.08
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 11722752
                    Iteration time: 2.56s
                        Total time: 3805.23s
                               ETA: 528025.4s

################################################################################
                    [1m Learning iteration 1431/200000 [0m

                       Computation: 3188 steps/s (collection: 0.533s, learning 2.036s)
               Value function loss: 86251.6306
                    Surrogate loss: 0.0147
             Mean action noise std: 0.89
                       Mean reward: 7630.14
               Mean episode length: 371.07
                 Mean success rate: 71.00
                  Mean reward/step: 19.23
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11730944
                    Iteration time: 2.57s
                        Total time: 3807.80s
                               ETA: 528010.3s

################################################################################
                    [1m Learning iteration 1432/200000 [0m

                       Computation: 3187 steps/s (collection: 0.540s, learning 2.030s)
               Value function loss: 78734.4698
                    Surrogate loss: 0.0145
             Mean action noise std: 0.89
                       Mean reward: 7399.25
               Mean episode length: 361.81
                 Mean success rate: 69.00
                  Mean reward/step: 19.59
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11739136
                    Iteration time: 2.57s
                        Total time: 3810.37s
                               ETA: 527995.3s

################################################################################
                    [1m Learning iteration 1433/200000 [0m

                       Computation: 3178 steps/s (collection: 0.519s, learning 2.059s)
               Value function loss: 75757.2080
                    Surrogate loss: 0.0132
             Mean action noise std: 0.89
                       Mean reward: 7173.98
               Mean episode length: 352.48
                 Mean success rate: 66.50
                  Mean reward/step: 20.21
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11747328
                    Iteration time: 2.58s
                        Total time: 3812.95s
                               ETA: 527981.3s

################################################################################
                    [1m Learning iteration 1434/200000 [0m

                       Computation: 3195 steps/s (collection: 0.522s, learning 2.042s)
               Value function loss: 67003.8341
                    Surrogate loss: 0.0139
             Mean action noise std: 0.89
                       Mean reward: 7158.39
               Mean episode length: 349.83
                 Mean success rate: 66.00
                  Mean reward/step: 21.29
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11755520
                    Iteration time: 2.56s
                        Total time: 3815.51s
                               ETA: 527965.5s

################################################################################
                    [1m Learning iteration 1435/200000 [0m

                       Computation: 3151 steps/s (collection: 0.528s, learning 2.071s)
               Value function loss: 67236.6271
                    Surrogate loss: 0.0128
             Mean action noise std: 0.89
                       Mean reward: 7146.74
               Mean episode length: 349.12
                 Mean success rate: 65.50
                  Mean reward/step: 22.04
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 11763712
                    Iteration time: 2.60s
                        Total time: 3818.11s
                               ETA: 527954.6s

################################################################################
                    [1m Learning iteration 1436/200000 [0m

                       Computation: 3194 steps/s (collection: 0.529s, learning 2.035s)
               Value function loss: 80146.9668
                    Surrogate loss: 0.0134
             Mean action noise std: 0.89
                       Mean reward: 6957.29
               Mean episode length: 336.70
                 Mean success rate: 63.00
                  Mean reward/step: 21.97
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11771904
                    Iteration time: 2.56s
                        Total time: 3820.67s
                               ETA: 527938.9s

################################################################################
                    [1m Learning iteration 1437/200000 [0m

                       Computation: 3207 steps/s (collection: 0.516s, learning 2.038s)
               Value function loss: 88224.5429
                    Surrogate loss: 0.0153
             Mean action noise std: 0.89
                       Mean reward: 7144.48
               Mean episode length: 343.74
                 Mean success rate: 66.00
                  Mean reward/step: 21.41
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11780096
                    Iteration time: 2.55s
                        Total time: 3823.23s
                               ETA: 527921.8s

################################################################################
                    [1m Learning iteration 1438/200000 [0m

                       Computation: 3141 steps/s (collection: 0.553s, learning 2.054s)
               Value function loss: 70591.4302
                    Surrogate loss: 0.0189
             Mean action noise std: 0.89
                       Mean reward: 7219.85
               Mean episode length: 344.63
                 Mean success rate: 65.00
                  Mean reward/step: 21.02
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11788288
                    Iteration time: 2.61s
                        Total time: 3825.84s
                               ETA: 527912.1s

################################################################################
                    [1m Learning iteration 1439/200000 [0m

                       Computation: 3161 steps/s (collection: 0.517s, learning 2.075s)
               Value function loss: 96114.8972
                    Surrogate loss: 0.0181
             Mean action noise std: 0.89
                       Mean reward: 6930.09
               Mean episode length: 336.57
                 Mean success rate: 62.50
                  Mean reward/step: 21.08
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 2.59s
                        Total time: 3828.43s
                               ETA: 527900.1s

################################################################################
                    [1m Learning iteration 1440/200000 [0m

                       Computation: 3147 steps/s (collection: 0.551s, learning 2.051s)
               Value function loss: 73476.7397
                    Surrogate loss: 0.0139
             Mean action noise std: 0.89
                       Mean reward: 6764.41
               Mean episode length: 331.62
                 Mean success rate: 61.00
                  Mean reward/step: 21.03
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11804672
                    Iteration time: 2.60s
                        Total time: 3831.03s
                               ETA: 527889.7s

################################################################################
                    [1m Learning iteration 1441/200000 [0m

                       Computation: 3082 steps/s (collection: 0.577s, learning 2.081s)
               Value function loss: 118896.1723
                    Surrogate loss: 0.0146
             Mean action noise std: 0.89
                       Mean reward: 6856.76
               Mean episode length: 331.63
                 Mean success rate: 61.50
                  Mean reward/step: 20.87
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 11812864
                    Iteration time: 2.66s
                        Total time: 3833.69s
                               ETA: 527886.9s

################################################################################
                    [1m Learning iteration 1442/200000 [0m

                       Computation: 3055 steps/s (collection: 0.577s, learning 2.104s)
               Value function loss: 64618.3628
                    Surrogate loss: 0.0154
             Mean action noise std: 0.89
                       Mean reward: 7132.84
               Mean episode length: 342.62
                 Mean success rate: 63.00
                  Mean reward/step: 20.38
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11821056
                    Iteration time: 2.68s
                        Total time: 3836.37s
                               ETA: 527887.4s

################################################################################
                    [1m Learning iteration 1443/200000 [0m

                       Computation: 3042 steps/s (collection: 0.525s, learning 2.168s)
               Value function loss: 64171.2206
                    Surrogate loss: 0.0144
             Mean action noise std: 0.89
                       Mean reward: 7359.08
               Mean episode length: 352.19
                 Mean success rate: 65.00
                  Mean reward/step: 20.70
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11829248
                    Iteration time: 2.69s
                        Total time: 3839.06s
                               ETA: 527889.4s

################################################################################
                    [1m Learning iteration 1444/200000 [0m

                       Computation: 2958 steps/s (collection: 0.626s, learning 2.143s)
               Value function loss: 82945.1620
                    Surrogate loss: 0.0142
             Mean action noise std: 0.89
                       Mean reward: 7204.71
               Mean episode length: 350.22
                 Mean success rate: 64.00
                  Mean reward/step: 20.56
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 11837440
                    Iteration time: 2.77s
                        Total time: 3841.83s
                               ETA: 527901.9s

################################################################################
                    [1m Learning iteration 1445/200000 [0m

                       Computation: 3032 steps/s (collection: 0.562s, learning 2.140s)
               Value function loss: 112560.7020
                    Surrogate loss: 0.0153
             Mean action noise std: 0.89
                       Mean reward: 7467.80
               Mean episode length: 354.95
                 Mean success rate: 64.50
                  Mean reward/step: 21.10
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11845632
                    Iteration time: 2.70s
                        Total time: 3844.53s
                               ETA: 527905.1s

################################################################################
                    [1m Learning iteration 1446/200000 [0m

                       Computation: 3049 steps/s (collection: 0.549s, learning 2.138s)
               Value function loss: 89480.5824
                    Surrogate loss: 0.0147
             Mean action noise std: 0.89
                       Mean reward: 7691.21
               Mean episode length: 361.47
                 Mean success rate: 66.50
                  Mean reward/step: 21.16
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11853824
                    Iteration time: 2.69s
                        Total time: 3847.22s
                               ETA: 527906.3s

################################################################################
                    [1m Learning iteration 1447/200000 [0m

                       Computation: 3060 steps/s (collection: 0.566s, learning 2.111s)
               Value function loss: 85648.1229
                    Surrogate loss: 0.0170
             Mean action noise std: 0.89
                       Mean reward: 7973.64
               Mean episode length: 371.77
                 Mean success rate: 69.00
                  Mean reward/step: 20.76
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11862016
                    Iteration time: 2.68s
                        Total time: 3849.89s
                               ETA: 527906.1s

################################################################################
                    [1m Learning iteration 1448/200000 [0m

                       Computation: 2880 steps/s (collection: 0.597s, learning 2.247s)
               Value function loss: 63802.9164
                    Surrogate loss: 0.0171
             Mean action noise std: 0.89
                       Mean reward: 7965.21
               Mean episode length: 371.75
                 Mean success rate: 69.00
                  Mean reward/step: 20.88
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 11870208
                    Iteration time: 2.84s
                        Total time: 3852.74s
                               ETA: 527928.9s

################################################################################
                    [1m Learning iteration 1449/200000 [0m

                       Computation: 2664 steps/s (collection: 0.858s, learning 2.216s)
               Value function loss: 69708.4025
                    Surrogate loss: 0.0133
             Mean action noise std: 0.89
                       Mean reward: 8306.86
               Mean episode length: 383.20
                 Mean success rate: 71.50
                  Mean reward/step: 21.79
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 11878400
                    Iteration time: 3.07s
                        Total time: 3855.81s
                               ETA: 527983.1s

################################################################################
                    [1m Learning iteration 1450/200000 [0m

                       Computation: 2966 steps/s (collection: 0.634s, learning 2.128s)
               Value function loss: 72941.6056
                    Surrogate loss: 0.0169
             Mean action noise std: 0.89
                       Mean reward: 8550.08
               Mean episode length: 391.23
                 Mean success rate: 73.50
                  Mean reward/step: 22.86
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11886592
                    Iteration time: 2.76s
                        Total time: 3858.57s
                               ETA: 527994.4s

################################################################################
                    [1m Learning iteration 1451/200000 [0m

                       Computation: 2990 steps/s (collection: 0.628s, learning 2.112s)
               Value function loss: 53290.7915
                    Surrogate loss: 0.0131
             Mean action noise std: 0.89
                       Mean reward: 8200.46
               Mean episode length: 380.19
                 Mean success rate: 70.00
                  Mean reward/step: 22.86
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 2.74s
                        Total time: 3861.31s
                               ETA: 528002.8s

################################################################################
                    [1m Learning iteration 1452/200000 [0m

                       Computation: 2855 steps/s (collection: 0.616s, learning 2.253s)
               Value function loss: 57826.5546
                    Surrogate loss: 0.0196
             Mean action noise std: 0.88
                       Mean reward: 7954.91
               Mean episode length: 370.74
                 Mean success rate: 67.50
                  Mean reward/step: 22.98
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 11902976
                    Iteration time: 2.87s
                        Total time: 3864.18s
                               ETA: 528028.8s

################################################################################
                    [1m Learning iteration 1453/200000 [0m

                       Computation: 3008 steps/s (collection: 0.614s, learning 2.109s)
               Value function loss: 83422.8153
                    Surrogate loss: 0.0181
             Mean action noise std: 0.88
                       Mean reward: 7905.11
               Mean episode length: 370.84
                 Mean success rate: 67.00
                  Mean reward/step: 22.67
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 11911168
                    Iteration time: 2.72s
                        Total time: 3866.91s
                               ETA: 528034.8s

################################################################################
                    [1m Learning iteration 1454/200000 [0m

                       Computation: 3039 steps/s (collection: 0.570s, learning 2.125s)
               Value function loss: 90058.9473
                    Surrogate loss: 0.0145
             Mean action noise std: 0.89
                       Mean reward: 8220.97
               Mean episode length: 384.62
                 Mean success rate: 70.00
                  Mean reward/step: 22.50
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 11919360
                    Iteration time: 2.69s
                        Total time: 3869.60s
                               ETA: 528037.0s

################################################################################
                    [1m Learning iteration 1455/200000 [0m

                       Computation: 2988 steps/s (collection: 0.643s, learning 2.099s)
               Value function loss: 112658.5271
                    Surrogate loss: 0.0152
             Mean action noise std: 0.88
                       Mean reward: 7742.50
               Mean episode length: 370.25
                 Mean success rate: 67.00
                  Mean reward/step: 21.62
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11927552
                    Iteration time: 2.74s
                        Total time: 3872.34s
                               ETA: 528045.5s

################################################################################
                    [1m Learning iteration 1456/200000 [0m

                       Computation: 2864 steps/s (collection: 0.670s, learning 2.190s)
               Value function loss: 77185.3756
                    Surrogate loss: 0.0169
             Mean action noise std: 0.88
                       Mean reward: 7835.80
               Mean episode length: 374.20
                 Mean success rate: 68.00
                  Mean reward/step: 21.57
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11935744
                    Iteration time: 2.86s
                        Total time: 3875.20s
                               ETA: 528070.1s

################################################################################
                    [1m Learning iteration 1457/200000 [0m

                       Computation: 2906 steps/s (collection: 0.658s, learning 2.161s)
               Value function loss: 102416.0692
                    Surrogate loss: 0.0174
             Mean action noise std: 0.88
                       Mean reward: 7754.96
               Mean episode length: 374.94
                 Mean success rate: 68.50
                  Mean reward/step: 21.74
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 11943936
                    Iteration time: 2.82s
                        Total time: 3878.02s
                               ETA: 528089.1s

################################################################################
                    [1m Learning iteration 1458/200000 [0m

                       Computation: 2987 steps/s (collection: 0.617s, learning 2.125s)
               Value function loss: 84262.1168
                    Surrogate loss: 0.0169
             Mean action noise std: 0.89
                       Mean reward: 7931.51
               Mean episode length: 377.61
                 Mean success rate: 69.50
                  Mean reward/step: 22.34
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 11952128
                    Iteration time: 2.74s
                        Total time: 3880.76s
                               ETA: 528097.7s

################################################################################
                    [1m Learning iteration 1459/200000 [0m

                       Computation: 3025 steps/s (collection: 0.589s, learning 2.118s)
               Value function loss: 110352.7482
                    Surrogate loss: 0.0159
             Mean action noise std: 0.89
                       Mean reward: 8155.49
               Mean episode length: 385.04
                 Mean success rate: 71.50
                  Mean reward/step: 22.02
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 11960320
                    Iteration time: 2.71s
                        Total time: 3883.47s
                               ETA: 528101.5s

################################################################################
                    [1m Learning iteration 1460/200000 [0m

                       Computation: 2944 steps/s (collection: 0.603s, learning 2.179s)
               Value function loss: 96999.9613
                    Surrogate loss: 0.0152
             Mean action noise std: 0.89
                       Mean reward: 8438.91
               Mean episode length: 392.14
                 Mean success rate: 74.50
                  Mean reward/step: 21.14
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 11968512
                    Iteration time: 2.78s
                        Total time: 3886.25s
                               ETA: 528115.4s

################################################################################
                    [1m Learning iteration 1461/200000 [0m

                       Computation: 2991 steps/s (collection: 0.625s, learning 2.114s)
               Value function loss: 104406.5412
                    Surrogate loss: 0.0173
             Mean action noise std: 0.89
                       Mean reward: 8847.39
               Mean episode length: 401.62
                 Mean success rate: 78.50
                  Mean reward/step: 20.30
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 11976704
                    Iteration time: 2.74s
                        Total time: 3888.99s
                               ETA: 528123.4s

################################################################################
                    [1m Learning iteration 1462/200000 [0m

                       Computation: 3008 steps/s (collection: 0.636s, learning 2.087s)
               Value function loss: 81433.0515
                    Surrogate loss: 0.0161
             Mean action noise std: 0.89
                       Mean reward: 8772.75
               Mean episode length: 397.35
                 Mean success rate: 78.00
                  Mean reward/step: 20.30
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 11984896
                    Iteration time: 2.72s
                        Total time: 3891.71s
                               ETA: 528129.2s

################################################################################
                    [1m Learning iteration 1463/200000 [0m

                       Computation: 2942 steps/s (collection: 0.591s, learning 2.193s)
               Value function loss: 80805.5330
                    Surrogate loss: 0.0163
             Mean action noise std: 0.89
                       Mean reward: 9090.72
               Mean episode length: 407.54
                 Mean success rate: 80.50
                  Mean reward/step: 20.19
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 2.78s
                        Total time: 3894.50s
                               ETA: 528143.4s

################################################################################
                    [1m Learning iteration 1464/200000 [0m

                       Computation: 3045 steps/s (collection: 0.620s, learning 2.069s)
               Value function loss: 75118.2667
                    Surrogate loss: 0.0175
             Mean action noise std: 0.89
                       Mean reward: 8794.98
               Mean episode length: 395.69
                 Mean success rate: 78.00
                  Mean reward/step: 20.85
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12001280
                    Iteration time: 2.69s
                        Total time: 3897.19s
                               ETA: 528144.8s

################################################################################
                    [1m Learning iteration 1465/200000 [0m

                       Computation: 3120 steps/s (collection: 0.551s, learning 2.074s)
               Value function loss: 52734.5944
                    Surrogate loss: 0.0175
             Mean action noise std: 0.89
                       Mean reward: 8443.57
               Mean episode length: 381.72
                 Mean success rate: 74.50
                  Mean reward/step: 21.10
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12009472
                    Iteration time: 2.63s
                        Total time: 3899.81s
                               ETA: 528137.4s

################################################################################
                    [1m Learning iteration 1466/200000 [0m

                       Computation: 3117 steps/s (collection: 0.547s, learning 2.081s)
               Value function loss: 78744.6581
                    Surrogate loss: 0.0143
             Mean action noise std: 0.89
                       Mean reward: 8166.53
               Mean episode length: 371.57
                 Mean success rate: 72.50
                  Mean reward/step: 21.57
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 12017664
                    Iteration time: 2.63s
                        Total time: 3902.44s
                               ETA: 528130.3s

################################################################################
                    [1m Learning iteration 1467/200000 [0m

                       Computation: 3093 steps/s (collection: 0.546s, learning 2.103s)
               Value function loss: 92593.7603
                    Surrogate loss: 0.0175
             Mean action noise std: 0.89
                       Mean reward: 8249.01
               Mean episode length: 374.56
                 Mean success rate: 72.50
                  Mean reward/step: 21.25
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12025856
                    Iteration time: 2.65s
                        Total time: 3905.09s
                               ETA: 528126.0s

################################################################################
                    [1m Learning iteration 1468/200000 [0m

                       Computation: 3119 steps/s (collection: 0.541s, learning 2.085s)
               Value function loss: 70477.3450
                    Surrogate loss: 0.0188
             Mean action noise std: 0.89
                       Mean reward: 7907.91
               Mean episode length: 362.51
                 Mean success rate: 70.00
                  Mean reward/step: 21.28
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12034048
                    Iteration time: 2.63s
                        Total time: 3907.71s
                               ETA: 528118.8s

################################################################################
                    [1m Learning iteration 1469/200000 [0m

                       Computation: 3077 steps/s (collection: 0.573s, learning 2.089s)
               Value function loss: 82117.6156
                    Surrogate loss: 0.0195
             Mean action noise std: 0.89
                       Mean reward: 7929.84
               Mean episode length: 365.40
                 Mean success rate: 70.50
                  Mean reward/step: 21.26
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12042240
                    Iteration time: 2.66s
                        Total time: 3910.38s
                               ETA: 528116.4s

################################################################################
                    [1m Learning iteration 1470/200000 [0m

                       Computation: 3051 steps/s (collection: 0.608s, learning 2.077s)
               Value function loss: 100350.7078
                    Surrogate loss: 0.0141
             Mean action noise std: 0.89
                       Mean reward: 7919.59
               Mean episode length: 369.00
                 Mean success rate: 70.00
                  Mean reward/step: 21.33
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12050432
                    Iteration time: 2.68s
                        Total time: 3913.06s
                               ETA: 528117.0s

################################################################################
                    [1m Learning iteration 1471/200000 [0m

                       Computation: 3035 steps/s (collection: 0.593s, learning 2.106s)
               Value function loss: 82595.0232
                    Surrogate loss: 0.0109
             Mean action noise std: 0.89
                       Mean reward: 8013.49
               Mean episode length: 373.30
                 Mean success rate: 72.00
                  Mean reward/step: 20.92
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12058624
                    Iteration time: 2.70s
                        Total time: 3915.76s
                               ETA: 528119.6s

################################################################################
                    [1m Learning iteration 1472/200000 [0m

                       Computation: 2920 steps/s (collection: 0.577s, learning 2.228s)
               Value function loss: 96084.3490
                    Surrogate loss: 0.0146
             Mean action noise std: 0.89
                       Mean reward: 7855.53
               Mean episode length: 371.34
                 Mean success rate: 70.50
                  Mean reward/step: 20.69
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12066816
                    Iteration time: 2.80s
                        Total time: 3918.57s
                               ETA: 528136.4s

################################################################################
                    [1m Learning iteration 1473/200000 [0m

                       Computation: 2987 steps/s (collection: 0.608s, learning 2.134s)
               Value function loss: 80116.9714
                    Surrogate loss: 0.0165
             Mean action noise std: 0.89
                       Mean reward: 8090.36
               Mean episode length: 381.85
                 Mean success rate: 73.00
                  Mean reward/step: 20.59
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12075008
                    Iteration time: 2.74s
                        Total time: 3921.31s
                               ETA: 528144.8s

################################################################################
                    [1m Learning iteration 1474/200000 [0m

                       Computation: 3022 steps/s (collection: 0.593s, learning 2.118s)
               Value function loss: 53625.2940
                    Surrogate loss: 0.0149
             Mean action noise std: 0.89
                       Mean reward: 7961.00
               Mean episode length: 381.83
                 Mean success rate: 72.00
                  Mean reward/step: 20.75
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12083200
                    Iteration time: 2.71s
                        Total time: 3924.02s
                               ETA: 528149.0s

################################################################################
                    [1m Learning iteration 1475/200000 [0m

                       Computation: 2922 steps/s (collection: 0.668s, learning 2.135s)
               Value function loss: 89044.9495
                    Surrogate loss: 0.0126
             Mean action noise std: 0.89
                       Mean reward: 8248.67
               Mean episode length: 394.32
                 Mean success rate: 75.00
                  Mean reward/step: 21.75
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 2.80s
                        Total time: 3926.82s
                               ETA: 528165.5s

################################################################################
                    [1m Learning iteration 1476/200000 [0m

                       Computation: 3067 steps/s (collection: 0.563s, learning 2.108s)
               Value function loss: 86111.6499
                    Surrogate loss: 0.0179
             Mean action noise std: 0.89
                       Mean reward: 8222.32
               Mean episode length: 395.19
                 Mean success rate: 74.50
                  Mean reward/step: 22.05
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 12099584
                    Iteration time: 2.67s
                        Total time: 3929.49s
                               ETA: 528164.2s

################################################################################
                    [1m Learning iteration 1477/200000 [0m

                       Computation: 2738 steps/s (collection: 0.721s, learning 2.270s)
               Value function loss: 72430.3040
                    Surrogate loss: 0.0142
             Mean action noise std: 0.89
                       Mean reward: 8254.45
               Mean episode length: 396.36
                 Mean success rate: 75.50
                  Mean reward/step: 22.13
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12107776
                    Iteration time: 2.99s
                        Total time: 3932.48s
                               ETA: 528206.0s

################################################################################
                    [1m Learning iteration 1478/200000 [0m

                       Computation: 2947 steps/s (collection: 0.698s, learning 2.081s)
               Value function loss: 84967.2714
                    Surrogate loss: 0.0168
             Mean action noise std: 0.89
                       Mean reward: 8176.27
               Mean episode length: 394.96
                 Mean success rate: 74.50
                  Mean reward/step: 21.75
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 12115968
                    Iteration time: 2.78s
                        Total time: 3935.26s
                               ETA: 528219.2s

################################################################################
                    [1m Learning iteration 1479/200000 [0m

                       Computation: 3083 steps/s (collection: 0.572s, learning 2.085s)
               Value function loss: 65123.6397
                    Surrogate loss: 0.0134
             Mean action noise std: 0.89
                       Mean reward: 7702.65
               Mean episode length: 377.83
                 Mean success rate: 70.50
                  Mean reward/step: 21.99
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12124160
                    Iteration time: 2.66s
                        Total time: 3937.92s
                               ETA: 528216.1s

################################################################################
                    [1m Learning iteration 1480/200000 [0m

                       Computation: 3104 steps/s (collection: 0.537s, learning 2.101s)
               Value function loss: 89761.0765
                    Surrogate loss: 0.0154
             Mean action noise std: 0.89
                       Mean reward: 7646.01
               Mean episode length: 373.46
                 Mean success rate: 70.00
                  Mean reward/step: 22.18
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12132352
                    Iteration time: 2.64s
                        Total time: 3940.56s
                               ETA: 528210.4s

################################################################################
                    [1m Learning iteration 1481/200000 [0m

                       Computation: 3065 steps/s (collection: 0.581s, learning 2.091s)
               Value function loss: 65310.0512
                    Surrogate loss: 0.0144
             Mean action noise std: 0.89
                       Mean reward: 7317.75
               Mean episode length: 363.72
                 Mean success rate: 66.50
                  Mean reward/step: 21.91
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12140544
                    Iteration time: 2.67s
                        Total time: 3943.23s
                               ETA: 528209.3s

################################################################################
                    [1m Learning iteration 1482/200000 [0m

                       Computation: 3038 steps/s (collection: 0.576s, learning 2.119s)
               Value function loss: 100698.9182
                    Surrogate loss: 0.0133
             Mean action noise std: 0.89
                       Mean reward: 7288.93
               Mean episode length: 357.03
                 Mean success rate: 66.50
                  Mean reward/step: 21.85
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 12148736
                    Iteration time: 2.70s
                        Total time: 3945.93s
                               ETA: 528211.3s

################################################################################
                    [1m Learning iteration 1483/200000 [0m

                       Computation: 2968 steps/s (collection: 0.644s, learning 2.116s)
               Value function loss: 86434.0048
                    Surrogate loss: 0.0135
             Mean action noise std: 0.89
                       Mean reward: 7401.43
               Mean episode length: 355.51
                 Mean success rate: 67.50
                  Mean reward/step: 22.06
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12156928
                    Iteration time: 2.76s
                        Total time: 3948.69s
                               ETA: 528221.9s

################################################################################
                    [1m Learning iteration 1484/200000 [0m

                       Computation: 2876 steps/s (collection: 0.606s, learning 2.242s)
               Value function loss: 130052.9530
                    Surrogate loss: 0.0119
             Mean action noise std: 0.89
                       Mean reward: 7365.75
               Mean episode length: 345.55
                 Mean success rate: 64.50
                  Mean reward/step: 21.81
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 12165120
                    Iteration time: 2.85s
                        Total time: 3951.53s
                               ETA: 528244.2s

################################################################################
                    [1m Learning iteration 1485/200000 [0m

                       Computation: 3037 steps/s (collection: 0.612s, learning 2.086s)
               Value function loss: 74933.7553
                    Surrogate loss: 0.0141
             Mean action noise std: 0.89
                       Mean reward: 7337.83
               Mean episode length: 343.40
                 Mean success rate: 64.50
                  Mean reward/step: 20.84
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12173312
                    Iteration time: 2.70s
                        Total time: 3954.23s
                               ETA: 528246.4s

################################################################################
                    [1m Learning iteration 1486/200000 [0m

                       Computation: 3099 steps/s (collection: 0.540s, learning 2.103s)
               Value function loss: 107489.1045
                    Surrogate loss: 0.0151
             Mean action noise std: 0.89
                       Mean reward: 7645.14
               Mean episode length: 352.47
                 Mean success rate: 66.00
                  Mean reward/step: 19.98
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12181504
                    Iteration time: 2.64s
                        Total time: 3956.87s
                               ETA: 528241.3s

################################################################################
                    [1m Learning iteration 1487/200000 [0m

                       Computation: 3118 steps/s (collection: 0.547s, learning 2.080s)
               Value function loss: 53989.6022
                    Surrogate loss: 0.0165
             Mean action noise std: 0.89
                       Mean reward: 7611.22
               Mean episode length: 353.19
                 Mean success rate: 65.50
                  Mean reward/step: 19.98
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 2.63s
                        Total time: 3959.50s
                               ETA: 528234.2s

################################################################################
                    [1m Learning iteration 1488/200000 [0m

                       Computation: 3026 steps/s (collection: 0.529s, learning 2.179s)
               Value function loss: 107016.9228
                    Surrogate loss: 0.0169
             Mean action noise std: 0.89
                       Mean reward: 7615.48
               Mean episode length: 353.18
                 Mean success rate: 65.50
                  Mean reward/step: 20.55
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12197888
                    Iteration time: 2.71s
                        Total time: 3962.21s
                               ETA: 528237.7s

################################################################################
                    [1m Learning iteration 1489/200000 [0m

                       Computation: 3050 steps/s (collection: 0.572s, learning 2.114s)
               Value function loss: 53857.0965
                    Surrogate loss: 0.0139
             Mean action noise std: 0.89
                       Mean reward: 7710.09
               Mean episode length: 355.51
                 Mean success rate: 66.00
                  Mean reward/step: 20.61
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12206080
                    Iteration time: 2.69s
                        Total time: 3964.89s
                               ETA: 528238.3s

################################################################################
                    [1m Learning iteration 1490/200000 [0m

                       Computation: 3106 steps/s (collection: 0.574s, learning 2.063s)
               Value function loss: 63782.2674
                    Surrogate loss: 0.0130
             Mean action noise std: 0.89
                       Mean reward: 7524.91
               Mean episode length: 349.50
                 Mean success rate: 64.00
                  Mean reward/step: 21.50
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12214272
                    Iteration time: 2.64s
                        Total time: 3967.53s
                               ETA: 528232.5s

################################################################################
                    [1m Learning iteration 1491/200000 [0m

                       Computation: 3157 steps/s (collection: 0.521s, learning 2.073s)
               Value function loss: 89663.0512
                    Surrogate loss: 0.0150
             Mean action noise std: 0.89
                       Mean reward: 7450.81
               Mean episode length: 347.87
                 Mean success rate: 62.50
                  Mean reward/step: 22.31
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12222464
                    Iteration time: 2.59s
                        Total time: 3970.13s
                               ETA: 528220.9s

################################################################################
                    [1m Learning iteration 1492/200000 [0m

                       Computation: 3030 steps/s (collection: 0.589s, learning 2.114s)
               Value function loss: 87744.8113
                    Surrogate loss: 0.0153
             Mean action noise std: 0.89
                       Mean reward: 7319.21
               Mean episode length: 344.24
                 Mean success rate: 62.00
                  Mean reward/step: 22.52
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 12230656
                    Iteration time: 2.70s
                        Total time: 3972.83s
                               ETA: 528223.9s

################################################################################
                    [1m Learning iteration 1493/200000 [0m

                       Computation: 2878 steps/s (collection: 0.687s, learning 2.159s)
               Value function loss: 77167.3491
                    Surrogate loss: 0.0127
             Mean action noise std: 0.89
                       Mean reward: 7523.87
               Mean episode length: 347.81
                 Mean success rate: 63.50
                  Mean reward/step: 22.90
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12238848
                    Iteration time: 2.85s
                        Total time: 3975.67s
                               ETA: 528245.8s

################################################################################
                    [1m Learning iteration 1494/200000 [0m

                       Computation: 2978 steps/s (collection: 0.572s, learning 2.178s)
               Value function loss: 90404.9205
                    Surrogate loss: 0.0140
             Mean action noise std: 0.89
                       Mean reward: 7365.84
               Mean episode length: 341.74
                 Mean success rate: 61.50
                  Mean reward/step: 23.12
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12247040
                    Iteration time: 2.75s
                        Total time: 3978.43s
                               ETA: 528255.1s

################################################################################
                    [1m Learning iteration 1495/200000 [0m

                       Computation: 2921 steps/s (collection: 0.569s, learning 2.235s)
               Value function loss: 58252.2311
                    Surrogate loss: 0.0131
             Mean action noise std: 0.89
                       Mean reward: 7161.09
               Mean episode length: 333.17
                 Mean success rate: 60.00
                  Mean reward/step: 23.33
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12255232
                    Iteration time: 2.80s
                        Total time: 3981.23s
                               ETA: 528271.3s

################################################################################
                    [1m Learning iteration 1496/200000 [0m

                       Computation: 2958 steps/s (collection: 0.619s, learning 2.150s)
               Value function loss: 79514.9610
                    Surrogate loss: 0.0139
             Mean action noise std: 0.89
                       Mean reward: 7331.78
               Mean episode length: 339.14
                 Mean success rate: 61.50
                  Mean reward/step: 22.95
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12263424
                    Iteration time: 2.77s
                        Total time: 3984.00s
                               ETA: 528282.9s

################################################################################
                    [1m Learning iteration 1497/200000 [0m

                       Computation: 3045 steps/s (collection: 0.575s, learning 2.115s)
               Value function loss: 95832.7215
                    Surrogate loss: 0.0126
             Mean action noise std: 0.89
                       Mean reward: 7612.65
               Mean episode length: 349.39
                 Mean success rate: 64.00
                  Mean reward/step: 22.59
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12271616
                    Iteration time: 2.69s
                        Total time: 3986.69s
                               ETA: 528284.1s

################################################################################
                    [1m Learning iteration 1498/200000 [0m

                       Computation: 3051 steps/s (collection: 0.571s, learning 2.113s)
               Value function loss: 87376.2455
                    Surrogate loss: 0.0152
             Mean action noise std: 0.89
                       Mean reward: 7845.47
               Mean episode length: 358.60
                 Mean success rate: 66.50
                  Mean reward/step: 22.19
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12279808
                    Iteration time: 2.68s
                        Total time: 3989.37s
                               ETA: 528284.5s

################################################################################
                    [1m Learning iteration 1499/200000 [0m

                       Computation: 3037 steps/s (collection: 0.559s, learning 2.138s)
               Value function loss: 89477.1156
                    Surrogate loss: 0.0172
             Mean action noise std: 0.89
                       Mean reward: 8171.92
               Mean episode length: 367.27
                 Mean success rate: 69.00
                  Mean reward/step: 21.72
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 2.70s
                        Total time: 3992.07s
                               ETA: 528286.6s

################################################################################
                    [1m Learning iteration 1500/200000 [0m

                       Computation: 3118 steps/s (collection: 0.567s, learning 2.061s)
               Value function loss: 89872.0276
                    Surrogate loss: 0.0163
             Mean action noise std: 0.89
                       Mean reward: 8657.65
               Mean episode length: 381.75
                 Mean success rate: 73.00
                  Mean reward/step: 21.66
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12296192
                    Iteration time: 2.63s
                        Total time: 3994.70s
                               ETA: 528279.4s

################################################################################
                    [1m Learning iteration 1501/200000 [0m

                       Computation: 3101 steps/s (collection: 0.544s, learning 2.097s)
               Value function loss: 77167.4727
                    Surrogate loss: 0.0159
             Mean action noise std: 0.89
                       Mean reward: 8685.14
               Mean episode length: 387.75
                 Mean success rate: 74.50
                  Mean reward/step: 21.30
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12304384
                    Iteration time: 2.64s
                        Total time: 3997.34s
                               ETA: 528274.1s

################################################################################
                    [1m Learning iteration 1502/200000 [0m

                       Computation: 3022 steps/s (collection: 0.601s, learning 2.110s)
               Value function loss: 89803.0123
                    Surrogate loss: 0.0132
             Mean action noise std: 0.89
                       Mean reward: 8164.57
               Mean episode length: 374.19
                 Mean success rate: 70.50
                  Mean reward/step: 21.06
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 12312576
                    Iteration time: 2.71s
                        Total time: 4000.05s
                               ETA: 528277.9s

################################################################################
                    [1m Learning iteration 1503/200000 [0m

                       Computation: 3062 steps/s (collection: 0.586s, learning 2.089s)
               Value function loss: 101839.9556
                    Surrogate loss: 0.0126
             Mean action noise std: 0.89
                       Mean reward: 8266.02
               Mean episode length: 378.27
                 Mean success rate: 72.00
                  Mean reward/step: 20.37
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12320768
                    Iteration time: 2.68s
                        Total time: 4002.72s
                               ETA: 528277.1s

################################################################################
                    [1m Learning iteration 1504/200000 [0m

                       Computation: 3022 steps/s (collection: 0.618s, learning 2.093s)
               Value function loss: 111355.8066
                    Surrogate loss: 0.0113
             Mean action noise std: 0.89
                       Mean reward: 8541.74
               Mean episode length: 383.54
                 Mean success rate: 73.50
                  Mean reward/step: 19.43
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 12328960
                    Iteration time: 2.71s
                        Total time: 4005.43s
                               ETA: 528280.9s

################################################################################
                    [1m Learning iteration 1505/200000 [0m

                       Computation: 3089 steps/s (collection: 0.579s, learning 2.073s)
               Value function loss: 58436.3895
                    Surrogate loss: 0.0169
             Mean action noise std: 0.89
                       Mean reward: 8145.02
               Mean episode length: 374.13
                 Mean success rate: 70.50
                  Mean reward/step: 19.61
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12337152
                    Iteration time: 2.65s
                        Total time: 4008.09s
                               ETA: 528277.0s

################################################################################
                    [1m Learning iteration 1506/200000 [0m

                       Computation: 2994 steps/s (collection: 0.600s, learning 2.136s)
               Value function loss: 62977.0313
                    Surrogate loss: 0.0210
             Mean action noise std: 0.89
                       Mean reward: 7676.87
               Mean episode length: 357.69
                 Mean success rate: 67.00
                  Mean reward/step: 20.03
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12345344
                    Iteration time: 2.74s
                        Total time: 4010.82s
                               ETA: 528284.1s

################################################################################
                    [1m Learning iteration 1507/200000 [0m

                       Computation: 3054 steps/s (collection: 0.567s, learning 2.115s)
               Value function loss: 89858.3279
                    Surrogate loss: 0.0176
             Mean action noise std: 0.89
                       Mean reward: 7337.25
               Mean episode length: 344.70
                 Mean success rate: 63.50
                  Mean reward/step: 20.60
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12353536
                    Iteration time: 2.68s
                        Total time: 4013.50s
                               ETA: 528284.1s

################################################################################
                    [1m Learning iteration 1508/200000 [0m

                       Computation: 3057 steps/s (collection: 0.594s, learning 2.086s)
               Value function loss: 87543.6667
                    Surrogate loss: 0.0153
             Mean action noise std: 0.89
                       Mean reward: 7362.05
               Mean episode length: 344.62
                 Mean success rate: 63.00
                  Mean reward/step: 20.72
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12361728
                    Iteration time: 2.68s
                        Total time: 4016.18s
                               ETA: 528283.8s

################################################################################
                    [1m Learning iteration 1509/200000 [0m

                       Computation: 3062 steps/s (collection: 0.553s, learning 2.122s)
               Value function loss: 83117.6461
                    Surrogate loss: 0.0132
             Mean action noise std: 0.89
                       Mean reward: 7103.62
               Mean episode length: 337.81
                 Mean success rate: 61.00
                  Mean reward/step: 21.50
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12369920
                    Iteration time: 2.68s
                        Total time: 4018.86s
                               ETA: 528283.0s

################################################################################
                    [1m Learning iteration 1510/200000 [0m

                       Computation: 3060 steps/s (collection: 0.587s, learning 2.090s)
               Value function loss: 93105.7776
                    Surrogate loss: 0.0126
             Mean action noise std: 0.89
                       Mean reward: 7295.03
               Mean episode length: 344.26
                 Mean success rate: 62.00
                  Mean reward/step: 22.00
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12378112
                    Iteration time: 2.68s
                        Total time: 4021.54s
                               ETA: 528282.3s

################################################################################
                    [1m Learning iteration 1511/200000 [0m

                       Computation: 3119 steps/s (collection: 0.535s, learning 2.091s)
               Value function loss: 75707.0556
                    Surrogate loss: 0.0116
             Mean action noise std: 0.89
                       Mean reward: 7332.59
               Mean episode length: 343.01
                 Mean success rate: 62.50
                  Mean reward/step: 21.91
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 2.63s
                        Total time: 4024.16s
                               ETA: 528275.1s

################################################################################
                    [1m Learning iteration 1512/200000 [0m

                       Computation: 3108 steps/s (collection: 0.542s, learning 2.093s)
               Value function loss: 75431.4574
                    Surrogate loss: 0.0153
             Mean action noise std: 0.89
                       Mean reward: 7012.13
               Mean episode length: 335.58
                 Mean success rate: 59.00
                  Mean reward/step: 21.64
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12394496
                    Iteration time: 2.64s
                        Total time: 4026.80s
                               ETA: 528268.9s

################################################################################
                    [1m Learning iteration 1513/200000 [0m

                       Computation: 2965 steps/s (collection: 0.605s, learning 2.157s)
               Value function loss: 88696.5018
                    Surrogate loss: 0.0135
             Mean action noise std: 0.89
                       Mean reward: 7157.70
               Mean episode length: 341.39
                 Mean success rate: 60.00
                  Mean reward/step: 21.76
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12402688
                    Iteration time: 2.76s
                        Total time: 4029.56s
                               ETA: 528279.5s

################################################################################
                    [1m Learning iteration 1514/200000 [0m

                       Computation: 3006 steps/s (collection: 0.625s, learning 2.100s)
               Value function loss: 77447.6522
                    Surrogate loss: 0.0140
             Mean action noise std: 0.89
                       Mean reward: 7374.08
               Mean episode length: 342.90
                 Mean success rate: 61.50
                  Mean reward/step: 21.73
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12410880
                    Iteration time: 2.72s
                        Total time: 4032.28s
                               ETA: 528285.1s

################################################################################
                    [1m Learning iteration 1515/200000 [0m

                       Computation: 3078 steps/s (collection: 0.531s, learning 2.130s)
               Value function loss: 111768.0096
                    Surrogate loss: 0.0182
             Mean action noise std: 0.89
                       Mean reward: 7611.55
               Mean episode length: 352.86
                 Mean success rate: 63.50
                  Mean reward/step: 21.19
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 12419072
                    Iteration time: 2.66s
                        Total time: 4034.95s
                               ETA: 528282.4s

################################################################################
                    [1m Learning iteration 1516/200000 [0m

                       Computation: 3088 steps/s (collection: 0.595s, learning 2.058s)
               Value function loss: 52559.3190
                    Surrogate loss: 0.0155
             Mean action noise std: 0.89
                       Mean reward: 7810.12
               Mean episode length: 356.57
                 Mean success rate: 64.50
                  Mean reward/step: 21.23
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 12427264
                    Iteration time: 2.65s
                        Total time: 4037.60s
                               ETA: 528278.6s

################################################################################
                    [1m Learning iteration 1517/200000 [0m

                       Computation: 3024 steps/s (collection: 0.551s, learning 2.158s)
               Value function loss: 86380.8970
                    Surrogate loss: 0.0154
             Mean action noise std: 0.89
                       Mean reward: 7433.21
               Mean episode length: 345.03
                 Mean success rate: 61.50
                  Mean reward/step: 21.41
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12435456
                    Iteration time: 2.71s
                        Total time: 4040.31s
                               ETA: 528282.1s

################################################################################
                    [1m Learning iteration 1518/200000 [0m

                       Computation: 3044 steps/s (collection: 0.593s, learning 2.098s)
               Value function loss: 81832.9724
                    Surrogate loss: 0.0118
             Mean action noise std: 0.89
                       Mean reward: 7371.79
               Mean episode length: 338.87
                 Mean success rate: 61.00
                  Mean reward/step: 20.75
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 12443648
                    Iteration time: 2.69s
                        Total time: 4043.00s
                               ETA: 528283.3s

################################################################################
                    [1m Learning iteration 1519/200000 [0m

                       Computation: 3090 steps/s (collection: 0.566s, learning 2.084s)
               Value function loss: 113966.0684
                    Surrogate loss: 0.0137
             Mean action noise std: 0.89
                       Mean reward: 7261.93
               Mean episode length: 341.34
                 Mean success rate: 60.50
                  Mean reward/step: 20.48
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 12451840
                    Iteration time: 2.65s
                        Total time: 4045.65s
                               ETA: 528279.2s

################################################################################
                    [1m Learning iteration 1520/200000 [0m

                       Computation: 3146 steps/s (collection: 0.529s, learning 2.074s)
               Value function loss: 87901.6882
                    Surrogate loss: 0.0186
             Mean action noise std: 0.89
                       Mean reward: 7374.29
               Mean episode length: 344.32
                 Mean success rate: 62.00
                  Mean reward/step: 19.53
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12460032
                    Iteration time: 2.60s
                        Total time: 4048.25s
                               ETA: 528268.9s

################################################################################
                    [1m Learning iteration 1521/200000 [0m

                       Computation: 3016 steps/s (collection: 0.610s, learning 2.105s)
               Value function loss: 101243.9809
                    Surrogate loss: 0.0182
             Mean action noise std: 0.89
                       Mean reward: 7286.78
               Mean episode length: 343.94
                 Mean success rate: 62.00
                  Mean reward/step: 19.33
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 12468224
                    Iteration time: 2.72s
                        Total time: 4050.97s
                               ETA: 528273.2s

################################################################################
                    [1m Learning iteration 1522/200000 [0m

                       Computation: 3038 steps/s (collection: 0.560s, learning 2.136s)
               Value function loss: 64054.1213
                    Surrogate loss: 0.0160
             Mean action noise std: 0.89
                       Mean reward: 7199.04
               Mean episode length: 340.96
                 Mean success rate: 61.50
                  Mean reward/step: 19.52
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12476416
                    Iteration time: 2.70s
                        Total time: 4053.66s
                               ETA: 528275.0s

################################################################################
                    [1m Learning iteration 1523/200000 [0m

                       Computation: 3106 steps/s (collection: 0.538s, learning 2.099s)
               Value function loss: 91369.9870
                    Surrogate loss: 0.0162
             Mean action noise std: 0.89
                       Mean reward: 7048.32
               Mean episode length: 337.50
                 Mean success rate: 60.00
                  Mean reward/step: 20.21
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 2.64s
                        Total time: 4056.30s
                               ETA: 528269.1s

################################################################################
                    [1m Learning iteration 1524/200000 [0m

                       Computation: 3013 steps/s (collection: 0.547s, learning 2.171s)
               Value function loss: 67842.5332
                    Surrogate loss: 0.0168
             Mean action noise std: 0.89
                       Mean reward: 6816.86
               Mean episode length: 329.58
                 Mean success rate: 58.00
                  Mean reward/step: 20.89
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12492800
                    Iteration time: 2.72s
                        Total time: 4059.02s
                               ETA: 528273.8s

################################################################################
                    [1m Learning iteration 1525/200000 [0m

                       Computation: 2985 steps/s (collection: 0.614s, learning 2.129s)
               Value function loss: 76246.5420
                    Surrogate loss: 0.0145
             Mean action noise std: 0.89
                       Mean reward: 6801.37
               Mean episode length: 334.06
                 Mean success rate: 58.00
                  Mean reward/step: 21.75
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12500992
                    Iteration time: 2.74s
                        Total time: 4061.76s
                               ETA: 528281.8s

################################################################################
                    [1m Learning iteration 1526/200000 [0m

                       Computation: 3073 steps/s (collection: 0.595s, learning 2.070s)
               Value function loss: 93977.4063
                    Surrogate loss: 0.0153
             Mean action noise std: 0.89
                       Mean reward: 6894.31
               Mean episode length: 341.12
                 Mean success rate: 59.50
                  Mean reward/step: 21.52
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12509184
                    Iteration time: 2.67s
                        Total time: 4064.43s
                               ETA: 528279.6s

################################################################################
                    [1m Learning iteration 1527/200000 [0m

                       Computation: 3105 steps/s (collection: 0.562s, learning 2.076s)
               Value function loss: 96624.1928
                    Surrogate loss: 0.0129
             Mean action noise std: 0.89
                       Mean reward: 7058.12
               Mean episode length: 342.80
                 Mean success rate: 60.50
                  Mean reward/step: 21.34
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12517376
                    Iteration time: 2.64s
                        Total time: 4067.06s
                               ETA: 528273.9s

################################################################################
                    [1m Learning iteration 1528/200000 [0m

                       Computation: 2947 steps/s (collection: 0.584s, learning 2.195s)
               Value function loss: 74206.2931
                    Surrogate loss: 0.0160
             Mean action noise std: 0.89
                       Mean reward: 6972.82
               Mean episode length: 335.71
                 Mean success rate: 59.50
                  Mean reward/step: 21.33
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12525568
                    Iteration time: 2.78s
                        Total time: 4069.84s
                               ETA: 528286.4s

################################################################################
                    [1m Learning iteration 1529/200000 [0m

                       Computation: 3067 steps/s (collection: 0.586s, learning 2.085s)
               Value function loss: 101408.6121
                    Surrogate loss: 0.0150
             Mean action noise std: 0.89
                       Mean reward: 7199.35
               Mean episode length: 343.46
                 Mean success rate: 61.00
                  Mean reward/step: 21.49
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12533760
                    Iteration time: 2.67s
                        Total time: 4072.51s
                               ETA: 528284.9s

################################################################################
                    [1m Learning iteration 1530/200000 [0m

                       Computation: 3133 steps/s (collection: 0.542s, learning 2.073s)
               Value function loss: 45604.7020
                    Surrogate loss: 0.0194
             Mean action noise std: 0.89
                       Mean reward: 7041.34
               Mean episode length: 337.62
                 Mean success rate: 60.00
                  Mean reward/step: 22.02
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12541952
                    Iteration time: 2.61s
                        Total time: 4075.13s
                               ETA: 528276.1s

################################################################################
                    [1m Learning iteration 1531/200000 [0m

                       Computation: 3141 steps/s (collection: 0.526s, learning 2.082s)
               Value function loss: 116283.8615
                    Surrogate loss: 0.0122
             Mean action noise std: 0.89
                       Mean reward: 7136.13
               Mean episode length: 340.75
                 Mean success rate: 60.50
                  Mean reward/step: 21.94
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 12550144
                    Iteration time: 2.61s
                        Total time: 4077.74s
                               ETA: 528266.4s

################################################################################
                    [1m Learning iteration 1532/200000 [0m

                       Computation: 3153 steps/s (collection: 0.528s, learning 2.070s)
               Value function loss: 53141.3051
                    Surrogate loss: 0.0138
             Mean action noise std: 0.89
                       Mean reward: 6892.53
               Mean episode length: 335.46
                 Mean success rate: 59.50
                  Mean reward/step: 21.96
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12558336
                    Iteration time: 2.60s
                        Total time: 4080.33s
                               ETA: 528255.5s

################################################################################
                    [1m Learning iteration 1533/200000 [0m

                       Computation: 3024 steps/s (collection: 0.595s, learning 2.113s)
               Value function loss: 90450.6845
                    Surrogate loss: 0.0128
             Mean action noise std: 0.89
                       Mean reward: 6968.24
               Mean episode length: 336.93
                 Mean success rate: 61.00
                  Mean reward/step: 22.30
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12566528
                    Iteration time: 2.71s
                        Total time: 4083.04s
                               ETA: 528258.8s

################################################################################
                    [1m Learning iteration 1534/200000 [0m

                       Computation: 3086 steps/s (collection: 0.555s, learning 2.099s)
               Value function loss: 89326.0661
                    Surrogate loss: 0.0143
             Mean action noise std: 0.89
                       Mean reward: 7269.93
               Mean episode length: 343.91
                 Mean success rate: 63.50
                  Mean reward/step: 22.16
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12574720
                    Iteration time: 2.65s
                        Total time: 4085.70s
                               ETA: 528255.2s

################################################################################
                    [1m Learning iteration 1535/200000 [0m

                       Computation: 3095 steps/s (collection: 0.539s, learning 2.107s)
               Value function loss: 129580.9396
                    Surrogate loss: 0.0124
             Mean action noise std: 0.89
                       Mean reward: 7501.30
               Mean episode length: 350.75
                 Mean success rate: 64.00
                  Mean reward/step: 22.04
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 2.65s
                        Total time: 4088.34s
                               ETA: 528250.5s

################################################################################
                    [1m Learning iteration 1536/200000 [0m

                       Computation: 3052 steps/s (collection: 0.603s, learning 2.081s)
               Value function loss: 97778.5865
                    Surrogate loss: 0.0160
             Mean action noise std: 0.88
                       Mean reward: 7407.03
               Mean episode length: 347.02
                 Mean success rate: 63.00
                  Mean reward/step: 21.94
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12591104
                    Iteration time: 2.68s
                        Total time: 4091.03s
                               ETA: 528250.8s

################################################################################
                    [1m Learning iteration 1537/200000 [0m

                       Computation: 2932 steps/s (collection: 0.651s, learning 2.143s)
               Value function loss: 118831.4249
                    Surrogate loss: 0.0112
             Mean action noise std: 0.89
                       Mean reward: 7495.29
               Mean episode length: 353.31
                 Mean success rate: 64.00
                  Mean reward/step: 21.40
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 12599296
                    Iteration time: 2.79s
                        Total time: 4093.82s
                               ETA: 528265.1s

################################################################################
                    [1m Learning iteration 1538/200000 [0m

                       Computation: 2828 steps/s (collection: 0.753s, learning 2.143s)
               Value function loss: 62222.1157
                    Surrogate loss: 0.0192
             Mean action noise std: 0.89
                       Mean reward: 7489.13
               Mean episode length: 353.51
                 Mean success rate: 64.50
                  Mean reward/step: 21.91
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 12607488
                    Iteration time: 2.90s
                        Total time: 4096.72s
                               ETA: 528292.6s

################################################################################
                    [1m Learning iteration 1539/200000 [0m

                       Computation: 3103 steps/s (collection: 0.570s, learning 2.070s)
               Value function loss: 114008.3746
                    Surrogate loss: 0.0179
             Mean action noise std: 0.89
                       Mean reward: 7610.57
               Mean episode length: 352.06
                 Mean success rate: 65.00
                  Mean reward/step: 21.21
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 12615680
                    Iteration time: 2.64s
                        Total time: 4099.35s
                               ETA: 528287.1s

################################################################################
                    [1m Learning iteration 1540/200000 [0m

                       Computation: 3113 steps/s (collection: 0.550s, learning 2.081s)
               Value function loss: 96455.7983
                    Surrogate loss: 0.0160
             Mean action noise std: 0.89
                       Mean reward: 8175.97
               Mean episode length: 369.77
                 Mean success rate: 69.50
                  Mean reward/step: 21.08
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12623872
                    Iteration time: 2.63s
                        Total time: 4101.99s
                               ETA: 528280.4s

################################################################################
                    [1m Learning iteration 1541/200000 [0m

                       Computation: 3125 steps/s (collection: 0.534s, learning 2.087s)
               Value function loss: 83546.3971
                    Surrogate loss: 0.0123
             Mean action noise std: 0.88
                       Mean reward: 8133.48
               Mean episode length: 366.69
                 Mean success rate: 68.00
                  Mean reward/step: 20.63
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12632064
                    Iteration time: 2.62s
                        Total time: 4104.61s
                               ETA: 528272.5s

################################################################################
                    [1m Learning iteration 1542/200000 [0m

                       Computation: 3034 steps/s (collection: 0.614s, learning 2.086s)
               Value function loss: 69375.4696
                    Surrogate loss: 0.0116
             Mean action noise std: 0.88
                       Mean reward: 7524.11
               Mean episode length: 344.25
                 Mean success rate: 64.00
                  Mean reward/step: 19.72
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 12640256
                    Iteration time: 2.70s
                        Total time: 4107.31s
                               ETA: 528274.7s

################################################################################
                    [1m Learning iteration 1543/200000 [0m

                       Computation: 3028 steps/s (collection: 0.593s, learning 2.112s)
               Value function loss: 89059.9288
                    Surrogate loss: 0.0127
             Mean action noise std: 0.88
                       Mean reward: 6997.48
               Mean episode length: 327.65
                 Mean success rate: 61.00
                  Mean reward/step: 18.60
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 12648448
                    Iteration time: 2.70s
                        Total time: 4110.01s
                               ETA: 528277.6s

################################################################################
                    [1m Learning iteration 1544/200000 [0m

                       Computation: 3066 steps/s (collection: 0.553s, learning 2.119s)
               Value function loss: 72614.7393
                    Surrogate loss: 0.0136
             Mean action noise std: 0.88
                       Mean reward: 6660.99
               Mean episode length: 313.67
                 Mean success rate: 59.50
                  Mean reward/step: 19.00
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 12656640
                    Iteration time: 2.67s
                        Total time: 4112.68s
                               ETA: 528276.2s

################################################################################
                    [1m Learning iteration 1545/200000 [0m

                       Computation: 2955 steps/s (collection: 0.706s, learning 2.066s)
               Value function loss: 82030.7771
                    Surrogate loss: 0.0112
             Mean action noise std: 0.88
                       Mean reward: 6036.97
               Mean episode length: 289.16
                 Mean success rate: 54.00
                  Mean reward/step: 18.93
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 12664832
                    Iteration time: 2.77s
                        Total time: 4115.46s
                               ETA: 528287.7s

################################################################################
                    [1m Learning iteration 1546/200000 [0m

                       Computation: 3114 steps/s (collection: 0.542s, learning 2.088s)
               Value function loss: 90412.7697
                    Surrogate loss: 0.0173
             Mean action noise std: 0.89
                       Mean reward: 5788.14
               Mean episode length: 281.11
                 Mean success rate: 51.50
                  Mean reward/step: 19.57
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 12673024
                    Iteration time: 2.63s
                        Total time: 4118.09s
                               ETA: 528281.0s

################################################################################
                    [1m Learning iteration 1547/200000 [0m

                       Computation: 3036 steps/s (collection: 0.624s, learning 2.074s)
               Value function loss: 84908.8697
                    Surrogate loss: 0.0147
             Mean action noise std: 0.89
                       Mean reward: 5705.17
               Mean episode length: 281.05
                 Mean success rate: 51.00
                  Mean reward/step: 18.92
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 2.70s
                        Total time: 4120.78s
                               ETA: 528282.9s

################################################################################
                    [1m Learning iteration 1548/200000 [0m

                       Computation: 2987 steps/s (collection: 0.644s, learning 2.098s)
               Value function loss: 93452.2280
                    Surrogate loss: 0.0174
             Mean action noise std: 0.89
                       Mean reward: 5870.97
               Mean episode length: 284.41
                 Mean success rate: 51.50
                  Mean reward/step: 20.01
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12689408
                    Iteration time: 2.74s
                        Total time: 4123.53s
                               ETA: 528290.5s

################################################################################
                    [1m Learning iteration 1549/200000 [0m

                       Computation: 3089 steps/s (collection: 0.563s, learning 2.088s)
               Value function loss: 95950.2626
                    Surrogate loss: 0.0158
             Mean action noise std: 0.89
                       Mean reward: 6120.51
               Mean episode length: 295.01
                 Mean success rate: 53.00
                  Mean reward/step: 20.13
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12697600
                    Iteration time: 2.65s
                        Total time: 4126.18s
                               ETA: 528286.5s

################################################################################
                    [1m Learning iteration 1550/200000 [0m

                       Computation: 3069 steps/s (collection: 0.568s, learning 2.101s)
               Value function loss: 83466.8099
                    Surrogate loss: 0.0127
             Mean action noise std: 0.89
                       Mean reward: 6081.95
               Mean episode length: 296.17
                 Mean success rate: 52.50
                  Mean reward/step: 19.89
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 12705792
                    Iteration time: 2.67s
                        Total time: 4128.85s
                               ETA: 528284.7s

################################################################################
                    [1m Learning iteration 1551/200000 [0m

                       Computation: 3078 steps/s (collection: 0.549s, learning 2.112s)
               Value function loss: 108646.0434
                    Surrogate loss: 0.0113
             Mean action noise std: 0.89
                       Mean reward: 6636.78
               Mean episode length: 316.03
                 Mean success rate: 56.50
                  Mean reward/step: 19.76
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 12713984
                    Iteration time: 2.66s
                        Total time: 4131.51s
                               ETA: 528281.9s

################################################################################
                    [1m Learning iteration 1552/200000 [0m

                       Computation: 3073 steps/s (collection: 0.563s, learning 2.103s)
               Value function loss: 91837.2947
                    Surrogate loss: 0.0156
             Mean action noise std: 0.89
                       Mean reward: 6955.94
               Mean episode length: 331.79
                 Mean success rate: 58.50
                  Mean reward/step: 20.04
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 12722176
                    Iteration time: 2.67s
                        Total time: 4134.17s
                               ETA: 528279.7s

################################################################################
                    [1m Learning iteration 1553/200000 [0m

                       Computation: 3084 steps/s (collection: 0.509s, learning 2.147s)
               Value function loss: 45630.0435
                    Surrogate loss: 0.0195
             Mean action noise std: 0.89
                       Mean reward: 7025.72
               Mean episode length: 338.36
                 Mean success rate: 59.00
                  Mean reward/step: 20.49
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12730368
                    Iteration time: 2.66s
                        Total time: 4136.83s
                               ETA: 528276.2s

################################################################################
                    [1m Learning iteration 1554/200000 [0m

                       Computation: 3068 steps/s (collection: 0.543s, learning 2.127s)
               Value function loss: 79898.1796
                    Surrogate loss: 0.0141
             Mean action noise std: 0.89
                       Mean reward: 7167.97
               Mean episode length: 346.41
                 Mean success rate: 61.00
                  Mean reward/step: 21.69
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12738560
                    Iteration time: 2.67s
                        Total time: 4139.50s
                               ETA: 528274.5s

################################################################################
                    [1m Learning iteration 1555/200000 [0m

                       Computation: 2935 steps/s (collection: 0.625s, learning 2.166s)
               Value function loss: 57038.2723
                    Surrogate loss: 0.0106
             Mean action noise std: 0.89
                       Mean reward: 6948.89
               Mean episode length: 342.26
                 Mean success rate: 60.50
                  Mean reward/step: 22.07
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12746752
                    Iteration time: 2.79s
                        Total time: 4142.29s
                               ETA: 528288.2s

################################################################################
                    [1m Learning iteration 1556/200000 [0m

                       Computation: 2971 steps/s (collection: 0.611s, learning 2.146s)
               Value function loss: 91150.3911
                    Surrogate loss: 0.0135
             Mean action noise std: 0.89
                       Mean reward: 6612.32
               Mean episode length: 337.20
                 Mean success rate: 58.50
                  Mean reward/step: 22.28
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 12754944
                    Iteration time: 2.76s
                        Total time: 4145.05s
                               ETA: 528297.6s

################################################################################
                    [1m Learning iteration 1557/200000 [0m

                       Computation: 3058 steps/s (collection: 0.546s, learning 2.133s)
               Value function loss: 69029.1202
                    Surrogate loss: 0.0129
             Mean action noise std: 0.89
                       Mean reward: 6400.86
               Mean episode length: 330.02
                 Mean success rate: 57.50
                  Mean reward/step: 22.04
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12763136
                    Iteration time: 2.68s
                        Total time: 4147.72s
                               ETA: 528297.1s

################################################################################
                    [1m Learning iteration 1558/200000 [0m

                       Computation: 3083 steps/s (collection: 0.535s, learning 2.121s)
               Value function loss: 95663.1861
                    Surrogate loss: 0.0135
             Mean action noise std: 0.89
                       Mean reward: 6664.02
               Mean episode length: 338.64
                 Mean success rate: 60.00
                  Mean reward/step: 21.98
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 12771328
                    Iteration time: 2.66s
                        Total time: 4150.38s
                               ETA: 528293.6s

################################################################################
                    [1m Learning iteration 1559/200000 [0m

                       Computation: 3025 steps/s (collection: 0.571s, learning 2.137s)
               Value function loss: 72542.2322
                    Surrogate loss: 0.0131
             Mean action noise std: 0.89
                       Mean reward: 6342.20
               Mean episode length: 331.49
                 Mean success rate: 57.50
                  Mean reward/step: 21.56
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 2.71s
                        Total time: 4153.09s
                               ETA: 528296.8s

################################################################################
                    [1m Learning iteration 1560/200000 [0m

                       Computation: 3056 steps/s (collection: 0.542s, learning 2.138s)
               Value function loss: 72351.6307
                    Surrogate loss: 0.0119
             Mean action noise std: 0.89
                       Mean reward: 6085.04
               Mean episode length: 315.70
                 Mean success rate: 55.00
                  Mean reward/step: 22.22
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12787712
                    Iteration time: 2.68s
                        Total time: 4155.77s
                               ETA: 528296.4s

################################################################################
                    [1m Learning iteration 1561/200000 [0m

                       Computation: 3009 steps/s (collection: 0.568s, learning 2.154s)
               Value function loss: 77135.3553
                    Surrogate loss: 0.0131
             Mean action noise std: 0.89
                       Mean reward: 6240.73
               Mean episode length: 320.27
                 Mean success rate: 56.50
                  Mean reward/step: 22.74
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12795904
                    Iteration time: 2.72s
                        Total time: 4158.49s
                               ETA: 528301.3s

################################################################################
                    [1m Learning iteration 1562/200000 [0m

                       Computation: 2978 steps/s (collection: 0.617s, learning 2.133s)
               Value function loss: 105995.2593
                    Surrogate loss: 0.0133
             Mean action noise std: 0.89
                       Mean reward: 6826.42
               Mean episode length: 330.44
                 Mean success rate: 60.50
                  Mean reward/step: 23.12
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12804096
                    Iteration time: 2.75s
                        Total time: 4161.24s
                               ETA: 528309.8s

################################################################################
                    [1m Learning iteration 1563/200000 [0m

                       Computation: 3046 steps/s (collection: 0.544s, learning 2.145s)
               Value function loss: 74416.3616
                    Surrogate loss: 0.0116
             Mean action noise std: 0.89
                       Mean reward: 7026.49
               Mean episode length: 339.88
                 Mean success rate: 61.50
                  Mean reward/step: 22.05
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 12812288
                    Iteration time: 2.69s
                        Total time: 4163.93s
                               ETA: 528310.5s

################################################################################
                    [1m Learning iteration 1564/200000 [0m

                       Computation: 3124 steps/s (collection: 0.536s, learning 2.085s)
               Value function loss: 73504.7183
                    Surrogate loss: 0.0141
             Mean action noise std: 0.88
                       Mean reward: 7193.52
               Mean episode length: 346.29
                 Mean success rate: 63.00
                  Mean reward/step: 22.12
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12820480
                    Iteration time: 2.62s
                        Total time: 4166.55s
                               ETA: 528302.7s

################################################################################
                    [1m Learning iteration 1565/200000 [0m

                       Computation: 3070 steps/s (collection: 0.587s, learning 2.081s)
               Value function loss: 108743.5418
                    Surrogate loss: 0.0119
             Mean action noise std: 0.88
                       Mean reward: 7708.76
               Mean episode length: 360.77
                 Mean success rate: 66.50
                  Mean reward/step: 21.35
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 12828672
                    Iteration time: 2.67s
                        Total time: 4169.22s
                               ETA: 528300.7s

################################################################################
                    [1m Learning iteration 1566/200000 [0m

                       Computation: 3148 steps/s (collection: 0.543s, learning 2.059s)
               Value function loss: 93458.7582
                    Surrogate loss: 0.0159
             Mean action noise std: 0.88
                       Mean reward: 7884.41
               Mean episode length: 364.02
                 Mean success rate: 67.00
                  Mean reward/step: 21.63
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 12836864
                    Iteration time: 2.60s
                        Total time: 4171.82s
                               ETA: 528290.4s

################################################################################
                    [1m Learning iteration 1567/200000 [0m

                       Computation: 3171 steps/s (collection: 0.522s, learning 2.061s)
               Value function loss: 60039.0303
                    Surrogate loss: 0.0149
             Mean action noise std: 0.88
                       Mean reward: 7950.72
               Mean episode length: 367.13
                 Mean success rate: 68.00
                  Mean reward/step: 21.86
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12845056
                    Iteration time: 2.58s
                        Total time: 4174.40s
                               ETA: 528277.6s

################################################################################
                    [1m Learning iteration 1568/200000 [0m

                       Computation: 3106 steps/s (collection: 0.540s, learning 2.096s)
               Value function loss: 75166.0703
                    Surrogate loss: 0.0109
             Mean action noise std: 0.88
                       Mean reward: 8301.28
               Mean episode length: 372.50
                 Mean success rate: 70.00
                  Mean reward/step: 22.33
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12853248
                    Iteration time: 2.64s
                        Total time: 4177.04s
                               ETA: 528271.8s

################################################################################
                    [1m Learning iteration 1569/200000 [0m

                       Computation: 3095 steps/s (collection: 0.560s, learning 2.086s)
               Value function loss: 63933.7826
                    Surrogate loss: 0.0122
             Mean action noise std: 0.88
                       Mean reward: 8446.85
               Mean episode length: 379.48
                 Mean success rate: 71.50
                  Mean reward/step: 23.21
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 12861440
                    Iteration time: 2.65s
                        Total time: 4179.69s
                               ETA: 528267.1s

################################################################################
                    [1m Learning iteration 1570/200000 [0m

                       Computation: 3091 steps/s (collection: 0.554s, learning 2.096s)
               Value function loss: 90435.2905
                    Surrogate loss: 0.0112
             Mean action noise std: 0.88
                       Mean reward: 8408.97
               Mean episode length: 378.77
                 Mean success rate: 71.50
                  Mean reward/step: 24.22
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 12869632
                    Iteration time: 2.65s
                        Total time: 4182.34s
                               ETA: 528262.8s

################################################################################
                    [1m Learning iteration 1571/200000 [0m

                       Computation: 3168 steps/s (collection: 0.530s, learning 2.056s)
               Value function loss: 73439.0828
                    Surrogate loss: 0.0133
             Mean action noise std: 0.88
                       Mean reward: 8371.92
               Mean episode length: 377.85
                 Mean success rate: 71.00
                  Mean reward/step: 23.67
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 2.59s
                        Total time: 4184.92s
                               ETA: 528250.4s

################################################################################
                    [1m Learning iteration 1572/200000 [0m

                       Computation: 3159 steps/s (collection: 0.527s, learning 2.066s)
               Value function loss: 80318.7466
                    Surrogate loss: 0.0127
             Mean action noise std: 0.89
                       Mean reward: 8378.95
               Mean episode length: 377.64
                 Mean success rate: 71.50
                  Mean reward/step: 22.61
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 12886016
                    Iteration time: 2.59s
                        Total time: 4187.51s
                               ETA: 528239.0s

################################################################################
                    [1m Learning iteration 1573/200000 [0m

                       Computation: 3093 steps/s (collection: 0.567s, learning 2.081s)
               Value function loss: 98851.0895
                    Surrogate loss: 0.0132
             Mean action noise std: 0.89
                       Mean reward: 8308.06
               Mean episode length: 370.12
                 Mean success rate: 70.50
                  Mean reward/step: 22.36
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 12894208
                    Iteration time: 2.65s
                        Total time: 4190.16s
                               ETA: 528234.5s

################################################################################
                    [1m Learning iteration 1574/200000 [0m

                       Computation: 3076 steps/s (collection: 0.592s, learning 2.070s)
               Value function loss: 108349.8229
                    Surrogate loss: 0.0146
             Mean action noise std: 0.89
                       Mean reward: 8381.03
               Mean episode length: 369.65
                 Mean success rate: 70.50
                  Mean reward/step: 21.83
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 12902400
                    Iteration time: 2.66s
                        Total time: 4192.82s
                               ETA: 528231.9s

################################################################################
                    [1m Learning iteration 1575/200000 [0m

                       Computation: 3081 steps/s (collection: 0.552s, learning 2.107s)
               Value function loss: 76163.5608
                    Surrogate loss: 0.0182
             Mean action noise std: 0.88
                       Mean reward: 8409.35
               Mean episode length: 372.81
                 Mean success rate: 71.50
                  Mean reward/step: 21.56
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 12910592
                    Iteration time: 2.66s
                        Total time: 4195.48s
                               ETA: 528228.8s

################################################################################
                    [1m Learning iteration 1576/200000 [0m

                       Computation: 3111 steps/s (collection: 0.545s, learning 2.088s)
               Value function loss: 78561.6223
                    Surrogate loss: 0.0166
             Mean action noise std: 0.88
                       Mean reward: 8299.94
               Mean episode length: 368.95
                 Mean success rate: 71.00
                  Mean reward/step: 21.18
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 12918784
                    Iteration time: 2.63s
                        Total time: 4198.12s
                               ETA: 528222.4s

################################################################################
                    [1m Learning iteration 1577/200000 [0m

                       Computation: 3016 steps/s (collection: 0.608s, learning 2.108s)
               Value function loss: 100947.1154
                    Surrogate loss: 0.0100
             Mean action noise std: 0.88
                       Mean reward: 8015.00
               Mean episode length: 358.31
                 Mean success rate: 69.50
                  Mean reward/step: 20.69
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12926976
                    Iteration time: 2.72s
                        Total time: 4200.83s
                               ETA: 528226.5s

################################################################################
                    [1m Learning iteration 1578/200000 [0m

                       Computation: 3117 steps/s (collection: 0.549s, learning 2.079s)
               Value function loss: 87055.5654
                    Surrogate loss: 0.0086
             Mean action noise std: 0.88
                       Mean reward: 8146.29
               Mean episode length: 363.95
                 Mean success rate: 70.50
                  Mean reward/step: 20.29
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 12935168
                    Iteration time: 2.63s
                        Total time: 4203.46s
                               ETA: 528219.6s

################################################################################
                    [1m Learning iteration 1579/200000 [0m

                       Computation: 3068 steps/s (collection: 0.534s, learning 2.136s)
               Value function loss: 67452.9675
                    Surrogate loss: 0.0139
             Mean action noise std: 0.88
                       Mean reward: 7972.09
               Mean episode length: 360.40
                 Mean success rate: 69.00
                  Mean reward/step: 20.50
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 12943360
                    Iteration time: 2.67s
                        Total time: 4206.13s
                               ETA: 528217.9s

################################################################################
                    [1m Learning iteration 1580/200000 [0m

                       Computation: 3005 steps/s (collection: 0.595s, learning 2.131s)
               Value function loss: 104335.5691
                    Surrogate loss: 0.0116
             Mean action noise std: 0.88
                       Mean reward: 7736.50
               Mean episode length: 353.75
                 Mean success rate: 67.00
                  Mean reward/step: 20.88
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 12951552
                    Iteration time: 2.73s
                        Total time: 4208.85s
                               ETA: 528223.2s

################################################################################
                    [1m Learning iteration 1581/200000 [0m

                       Computation: 3072 steps/s (collection: 0.558s, learning 2.108s)
               Value function loss: 97686.8928
                    Surrogate loss: 0.0109
             Mean action noise std: 0.88
                       Mean reward: 7924.21
               Mean episode length: 358.60
                 Mean success rate: 68.50
                  Mean reward/step: 20.30
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 12959744
                    Iteration time: 2.67s
                        Total time: 4211.52s
                               ETA: 528221.1s

################################################################################
                    [1m Learning iteration 1582/200000 [0m

                       Computation: 3090 steps/s (collection: 0.570s, learning 2.081s)
               Value function loss: 108528.9862
                    Surrogate loss: 0.0122
             Mean action noise std: 0.88
                       Mean reward: 7995.66
               Mean episode length: 363.06
                 Mean success rate: 69.50
                  Mean reward/step: 18.80
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 12967936
                    Iteration time: 2.65s
                        Total time: 4214.17s
                               ETA: 528217.0s

################################################################################
                    [1m Learning iteration 1583/200000 [0m

                       Computation: 3113 steps/s (collection: 0.528s, learning 2.103s)
               Value function loss: 54900.4962
                    Surrogate loss: 0.0114
             Mean action noise std: 0.88
                       Mean reward: 7887.22
               Mean episode length: 360.60
                 Mean success rate: 69.50
                  Mean reward/step: 19.44
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 2.63s
                        Total time: 4216.80s
                               ETA: 528210.5s

################################################################################
                    [1m Learning iteration 1584/200000 [0m

                       Computation: 3143 steps/s (collection: 0.517s, learning 2.089s)
               Value function loss: 49001.0734
                    Surrogate loss: 0.0126
             Mean action noise std: 0.88
                       Mean reward: 7714.41
               Mean episode length: 354.39
                 Mean success rate: 68.00
                  Mean reward/step: 21.31
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 12984320
                    Iteration time: 2.61s
                        Total time: 4219.41s
                               ETA: 528200.8s

################################################################################
                    [1m Learning iteration 1585/200000 [0m

                       Computation: 3094 steps/s (collection: 0.556s, learning 2.091s)
               Value function loss: 81291.3979
                    Surrogate loss: 0.0112
             Mean action noise std: 0.88
                       Mean reward: 7896.35
               Mean episode length: 359.19
                 Mean success rate: 69.00
                  Mean reward/step: 22.16
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 12992512
                    Iteration time: 2.65s
                        Total time: 4222.06s
                               ETA: 528196.3s

################################################################################
                    [1m Learning iteration 1586/200000 [0m

                       Computation: 3086 steps/s (collection: 0.566s, learning 2.088s)
               Value function loss: 66772.9734
                    Surrogate loss: 0.0106
             Mean action noise std: 0.88
                       Mean reward: 7772.41
               Mean episode length: 354.81
                 Mean success rate: 67.00
                  Mean reward/step: 21.49
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13000704
                    Iteration time: 2.65s
                        Total time: 4224.71s
                               ETA: 528192.7s

################################################################################
                    [1m Learning iteration 1587/200000 [0m

                       Computation: 3058 steps/s (collection: 0.556s, learning 2.122s)
               Value function loss: 109000.0177
                    Surrogate loss: 0.0113
             Mean action noise std: 0.88
                       Mean reward: 8084.18
               Mean episode length: 364.71
                 Mean success rate: 69.00
                  Mean reward/step: 21.22
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13008896
                    Iteration time: 2.68s
                        Total time: 4227.39s
                               ETA: 528192.1s

################################################################################
                    [1m Learning iteration 1588/200000 [0m

                       Computation: 3046 steps/s (collection: 0.558s, learning 2.132s)
               Value function loss: 74844.5052
                    Surrogate loss: 0.0135
             Mean action noise std: 0.88
                       Mean reward: 7863.44
               Mean episode length: 363.21
                 Mean success rate: 68.50
                  Mean reward/step: 20.24
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13017088
                    Iteration time: 2.69s
                        Total time: 4230.08s
                               ETA: 528192.8s

################################################################################
                    [1m Learning iteration 1589/200000 [0m

                       Computation: 2991 steps/s (collection: 0.606s, learning 2.132s)
               Value function loss: 85231.1311
                    Surrogate loss: 0.0101
             Mean action noise std: 0.88
                       Mean reward: 7551.63
               Mean episode length: 355.43
                 Mean success rate: 66.00
                  Mean reward/step: 20.22
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 13025280
                    Iteration time: 2.74s
                        Total time: 4232.82s
                               ETA: 528199.7s

################################################################################
                    [1m Learning iteration 1590/200000 [0m

                       Computation: 3060 steps/s (collection: 0.572s, learning 2.104s)
               Value function loss: 86931.6462
                    Surrogate loss: 0.0121
             Mean action noise std: 0.88
                       Mean reward: 7649.90
               Mean episode length: 363.05
                 Mean success rate: 67.00
                  Mean reward/step: 19.53
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13033472
                    Iteration time: 2.68s
                        Total time: 4235.49s
                               ETA: 528198.8s

################################################################################
                    [1m Learning iteration 1591/200000 [0m

                       Computation: 3051 steps/s (collection: 0.547s, learning 2.137s)
               Value function loss: 75871.3995
                    Surrogate loss: 0.0154
             Mean action noise std: 0.88
                       Mean reward: 7009.91
               Mean episode length: 346.28
                 Mean success rate: 62.50
                  Mean reward/step: 19.71
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 13041664
                    Iteration time: 2.68s
                        Total time: 4238.18s
                               ETA: 528198.9s

################################################################################
                    [1m Learning iteration 1592/200000 [0m

                       Computation: 2949 steps/s (collection: 0.638s, learning 2.140s)
               Value function loss: 86317.1114
                    Surrogate loss: 0.0148
             Mean action noise std: 0.88
                       Mean reward: 7172.64
               Mean episode length: 350.34
                 Mean success rate: 64.00
                  Mean reward/step: 19.96
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13049856
                    Iteration time: 2.78s
                        Total time: 4240.95s
                               ETA: 528210.5s

################################################################################
                    [1m Learning iteration 1593/200000 [0m

                       Computation: 3047 steps/s (collection: 0.595s, learning 2.093s)
               Value function loss: 79771.8945
                    Surrogate loss: 0.0132
             Mean action noise std: 0.88
                       Mean reward: 6979.04
               Mean episode length: 347.57
                 Mean success rate: 63.00
                  Mean reward/step: 19.82
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13058048
                    Iteration time: 2.69s
                        Total time: 4243.64s
                               ETA: 528211.1s

################################################################################
                    [1m Learning iteration 1594/200000 [0m

                       Computation: 3140 steps/s (collection: 0.578s, learning 2.031s)
               Value function loss: 65227.2699
                    Surrogate loss: 0.0132
             Mean action noise std: 0.88
                       Mean reward: 6971.19
               Mean episode length: 347.30
                 Mean success rate: 63.50
                  Mean reward/step: 20.05
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13066240
                    Iteration time: 2.61s
                        Total time: 4246.25s
                               ETA: 528201.7s

################################################################################
                    [1m Learning iteration 1595/200000 [0m

                       Computation: 3184 steps/s (collection: 0.525s, learning 2.047s)
               Value function loss: 48422.2499
                    Surrogate loss: 0.0125
             Mean action noise std: 0.88
                       Mean reward: 6184.74
               Mean episode length: 319.36
                 Mean success rate: 57.00
                  Mean reward/step: 20.79
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 2.57s
                        Total time: 4248.82s
                               ETA: 528187.9s

################################################################################
                    [1m Learning iteration 1596/200000 [0m

                       Computation: 3165 steps/s (collection: 0.564s, learning 2.024s)
               Value function loss: 126366.3588
                    Surrogate loss: 0.0123
             Mean action noise std: 0.88
                       Mean reward: 6611.07
               Mean episode length: 336.15
                 Mean success rate: 61.00
                  Mean reward/step: 20.99
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 13082624
                    Iteration time: 2.59s
                        Total time: 4251.41s
                               ETA: 528176.0s

################################################################################
                    [1m Learning iteration 1597/200000 [0m

                       Computation: 3230 steps/s (collection: 0.511s, learning 2.025s)
               Value function loss: 83287.5752
                    Surrogate loss: 0.0117
             Mean action noise std: 0.88
                       Mean reward: 6858.85
               Mean episode length: 345.51
                 Mean success rate: 63.00
                  Mean reward/step: 20.91
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 13090816
                    Iteration time: 2.54s
                        Total time: 4253.95s
                               ETA: 528157.7s

################################################################################
                    [1m Learning iteration 1598/200000 [0m

                       Computation: 3242 steps/s (collection: 0.511s, learning 2.015s)
               Value function loss: 67974.8319
                    Surrogate loss: 0.0110
             Mean action noise std: 0.88
                       Mean reward: 6900.42
               Mean episode length: 342.35
                 Mean success rate: 62.50
                  Mean reward/step: 21.13
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13099008
                    Iteration time: 2.53s
                        Total time: 4256.47s
                               ETA: 528138.2s

################################################################################
                    [1m Learning iteration 1599/200000 [0m

                       Computation: 3219 steps/s (collection: 0.511s, learning 2.034s)
               Value function loss: 49701.1714
                    Surrogate loss: 0.0100
             Mean action noise std: 0.88
                       Mean reward: 6646.25
               Mean episode length: 335.96
                 Mean success rate: 60.00
                  Mean reward/step: 22.29
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13107200
                    Iteration time: 2.54s
                        Total time: 4259.02s
                               ETA: 528121.0s

################################################################################
                    [1m Learning iteration 1600/200000 [0m

                       Computation: 3157 steps/s (collection: 0.540s, learning 2.054s)
               Value function loss: 53063.2768
                    Surrogate loss: 0.0100
             Mean action noise std: 0.88
                       Mean reward: 6801.33
               Mean episode length: 340.77
                 Mean success rate: 61.00
                  Mean reward/step: 23.24
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13115392
                    Iteration time: 2.59s
                        Total time: 4261.61s
                               ETA: 528110.0s

################################################################################
                    [1m Learning iteration 1601/200000 [0m

                       Computation: 3168 steps/s (collection: 0.522s, learning 2.064s)
               Value function loss: 92373.5458
                    Surrogate loss: 0.0132
             Mean action noise std: 0.88
                       Mean reward: 7077.35
               Mean episode length: 348.90
                 Mean success rate: 62.00
                  Mean reward/step: 23.32
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13123584
                    Iteration time: 2.59s
                        Total time: 4264.20s
                               ETA: 528097.9s

################################################################################
                    [1m Learning iteration 1602/200000 [0m

                       Computation: 3092 steps/s (collection: 0.554s, learning 2.095s)
               Value function loss: 54889.6431
                    Surrogate loss: 0.0116
             Mean action noise std: 0.88
                       Mean reward: 6850.63
               Mean episode length: 340.12
                 Mean success rate: 60.50
                  Mean reward/step: 23.93
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13131776
                    Iteration time: 2.65s
                        Total time: 4266.85s
                               ETA: 528093.6s

################################################################################
                    [1m Learning iteration 1603/200000 [0m

                       Computation: 3122 steps/s (collection: 0.549s, learning 2.075s)
               Value function loss: 78260.4657
                    Surrogate loss: 0.0149
             Mean action noise std: 0.88
                       Mean reward: 7065.40
               Mean episode length: 344.94
                 Mean success rate: 61.50
                  Mean reward/step: 23.66
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13139968
                    Iteration time: 2.62s
                        Total time: 4269.47s
                               ETA: 528086.2s

################################################################################
                    [1m Learning iteration 1604/200000 [0m

                       Computation: 3107 steps/s (collection: 0.550s, learning 2.086s)
               Value function loss: 112631.0910
                    Surrogate loss: 0.0131
             Mean action noise std: 0.88
                       Mean reward: 7688.45
               Mean episode length: 362.46
                 Mean success rate: 66.00
                  Mean reward/step: 23.50
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13148160
                    Iteration time: 2.64s
                        Total time: 4272.11s
                               ETA: 528080.4s

################################################################################
                    [1m Learning iteration 1605/200000 [0m

                       Computation: 3194 steps/s (collection: 0.526s, learning 2.039s)
               Value function loss: 80642.7058
                    Surrogate loss: 0.0138
             Mean action noise std: 0.88
                       Mean reward: 7932.92
               Mean episode length: 367.43
                 Mean success rate: 69.00
                  Mean reward/step: 21.99
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13156352
                    Iteration time: 2.56s
                        Total time: 4274.67s
                               ETA: 528065.7s

################################################################################
                    [1m Learning iteration 1606/200000 [0m

                       Computation: 3173 steps/s (collection: 0.523s, learning 2.058s)
               Value function loss: 79509.7352
                    Surrogate loss: 0.0124
             Mean action noise std: 0.88
                       Mean reward: 8170.98
               Mean episode length: 371.39
                 Mean success rate: 70.00
                  Mean reward/step: 22.01
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13164544
                    Iteration time: 2.58s
                        Total time: 4277.25s
                               ETA: 528053.1s

################################################################################
                    [1m Learning iteration 1607/200000 [0m

                       Computation: 3147 steps/s (collection: 0.542s, learning 2.060s)
               Value function loss: 83125.6673
                    Surrogate loss: 0.0122
             Mean action noise std: 0.88
                       Mean reward: 8024.10
               Mean episode length: 368.93
                 Mean success rate: 69.50
                  Mean reward/step: 21.74
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 2.60s
                        Total time: 4279.86s
                               ETA: 528043.1s

################################################################################
                    [1m Learning iteration 1608/200000 [0m

                       Computation: 2988 steps/s (collection: 0.658s, learning 2.083s)
               Value function loss: 102479.4426
                    Surrogate loss: 0.0119
             Mean action noise std: 0.88
                       Mean reward: 8146.12
               Mean episode length: 370.57
                 Mean success rate: 72.00
                  Mean reward/step: 21.08
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 13180928
                    Iteration time: 2.74s
                        Total time: 4282.60s
                               ETA: 528050.2s

################################################################################
                    [1m Learning iteration 1609/200000 [0m

                       Computation: 3244 steps/s (collection: 0.491s, learning 2.034s)
               Value function loss: 89887.5788
                    Surrogate loss: 0.0130
             Mean action noise std: 0.88
                       Mean reward: 8248.40
               Mean episode length: 371.08
                 Mean success rate: 73.00
                  Mean reward/step: 20.56
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 13189120
                    Iteration time: 2.52s
                        Total time: 4285.12s
                               ETA: 528030.7s

################################################################################
                    [1m Learning iteration 1610/200000 [0m

                       Computation: 3262 steps/s (collection: 0.497s, learning 2.013s)
               Value function loss: 107586.9840
                    Surrogate loss: 0.0147
             Mean action noise std: 0.88
                       Mean reward: 8414.22
               Mean episode length: 374.91
                 Mean success rate: 74.00
                  Mean reward/step: 19.92
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13197312
                    Iteration time: 2.51s
                        Total time: 4287.63s
                               ETA: 528009.5s

################################################################################
                    [1m Learning iteration 1611/200000 [0m

                       Computation: 3166 steps/s (collection: 0.562s, learning 2.025s)
               Value function loss: 99157.2979
                    Surrogate loss: 0.0150
             Mean action noise std: 0.88
                       Mean reward: 8553.79
               Mean episode length: 380.80
                 Mean success rate: 75.00
                  Mean reward/step: 19.25
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13205504
                    Iteration time: 2.59s
                        Total time: 4290.22s
                               ETA: 527997.6s

################################################################################
                    [1m Learning iteration 1612/200000 [0m

                       Computation: 3266 steps/s (collection: 0.462s, learning 2.045s)
               Value function loss: 77150.6677
                    Surrogate loss: 0.0129
             Mean action noise std: 0.88
                       Mean reward: 8317.38
               Mean episode length: 375.46
                 Mean success rate: 73.00
                  Mean reward/step: 19.15
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13213696
                    Iteration time: 2.51s
                        Total time: 4292.73s
                               ETA: 527976.0s

################################################################################
                    [1m Learning iteration 1613/200000 [0m

                       Computation: 3237 steps/s (collection: 0.500s, learning 2.031s)
               Value function loss: 83752.7774
                    Surrogate loss: 0.0137
             Mean action noise std: 0.88
                       Mean reward: 7934.65
               Mean episode length: 364.19
                 Mean success rate: 70.00
                  Mean reward/step: 19.67
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 13221888
                    Iteration time: 2.53s
                        Total time: 4295.26s
                               ETA: 527957.3s

################################################################################
                    [1m Learning iteration 1614/200000 [0m

                       Computation: 3205 steps/s (collection: 0.497s, learning 2.059s)
               Value function loss: 73415.7058
                    Surrogate loss: 0.0145
             Mean action noise std: 0.88
                       Mean reward: 7932.45
               Mean episode length: 365.20
                 Mean success rate: 69.50
                  Mean reward/step: 19.78
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13230080
                    Iteration time: 2.56s
                        Total time: 4297.81s
                               ETA: 527941.7s

################################################################################
                    [1m Learning iteration 1615/200000 [0m

                       Computation: 3182 steps/s (collection: 0.551s, learning 2.023s)
               Value function loss: 57433.1126
                    Surrogate loss: 0.0157
             Mean action noise std: 0.88
                       Mean reward: 7382.58
               Mean episode length: 341.16
                 Mean success rate: 64.00
                  Mean reward/step: 20.82
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13238272
                    Iteration time: 2.57s
                        Total time: 4300.39s
                               ETA: 527928.3s

################################################################################
                    [1m Learning iteration 1616/200000 [0m

                       Computation: 3073 steps/s (collection: 0.535s, learning 2.131s)
               Value function loss: 72967.1478
                    Surrogate loss: 0.0125
             Mean action noise std: 0.88
                       Mean reward: 7342.59
               Mean episode length: 338.94
                 Mean success rate: 63.00
                  Mean reward/step: 21.20
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13246464
                    Iteration time: 2.67s
                        Total time: 4303.05s
                               ETA: 527926.2s

################################################################################
                    [1m Learning iteration 1617/200000 [0m

                       Computation: 3076 steps/s (collection: 0.582s, learning 2.080s)
               Value function loss: 95679.3847
                    Surrogate loss: 0.0157
             Mean action noise std: 0.88
                       Mean reward: 7143.76
               Mean episode length: 335.49
                 Mean success rate: 62.50
                  Mean reward/step: 21.20
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 13254656
                    Iteration time: 2.66s
                        Total time: 4305.71s
                               ETA: 527923.7s

################################################################################
                    [1m Learning iteration 1618/200000 [0m

                       Computation: 3090 steps/s (collection: 0.580s, learning 2.070s)
               Value function loss: 74217.5736
                    Surrogate loss: 0.0125
             Mean action noise std: 0.88
                       Mean reward: 6821.03
               Mean episode length: 328.80
                 Mean success rate: 61.00
                  Mean reward/step: 21.20
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13262848
                    Iteration time: 2.65s
                        Total time: 4308.36s
                               ETA: 527919.7s

################################################################################
                    [1m Learning iteration 1619/200000 [0m

                       Computation: 3123 steps/s (collection: 0.555s, learning 2.068s)
               Value function loss: 73488.5661
                    Surrogate loss: 0.0134
             Mean action noise std: 0.88
                       Mean reward: 6590.39
               Mean episode length: 320.90
                 Mean success rate: 59.50
                  Mean reward/step: 20.99
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 2.62s
                        Total time: 4310.99s
                               ETA: 527912.4s

################################################################################
                    [1m Learning iteration 1620/200000 [0m

                       Computation: 3079 steps/s (collection: 0.583s, learning 2.077s)
               Value function loss: 72566.9435
                    Surrogate loss: 0.0147
             Mean action noise std: 0.88
                       Mean reward: 6649.66
               Mean episode length: 321.25
                 Mean success rate: 61.00
                  Mean reward/step: 21.03
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13279232
                    Iteration time: 2.66s
                        Total time: 4313.65s
                               ETA: 527909.6s

################################################################################
                    [1m Learning iteration 1621/200000 [0m

                       Computation: 3092 steps/s (collection: 0.543s, learning 2.106s)
               Value function loss: 66621.0793
                    Surrogate loss: 0.0113
             Mean action noise std: 0.88
                       Mean reward: 6890.26
               Mean episode length: 326.57
                 Mean success rate: 62.00
                  Mean reward/step: 22.03
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13287424
                    Iteration time: 2.65s
                        Total time: 4316.30s
                               ETA: 527905.4s

################################################################################
                    [1m Learning iteration 1622/200000 [0m

                       Computation: 3158 steps/s (collection: 0.508s, learning 2.086s)
               Value function loss: 68291.1551
                    Surrogate loss: 0.0134
             Mean action noise std: 0.88
                       Mean reward: 7027.07
               Mean episode length: 331.90
                 Mean success rate: 63.00
                  Mean reward/step: 22.22
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13295616
                    Iteration time: 2.59s
                        Total time: 4318.89s
                               ETA: 527894.5s

################################################################################
                    [1m Learning iteration 1623/200000 [0m

                       Computation: 2980 steps/s (collection: 0.602s, learning 2.146s)
               Value function loss: 83696.5015
                    Surrogate loss: 0.0113
             Mean action noise std: 0.88
                       Mean reward: 6961.46
               Mean episode length: 334.07
                 Mean success rate: 64.00
                  Mean reward/step: 22.28
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13303808
                    Iteration time: 2.75s
                        Total time: 4321.64s
                               ETA: 527902.6s

################################################################################
                    [1m Learning iteration 1624/200000 [0m

                       Computation: 3028 steps/s (collection: 0.591s, learning 2.114s)
               Value function loss: 83269.1807
                    Surrogate loss: 0.0092
             Mean action noise std: 0.88
                       Mean reward: 7268.55
               Mean episode length: 347.44
                 Mean success rate: 67.00
                  Mean reward/step: 22.46
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13312000
                    Iteration time: 2.71s
                        Total time: 4324.34s
                               ETA: 527905.3s

################################################################################
                    [1m Learning iteration 1625/200000 [0m

                       Computation: 3032 steps/s (collection: 0.563s, learning 2.138s)
               Value function loss: 76513.8461
                    Surrogate loss: 0.0153
             Mean action noise std: 0.88
                       Mean reward: 7477.31
               Mean episode length: 355.40
                 Mean success rate: 69.50
                  Mean reward/step: 21.79
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13320192
                    Iteration time: 2.70s
                        Total time: 4327.05s
                               ETA: 527907.6s

################################################################################
                    [1m Learning iteration 1626/200000 [0m

                       Computation: 2434 steps/s (collection: 0.582s, learning 2.783s)
               Value function loss: 102314.5919
                    Surrogate loss: 0.0138
             Mean action noise std: 0.88
                       Mean reward: 7746.23
               Mean episode length: 364.19
                 Mean success rate: 70.50
                  Mean reward/step: 21.38
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13328384
                    Iteration time: 3.36s
                        Total time: 4330.41s
                               ETA: 527990.7s

################################################################################
                    [1m Learning iteration 1627/200000 [0m

                       Computation: 2395 steps/s (collection: 0.955s, learning 2.465s)
               Value function loss: 106864.6236
                    Surrogate loss: 0.0164
             Mean action noise std: 0.88
                       Mean reward: 8230.84
               Mean episode length: 383.37
                 Mean success rate: 73.50
                  Mean reward/step: 21.08
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13336576
                    Iteration time: 3.42s
                        Total time: 4333.83s
                               ETA: 528080.4s

################################################################################
                    [1m Learning iteration 1628/200000 [0m

                       Computation: 3109 steps/s (collection: 0.554s, learning 2.081s)
               Value function loss: 72697.0871
                    Surrogate loss: 0.0138
             Mean action noise std: 0.88
                       Mean reward: 8216.99
               Mean episode length: 378.09
                 Mean success rate: 73.00
                  Mean reward/step: 20.34
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13344768
                    Iteration time: 2.63s
                        Total time: 4336.47s
                               ETA: 528074.5s

################################################################################
                    [1m Learning iteration 1629/200000 [0m

                       Computation: 3025 steps/s (collection: 0.534s, learning 2.174s)
               Value function loss: 73617.9944
                    Surrogate loss: 0.0129
             Mean action noise std: 0.88
                       Mean reward: 8297.58
               Mean episode length: 383.12
                 Mean success rate: 73.50
                  Mean reward/step: 20.66
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13352960
                    Iteration time: 2.71s
                        Total time: 4339.17s
                               ETA: 528077.4s

################################################################################
                    [1m Learning iteration 1630/200000 [0m

                       Computation: 2720 steps/s (collection: 0.703s, learning 2.309s)
               Value function loss: 87665.3304
                    Surrogate loss: 0.0155
             Mean action noise std: 0.88
                       Mean reward: 8228.61
               Mean episode length: 385.37
                 Mean success rate: 75.00
                  Mean reward/step: 21.19
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13361152
                    Iteration time: 3.01s
                        Total time: 4342.18s
                               ETA: 528117.2s

################################################################################
                    [1m Learning iteration 1631/200000 [0m

                       Computation: 2952 steps/s (collection: 0.685s, learning 2.089s)
               Value function loss: 61918.2687
                    Surrogate loss: 0.0115
             Mean action noise std: 0.88
                       Mean reward: 7976.11
               Mean episode length: 375.36
                 Mean success rate: 73.00
                  Mean reward/step: 21.09
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 2.77s
                        Total time: 4344.96s
                               ETA: 528128.2s

################################################################################
                    [1m Learning iteration 1632/200000 [0m

                       Computation: 3062 steps/s (collection: 0.580s, learning 2.095s)
               Value function loss: 89659.2517
                    Surrogate loss: 0.0143
             Mean action noise std: 0.88
                       Mean reward: 7633.15
               Mean episode length: 360.95
                 Mean success rate: 70.00
                  Mean reward/step: 21.02
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 13377536
                    Iteration time: 2.68s
                        Total time: 4347.63s
                               ETA: 528127.0s

################################################################################
                    [1m Learning iteration 1633/200000 [0m

                       Computation: 3080 steps/s (collection: 0.561s, learning 2.098s)
               Value function loss: 83132.8842
                    Surrogate loss: 0.0169
             Mean action noise std: 0.88
                       Mean reward: 7817.22
               Mean episode length: 366.82
                 Mean success rate: 71.00
                  Mean reward/step: 20.14
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13385728
                    Iteration time: 2.66s
                        Total time: 4350.29s
                               ETA: 528124.0s

################################################################################
                    [1m Learning iteration 1634/200000 [0m

                       Computation: 3099 steps/s (collection: 0.521s, learning 2.122s)
               Value function loss: 81048.6392
                    Surrogate loss: 0.0103
             Mean action noise std: 0.88
                       Mean reward: 7775.54
               Mean episode length: 370.28
                 Mean success rate: 71.00
                  Mean reward/step: 20.01
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13393920
                    Iteration time: 2.64s
                        Total time: 4352.94s
                               ETA: 528119.0s

################################################################################
                    [1m Learning iteration 1635/200000 [0m

                       Computation: 3057 steps/s (collection: 0.592s, learning 2.087s)
               Value function loss: 79517.5412
                    Surrogate loss: 0.0122
             Mean action noise std: 0.88
                       Mean reward: 7360.86
               Mean episode length: 359.59
                 Mean success rate: 68.00
                  Mean reward/step: 20.68
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13402112
                    Iteration time: 2.68s
                        Total time: 4355.62s
                               ETA: 528118.4s

################################################################################
                    [1m Learning iteration 1636/200000 [0m

                       Computation: 3128 steps/s (collection: 0.535s, learning 2.084s)
               Value function loss: 61880.3983
                    Surrogate loss: 0.0121
             Mean action noise std: 0.88
                       Mean reward: 7089.55
               Mean episode length: 347.56
                 Mean success rate: 65.50
                  Mean reward/step: 20.83
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13410304
                    Iteration time: 2.62s
                        Total time: 4358.23s
                               ETA: 528110.4s

################################################################################
                    [1m Learning iteration 1637/200000 [0m

                       Computation: 3231 steps/s (collection: 0.509s, learning 2.026s)
               Value function loss: 69358.9020
                    Surrogate loss: 0.0108
             Mean action noise std: 0.88
                       Mean reward: 6683.19
               Mean episode length: 332.57
                 Mean success rate: 61.50
                  Mean reward/step: 21.43
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13418496
                    Iteration time: 2.54s
                        Total time: 4360.77s
                               ETA: 528092.3s

################################################################################
                    [1m Learning iteration 1638/200000 [0m

                       Computation: 3032 steps/s (collection: 0.618s, learning 2.083s)
               Value function loss: 90570.3696
                    Surrogate loss: 0.0146
             Mean action noise std: 0.88
                       Mean reward: 6873.13
               Mean episode length: 332.79
                 Mean success rate: 61.00
                  Mean reward/step: 21.42
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13426688
                    Iteration time: 2.70s
                        Total time: 4363.47s
                               ETA: 528094.4s

################################################################################
                    [1m Learning iteration 1639/200000 [0m

                       Computation: 3096 steps/s (collection: 0.597s, learning 2.049s)
               Value function loss: 83198.5071
                    Surrogate loss: 0.0136
             Mean action noise std: 0.88
                       Mean reward: 7345.79
               Mean episode length: 345.48
                 Mean success rate: 64.00
                  Mean reward/step: 21.52
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13434880
                    Iteration time: 2.65s
                        Total time: 4366.12s
                               ETA: 528089.7s

################################################################################
                    [1m Learning iteration 1640/200000 [0m

                       Computation: 3235 steps/s (collection: 0.483s, learning 2.049s)
               Value function loss: 74486.7211
                    Surrogate loss: 0.0121
             Mean action noise std: 0.88
                       Mean reward: 7393.47
               Mean episode length: 348.65
                 Mean success rate: 63.50
                  Mean reward/step: 21.49
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13443072
                    Iteration time: 2.53s
                        Total time: 4368.65s
                               ETA: 528071.4s

################################################################################
                    [1m Learning iteration 1641/200000 [0m

                       Computation: 3269 steps/s (collection: 0.475s, learning 2.031s)
               Value function loss: 83908.7963
                    Surrogate loss: 0.0127
             Mean action noise std: 0.88
                       Mean reward: 6988.43
               Mean episode length: 335.94
                 Mean success rate: 60.00
                  Mean reward/step: 21.98
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13451264
                    Iteration time: 2.51s
                        Total time: 4371.15s
                               ETA: 528049.8s

################################################################################
                    [1m Learning iteration 1642/200000 [0m

                       Computation: 3168 steps/s (collection: 0.486s, learning 2.100s)
               Value function loss: 80447.0312
                    Surrogate loss: 0.0139
             Mean action noise std: 0.88
                       Mean reward: 7037.38
               Mean episode length: 334.98
                 Mean success rate: 59.50
                  Mean reward/step: 22.13
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13459456
                    Iteration time: 2.59s
                        Total time: 4373.74s
                               ETA: 528037.9s

################################################################################
                    [1m Learning iteration 1643/200000 [0m

                       Computation: 3171 steps/s (collection: 0.529s, learning 2.054s)
               Value function loss: 103905.8796
                    Surrogate loss: 0.0120
             Mean action noise std: 0.88
                       Mean reward: 7192.38
               Mean episode length: 337.61
                 Mean success rate: 60.50
                  Mean reward/step: 21.44
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 2.58s
                        Total time: 4376.32s
                               ETA: 528025.7s

################################################################################
                    [1m Learning iteration 1644/200000 [0m

                       Computation: 3083 steps/s (collection: 0.549s, learning 2.108s)
               Value function loss: 90996.1061
                    Surrogate loss: 0.0113
             Mean action noise std: 0.88
                       Mean reward: 7430.96
               Mean episode length: 345.11
                 Mean success rate: 63.50
                  Mean reward/step: 21.02
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13475840
                    Iteration time: 2.66s
                        Total time: 4378.98s
                               ETA: 528022.4s

################################################################################
                    [1m Learning iteration 1645/200000 [0m

                       Computation: 3167 steps/s (collection: 0.528s, learning 2.058s)
               Value function loss: 50202.7543
                    Surrogate loss: 0.0127
             Mean action noise std: 0.88
                       Mean reward: 7167.91
               Mean episode length: 336.18
                 Mean success rate: 62.50
                  Mean reward/step: 21.36
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13484032
                    Iteration time: 2.59s
                        Total time: 4381.57s
                               ETA: 528010.6s

################################################################################
                    [1m Learning iteration 1646/200000 [0m

                       Computation: 3164 steps/s (collection: 0.510s, learning 2.079s)
               Value function loss: 74341.3574
                    Surrogate loss: 0.0120
             Mean action noise std: 0.88
                       Mean reward: 7558.36
               Mean episode length: 355.13
                 Mean success rate: 66.00
                  Mean reward/step: 22.59
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13492224
                    Iteration time: 2.59s
                        Total time: 4384.15s
                               ETA: 527999.1s

################################################################################
                    [1m Learning iteration 1647/200000 [0m

                       Computation: 3173 steps/s (collection: 0.519s, learning 2.062s)
               Value function loss: 105657.2154
                    Surrogate loss: 0.0118
             Mean action noise std: 0.88
                       Mean reward: 7768.41
               Mean episode length: 361.44
                 Mean success rate: 67.50
                  Mean reward/step: 23.06
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13500416
                    Iteration time: 2.58s
                        Total time: 4386.74s
                               ETA: 527986.8s

################################################################################
                    [1m Learning iteration 1648/200000 [0m

                       Computation: 3159 steps/s (collection: 0.542s, learning 2.051s)
               Value function loss: 79361.3634
                    Surrogate loss: 0.0121
             Mean action noise std: 0.88
                       Mean reward: 7563.36
               Mean episode length: 358.70
                 Mean success rate: 66.50
                  Mean reward/step: 22.19
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 13508608
                    Iteration time: 2.59s
                        Total time: 4389.33s
                               ETA: 527975.9s

################################################################################
                    [1m Learning iteration 1649/200000 [0m

                       Computation: 3192 steps/s (collection: 0.538s, learning 2.028s)
               Value function loss: 93697.4274
                    Surrogate loss: 0.0140
             Mean action noise std: 0.88
                       Mean reward: 7711.34
               Mean episode length: 363.77
                 Mean success rate: 68.00
                  Mean reward/step: 21.73
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13516800
                    Iteration time: 2.57s
                        Total time: 4391.89s
                               ETA: 527961.7s

################################################################################
                    [1m Learning iteration 1650/200000 [0m

                       Computation: 3160 steps/s (collection: 0.541s, learning 2.051s)
               Value function loss: 77289.3088
                    Surrogate loss: 0.0113
             Mean action noise std: 0.88
                       Mean reward: 7675.77
               Mean episode length: 362.85
                 Mean success rate: 67.00
                  Mean reward/step: 21.48
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13524992
                    Iteration time: 2.59s
                        Total time: 4394.49s
                               ETA: 527950.7s

################################################################################
                    [1m Learning iteration 1651/200000 [0m

                       Computation: 3171 steps/s (collection: 0.501s, learning 2.082s)
               Value function loss: 93798.7450
                    Surrogate loss: 0.0123
             Mean action noise std: 0.88
                       Mean reward: 7731.82
               Mean episode length: 364.08
                 Mean success rate: 68.00
                  Mean reward/step: 21.69
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 13533184
                    Iteration time: 2.58s
                        Total time: 4397.07s
                               ETA: 527938.5s

################################################################################
                    [1m Learning iteration 1652/200000 [0m

                       Computation: 3145 steps/s (collection: 0.527s, learning 2.077s)
               Value function loss: 77131.9746
                    Surrogate loss: 0.0124
             Mean action noise std: 0.88
                       Mean reward: 7602.33
               Mean episode length: 359.19
                 Mean success rate: 66.50
                  Mean reward/step: 22.08
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13541376
                    Iteration time: 2.60s
                        Total time: 4399.67s
                               ETA: 527929.0s

################################################################################
                    [1m Learning iteration 1653/200000 [0m

                       Computation: 2811 steps/s (collection: 0.639s, learning 2.275s)
               Value function loss: 74171.7211
                    Surrogate loss: 0.0123
             Mean action noise std: 0.88
                       Mean reward: 7652.60
               Mean episode length: 360.19
                 Mean success rate: 67.00
                  Mean reward/step: 22.91
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13549568
                    Iteration time: 2.91s
                        Total time: 4402.59s
                               ETA: 527956.5s

################################################################################
                    [1m Learning iteration 1654/200000 [0m

                       Computation: 3037 steps/s (collection: 0.625s, learning 2.072s)
               Value function loss: 77829.0338
                    Surrogate loss: 0.0120
             Mean action noise std: 0.88
                       Mean reward: 7965.30
               Mean episode length: 372.15
                 Mean success rate: 68.50
                  Mean reward/step: 23.32
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13557760
                    Iteration time: 2.70s
                        Total time: 4405.29s
                               ETA: 527958.1s

################################################################################
                    [1m Learning iteration 1655/200000 [0m

                       Computation: 3233 steps/s (collection: 0.474s, learning 2.060s)
               Value function loss: 87081.6372
                    Surrogate loss: 0.0131
             Mean action noise std: 0.88
                       Mean reward: 7992.50
               Mean episode length: 368.74
                 Mean success rate: 68.00
                  Mean reward/step: 23.30
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 2.53s
                        Total time: 4407.82s
                               ETA: 527940.1s

################################################################################
                    [1m Learning iteration 1656/200000 [0m

                       Computation: 3224 steps/s (collection: 0.487s, learning 2.054s)
               Value function loss: 102111.2726
                    Surrogate loss: 0.0155
             Mean action noise std: 0.88
                       Mean reward: 7939.53
               Mean episode length: 365.90
                 Mean success rate: 67.50
                  Mean reward/step: 22.12
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13574144
                    Iteration time: 2.54s
                        Total time: 4410.36s
                               ETA: 527923.0s

################################################################################
                    [1m Learning iteration 1657/200000 [0m

                       Computation: 3179 steps/s (collection: 0.540s, learning 2.036s)
               Value function loss: 132287.9592
                    Surrogate loss: 0.0101
             Mean action noise std: 0.88
                       Mean reward: 8009.10
               Mean episode length: 368.31
                 Mean success rate: 68.50
                  Mean reward/step: 21.07
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 13582336
                    Iteration time: 2.58s
                        Total time: 4412.94s
                               ETA: 527910.1s

################################################################################
                    [1m Learning iteration 1658/200000 [0m

                       Computation: 3182 steps/s (collection: 0.537s, learning 2.037s)
               Value function loss: 88999.0535
                    Surrogate loss: 0.0133
             Mean action noise std: 0.88
                       Mean reward: 8235.84
               Mean episode length: 371.33
                 Mean success rate: 70.50
                  Mean reward/step: 20.24
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 13590528
                    Iteration time: 2.57s
                        Total time: 4415.51s
                               ETA: 527896.9s

################################################################################
                    [1m Learning iteration 1659/200000 [0m

                       Computation: 3153 steps/s (collection: 0.532s, learning 2.066s)
               Value function loss: 76054.3493
                    Surrogate loss: 0.0187
             Mean action noise std: 0.88
                       Mean reward: 8292.23
               Mean episode length: 371.27
                 Mean success rate: 71.00
                  Mean reward/step: 19.75
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13598720
                    Iteration time: 2.60s
                        Total time: 4418.11s
                               ETA: 527886.6s

################################################################################
                    [1m Learning iteration 1660/200000 [0m

                       Computation: 3215 steps/s (collection: 0.509s, learning 2.039s)
               Value function loss: 87386.6052
                    Surrogate loss: 0.0140
             Mean action noise std: 0.88
                       Mean reward: 8324.67
               Mean episode length: 373.63
                 Mean success rate: 71.50
                  Mean reward/step: 19.48
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13606912
                    Iteration time: 2.55s
                        Total time: 4420.65s
                               ETA: 527870.4s

################################################################################
                    [1m Learning iteration 1661/200000 [0m

                       Computation: 3170 steps/s (collection: 0.526s, learning 2.058s)
               Value function loss: 73051.5566
                    Surrogate loss: 0.0120
             Mean action noise std: 0.88
                       Mean reward: 8118.57
               Mean episode length: 365.32
                 Mean success rate: 70.00
                  Mean reward/step: 19.94
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13615104
                    Iteration time: 2.58s
                        Total time: 4423.24s
                               ETA: 527858.4s

################################################################################
                    [1m Learning iteration 1662/200000 [0m

                       Computation: 3211 steps/s (collection: 0.493s, learning 2.058s)
               Value function loss: 59749.7192
                    Surrogate loss: 0.0138
             Mean action noise std: 0.88
                       Mean reward: 8008.83
               Mean episode length: 360.19
                 Mean success rate: 68.50
                  Mean reward/step: 20.08
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13623296
                    Iteration time: 2.55s
                        Total time: 4425.79s
                               ETA: 527842.6s

################################################################################
                    [1m Learning iteration 1663/200000 [0m

                       Computation: 3142 steps/s (collection: 0.516s, learning 2.090s)
               Value function loss: 88986.2354
                    Surrogate loss: 0.0146
             Mean action noise std: 0.88
                       Mean reward: 7795.46
               Mean episode length: 354.74
                 Mean success rate: 67.00
                  Mean reward/step: 20.73
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 13631488
                    Iteration time: 2.61s
                        Total time: 4428.40s
                               ETA: 527833.4s

################################################################################
                    [1m Learning iteration 1664/200000 [0m

                       Computation: 3020 steps/s (collection: 0.617s, learning 2.095s)
               Value function loss: 79890.0517
                    Surrogate loss: 0.0129
             Mean action noise std: 0.88
                       Mean reward: 8131.37
               Mean episode length: 365.55
                 Mean success rate: 70.00
                  Mean reward/step: 20.21
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13639680
                    Iteration time: 2.71s
                        Total time: 4431.11s
                               ETA: 527836.8s

################################################################################
                    [1m Learning iteration 1665/200000 [0m

                       Computation: 3087 steps/s (collection: 0.565s, learning 2.088s)
               Value function loss: 99858.3437
                    Surrogate loss: 0.0127
             Mean action noise std: 0.88
                       Mean reward: 8022.91
               Mean episode length: 364.33
                 Mean success rate: 69.00
                  Mean reward/step: 19.51
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 13647872
                    Iteration time: 2.65s
                        Total time: 4433.76s
                               ETA: 527833.2s

################################################################################
                    [1m Learning iteration 1666/200000 [0m

                       Computation: 3178 steps/s (collection: 0.524s, learning 2.053s)
               Value function loss: 83164.3317
                    Surrogate loss: 0.0133
             Mean action noise std: 0.88
                       Mean reward: 7897.89
               Mean episode length: 358.42
                 Mean success rate: 67.50
                  Mean reward/step: 19.89
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 13656064
                    Iteration time: 2.58s
                        Total time: 4436.34s
                               ETA: 527820.5s

################################################################################
                    [1m Learning iteration 1667/200000 [0m

                       Computation: 3042 steps/s (collection: 0.574s, learning 2.119s)
               Value function loss: 74087.7409
                    Surrogate loss: 0.0139
             Mean action noise std: 0.88
                       Mean reward: 7958.86
               Mean episode length: 363.12
                 Mean success rate: 69.00
                  Mean reward/step: 20.74
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 2.69s
                        Total time: 4439.03s
                               ETA: 527821.5s

################################################################################
                    [1m Learning iteration 1668/200000 [0m

                       Computation: 3027 steps/s (collection: 0.593s, learning 2.112s)
               Value function loss: 64713.0459
                    Surrogate loss: 0.0117
             Mean action noise std: 0.88
                       Mean reward: 8049.97
               Mean episode length: 368.08
                 Mean success rate: 70.50
                  Mean reward/step: 20.77
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13672448
                    Iteration time: 2.71s
                        Total time: 4441.74s
                               ETA: 527824.1s

################################################################################
                    [1m Learning iteration 1669/200000 [0m

                       Computation: 3150 steps/s (collection: 0.532s, learning 2.069s)
               Value function loss: 66682.0938
                    Surrogate loss: 0.0150
             Mean action noise std: 0.88
                       Mean reward: 7903.15
               Mean episode length: 366.63
                 Mean success rate: 69.50
                  Mean reward/step: 21.18
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 13680640
                    Iteration time: 2.60s
                        Total time: 4444.34s
                               ETA: 527814.3s

################################################################################
                    [1m Learning iteration 1670/200000 [0m

                       Computation: 3227 steps/s (collection: 0.522s, learning 2.016s)
               Value function loss: 51729.2443
                    Surrogate loss: 0.0143
             Mean action noise std: 0.88
                       Mean reward: 7991.11
               Mean episode length: 369.31
                 Mean success rate: 70.00
                  Mean reward/step: 21.23
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 13688832
                    Iteration time: 2.54s
                        Total time: 4446.87s
                               ETA: 527797.0s

################################################################################
                    [1m Learning iteration 1671/200000 [0m

                       Computation: 3219 steps/s (collection: 0.494s, learning 2.051s)
               Value function loss: 84880.1198
                    Surrogate loss: 0.0131
             Mean action noise std: 0.88
                       Mean reward: 7934.59
               Mean episode length: 375.81
                 Mean success rate: 70.50
                  Mean reward/step: 21.34
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 13697024
                    Iteration time: 2.54s
                        Total time: 4449.42s
                               ETA: 527780.5s

################################################################################
                    [1m Learning iteration 1672/200000 [0m

                       Computation: 3217 steps/s (collection: 0.508s, learning 2.038s)
               Value function loss: 72884.6632
                    Surrogate loss: 0.0128
             Mean action noise std: 0.88
                       Mean reward: 7874.87
               Mean episode length: 378.74
                 Mean success rate: 71.50
                  Mean reward/step: 20.38
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13705216
                    Iteration time: 2.55s
                        Total time: 4451.97s
                               ETA: 527764.2s

################################################################################
                    [1m Learning iteration 1673/200000 [0m

                       Computation: 3205 steps/s (collection: 0.530s, learning 2.025s)
               Value function loss: 76770.7780
                    Surrogate loss: 0.0126
             Mean action noise std: 0.88
                       Mean reward: 7782.34
               Mean episode length: 379.97
                 Mean success rate: 72.50
                  Mean reward/step: 20.28
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 13713408
                    Iteration time: 2.56s
                        Total time: 4454.52s
                               ETA: 527749.0s

################################################################################
                    [1m Learning iteration 1674/200000 [0m

                       Computation: 3227 steps/s (collection: 0.512s, learning 2.026s)
               Value function loss: 106310.0366
                    Surrogate loss: 0.0143
             Mean action noise std: 0.88
                       Mean reward: 7680.93
               Mean episode length: 372.04
                 Mean success rate: 72.50
                  Mean reward/step: 19.15
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 13721600
                    Iteration time: 2.54s
                        Total time: 4457.06s
                               ETA: 527731.7s

################################################################################
                    [1m Learning iteration 1675/200000 [0m

                       Computation: 3256 steps/s (collection: 0.458s, learning 2.057s)
               Value function loss: 52562.5594
                    Surrogate loss: 0.0136
             Mean action noise std: 0.88
                       Mean reward: 7552.37
               Mean episode length: 371.31
                 Mean success rate: 72.50
                  Mean reward/step: 19.47
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13729792
                    Iteration time: 2.52s
                        Total time: 4459.57s
                               ETA: 527711.8s

################################################################################
                    [1m Learning iteration 1676/200000 [0m

                       Computation: 3130 steps/s (collection: 0.557s, learning 2.060s)
               Value function loss: 70921.6483
                    Surrogate loss: 0.0129
             Mean action noise std: 0.88
                       Mean reward: 7572.21
               Mean episode length: 373.65
                 Mean success rate: 72.50
                  Mean reward/step: 19.65
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13737984
                    Iteration time: 2.62s
                        Total time: 4462.19s
                               ETA: 527704.0s

################################################################################
                    [1m Learning iteration 1677/200000 [0m

                       Computation: 3196 steps/s (collection: 0.513s, learning 2.050s)
               Value function loss: 68804.8821
                    Surrogate loss: 0.0150
             Mean action noise std: 0.88
                       Mean reward: 7673.06
               Mean episode length: 374.92
                 Mean success rate: 72.50
                  Mean reward/step: 20.81
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 13746176
                    Iteration time: 2.56s
                        Total time: 4464.75s
                               ETA: 527689.7s

################################################################################
                    [1m Learning iteration 1678/200000 [0m

                       Computation: 3030 steps/s (collection: 0.491s, learning 2.213s)
               Value function loss: 75072.3658
                    Surrogate loss: 0.0131
             Mean action noise std: 0.88
                       Mean reward: 7219.85
               Mean episode length: 362.00
                 Mean success rate: 69.50
                  Mean reward/step: 22.13
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13754368
                    Iteration time: 2.70s
                        Total time: 4467.46s
                               ETA: 527692.1s

################################################################################
                    [1m Learning iteration 1679/200000 [0m

                       Computation: 2946 steps/s (collection: 0.640s, learning 2.141s)
               Value function loss: 119318.0985
                    Surrogate loss: 0.0153
             Mean action noise std: 0.88
                       Mean reward: 7633.91
               Mean episode length: 375.39
                 Mean success rate: 73.00
                  Mean reward/step: 21.92
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 2.78s
                        Total time: 4470.24s
                               ETA: 527703.6s

################################################################################
                    [1m Learning iteration 1680/200000 [0m

                       Computation: 2950 steps/s (collection: 0.593s, learning 2.184s)
               Value function loss: 76667.7414
                    Surrogate loss: 0.0150
             Mean action noise std: 0.88
                       Mean reward: 7749.05
               Mean episode length: 374.14
                 Mean success rate: 74.00
                  Mean reward/step: 20.92
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13770752
                    Iteration time: 2.78s
                        Total time: 4473.01s
                               ETA: 527714.6s

################################################################################
                    [1m Learning iteration 1681/200000 [0m

                       Computation: 3091 steps/s (collection: 0.587s, learning 2.063s)
               Value function loss: 82839.5940
                    Surrogate loss: 0.0163
             Mean action noise std: 0.88
                       Mean reward: 7644.13
               Mean episode length: 367.77
                 Mean success rate: 73.50
                  Mean reward/step: 20.27
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 13778944
                    Iteration time: 2.65s
                        Total time: 4475.66s
                               ETA: 527710.7s

################################################################################
                    [1m Learning iteration 1682/200000 [0m

                       Computation: 3177 steps/s (collection: 0.532s, learning 2.046s)
               Value function loss: 81594.9354
                    Surrogate loss: 0.0150
             Mean action noise std: 0.88
                       Mean reward: 7806.60
               Mean episode length: 369.54
                 Mean success rate: 74.00
                  Mean reward/step: 20.47
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 13787136
                    Iteration time: 2.58s
                        Total time: 4478.24s
                               ETA: 527698.3s

################################################################################
                    [1m Learning iteration 1683/200000 [0m

                       Computation: 3085 steps/s (collection: 0.611s, learning 2.044s)
               Value function loss: 72300.6107
                    Surrogate loss: 0.0180
             Mean action noise std: 0.88
                       Mean reward: 7479.85
               Mean episode length: 362.56
                 Mean success rate: 71.00
                  Mean reward/step: 21.13
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 13795328
                    Iteration time: 2.66s
                        Total time: 4480.90s
                               ETA: 527694.9s

################################################################################
                    [1m Learning iteration 1684/200000 [0m

                       Computation: 3228 steps/s (collection: 0.519s, learning 2.018s)
               Value function loss: 72332.5693
                    Surrogate loss: 0.0154
             Mean action noise std: 0.88
                       Mean reward: 7388.68
               Mean episode length: 360.04
                 Mean success rate: 69.50
                  Mean reward/step: 21.78
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13803520
                    Iteration time: 2.54s
                        Total time: 4483.44s
                               ETA: 527677.7s

################################################################################
                    [1m Learning iteration 1685/200000 [0m

                       Computation: 3156 steps/s (collection: 0.565s, learning 2.030s)
               Value function loss: 38344.4012
                    Surrogate loss: 0.0146
             Mean action noise std: 0.88
                       Mean reward: 7122.72
               Mean episode length: 351.31
                 Mean success rate: 67.00
                  Mean reward/step: 22.29
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13811712
                    Iteration time: 2.60s
                        Total time: 4486.03s
                               ETA: 527667.3s

################################################################################
                    [1m Learning iteration 1686/200000 [0m

                       Computation: 3175 steps/s (collection: 0.531s, learning 2.049s)
               Value function loss: 89260.2177
                    Surrogate loss: 0.0117
             Mean action noise std: 0.88
                       Mean reward: 6948.87
               Mean episode length: 346.93
                 Mean success rate: 65.50
                  Mean reward/step: 22.63
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13819904
                    Iteration time: 2.58s
                        Total time: 4488.61s
                               ETA: 527655.2s

################################################################################
                    [1m Learning iteration 1687/200000 [0m

                       Computation: 3274 steps/s (collection: 0.484s, learning 2.018s)
               Value function loss: 100261.2191
                    Surrogate loss: 0.0128
             Mean action noise std: 0.88
                       Mean reward: 6781.60
               Mean episode length: 337.59
                 Mean success rate: 63.50
                  Mean reward/step: 22.02
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13828096
                    Iteration time: 2.50s
                        Total time: 4491.11s
                               ETA: 527633.9s

################################################################################
                    [1m Learning iteration 1688/200000 [0m

                       Computation: 3238 steps/s (collection: 0.490s, learning 2.039s)
               Value function loss: 95721.7078
                    Surrogate loss: 0.0129
             Mean action noise std: 0.88
                       Mean reward: 7003.03
               Mean episode length: 344.83
                 Mean success rate: 65.00
                  Mean reward/step: 21.53
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 13836288
                    Iteration time: 2.53s
                        Total time: 4493.64s
                               ETA: 527615.8s

################################################################################
                    [1m Learning iteration 1689/200000 [0m

                       Computation: 3041 steps/s (collection: 0.560s, learning 2.133s)
               Value function loss: 87270.9486
                    Surrogate loss: 0.0110
             Mean action noise std: 0.88
                       Mean reward: 6816.52
               Mean episode length: 336.19
                 Mean success rate: 62.50
                  Mean reward/step: 21.37
       Mean episode length/episode: 27.96
--------------------------------------------------------------------------------
                   Total timesteps: 13844480
                    Iteration time: 2.69s
                        Total time: 4496.33s
                               ETA: 527617.0s

################################################################################
                    [1m Learning iteration 1690/200000 [0m

                       Computation: 3114 steps/s (collection: 0.511s, learning 2.119s)
               Value function loss: 79836.9672
                    Surrogate loss: 0.0132
             Mean action noise std: 0.88
                       Mean reward: 6592.54
               Mean episode length: 326.85
                 Mean success rate: 61.00
                  Mean reward/step: 21.42
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 13852672
                    Iteration time: 2.63s
                        Total time: 4498.97s
                               ETA: 527610.8s

################################################################################
                    [1m Learning iteration 1691/200000 [0m

                       Computation: 3219 steps/s (collection: 0.470s, learning 2.075s)
               Value function loss: 88788.0906
                    Surrogate loss: 0.0129
             Mean action noise std: 0.88
                       Mean reward: 7069.01
               Mean episode length: 339.76
                 Mean success rate: 64.00
                  Mean reward/step: 21.48
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 2.54s
                        Total time: 4501.51s
                               ETA: 527594.5s

################################################################################
                    [1m Learning iteration 1692/200000 [0m

                       Computation: 3123 steps/s (collection: 0.501s, learning 2.121s)
               Value function loss: 48109.4932
                    Surrogate loss: 0.0135
             Mean action noise std: 0.88
                       Mean reward: 6928.51
               Mean episode length: 332.41
                 Mean success rate: 63.00
                  Mean reward/step: 22.25
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 13869056
                    Iteration time: 2.62s
                        Total time: 4504.13s
                               ETA: 527587.4s

################################################################################
                    [1m Learning iteration 1693/200000 [0m

                       Computation: 3047 steps/s (collection: 0.569s, learning 2.119s)
               Value function loss: 67799.1621
                    Surrogate loss: 0.0120
             Mean action noise std: 0.88
                       Mean reward: 6966.02
               Mean episode length: 329.94
                 Mean success rate: 63.50
                  Mean reward/step: 23.65
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 13877248
                    Iteration time: 2.69s
                        Total time: 4506.82s
                               ETA: 527588.0s

################################################################################
                    [1m Learning iteration 1694/200000 [0m

                       Computation: 3059 steps/s (collection: 0.532s, learning 2.146s)
               Value function loss: 61350.7868
                    Surrogate loss: 0.0127
             Mean action noise std: 0.88
                       Mean reward: 7194.78
               Mean episode length: 338.86
                 Mean success rate: 66.50
                  Mean reward/step: 23.32
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 13885440
                    Iteration time: 2.68s
                        Total time: 4509.50s
                               ETA: 527587.3s

################################################################################
                    [1m Learning iteration 1695/200000 [0m

                       Computation: 3071 steps/s (collection: 0.540s, learning 2.128s)
               Value function loss: 88459.4186
                    Surrogate loss: 0.0105
             Mean action noise std: 0.88
                       Mean reward: 7563.27
               Mean episode length: 350.35
                 Mean success rate: 69.00
                  Mean reward/step: 23.22
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13893632
                    Iteration time: 2.67s
                        Total time: 4512.17s
                               ETA: 527585.5s

################################################################################
                    [1m Learning iteration 1696/200000 [0m

                       Computation: 3045 steps/s (collection: 0.571s, learning 2.118s)
               Value function loss: 81843.0784
                    Surrogate loss: 0.0150
             Mean action noise std: 0.88
                       Mean reward: 7701.66
               Mean episode length: 354.58
                 Mean success rate: 70.00
                  Mean reward/step: 23.32
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13901824
                    Iteration time: 2.69s
                        Total time: 4514.86s
                               ETA: 527586.2s

################################################################################
                    [1m Learning iteration 1697/200000 [0m

                       Computation: 3136 steps/s (collection: 0.530s, learning 2.082s)
               Value function loss: 96494.0570
                    Surrogate loss: 0.0119
             Mean action noise std: 0.88
                       Mean reward: 7701.11
               Mean episode length: 352.42
                 Mean success rate: 68.50
                  Mean reward/step: 22.62
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 13910016
                    Iteration time: 2.61s
                        Total time: 4517.47s
                               ETA: 527577.9s

################################################################################
                    [1m Learning iteration 1698/200000 [0m

                       Computation: 3117 steps/s (collection: 0.534s, learning 2.093s)
               Value function loss: 86017.1471
                    Surrogate loss: 0.0124
             Mean action noise std: 0.88
                       Mean reward: 8118.08
               Mean episode length: 366.71
                 Mean success rate: 71.50
                  Mean reward/step: 22.09
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13918208
                    Iteration time: 2.63s
                        Total time: 4520.09s
                               ETA: 527571.4s

################################################################################
                    [1m Learning iteration 1699/200000 [0m

                       Computation: 3034 steps/s (collection: 0.555s, learning 2.144s)
               Value function loss: 85696.5021
                    Surrogate loss: 0.0162
             Mean action noise std: 0.88
                       Mean reward: 8326.41
               Mean episode length: 375.71
                 Mean success rate: 73.50
                  Mean reward/step: 22.50
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13926400
                    Iteration time: 2.70s
                        Total time: 4522.79s
                               ETA: 527573.3s

################################################################################
                    [1m Learning iteration 1700/200000 [0m

                       Computation: 3189 steps/s (collection: 0.530s, learning 2.038s)
               Value function loss: 81800.7124
                    Surrogate loss: 0.0129
             Mean action noise std: 0.88
                       Mean reward: 8765.52
               Mean episode length: 389.49
                 Mean success rate: 76.50
                  Mean reward/step: 22.43
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13934592
                    Iteration time: 2.57s
                        Total time: 4525.36s
                               ETA: 527559.9s

################################################################################
                    [1m Learning iteration 1701/200000 [0m

                       Computation: 3172 steps/s (collection: 0.542s, learning 2.040s)
               Value function loss: 54057.9246
                    Surrogate loss: 0.0138
             Mean action noise std: 0.88
                       Mean reward: 8826.16
               Mean episode length: 393.24
                 Mean success rate: 77.50
                  Mean reward/step: 22.80
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 13942784
                    Iteration time: 2.58s
                        Total time: 4527.94s
                               ETA: 527548.1s

################################################################################
                    [1m Learning iteration 1702/200000 [0m

                       Computation: 3198 steps/s (collection: 0.527s, learning 2.035s)
               Value function loss: 62339.3721
                    Surrogate loss: 0.0144
             Mean action noise std: 0.88
                       Mean reward: 9040.43
               Mean episode length: 399.68
                 Mean success rate: 78.50
                  Mean reward/step: 23.03
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 13950976
                    Iteration time: 2.56s
                        Total time: 4530.51s
                               ETA: 527533.9s

################################################################################
                    [1m Learning iteration 1703/200000 [0m

                       Computation: 3182 steps/s (collection: 0.527s, learning 2.047s)
               Value function loss: 81509.3095
                    Surrogate loss: 0.0110
             Mean action noise std: 0.88
                       Mean reward: 9427.11
               Mean episode length: 412.76
                 Mean success rate: 81.00
                  Mean reward/step: 23.12
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 2.57s
                        Total time: 4533.08s
                               ETA: 527521.2s

################################################################################
                    [1m Learning iteration 1704/200000 [0m

                       Computation: 3242 steps/s (collection: 0.479s, learning 2.047s)
               Value function loss: 116786.6594
                    Surrogate loss: 0.0141
             Mean action noise std: 0.88
                       Mean reward: 9593.79
               Mean episode length: 418.12
                 Mean success rate: 80.50
                  Mean reward/step: 23.29
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 13967360
                    Iteration time: 2.53s
                        Total time: 4535.61s
                               ETA: 527503.0s

################################################################################
                    [1m Learning iteration 1705/200000 [0m

                       Computation: 3191 steps/s (collection: 0.515s, learning 2.052s)
               Value function loss: 125135.6264
                    Surrogate loss: 0.0134
             Mean action noise std: 0.88
                       Mean reward: 9679.66
               Mean episode length: 421.58
                 Mean success rate: 81.00
                  Mean reward/step: 22.02
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 13975552
                    Iteration time: 2.57s
                        Total time: 4538.17s
                               ETA: 527489.4s

################################################################################
                    [1m Learning iteration 1706/200000 [0m

                       Computation: 3161 steps/s (collection: 0.544s, learning 2.047s)
               Value function loss: 75751.5159
                    Surrogate loss: 0.0144
             Mean action noise std: 0.88
                       Mean reward: 9633.68
               Mean episode length: 420.40
                 Mean success rate: 81.00
                  Mean reward/step: 21.13
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 13983744
                    Iteration time: 2.59s
                        Total time: 4540.76s
                               ETA: 527478.8s

################################################################################
                    [1m Learning iteration 1707/200000 [0m

                       Computation: 3206 steps/s (collection: 0.526s, learning 2.029s)
               Value function loss: 78377.0959
                    Surrogate loss: 0.0133
             Mean action noise std: 0.88
                       Mean reward: 9222.40
               Mean episode length: 406.74
                 Mean success rate: 79.00
                  Mean reward/step: 21.36
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 13991936
                    Iteration time: 2.55s
                        Total time: 4543.32s
                               ETA: 527463.9s

################################################################################
                    [1m Learning iteration 1708/200000 [0m

                       Computation: 3211 steps/s (collection: 0.497s, learning 2.054s)
               Value function loss: 80589.7821
                    Surrogate loss: 0.0137
             Mean action noise std: 0.88
                       Mean reward: 9100.84
               Mean episode length: 395.75
                 Mean success rate: 76.50
                  Mean reward/step: 22.61
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14000128
                    Iteration time: 2.55s
                        Total time: 4545.87s
                               ETA: 527448.6s

################################################################################
                    [1m Learning iteration 1709/200000 [0m

                       Computation: 3201 steps/s (collection: 0.497s, learning 2.062s)
               Value function loss: 89421.9525
                    Surrogate loss: 0.0141
             Mean action noise std: 0.88
                       Mean reward: 8725.93
               Mean episode length: 387.88
                 Mean success rate: 74.00
                  Mean reward/step: 23.54
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14008320
                    Iteration time: 2.56s
                        Total time: 4548.43s
                               ETA: 527434.2s

################################################################################
                    [1m Learning iteration 1710/200000 [0m

                       Computation: 3248 steps/s (collection: 0.480s, learning 2.042s)
               Value function loss: 65729.5112
                    Surrogate loss: 0.0128
             Mean action noise std: 0.88
                       Mean reward: 8481.06
               Mean episode length: 378.26
                 Mean success rate: 71.50
                  Mean reward/step: 23.17
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14016512
                    Iteration time: 2.52s
                        Total time: 4550.95s
                               ETA: 527415.6s

################################################################################
                    [1m Learning iteration 1711/200000 [0m

                       Computation: 3258 steps/s (collection: 0.484s, learning 2.030s)
               Value function loss: 72325.5374
                    Surrogate loss: 0.0121
             Mean action noise std: 0.88
                       Mean reward: 8346.28
               Mean episode length: 375.04
                 Mean success rate: 71.00
                  Mean reward/step: 22.94
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14024704
                    Iteration time: 2.51s
                        Total time: 4553.47s
                               ETA: 527396.0s

################################################################################
                    [1m Learning iteration 1712/200000 [0m

                       Computation: 3095 steps/s (collection: 0.514s, learning 2.132s)
               Value function loss: 109173.6367
                    Surrogate loss: 0.0144
             Mean action noise std: 0.88
                       Mean reward: 8432.49
               Mean episode length: 375.18
                 Mean success rate: 72.00
                  Mean reward/step: 23.07
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 14032896
                    Iteration time: 2.65s
                        Total time: 4556.11s
                               ETA: 527391.8s

################################################################################
                    [1m Learning iteration 1713/200000 [0m

                       Computation: 3250 steps/s (collection: 0.493s, learning 2.027s)
               Value function loss: 74756.6782
                    Surrogate loss: 0.0113
             Mean action noise std: 0.88
                       Mean reward: 8725.32
               Mean episode length: 384.01
                 Mean success rate: 74.50
                  Mean reward/step: 22.18
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14041088
                    Iteration time: 2.52s
                        Total time: 4558.63s
                               ETA: 527373.0s

################################################################################
                    [1m Learning iteration 1714/200000 [0m

                       Computation: 3167 steps/s (collection: 0.489s, learning 2.098s)
               Value function loss: 108567.4297
                    Surrogate loss: 0.0154
             Mean action noise std: 0.88
                       Mean reward: 8791.51
               Mean episode length: 383.08
                 Mean success rate: 75.00
                  Mean reward/step: 21.83
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14049280
                    Iteration time: 2.59s
                        Total time: 4561.22s
                               ETA: 527361.8s

################################################################################
                    [1m Learning iteration 1715/200000 [0m

                       Computation: 3160 steps/s (collection: 0.475s, learning 2.118s)
               Value function loss: 70702.0567
                    Surrogate loss: 0.0124
             Mean action noise std: 0.88
                       Mean reward: 8552.81
               Mean episode length: 376.14
                 Mean success rate: 74.00
                  Mean reward/step: 21.86
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 2.59s
                        Total time: 4563.81s
                               ETA: 527351.4s

################################################################################
                    [1m Learning iteration 1716/200000 [0m

                       Computation: 3076 steps/s (collection: 0.562s, learning 2.100s)
               Value function loss: 55407.0229
                    Surrogate loss: 0.0136
             Mean action noise std: 0.88
                       Mean reward: 8623.84
               Mean episode length: 376.27
                 Mean success rate: 74.50
                  Mean reward/step: 22.28
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14065664
                    Iteration time: 2.66s
                        Total time: 4566.47s
                               ETA: 527349.1s

################################################################################
                    [1m Learning iteration 1717/200000 [0m

                       Computation: 3125 steps/s (collection: 0.530s, learning 2.091s)
               Value function loss: 79069.7217
                    Surrogate loss: 0.0121
             Mean action noise std: 0.88
                       Mean reward: 8626.76
               Mean episode length: 380.37
                 Mean success rate: 75.50
                  Mean reward/step: 22.93
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14073856
                    Iteration time: 2.62s
                        Total time: 4569.09s
                               ETA: 527342.0s

################################################################################
                    [1m Learning iteration 1718/200000 [0m

                       Computation: 3214 steps/s (collection: 0.487s, learning 2.061s)
               Value function loss: 56186.2731
                    Surrogate loss: 0.0106
             Mean action noise std: 0.88
                       Mean reward: 8806.15
               Mean episode length: 390.64
                 Mean success rate: 77.50
                  Mean reward/step: 22.97
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 14082048
                    Iteration time: 2.55s
                        Total time: 4571.64s
                               ETA: 527326.5s

################################################################################
                    [1m Learning iteration 1719/200000 [0m

                       Computation: 3169 steps/s (collection: 0.475s, learning 2.110s)
               Value function loss: 81913.8140
                    Surrogate loss: 0.0149
             Mean action noise std: 0.88
                       Mean reward: 8967.45
               Mean episode length: 394.80
                 Mean success rate: 78.50
                  Mean reward/step: 22.65
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14090240
                    Iteration time: 2.58s
                        Total time: 4574.23s
                               ETA: 527315.2s

################################################################################
                    [1m Learning iteration 1720/200000 [0m

                       Computation: 3127 steps/s (collection: 0.512s, learning 2.107s)
               Value function loss: 116358.9187
                    Surrogate loss: 0.0107
             Mean action noise std: 0.88
                       Mean reward: 9271.80
               Mean episode length: 405.88
                 Mean success rate: 81.00
                  Mean reward/step: 22.63
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 14098432
                    Iteration time: 2.62s
                        Total time: 4576.85s
                               ETA: 527308.0s

################################################################################
                    [1m Learning iteration 1721/200000 [0m

                       Computation: 3103 steps/s (collection: 0.523s, learning 2.116s)
               Value function loss: 86793.3473
                    Surrogate loss: 0.0113
             Mean action noise std: 0.88
                       Mean reward: 9124.93
               Mean episode length: 402.03
                 Mean success rate: 80.00
                  Mean reward/step: 22.30
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14106624
                    Iteration time: 2.64s
                        Total time: 4579.49s
                               ETA: 527303.0s

################################################################################
                    [1m Learning iteration 1722/200000 [0m

                       Computation: 3174 steps/s (collection: 0.489s, learning 2.091s)
               Value function loss: 87506.8387
                    Surrogate loss: 0.0120
             Mean action noise std: 0.88
                       Mean reward: 9068.58
               Mean episode length: 400.38
                 Mean success rate: 80.00
                  Mean reward/step: 22.24
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14114816
                    Iteration time: 2.58s
                        Total time: 4582.07s
                               ETA: 527291.3s

################################################################################
                    [1m Learning iteration 1723/200000 [0m

                       Computation: 3208 steps/s (collection: 0.466s, learning 2.087s)
               Value function loss: 67087.8330
                    Surrogate loss: 0.0121
             Mean action noise std: 0.88
                       Mean reward: 9067.67
               Mean episode length: 401.11
                 Mean success rate: 79.50
                  Mean reward/step: 22.10
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14123008
                    Iteration time: 2.55s
                        Total time: 4584.62s
                               ETA: 527276.4s

################################################################################
                    [1m Learning iteration 1724/200000 [0m

                       Computation: 3130 steps/s (collection: 0.518s, learning 2.099s)
               Value function loss: 64474.6150
                    Surrogate loss: 0.0158
             Mean action noise std: 0.88
                       Mean reward: 8493.02
               Mean episode length: 384.76
                 Mean success rate: 74.50
                  Mean reward/step: 22.58
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14131200
                    Iteration time: 2.62s
                        Total time: 4587.24s
                               ETA: 527268.9s

################################################################################
                    [1m Learning iteration 1725/200000 [0m

                       Computation: 3114 steps/s (collection: 0.509s, learning 2.121s)
               Value function loss: 89918.5285
                    Surrogate loss: 0.0119
             Mean action noise std: 0.88
                       Mean reward: 8707.30
               Mean episode length: 392.59
                 Mean success rate: 75.50
                  Mean reward/step: 22.88
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14139392
                    Iteration time: 2.63s
                        Total time: 4589.87s
                               ETA: 527263.0s

################################################################################
                    [1m Learning iteration 1726/200000 [0m

                       Computation: 3207 steps/s (collection: 0.484s, learning 2.070s)
               Value function loss: 61597.2674
                    Surrogate loss: 0.0133
             Mean action noise std: 0.88
                       Mean reward: 8822.42
               Mean episode length: 395.65
                 Mean success rate: 76.00
                  Mean reward/step: 23.48
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 14147584
                    Iteration time: 2.55s
                        Total time: 4592.42s
                               ETA: 527248.2s

################################################################################
                    [1m Learning iteration 1727/200000 [0m

                       Computation: 3193 steps/s (collection: 0.508s, learning 2.057s)
               Value function loss: 85276.0806
                    Surrogate loss: 0.0114
             Mean action noise std: 0.88
                       Mean reward: 8899.08
               Mean episode length: 398.24
                 Mean success rate: 75.00
                  Mean reward/step: 24.14
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 2.57s
                        Total time: 4594.99s
                               ETA: 527234.8s

################################################################################
                    [1m Learning iteration 1728/200000 [0m

                       Computation: 3151 steps/s (collection: 0.524s, learning 2.075s)
               Value function loss: 130407.0245
                    Surrogate loss: 0.0111
             Mean action noise std: 0.88
                       Mean reward: 8979.63
               Mean episode length: 397.57
                 Mean success rate: 75.50
                  Mean reward/step: 23.82
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 14163968
                    Iteration time: 2.60s
                        Total time: 4597.59s
                               ETA: 527225.2s

################################################################################
                    [1m Learning iteration 1729/200000 [0m

                       Computation: 3144 steps/s (collection: 0.525s, learning 2.081s)
               Value function loss: 92830.5972
                    Surrogate loss: 0.0114
             Mean action noise std: 0.88
                       Mean reward: 8910.65
               Mean episode length: 396.16
                 Mean success rate: 75.50
                  Mean reward/step: 23.85
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14172160
                    Iteration time: 2.61s
                        Total time: 4600.19s
                               ETA: 527216.4s

################################################################################
                    [1m Learning iteration 1730/200000 [0m

                       Computation: 3190 steps/s (collection: 0.503s, learning 2.064s)
               Value function loss: 95904.1593
                    Surrogate loss: 0.0136
             Mean action noise std: 0.88
                       Mean reward: 9269.85
               Mean episode length: 405.48
                 Mean success rate: 77.50
                  Mean reward/step: 23.79
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14180352
                    Iteration time: 2.57s
                        Total time: 4602.76s
                               ETA: 527203.3s

################################################################################
                    [1m Learning iteration 1731/200000 [0m

                       Computation: 3091 steps/s (collection: 0.527s, learning 2.123s)
               Value function loss: 85760.4295
                    Surrogate loss: 0.0122
             Mean action noise std: 0.88
                       Mean reward: 9226.62
               Mean episode length: 401.72
                 Mean success rate: 76.50
                  Mean reward/step: 24.16
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 14188544
                    Iteration time: 2.65s
                        Total time: 4605.41s
                               ETA: 527199.6s

################################################################################
                    [1m Learning iteration 1732/200000 [0m

                       Computation: 3152 steps/s (collection: 0.515s, learning 2.083s)
               Value function loss: 48767.4909
                    Surrogate loss: 0.0087
             Mean action noise std: 0.88
                       Mean reward: 9278.51
               Mean episode length: 403.78
                 Mean success rate: 77.00
                  Mean reward/step: 24.46
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 14196736
                    Iteration time: 2.60s
                        Total time: 4608.01s
                               ETA: 527190.0s

################################################################################
                    [1m Learning iteration 1733/200000 [0m

                       Computation: 3048 steps/s (collection: 0.607s, learning 2.081s)
               Value function loss: 90827.8647
                    Surrogate loss: 0.0080
             Mean action noise std: 0.88
                       Mean reward: 9453.00
               Mean episode length: 408.19
                 Mean success rate: 78.00
                  Mean reward/step: 24.53
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14204928
                    Iteration time: 2.69s
                        Total time: 4610.69s
                               ETA: 527190.6s

################################################################################
                    [1m Learning iteration 1734/200000 [0m

                       Computation: 3071 steps/s (collection: 0.535s, learning 2.133s)
               Value function loss: 90957.8515
                    Surrogate loss: 0.0087
             Mean action noise std: 0.88
                       Mean reward: 9535.18
               Mean episode length: 410.13
                 Mean success rate: 78.00
                  Mean reward/step: 24.04
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14213120
                    Iteration time: 2.67s
                        Total time: 4613.36s
                               ETA: 527188.9s

################################################################################
                    [1m Learning iteration 1735/200000 [0m

                       Computation: 3173 steps/s (collection: 0.480s, learning 2.101s)
               Value function loss: 85838.3075
                    Surrogate loss: 0.0098
             Mean action noise std: 0.88
                       Mean reward: 9901.62
               Mean episode length: 418.70
                 Mean success rate: 81.50
                  Mean reward/step: 23.21
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14221312
                    Iteration time: 2.58s
                        Total time: 4615.94s
                               ETA: 527177.4s

################################################################################
                    [1m Learning iteration 1736/200000 [0m

                       Computation: 3151 steps/s (collection: 0.544s, learning 2.055s)
               Value function loss: 130242.3727
                    Surrogate loss: 0.0094
             Mean action noise std: 0.88
                       Mean reward: 9760.71
               Mean episode length: 416.43
                 Mean success rate: 81.00
                  Mean reward/step: 22.26
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 14229504
                    Iteration time: 2.60s
                        Total time: 4618.54s
                               ETA: 527167.9s

################################################################################
                    [1m Learning iteration 1737/200000 [0m

                       Computation: 3162 steps/s (collection: 0.516s, learning 2.074s)
               Value function loss: 108726.1674
                    Surrogate loss: 0.0106
             Mean action noise std: 0.88
                       Mean reward: 10090.37
               Mean episode length: 423.55
                 Mean success rate: 83.50
                  Mean reward/step: 21.80
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14237696
                    Iteration time: 2.59s
                        Total time: 4621.13s
                               ETA: 527157.4s

################################################################################
                    [1m Learning iteration 1738/200000 [0m

                       Computation: 3145 steps/s (collection: 0.527s, learning 2.078s)
               Value function loss: 92683.0568
                    Surrogate loss: 0.0102
             Mean action noise std: 0.88
                       Mean reward: 10007.20
               Mean episode length: 422.75
                 Mean success rate: 83.00
                  Mean reward/step: 21.46
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14245888
                    Iteration time: 2.60s
                        Total time: 4623.74s
                               ETA: 527148.6s

################################################################################
                    [1m Learning iteration 1739/200000 [0m

                       Computation: 3239 steps/s (collection: 0.480s, learning 2.049s)
               Value function loss: 79088.1155
                    Surrogate loss: 0.0117
             Mean action noise std: 0.88
                       Mean reward: 9905.87
               Mean episode length: 421.52
                 Mean success rate: 82.50
                  Mean reward/step: 21.76
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 2.53s
                        Total time: 4626.27s
                               ETA: 527131.1s

################################################################################
                    [1m Learning iteration 1740/200000 [0m

                       Computation: 3111 steps/s (collection: 0.559s, learning 2.074s)
               Value function loss: 109173.2451
                    Surrogate loss: 0.0112
             Mean action noise std: 0.88
                       Mean reward: 10085.55
               Mean episode length: 424.39
                 Mean success rate: 83.00
                  Mean reward/step: 22.32
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14262272
                    Iteration time: 2.63s
                        Total time: 4628.90s
                               ETA: 527125.5s

################################################################################
                    [1m Learning iteration 1741/200000 [0m

                       Computation: 3144 steps/s (collection: 0.504s, learning 2.102s)
               Value function loss: 61628.4726
                    Surrogate loss: 0.0123
             Mean action noise std: 0.88
                       Mean reward: 9840.15
               Mean episode length: 416.00
                 Mean success rate: 81.50
                  Mean reward/step: 22.57
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14270464
                    Iteration time: 2.61s
                        Total time: 4631.50s
                               ETA: 527116.7s

################################################################################
                    [1m Learning iteration 1742/200000 [0m

                       Computation: 3149 steps/s (collection: 0.535s, learning 2.066s)
               Value function loss: 70521.4713
                    Surrogate loss: 0.0123
             Mean action noise std: 0.88
                       Mean reward: 9369.93
               Mean episode length: 402.26
                 Mean success rate: 78.50
                  Mean reward/step: 22.98
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14278656
                    Iteration time: 2.60s
                        Total time: 4634.10s
                               ETA: 527107.5s

################################################################################
                    [1m Learning iteration 1743/200000 [0m

                       Computation: 3174 steps/s (collection: 0.518s, learning 2.062s)
               Value function loss: 125143.3215
                    Surrogate loss: 0.0122
             Mean action noise std: 0.88
                       Mean reward: 9132.02
               Mean episode length: 396.37
                 Mean success rate: 76.00
                  Mean reward/step: 22.93
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 14286848
                    Iteration time: 2.58s
                        Total time: 4636.69s
                               ETA: 527095.9s

################################################################################
                    [1m Learning iteration 1744/200000 [0m

                       Computation: 3041 steps/s (collection: 0.589s, learning 2.104s)
               Value function loss: 91464.9414
                    Surrogate loss: 0.0111
             Mean action noise std: 0.88
                       Mean reward: 9138.02
               Mean episode length: 393.40
                 Mean success rate: 76.00
                  Mean reward/step: 21.41
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14295040
                    Iteration time: 2.69s
                        Total time: 4639.38s
                               ETA: 527097.2s

################################################################################
                    [1m Learning iteration 1745/200000 [0m

                       Computation: 3144 steps/s (collection: 0.500s, learning 2.105s)
               Value function loss: 72122.9993
                    Surrogate loss: 0.0123
             Mean action noise std: 0.88
                       Mean reward: 8668.04
               Mean episode length: 379.76
                 Mean success rate: 73.00
                  Mean reward/step: 21.39
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14303232
                    Iteration time: 2.60s
                        Total time: 4641.98s
                               ETA: 527088.4s

################################################################################
                    [1m Learning iteration 1746/200000 [0m

                       Computation: 3220 steps/s (collection: 0.494s, learning 2.050s)
               Value function loss: 66991.7671
                    Surrogate loss: 0.0153
             Mean action noise std: 0.88
                       Mean reward: 8514.43
               Mean episode length: 375.98
                 Mean success rate: 72.00
                  Mean reward/step: 21.68
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14311424
                    Iteration time: 2.54s
                        Total time: 4644.53s
                               ETA: 527072.8s

################################################################################
                    [1m Learning iteration 1747/200000 [0m

                       Computation: 3195 steps/s (collection: 0.525s, learning 2.038s)
               Value function loss: 91495.7012
                    Surrogate loss: 0.0141
             Mean action noise std: 0.88
                       Mean reward: 8068.24
               Mean episode length: 355.92
                 Mean success rate: 68.50
                  Mean reward/step: 22.89
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 14319616
                    Iteration time: 2.56s
                        Total time: 4647.09s
                               ETA: 527059.3s

################################################################################
                    [1m Learning iteration 1748/200000 [0m

                       Computation: 3265 steps/s (collection: 0.461s, learning 2.048s)
               Value function loss: 75547.4246
                    Surrogate loss: 0.0114
             Mean action noise std: 0.88
                       Mean reward: 8014.65
               Mean episode length: 354.12
                 Mean success rate: 68.00
                  Mean reward/step: 23.28
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 14327808
                    Iteration time: 2.51s
                        Total time: 4649.60s
                               ETA: 527039.7s

################################################################################
                    [1m Learning iteration 1749/200000 [0m

                       Computation: 3212 steps/s (collection: 0.482s, learning 2.068s)
               Value function loss: 89185.0965
                    Surrogate loss: 0.0117
             Mean action noise std: 0.88
                       Mean reward: 7779.18
               Mean episode length: 350.76
                 Mean success rate: 67.00
                  Mean reward/step: 23.63
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14336000
                    Iteration time: 2.55s
                        Total time: 4652.15s
                               ETA: 527024.7s

################################################################################
                    [1m Learning iteration 1750/200000 [0m

                       Computation: 3196 steps/s (collection: 0.491s, learning 2.072s)
               Value function loss: 74821.4824
                    Surrogate loss: 0.0137
             Mean action noise std: 0.88
                       Mean reward: 7890.96
               Mean episode length: 357.73
                 Mean success rate: 68.00
                  Mean reward/step: 23.62
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14344192
                    Iteration time: 2.56s
                        Total time: 4654.71s
                               ETA: 527011.2s

################################################################################
                    [1m Learning iteration 1751/200000 [0m

                       Computation: 3159 steps/s (collection: 0.495s, learning 2.098s)
               Value function loss: 101597.5592
                    Surrogate loss: 0.0140
             Mean action noise std: 0.88
                       Mean reward: 8285.78
               Mean episode length: 370.11
                 Mean success rate: 70.50
                  Mean reward/step: 23.93
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 2.59s
                        Total time: 4657.30s
                               ETA: 527001.1s

################################################################################
                    [1m Learning iteration 1752/200000 [0m

                       Computation: 3167 steps/s (collection: 0.510s, learning 2.076s)
               Value function loss: 69201.4421
                    Surrogate loss: 0.0154
             Mean action noise std: 0.88
                       Mean reward: 7904.33
               Mean episode length: 359.70
                 Mean success rate: 68.50
                  Mean reward/step: 23.71
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14360576
                    Iteration time: 2.59s
                        Total time: 4659.89s
                               ETA: 526990.3s

################################################################################
                    [1m Learning iteration 1753/200000 [0m

                       Computation: 3203 steps/s (collection: 0.492s, learning 2.065s)
               Value function loss: 78849.3461
                    Surrogate loss: 0.0125
             Mean action noise std: 0.88
                       Mean reward: 8087.93
               Mean episode length: 365.95
                 Mean success rate: 69.50
                  Mean reward/step: 23.72
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14368768
                    Iteration time: 2.56s
                        Total time: 4662.45s
                               ETA: 526976.3s

################################################################################
                    [1m Learning iteration 1754/200000 [0m

                       Computation: 3130 steps/s (collection: 0.545s, learning 2.072s)
               Value function loss: 107909.2119
                    Surrogate loss: 0.0142
             Mean action noise std: 0.88
                       Mean reward: 8249.90
               Mean episode length: 371.68
                 Mean success rate: 70.50
                  Mean reward/step: 23.46
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14376960
                    Iteration time: 2.62s
                        Total time: 4665.07s
                               ETA: 526969.0s

################################################################################
                    [1m Learning iteration 1755/200000 [0m

                       Computation: 3015 steps/s (collection: 0.583s, learning 2.133s)
               Value function loss: 64894.8709
                    Surrogate loss: 0.0168
             Mean action noise std: 0.88
                       Mean reward: 8353.69
               Mean episode length: 375.69
                 Mean success rate: 71.00
                  Mean reward/step: 23.43
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14385152
                    Iteration time: 2.72s
                        Total time: 4667.78s
                               ETA: 526972.9s

################################################################################
                    [1m Learning iteration 1756/200000 [0m

                       Computation: 3203 steps/s (collection: 0.484s, learning 2.073s)
               Value function loss: 98555.4744
                    Surrogate loss: 0.0152
             Mean action noise std: 0.88
                       Mean reward: 8567.27
               Mean episode length: 384.34
                 Mean success rate: 72.50
                  Mean reward/step: 24.08
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14393344
                    Iteration time: 2.56s
                        Total time: 4670.34s
                               ETA: 526958.9s

################################################################################
                    [1m Learning iteration 1757/200000 [0m

                       Computation: 3169 steps/s (collection: 0.483s, learning 2.101s)
               Value function loss: 86397.3071
                    Surrogate loss: 0.0141
             Mean action noise std: 0.88
                       Mean reward: 8948.08
               Mean episode length: 396.38
                 Mean success rate: 75.00
                  Mean reward/step: 24.35
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14401536
                    Iteration time: 2.58s
                        Total time: 4672.92s
                               ETA: 526947.9s

################################################################################
                    [1m Learning iteration 1758/200000 [0m

                       Computation: 3124 steps/s (collection: 0.548s, learning 2.074s)
               Value function loss: 90976.2283
                    Surrogate loss: 0.0112
             Mean action noise std: 0.88
                       Mean reward: 9090.62
               Mean episode length: 400.85
                 Mean success rate: 76.50
                  Mean reward/step: 23.96
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14409728
                    Iteration time: 2.62s
                        Total time: 4675.55s
                               ETA: 526941.2s

################################################################################
                    [1m Learning iteration 1759/200000 [0m

                       Computation: 3182 steps/s (collection: 0.508s, learning 2.066s)
               Value function loss: 157644.1303
                    Surrogate loss: 0.0120
             Mean action noise std: 0.88
                       Mean reward: 9598.74
               Mean episode length: 413.87
                 Mean success rate: 79.50
                  Mean reward/step: 23.77
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14417920
                    Iteration time: 2.57s
                        Total time: 4678.12s
                               ETA: 526929.0s

################################################################################
                    [1m Learning iteration 1760/200000 [0m

                       Computation: 3195 steps/s (collection: 0.464s, learning 2.099s)
               Value function loss: 90613.1815
                    Surrogate loss: 0.0131
             Mean action noise std: 0.88
                       Mean reward: 9706.37
               Mean episode length: 416.64
                 Mean success rate: 80.50
                  Mean reward/step: 22.68
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14426112
                    Iteration time: 2.56s
                        Total time: 4680.68s
                               ETA: 526915.7s

################################################################################
                    [1m Learning iteration 1761/200000 [0m

                       Computation: 3228 steps/s (collection: 0.496s, learning 2.042s)
               Value function loss: 96878.3931
                    Surrogate loss: 0.0111
             Mean action noise std: 0.88
                       Mean reward: 9539.81
               Mean episode length: 410.20
                 Mean success rate: 79.00
                  Mean reward/step: 23.25
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14434304
                    Iteration time: 2.54s
                        Total time: 4683.22s
                               ETA: 526899.5s

################################################################################
                    [1m Learning iteration 1762/200000 [0m

                       Computation: 3215 steps/s (collection: 0.474s, learning 2.073s)
               Value function loss: 73258.7119
                    Surrogate loss: 0.0133
             Mean action noise std: 0.88
                       Mean reward: 9720.89
               Mean episode length: 410.79
                 Mean success rate: 79.50
                  Mean reward/step: 23.87
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14442496
                    Iteration time: 2.55s
                        Total time: 4685.77s
                               ETA: 526884.4s

################################################################################
                    [1m Learning iteration 1763/200000 [0m

                       Computation: 3200 steps/s (collection: 0.505s, learning 2.055s)
               Value function loss: 79126.5600
                    Surrogate loss: 0.0123
             Mean action noise std: 0.88
                       Mean reward: 9757.21
               Mean episode length: 409.48
                 Mean success rate: 79.50
                  Mean reward/step: 24.08
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 2.56s
                        Total time: 4688.33s
                               ETA: 526870.8s

################################################################################
                    [1m Learning iteration 1764/200000 [0m

                       Computation: 3232 steps/s (collection: 0.478s, learning 2.056s)
               Value function loss: 91795.2078
                    Surrogate loss: 0.0148
             Mean action noise std: 0.88
                       Mean reward: 9837.27
               Mean episode length: 412.38
                 Mean success rate: 80.50
                  Mean reward/step: 23.78
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14458880
                    Iteration time: 2.53s
                        Total time: 4690.86s
                               ETA: 526854.2s

################################################################################
                    [1m Learning iteration 1765/200000 [0m

                       Computation: 3201 steps/s (collection: 0.502s, learning 2.058s)
               Value function loss: 63444.3378
                    Surrogate loss: 0.0133
             Mean action noise std: 0.88
                       Mean reward: 9707.60
               Mean episode length: 405.12
                 Mean success rate: 79.50
                  Mean reward/step: 23.46
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14467072
                    Iteration time: 2.56s
                        Total time: 4693.42s
                               ETA: 526840.5s

################################################################################
                    [1m Learning iteration 1766/200000 [0m

                       Computation: 3252 steps/s (collection: 0.469s, learning 2.050s)
               Value function loss: 79720.4110
                    Surrogate loss: 0.0121
             Mean action noise std: 0.88
                       Mean reward: 9505.49
               Mean episode length: 397.42
                 Mean success rate: 78.00
                  Mean reward/step: 23.81
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14475264
                    Iteration time: 2.52s
                        Total time: 4695.94s
                               ETA: 526822.2s

################################################################################
                    [1m Learning iteration 1767/200000 [0m

                       Computation: 3189 steps/s (collection: 0.474s, learning 2.094s)
               Value function loss: 102952.6080
                    Surrogate loss: 0.0116
             Mean action noise std: 0.88
                       Mean reward: 9719.17
               Mean episode length: 405.35
                 Mean success rate: 79.50
                  Mean reward/step: 23.35
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14483456
                    Iteration time: 2.57s
                        Total time: 4698.51s
                               ETA: 526809.6s

################################################################################
                    [1m Learning iteration 1768/200000 [0m

                       Computation: 3212 steps/s (collection: 0.489s, learning 2.061s)
               Value function loss: 94416.3618
                    Surrogate loss: 0.0140
             Mean action noise std: 0.88
                       Mean reward: 9644.14
               Mean episode length: 404.50
                 Mean success rate: 79.50
                  Mean reward/step: 23.00
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14491648
                    Iteration time: 2.55s
                        Total time: 4701.06s
                               ETA: 526794.8s

################################################################################
                    [1m Learning iteration 1769/200000 [0m

                       Computation: 3226 steps/s (collection: 0.496s, learning 2.043s)
               Value function loss: 115110.6277
                    Surrogate loss: 0.0127
             Mean action noise std: 0.88
                       Mean reward: 9393.78
               Mean episode length: 396.39
                 Mean success rate: 78.00
                  Mean reward/step: 22.55
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14499840
                    Iteration time: 2.54s
                        Total time: 4703.60s
                               ETA: 526778.9s

################################################################################
                    [1m Learning iteration 1770/200000 [0m

                       Computation: 3237 steps/s (collection: 0.446s, learning 2.084s)
               Value function loss: 91415.7894
                    Surrogate loss: 0.0124
             Mean action noise std: 0.88
                       Mean reward: 9471.99
               Mean episode length: 397.77
                 Mean success rate: 78.50
                  Mean reward/step: 22.01
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14508032
                    Iteration time: 2.53s
                        Total time: 4706.13s
                               ETA: 526762.0s

################################################################################
                    [1m Learning iteration 1771/200000 [0m

                       Computation: 3206 steps/s (collection: 0.496s, learning 2.060s)
               Value function loss: 99594.1521
                    Surrogate loss: 0.0123
             Mean action noise std: 0.88
                       Mean reward: 9707.95
               Mean episode length: 405.50
                 Mean success rate: 80.00
                  Mean reward/step: 22.21
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14516224
                    Iteration time: 2.56s
                        Total time: 4708.68s
                               ETA: 526747.9s

################################################################################
                    [1m Learning iteration 1772/200000 [0m

                       Computation: 3275 steps/s (collection: 0.447s, learning 2.054s)
               Value function loss: 87155.9876
                    Surrogate loss: 0.0138
             Mean action noise std: 0.88
                       Mean reward: 9683.57
               Mean episode length: 408.21
                 Mean success rate: 80.50
                  Mean reward/step: 21.98
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14524416
                    Iteration time: 2.50s
                        Total time: 4711.18s
                               ETA: 526727.8s

################################################################################
                    [1m Learning iteration 1773/200000 [0m

                       Computation: 3227 steps/s (collection: 0.463s, learning 2.075s)
               Value function loss: 75774.8008
                    Surrogate loss: 0.0166
             Mean action noise std: 0.88
                       Mean reward: 9356.30
               Mean episode length: 399.15
                 Mean success rate: 78.50
                  Mean reward/step: 22.26
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14532608
                    Iteration time: 2.54s
                        Total time: 4713.72s
                               ETA: 526711.8s

################################################################################
                    [1m Learning iteration 1774/200000 [0m

                       Computation: 3231 steps/s (collection: 0.490s, learning 2.045s)
               Value function loss: 85279.3048
                    Surrogate loss: 0.0166
             Mean action noise std: 0.88
                       Mean reward: 9139.56
               Mean episode length: 393.45
                 Mean success rate: 76.50
                  Mean reward/step: 22.19
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14540800
                    Iteration time: 2.54s
                        Total time: 4716.26s
                               ETA: 526695.5s

################################################################################
                    [1m Learning iteration 1775/200000 [0m

                       Computation: 3188 steps/s (collection: 0.500s, learning 2.069s)
               Value function loss: 97561.4748
                    Surrogate loss: 0.0124
             Mean action noise std: 0.88
                       Mean reward: 9392.80
               Mean episode length: 404.91
                 Mean success rate: 78.50
                  Mean reward/step: 22.05
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 2.57s
                        Total time: 4718.83s
                               ETA: 526683.1s

################################################################################
                    [1m Learning iteration 1776/200000 [0m

                       Computation: 3196 steps/s (collection: 0.490s, learning 2.072s)
               Value function loss: 90203.0724
                    Surrogate loss: 0.0164
             Mean action noise std: 0.88
                       Mean reward: 8863.13
               Mean episode length: 388.68
                 Mean success rate: 73.50
                  Mean reward/step: 22.18
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14557184
                    Iteration time: 2.56s
                        Total time: 4721.39s
                               ETA: 526669.9s

################################################################################
                    [1m Learning iteration 1777/200000 [0m

                       Computation: 3114 steps/s (collection: 0.525s, learning 2.105s)
               Value function loss: 85754.1201
                    Surrogate loss: 0.0127
             Mean action noise std: 0.88
                       Mean reward: 8574.83
               Mean episode length: 376.52
                 Mean success rate: 71.00
                  Mean reward/step: 22.65
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14565376
                    Iteration time: 2.63s
                        Total time: 4724.02s
                               ETA: 526664.2s

################################################################################
                    [1m Learning iteration 1778/200000 [0m

                       Computation: 3200 steps/s (collection: 0.489s, learning 2.071s)
               Value function loss: 112486.7902
                    Surrogate loss: 0.0154
             Mean action noise std: 0.88
                       Mean reward: 8596.87
               Mean episode length: 380.06
                 Mean success rate: 72.00
                  Mean reward/step: 23.60
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14573568
                    Iteration time: 2.56s
                        Total time: 4726.58s
                               ETA: 526650.8s

################################################################################
                    [1m Learning iteration 1779/200000 [0m

                       Computation: 3175 steps/s (collection: 0.516s, learning 2.064s)
               Value function loss: 55478.9394
                    Surrogate loss: 0.0158
             Mean action noise std: 0.88
                       Mean reward: 8215.94
               Mean episode length: 368.33
                 Mean success rate: 69.50
                  Mean reward/step: 23.60
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14581760
                    Iteration time: 2.58s
                        Total time: 4729.16s
                               ETA: 526639.5s

################################################################################
                    [1m Learning iteration 1780/200000 [0m

                       Computation: 3218 steps/s (collection: 0.483s, learning 2.062s)
               Value function loss: 71074.3337
                    Surrogate loss: 0.0145
             Mean action noise std: 0.88
                       Mean reward: 7745.01
               Mean episode length: 353.98
                 Mean success rate: 65.50
                  Mean reward/step: 24.17
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14589952
                    Iteration time: 2.55s
                        Total time: 4731.70s
                               ETA: 526624.4s

################################################################################
                    [1m Learning iteration 1781/200000 [0m

                       Computation: 3159 steps/s (collection: 0.495s, learning 2.098s)
               Value function loss: 106422.7566
                    Surrogate loss: 0.0144
             Mean action noise std: 0.88
                       Mean reward: 8119.45
               Mean episode length: 365.00
                 Mean success rate: 68.50
                  Mean reward/step: 23.93
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14598144
                    Iteration time: 2.59s
                        Total time: 4734.30s
                               ETA: 526614.7s

################################################################################
                    [1m Learning iteration 1782/200000 [0m

                       Computation: 3124 steps/s (collection: 0.528s, learning 2.093s)
               Value function loss: 109516.0588
                    Surrogate loss: 0.0154
             Mean action noise std: 0.88
                       Mean reward: 8200.52
               Mean episode length: 366.49
                 Mean success rate: 68.50
                  Mean reward/step: 23.17
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14606336
                    Iteration time: 2.62s
                        Total time: 4736.92s
                               ETA: 526608.1s

################################################################################
                    [1m Learning iteration 1783/200000 [0m

                       Computation: 3125 steps/s (collection: 0.524s, learning 2.098s)
               Value function loss: 78423.7309
                    Surrogate loss: 0.0122
             Mean action noise std: 0.88
                       Mean reward: 8383.03
               Mean episode length: 376.94
                 Mean success rate: 70.00
                  Mean reward/step: 22.56
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14614528
                    Iteration time: 2.62s
                        Total time: 4739.54s
                               ETA: 526601.5s

################################################################################
                    [1m Learning iteration 1784/200000 [0m

                       Computation: 3122 steps/s (collection: 0.524s, learning 2.100s)
               Value function loss: 63902.0189
                    Surrogate loss: 0.0141
             Mean action noise std: 0.88
                       Mean reward: 8140.36
               Mean episode length: 366.56
                 Mean success rate: 68.00
                  Mean reward/step: 22.58
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14622720
                    Iteration time: 2.62s
                        Total time: 4742.16s
                               ETA: 526595.1s

################################################################################
                    [1m Learning iteration 1785/200000 [0m

                       Computation: 3103 steps/s (collection: 0.549s, learning 2.090s)
               Value function loss: 124997.4815
                    Surrogate loss: 0.0170
             Mean action noise std: 0.88
                       Mean reward: 8590.78
               Mean episode length: 378.63
                 Mean success rate: 72.00
                  Mean reward/step: 22.41
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14630912
                    Iteration time: 2.64s
                        Total time: 4744.80s
                               ETA: 526590.6s

################################################################################
                    [1m Learning iteration 1786/200000 [0m

                       Computation: 3198 steps/s (collection: 0.510s, learning 2.052s)
               Value function loss: 86140.4819
                    Surrogate loss: 0.0159
             Mean action noise std: 0.88
                       Mean reward: 8550.20
               Mean episode length: 375.67
                 Mean success rate: 72.00
                  Mean reward/step: 22.21
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14639104
                    Iteration time: 2.56s
                        Total time: 4747.36s
                               ETA: 526577.4s

################################################################################
                    [1m Learning iteration 1787/200000 [0m

                       Computation: 2954 steps/s (collection: 0.634s, learning 2.139s)
               Value function loss: 96847.2480
                    Surrogate loss: 0.0148
             Mean action noise std: 0.88
                       Mean reward: 8221.87
               Mean episode length: 364.40
                 Mean success rate: 68.00
                  Mean reward/step: 22.22
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 2.77s
                        Total time: 4750.14s
                               ETA: 526587.6s

################################################################################
                    [1m Learning iteration 1788/200000 [0m

                       Computation: 3095 steps/s (collection: 0.586s, learning 2.060s)
               Value function loss: 98688.6441
                    Surrogate loss: 0.0132
             Mean action noise std: 0.88
                       Mean reward: 8866.73
               Mean episode length: 384.50
                 Mean success rate: 72.00
                  Mean reward/step: 22.08
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14655488
                    Iteration time: 2.65s
                        Total time: 4752.78s
                               ETA: 526583.8s

################################################################################
                    [1m Learning iteration 1789/200000 [0m

                       Computation: 3250 steps/s (collection: 0.495s, learning 2.025s)
               Value function loss: 57441.6044
                    Surrogate loss: 0.0139
             Mean action noise std: 0.88
                       Mean reward: 8666.17
               Mean episode length: 379.80
                 Mean success rate: 71.50
                  Mean reward/step: 22.34
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14663680
                    Iteration time: 2.52s
                        Total time: 4755.30s
                               ETA: 526566.0s

################################################################################
                    [1m Learning iteration 1790/200000 [0m

                       Computation: 3194 steps/s (collection: 0.500s, learning 2.065s)
               Value function loss: 119413.4494
                    Surrogate loss: 0.0125
             Mean action noise std: 0.88
                       Mean reward: 8831.42
               Mean episode length: 386.33
                 Mean success rate: 72.50
                  Mean reward/step: 23.24
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14671872
                    Iteration time: 2.56s
                        Total time: 4757.87s
                               ETA: 526553.2s

################################################################################
                    [1m Learning iteration 1791/200000 [0m

                       Computation: 3131 steps/s (collection: 0.534s, learning 2.082s)
               Value function loss: 71496.2305
                    Surrogate loss: 0.0137
             Mean action noise std: 0.88
                       Mean reward: 8791.89
               Mean episode length: 384.69
                 Mean success rate: 72.50
                  Mean reward/step: 22.46
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 14680064
                    Iteration time: 2.62s
                        Total time: 4760.48s
                               ETA: 526546.1s

################################################################################
                    [1m Learning iteration 1792/200000 [0m

                       Computation: 3189 steps/s (collection: 0.511s, learning 2.058s)
               Value function loss: 83763.6801
                    Surrogate loss: 0.0133
             Mean action noise std: 0.88
                       Mean reward: 8733.68
               Mean episode length: 381.34
                 Mean success rate: 72.00
                  Mean reward/step: 22.42
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14688256
                    Iteration time: 2.57s
                        Total time: 4763.05s
                               ETA: 526533.7s

################################################################################
                    [1m Learning iteration 1793/200000 [0m

                       Computation: 3160 steps/s (collection: 0.511s, learning 2.082s)
               Value function loss: 64963.1779
                    Surrogate loss: 0.0099
             Mean action noise std: 0.88
                       Mean reward: 9020.21
               Mean episode length: 389.93
                 Mean success rate: 74.00
                  Mean reward/step: 23.02
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14696448
                    Iteration time: 2.59s
                        Total time: 4765.64s
                               ETA: 526523.9s

################################################################################
                    [1m Learning iteration 1794/200000 [0m

                       Computation: 3129 steps/s (collection: 0.571s, learning 2.047s)
               Value function loss: 91882.5776
                    Surrogate loss: 0.0092
             Mean action noise std: 0.88
                       Mean reward: 8871.62
               Mean episode length: 386.76
                 Mean success rate: 73.00
                  Mean reward/step: 23.81
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14704640
                    Iteration time: 2.62s
                        Total time: 4768.26s
                               ETA: 526517.0s

################################################################################
                    [1m Learning iteration 1795/200000 [0m

                       Computation: 3111 steps/s (collection: 0.518s, learning 2.115s)
               Value function loss: 66225.9131
                    Surrogate loss: 0.0131
             Mean action noise std: 0.88
                       Mean reward: 9075.15
               Mean episode length: 398.66
                 Mean success rate: 74.50
                  Mean reward/step: 24.50
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14712832
                    Iteration time: 2.63s
                        Total time: 4770.89s
                               ETA: 526511.7s

################################################################################
                    [1m Learning iteration 1796/200000 [0m

                       Computation: 3060 steps/s (collection: 0.556s, learning 2.121s)
               Value function loss: 71382.9004
                    Surrogate loss: 0.0150
             Mean action noise std: 0.88
                       Mean reward: 9166.13
               Mean episode length: 402.95
                 Mean success rate: 75.50
                  Mean reward/step: 25.71
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 14721024
                    Iteration time: 2.68s
                        Total time: 4773.57s
                               ETA: 526511.3s

################################################################################
                    [1m Learning iteration 1797/200000 [0m

                       Computation: 3143 steps/s (collection: 0.522s, learning 2.084s)
               Value function loss: 86043.1604
                    Surrogate loss: 0.0100
             Mean action noise std: 0.88
                       Mean reward: 9544.32
               Mean episode length: 415.49
                 Mean success rate: 78.50
                  Mean reward/step: 26.19
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14729216
                    Iteration time: 2.61s
                        Total time: 4776.18s
                               ETA: 526503.1s

################################################################################
                    [1m Learning iteration 1798/200000 [0m

                       Computation: 3176 steps/s (collection: 0.494s, learning 2.084s)
               Value function loss: 92586.7784
                    Surrogate loss: 0.0113
             Mean action noise std: 0.88
                       Mean reward: 9914.96
               Mean episode length: 424.70
                 Mean success rate: 82.00
                  Mean reward/step: 25.85
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14737408
                    Iteration time: 2.58s
                        Total time: 4778.76s
                               ETA: 526491.9s

################################################################################
                    [1m Learning iteration 1799/200000 [0m

                       Computation: 3230 steps/s (collection: 0.480s, learning 2.056s)
               Value function loss: 99527.0922
                    Surrogate loss: 0.0146
             Mean action noise std: 0.88
                       Mean reward: 9824.50
               Mean episode length: 423.20
                 Mean success rate: 81.50
                  Mean reward/step: 25.36
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 2.54s
                        Total time: 4781.29s
                               ETA: 526475.9s

################################################################################
                    [1m Learning iteration 1800/200000 [0m

                       Computation: 3272 steps/s (collection: 0.470s, learning 2.033s)
               Value function loss: 71938.8682
                    Surrogate loss: 0.0129
             Mean action noise std: 0.88
                       Mean reward: 9846.92
               Mean episode length: 425.08
                 Mean success rate: 81.50
                  Mean reward/step: 25.66
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14753792
                    Iteration time: 2.50s
                        Total time: 4783.79s
                               ETA: 526456.4s

################################################################################
                    [1m Learning iteration 1801/200000 [0m

                       Computation: 3115 steps/s (collection: 0.558s, learning 2.071s)
               Value function loss: 129224.4949
                    Surrogate loss: 0.0151
             Mean action noise std: 0.88
                       Mean reward: 9683.64
               Mean episode length: 415.94
                 Mean success rate: 79.50
                  Mean reward/step: 25.26
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 14761984
                    Iteration time: 2.63s
                        Total time: 4786.42s
                               ETA: 526450.8s

################################################################################
                    [1m Learning iteration 1802/200000 [0m

                       Computation: 3193 steps/s (collection: 0.502s, learning 2.063s)
               Value function loss: 98049.0382
                    Surrogate loss: 0.0164
             Mean action noise std: 0.88
                       Mean reward: 9954.27
               Mean episode length: 423.86
                 Mean success rate: 81.00
                  Mean reward/step: 24.52
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14770176
                    Iteration time: 2.57s
                        Total time: 4788.99s
                               ETA: 526438.2s

################################################################################
                    [1m Learning iteration 1803/200000 [0m

                       Computation: 3143 steps/s (collection: 0.542s, learning 2.064s)
               Value function loss: 135288.7510
                    Surrogate loss: 0.0152
             Mean action noise std: 0.88
                       Mean reward: 10156.29
               Mean episode length: 429.06
                 Mean success rate: 82.50
                  Mean reward/step: 23.78
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 14778368
                    Iteration time: 2.61s
                        Total time: 4791.60s
                               ETA: 526430.1s

################################################################################
                    [1m Learning iteration 1804/200000 [0m

                       Computation: 3151 steps/s (collection: 0.538s, learning 2.061s)
               Value function loss: 95048.5655
                    Surrogate loss: 0.0157
             Mean action noise std: 0.88
                       Mean reward: 10281.27
               Mean episode length: 433.55
                 Mean success rate: 83.50
                  Mean reward/step: 23.47
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14786560
                    Iteration time: 2.60s
                        Total time: 4794.19s
                               ETA: 526421.1s

################################################################################
                    [1m Learning iteration 1805/200000 [0m

                       Computation: 3179 steps/s (collection: 0.509s, learning 2.067s)
               Value function loss: 81039.6531
                    Surrogate loss: 0.0136
             Mean action noise std: 0.88
                       Mean reward: 10388.47
               Mean episode length: 433.74
                 Mean success rate: 84.00
                  Mean reward/step: 23.51
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14794752
                    Iteration time: 2.58s
                        Total time: 4796.77s
                               ETA: 526409.7s

################################################################################
                    [1m Learning iteration 1806/200000 [0m

                       Computation: 3020 steps/s (collection: 0.615s, learning 2.098s)
               Value function loss: 137559.6467
                    Surrogate loss: 0.0162
             Mean action noise std: 0.88
                       Mean reward: 10539.97
               Mean episode length: 432.68
                 Mean success rate: 84.50
                  Mean reward/step: 23.51
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 14802944
                    Iteration time: 2.71s
                        Total time: 4799.48s
                               ETA: 526413.3s

################################################################################
                    [1m Learning iteration 1807/200000 [0m

                       Computation: 3072 steps/s (collection: 0.563s, learning 2.103s)
               Value function loss: 99157.6889
                    Surrogate loss: 0.0159
             Mean action noise std: 0.88
                       Mean reward: 10162.76
               Mean episode length: 419.81
                 Mean success rate: 81.00
                  Mean reward/step: 22.57
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14811136
                    Iteration time: 2.67s
                        Total time: 4802.15s
                               ETA: 526411.7s

################################################################################
                    [1m Learning iteration 1808/200000 [0m

                       Computation: 3163 steps/s (collection: 0.549s, learning 2.041s)
               Value function loss: 110756.7506
                    Surrogate loss: 0.0180
             Mean action noise std: 0.88
                       Mean reward: 10006.99
               Mean episode length: 413.58
                 Mean success rate: 79.50
                  Mean reward/step: 22.30
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14819328
                    Iteration time: 2.59s
                        Total time: 4804.74s
                               ETA: 526401.8s

################################################################################
                    [1m Learning iteration 1809/200000 [0m

                       Computation: 3231 steps/s (collection: 0.497s, learning 2.038s)
               Value function loss: 77397.4203
                    Surrogate loss: 0.0140
             Mean action noise std: 0.88
                       Mean reward: 10444.59
               Mean episode length: 425.06
                 Mean success rate: 82.50
                  Mean reward/step: 22.57
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 14827520
                    Iteration time: 2.53s
                        Total time: 4807.27s
                               ETA: 526385.8s

################################################################################
                    [1m Learning iteration 1810/200000 [0m

                       Computation: 3233 steps/s (collection: 0.487s, learning 2.046s)
               Value function loss: 79918.9623
                    Surrogate loss: 0.0163
             Mean action noise std: 0.88
                       Mean reward: 10253.82
               Mean episode length: 421.31
                 Mean success rate: 81.50
                  Mean reward/step: 23.88
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14835712
                    Iteration time: 2.53s
                        Total time: 4809.81s
                               ETA: 526369.7s

################################################################################
                    [1m Learning iteration 1811/200000 [0m

                       Computation: 3258 steps/s (collection: 0.477s, learning 2.037s)
               Value function loss: 46774.9512
                    Surrogate loss: 0.0150
             Mean action noise std: 0.88
                       Mean reward: 10172.39
               Mean episode length: 418.34
                 Mean success rate: 80.50
                  Mean reward/step: 24.64
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 2.51s
                        Total time: 4812.32s
                               ETA: 526351.6s

################################################################################
                    [1m Learning iteration 1812/200000 [0m

                       Computation: 3261 steps/s (collection: 0.471s, learning 2.041s)
               Value function loss: 87698.0237
                    Surrogate loss: 0.0134
             Mean action noise std: 0.88
                       Mean reward: 10057.07
               Mean episode length: 412.72
                 Mean success rate: 79.00
                  Mean reward/step: 24.80
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14852096
                    Iteration time: 2.51s
                        Total time: 4814.83s
                               ETA: 526333.2s

################################################################################
                    [1m Learning iteration 1813/200000 [0m

                       Computation: 3267 steps/s (collection: 0.467s, learning 2.040s)
               Value function loss: 85307.7842
                    Surrogate loss: 0.0142
             Mean action noise std: 0.88
                       Mean reward: 9865.08
               Mean episode length: 405.70
                 Mean success rate: 77.00
                  Mean reward/step: 25.49
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14860288
                    Iteration time: 2.51s
                        Total time: 4817.34s
                               ETA: 526314.3s

################################################################################
                    [1m Learning iteration 1814/200000 [0m

                       Computation: 3219 steps/s (collection: 0.493s, learning 2.052s)
               Value function loss: 93710.8283
                    Surrogate loss: 0.0131
             Mean action noise std: 0.88
                       Mean reward: 9562.85
               Mean episode length: 396.94
                 Mean success rate: 75.00
                  Mean reward/step: 25.58
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 14868480
                    Iteration time: 2.54s
                        Total time: 4819.88s
                               ETA: 526299.5s

################################################################################
                    [1m Learning iteration 1815/200000 [0m

                       Computation: 3204 steps/s (collection: 0.520s, learning 2.036s)
               Value function loss: 86219.8682
                    Surrogate loss: 0.0148
             Mean action noise std: 0.88
                       Mean reward: 9477.05
               Mean episode length: 395.75
                 Mean success rate: 74.50
                  Mean reward/step: 25.53
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 14876672
                    Iteration time: 2.56s
                        Total time: 4822.44s
                               ETA: 526286.0s

################################################################################
                    [1m Learning iteration 1816/200000 [0m

                       Computation: 3243 steps/s (collection: 0.456s, learning 2.069s)
               Value function loss: 115408.7570
                    Surrogate loss: 0.0149
             Mean action noise std: 0.88
                       Mean reward: 9475.65
               Mean episode length: 395.22
                 Mean success rate: 74.50
                  Mean reward/step: 25.23
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14884864
                    Iteration time: 2.53s
                        Total time: 4824.97s
                               ETA: 526269.1s

################################################################################
                    [1m Learning iteration 1817/200000 [0m

                       Computation: 3177 steps/s (collection: 0.513s, learning 2.066s)
               Value function loss: 95664.5310
                    Surrogate loss: 0.0127
             Mean action noise std: 0.88
                       Mean reward: 9746.45
               Mean episode length: 404.21
                 Mean success rate: 76.50
                  Mean reward/step: 24.89
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14893056
                    Iteration time: 2.58s
                        Total time: 4827.54s
                               ETA: 526258.1s

################################################################################
                    [1m Learning iteration 1818/200000 [0m

                       Computation: 3088 steps/s (collection: 0.545s, learning 2.107s)
               Value function loss: 91385.8326
                    Surrogate loss: 0.0144
             Mean action noise std: 0.88
                       Mean reward: 9925.91
               Mean episode length: 411.37
                 Mean success rate: 78.00
                  Mean reward/step: 25.92
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14901248
                    Iteration time: 2.65s
                        Total time: 4830.20s
                               ETA: 526255.1s

################################################################################
                    [1m Learning iteration 1819/200000 [0m

                       Computation: 3177 steps/s (collection: 0.483s, learning 2.095s)
               Value function loss: 91930.1609
                    Surrogate loss: 0.0153
             Mean action noise std: 0.88
                       Mean reward: 10181.55
               Mean episode length: 419.42
                 Mean success rate: 80.00
                  Mean reward/step: 24.77
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14909440
                    Iteration time: 2.58s
                        Total time: 4832.77s
                               ETA: 526244.0s

################################################################################
                    [1m Learning iteration 1820/200000 [0m

                       Computation: 3179 steps/s (collection: 0.449s, learning 2.128s)
               Value function loss: 87747.2354
                    Surrogate loss: 0.0141
             Mean action noise std: 0.88
                       Mean reward: 9854.74
               Mean episode length: 406.47
                 Mean success rate: 77.50
                  Mean reward/step: 24.51
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14917632
                    Iteration time: 2.58s
                        Total time: 4835.35s
                               ETA: 526232.8s

################################################################################
                    [1m Learning iteration 1821/200000 [0m

                       Computation: 3142 steps/s (collection: 0.499s, learning 2.108s)
               Value function loss: 94012.1006
                    Surrogate loss: 0.0117
             Mean action noise std: 0.88
                       Mean reward: 10034.75
               Mean episode length: 410.75
                 Mean success rate: 79.00
                  Mean reward/step: 24.65
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 14925824
                    Iteration time: 2.61s
                        Total time: 4837.96s
                               ETA: 526224.9s

################################################################################
                    [1m Learning iteration 1822/200000 [0m

                       Computation: 3101 steps/s (collection: 0.557s, learning 2.084s)
               Value function loss: 126967.8938
                    Surrogate loss: 0.0128
             Mean action noise std: 0.88
                       Mean reward: 10179.57
               Mean episode length: 417.52
                 Mean success rate: 80.50
                  Mean reward/step: 24.44
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 14934016
                    Iteration time: 2.64s
                        Total time: 4840.60s
                               ETA: 526220.7s

################################################################################
                    [1m Learning iteration 1823/200000 [0m

                       Computation: 3166 steps/s (collection: 0.489s, learning 2.098s)
               Value function loss: 98853.6159
                    Surrogate loss: 0.0149
             Mean action noise std: 0.88
                       Mean reward: 10456.80
               Mean episode length: 424.15
                 Mean success rate: 82.50
                  Mean reward/step: 23.43
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 2.59s
                        Total time: 4843.19s
                               ETA: 526210.6s

################################################################################
                    [1m Learning iteration 1824/200000 [0m

                       Computation: 3170 steps/s (collection: 0.501s, learning 2.084s)
               Value function loss: 93826.8434
                    Surrogate loss: 0.0131
             Mean action noise std: 0.88
                       Mean reward: 10449.82
               Mean episode length: 427.41
                 Mean success rate: 83.50
                  Mean reward/step: 23.44
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 14950400
                    Iteration time: 2.58s
                        Total time: 4845.77s
                               ETA: 526200.3s

################################################################################
                    [1m Learning iteration 1825/200000 [0m

                       Computation: 3146 steps/s (collection: 0.512s, learning 2.091s)
               Value function loss: 100516.1100
                    Surrogate loss: 0.0163
             Mean action noise std: 0.87
                       Mean reward: 10352.31
               Mean episode length: 424.38
                 Mean success rate: 82.50
                  Mean reward/step: 24.13
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 14958592
                    Iteration time: 2.60s
                        Total time: 4848.37s
                               ETA: 526192.0s

################################################################################
                    [1m Learning iteration 1826/200000 [0m

                       Computation: 3110 steps/s (collection: 0.547s, learning 2.087s)
               Value function loss: 69178.6194
                    Surrogate loss: 0.0152
             Mean action noise std: 0.88
                       Mean reward: 10038.77
               Mean episode length: 411.74
                 Mean success rate: 79.50
                  Mean reward/step: 24.45
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 14966784
                    Iteration time: 2.63s
                        Total time: 4851.01s
                               ETA: 526187.0s

################################################################################
                    [1m Learning iteration 1827/200000 [0m

                       Computation: 3195 steps/s (collection: 0.507s, learning 2.056s)
               Value function loss: 49541.2503
                    Surrogate loss: 0.0109
             Mean action noise std: 0.87
                       Mean reward: 9807.58
               Mean episode length: 404.12
                 Mean success rate: 77.50
                  Mean reward/step: 24.94
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 14974976
                    Iteration time: 2.56s
                        Total time: 4853.57s
                               ETA: 526174.4s

################################################################################
                    [1m Learning iteration 1828/200000 [0m

                       Computation: 3082 steps/s (collection: 0.549s, learning 2.109s)
               Value function loss: 107413.9316
                    Surrogate loss: 0.0139
             Mean action noise std: 0.87
                       Mean reward: 9569.05
               Mean episode length: 395.44
                 Mean success rate: 75.50
                  Mean reward/step: 25.38
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 14983168
                    Iteration time: 2.66s
                        Total time: 4856.23s
                               ETA: 526172.0s

################################################################################
                    [1m Learning iteration 1829/200000 [0m

                       Computation: 3061 steps/s (collection: 0.502s, learning 2.174s)
               Value function loss: 91946.7303
                    Surrogate loss: 0.0128
             Mean action noise std: 0.87
                       Mean reward: 9313.05
               Mean episode length: 386.38
                 Mean success rate: 74.00
                  Mean reward/step: 24.71
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 14991360
                    Iteration time: 2.68s
                        Total time: 4858.90s
                               ETA: 526171.6s

################################################################################
                    [1m Learning iteration 1830/200000 [0m

                       Computation: 2982 steps/s (collection: 0.611s, learning 2.136s)
               Value function loss: 87158.9667
                    Surrogate loss: 0.0166
             Mean action noise std: 0.88
                       Mean reward: 9116.29
               Mean episode length: 378.96
                 Mean success rate: 72.50
                  Mean reward/step: 23.84
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 14999552
                    Iteration time: 2.75s
                        Total time: 4861.65s
                               ETA: 526178.8s

################################################################################
                    [1m Learning iteration 1831/200000 [0m

                       Computation: 3104 steps/s (collection: 0.498s, learning 2.141s)
               Value function loss: 81172.3382
                    Surrogate loss: 0.0140
             Mean action noise std: 0.88
                       Mean reward: 9009.35
               Mean episode length: 376.46
                 Mean success rate: 72.00
                  Mean reward/step: 24.30
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15007744
                    Iteration time: 2.64s
                        Total time: 4864.29s
                               ETA: 526174.4s

################################################################################
                    [1m Learning iteration 1832/200000 [0m

                       Computation: 3137 steps/s (collection: 0.502s, learning 2.108s)
               Value function loss: 115249.8414
                    Surrogate loss: 0.0147
             Mean action noise std: 0.88
                       Mean reward: 8860.55
               Mean episode length: 370.38
                 Mean success rate: 71.00
                  Mean reward/step: 24.22
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 15015936
                    Iteration time: 2.61s
                        Total time: 4866.90s
                               ETA: 526167.0s

################################################################################
                    [1m Learning iteration 1833/200000 [0m

                       Computation: 3066 steps/s (collection: 0.541s, learning 2.131s)
               Value function loss: 70287.6477
                    Surrogate loss: 0.0113
             Mean action noise std: 0.88
                       Mean reward: 8496.79
               Mean episode length: 357.03
                 Mean success rate: 68.50
                  Mean reward/step: 24.44
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15024128
                    Iteration time: 2.67s
                        Total time: 4869.57s
                               ETA: 526166.1s

################################################################################
                    [1m Learning iteration 1834/200000 [0m

                       Computation: 3085 steps/s (collection: 0.537s, learning 2.118s)
               Value function loss: 108640.7394
                    Surrogate loss: 0.0113
             Mean action noise std: 0.88
                       Mean reward: 8989.14
               Mean episode length: 369.55
                 Mean success rate: 71.50
                  Mean reward/step: 24.57
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15032320
                    Iteration time: 2.66s
                        Total time: 4872.23s
                               ETA: 526163.4s

################################################################################
                    [1m Learning iteration 1835/200000 [0m

                       Computation: 3082 steps/s (collection: 0.522s, learning 2.136s)
               Value function loss: 85393.5800
                    Surrogate loss: 0.0157
             Mean action noise std: 0.88
                       Mean reward: 9133.40
               Mean episode length: 373.37
                 Mean success rate: 72.50
                  Mean reward/step: 24.28
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 2.66s
                        Total time: 4874.89s
                               ETA: 526161.0s

################################################################################
                    [1m Learning iteration 1836/200000 [0m

                       Computation: 3118 steps/s (collection: 0.514s, learning 2.113s)
               Value function loss: 79148.8854
                    Surrogate loss: 0.0123
             Mean action noise std: 0.88
                       Mean reward: 9149.91
               Mean episode length: 374.15
                 Mean success rate: 73.00
                  Mean reward/step: 24.49
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15048704
                    Iteration time: 2.63s
                        Total time: 4877.51s
                               ETA: 526155.3s

################################################################################
                    [1m Learning iteration 1837/200000 [0m

                       Computation: 3093 steps/s (collection: 0.586s, learning 2.062s)
               Value function loss: 109329.6906
                    Surrogate loss: 0.0122
             Mean action noise std: 0.88
                       Mean reward: 9097.36
               Mean episode length: 373.98
                 Mean success rate: 73.00
                  Mean reward/step: 24.77
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15056896
                    Iteration time: 2.65s
                        Total time: 4880.16s
                               ETA: 526151.9s

################################################################################
                    [1m Learning iteration 1838/200000 [0m

                       Computation: 3058 steps/s (collection: 0.526s, learning 2.152s)
               Value function loss: 103408.2938
                    Surrogate loss: 0.0122
             Mean action noise std: 0.88
                       Mean reward: 9488.47
               Mean episode length: 385.22
                 Mean success rate: 75.50
                  Mean reward/step: 24.40
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15065088
                    Iteration time: 2.68s
                        Total time: 4882.84s
                               ETA: 526151.7s

################################################################################
                    [1m Learning iteration 1839/200000 [0m

                       Computation: 3203 steps/s (collection: 0.544s, learning 2.013s)
               Value function loss: 79037.6809
                    Surrogate loss: 0.0115
             Mean action noise std: 0.88
                       Mean reward: 9651.59
               Mean episode length: 390.39
                 Mean success rate: 76.50
                  Mean reward/step: 24.09
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15073280
                    Iteration time: 2.56s
                        Total time: 4885.40s
                               ETA: 526138.5s

################################################################################
                    [1m Learning iteration 1840/200000 [0m

                       Computation: 3147 steps/s (collection: 0.562s, learning 2.041s)
               Value function loss: 87631.2217
                    Surrogate loss: 0.0116
             Mean action noise std: 0.88
                       Mean reward: 9635.68
               Mean episode length: 388.26
                 Mean success rate: 76.00
                  Mean reward/step: 23.50
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15081472
                    Iteration time: 2.60s
                        Total time: 4888.00s
                               ETA: 526130.2s

################################################################################
                    [1m Learning iteration 1841/200000 [0m

                       Computation: 3131 steps/s (collection: 0.531s, learning 2.085s)
               Value function loss: 72328.4350
                    Surrogate loss: 0.0135
             Mean action noise std: 0.88
                       Mean reward: 9776.86
               Mean episode length: 391.10
                 Mean success rate: 76.50
                  Mean reward/step: 23.56
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15089664
                    Iteration time: 2.62s
                        Total time: 4890.61s
                               ETA: 526123.3s

################################################################################
                    [1m Learning iteration 1842/200000 [0m

                       Computation: 3200 steps/s (collection: 0.526s, learning 2.033s)
               Value function loss: 70428.7812
                    Surrogate loss: 0.0109
             Mean action noise std: 0.88
                       Mean reward: 9741.35
               Mean episode length: 390.87
                 Mean success rate: 76.50
                  Mean reward/step: 24.12
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15097856
                    Iteration time: 2.56s
                        Total time: 4893.17s
                               ETA: 526110.3s

################################################################################
                    [1m Learning iteration 1843/200000 [0m

                       Computation: 3181 steps/s (collection: 0.548s, learning 2.027s)
               Value function loss: 73532.5230
                    Surrogate loss: 0.0117
             Mean action noise std: 0.88
                       Mean reward: 9585.33
               Mean episode length: 387.21
                 Mean success rate: 74.50
                  Mean reward/step: 24.80
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15106048
                    Iteration time: 2.57s
                        Total time: 4895.75s
                               ETA: 526099.1s

################################################################################
                    [1m Learning iteration 1844/200000 [0m

                       Computation: 3096 steps/s (collection: 0.581s, learning 2.064s)
               Value function loss: 98271.2964
                    Surrogate loss: 0.0126
             Mean action noise std: 0.88
                       Mean reward: 9311.32
               Mean episode length: 379.66
                 Mean success rate: 72.50
                  Mean reward/step: 25.16
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 15114240
                    Iteration time: 2.65s
                        Total time: 4898.39s
                               ETA: 526095.4s

################################################################################
                    [1m Learning iteration 1845/200000 [0m

                       Computation: 3242 steps/s (collection: 0.496s, learning 2.030s)
               Value function loss: 111290.9098
                    Surrogate loss: 0.0137
             Mean action noise std: 0.88
                       Mean reward: 9294.71
               Mean episode length: 380.23
                 Mean success rate: 73.00
                  Mean reward/step: 23.92
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 15122432
                    Iteration time: 2.53s
                        Total time: 4900.92s
                               ETA: 526078.9s

################################################################################
                    [1m Learning iteration 1846/200000 [0m

                       Computation: 3225 steps/s (collection: 0.501s, learning 2.038s)
               Value function loss: 99303.2734
                    Surrogate loss: 0.0147
             Mean action noise std: 0.88
                       Mean reward: 9099.90
               Mean episode length: 372.70
                 Mean success rate: 71.50
                  Mean reward/step: 22.70
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15130624
                    Iteration time: 2.54s
                        Total time: 4903.46s
                               ETA: 526063.8s

################################################################################
                    [1m Learning iteration 1847/200000 [0m

                       Computation: 3223 steps/s (collection: 0.493s, learning 2.048s)
               Value function loss: 99664.9176
                    Surrogate loss: 0.0132
             Mean action noise std: 0.88
                       Mean reward: 8222.42
               Mean episode length: 348.81
                 Mean success rate: 65.50
                  Mean reward/step: 22.32
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 2.54s
                        Total time: 4906.00s
                               ETA: 526049.0s

################################################################################
                    [1m Learning iteration 1848/200000 [0m

                       Computation: 3178 steps/s (collection: 0.514s, learning 2.063s)
               Value function loss: 99427.4981
                    Surrogate loss: 0.0134
             Mean action noise std: 0.88
                       Mean reward: 8377.96
               Mean episode length: 356.52
                 Mean success rate: 66.50
                  Mean reward/step: 21.48
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15147008
                    Iteration time: 2.58s
                        Total time: 4908.58s
                               ETA: 526038.1s

################################################################################
                    [1m Learning iteration 1849/200000 [0m

                       Computation: 3120 steps/s (collection: 0.548s, learning 2.077s)
               Value function loss: 82778.4361
                    Surrogate loss: 0.0128
             Mean action noise std: 0.88
                       Mean reward: 8106.29
               Mean episode length: 347.60
                 Mean success rate: 65.00
                  Mean reward/step: 21.59
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15155200
                    Iteration time: 2.62s
                        Total time: 4911.20s
                               ETA: 526032.2s

################################################################################
                    [1m Learning iteration 1850/200000 [0m

                       Computation: 3225 steps/s (collection: 0.499s, learning 2.041s)
               Value function loss: 109739.2979
                    Surrogate loss: 0.0124
             Mean action noise std: 0.88
                       Mean reward: 8282.01
               Mean episode length: 354.12
                 Mean success rate: 66.50
                  Mean reward/step: 21.33
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 15163392
                    Iteration time: 2.54s
                        Total time: 4913.74s
                               ETA: 526017.2s

################################################################################
                    [1m Learning iteration 1851/200000 [0m

                       Computation: 3169 steps/s (collection: 0.503s, learning 2.082s)
               Value function loss: 85507.6653
                    Surrogate loss: 0.0146
             Mean action noise std: 0.88
                       Mean reward: 8139.01
               Mean episode length: 347.94
                 Mean success rate: 66.00
                  Mean reward/step: 21.83
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15171584
                    Iteration time: 2.58s
                        Total time: 4916.33s
                               ETA: 526007.1s

################################################################################
                    [1m Learning iteration 1852/200000 [0m

                       Computation: 3091 steps/s (collection: 0.542s, learning 2.108s)
               Value function loss: 62167.9932
                    Surrogate loss: 0.0147
             Mean action noise std: 0.88
                       Mean reward: 8007.13
               Mean episode length: 344.65
                 Mean success rate: 67.00
                  Mean reward/step: 21.16
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15179776
                    Iteration time: 2.65s
                        Total time: 4918.98s
                               ETA: 526004.0s

################################################################################
                    [1m Learning iteration 1853/200000 [0m

                       Computation: 3078 steps/s (collection: 0.581s, learning 2.080s)
               Value function loss: 93793.2246
                    Surrogate loss: 0.0113
             Mean action noise std: 0.88
                       Mean reward: 7674.14
               Mean episode length: 335.25
                 Mean success rate: 64.00
                  Mean reward/step: 21.27
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 15187968
                    Iteration time: 2.66s
                        Total time: 4921.64s
                               ETA: 526002.0s

################################################################################
                    [1m Learning iteration 1854/200000 [0m

                       Computation: 3105 steps/s (collection: 0.531s, learning 2.107s)
               Value function loss: 80546.9031
                    Surrogate loss: 0.0139
             Mean action noise std: 0.88
                       Mean reward: 7441.31
               Mean episode length: 328.94
                 Mean success rate: 62.50
                  Mean reward/step: 21.41
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15196160
                    Iteration time: 2.64s
                        Total time: 4924.28s
                               ETA: 525997.5s

################################################################################
                    [1m Learning iteration 1855/200000 [0m

                       Computation: 3076 steps/s (collection: 0.566s, learning 2.097s)
               Value function loss: 90027.9768
                    Surrogate loss: 0.0147
             Mean action noise std: 0.88
                       Mean reward: 7635.57
               Mean episode length: 334.06
                 Mean success rate: 64.00
                  Mean reward/step: 21.33
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 15204352
                    Iteration time: 2.66s
                        Total time: 4926.94s
                               ETA: 525995.7s

################################################################################
                    [1m Learning iteration 1856/200000 [0m

                       Computation: 3093 steps/s (collection: 0.537s, learning 2.112s)
               Value function loss: 90915.9887
                    Surrogate loss: 0.0179
             Mean action noise std: 0.88
                       Mean reward: 7089.45
               Mean episode length: 320.88
                 Mean success rate: 60.50
                  Mean reward/step: 21.85
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 15212544
                    Iteration time: 2.65s
                        Total time: 4929.59s
                               ETA: 525992.4s

################################################################################
                    [1m Learning iteration 1857/200000 [0m

                       Computation: 3154 steps/s (collection: 0.506s, learning 2.091s)
               Value function loss: 53261.8020
                    Surrogate loss: 0.0116
             Mean action noise std: 0.88
                       Mean reward: 6763.08
               Mean episode length: 310.38
                 Mean success rate: 57.50
                  Mean reward/step: 21.58
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15220736
                    Iteration time: 2.60s
                        Total time: 4932.18s
                               ETA: 525983.6s

################################################################################
                    [1m Learning iteration 1858/200000 [0m

                       Computation: 3151 steps/s (collection: 0.516s, learning 2.084s)
               Value function loss: 64797.3406
                    Surrogate loss: 0.0114
             Mean action noise std: 0.88
                       Mean reward: 6498.73
               Mean episode length: 303.49
                 Mean success rate: 56.50
                  Mean reward/step: 22.55
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15228928
                    Iteration time: 2.60s
                        Total time: 4934.78s
                               ETA: 525975.1s

################################################################################
                    [1m Learning iteration 1859/200000 [0m

                       Computation: 3104 steps/s (collection: 0.535s, learning 2.103s)
               Value function loss: 62840.3096
                    Surrogate loss: 0.0179
             Mean action noise std: 0.88
                       Mean reward: 6225.39
               Mean episode length: 294.92
                 Mean success rate: 54.50
                  Mean reward/step: 22.70
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 2.64s
                        Total time: 4937.42s
                               ETA: 525970.7s

################################################################################
                    [1m Learning iteration 1860/200000 [0m

                       Computation: 3150 steps/s (collection: 0.531s, learning 2.070s)
               Value function loss: 78347.5619
                    Surrogate loss: 0.0143
             Mean action noise std: 0.88
                       Mean reward: 6551.31
               Mean episode length: 308.85
                 Mean success rate: 56.00
                  Mean reward/step: 22.50
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15245312
                    Iteration time: 2.60s
                        Total time: 4940.02s
                               ETA: 525962.3s

################################################################################
                    [1m Learning iteration 1861/200000 [0m

                       Computation: 3134 steps/s (collection: 0.536s, learning 2.078s)
               Value function loss: 63414.9234
                    Surrogate loss: 0.0129
             Mean action noise std: 0.88
                       Mean reward: 6636.46
               Mean episode length: 311.52
                 Mean success rate: 57.00
                  Mean reward/step: 22.65
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15253504
                    Iteration time: 2.61s
                        Total time: 4942.63s
                               ETA: 525955.3s

################################################################################
                    [1m Learning iteration 1862/200000 [0m

                       Computation: 3190 steps/s (collection: 0.544s, learning 2.023s)
               Value function loss: 107639.5662
                    Surrogate loss: 0.0115
             Mean action noise std: 0.88
                       Mean reward: 6652.15
               Mean episode length: 310.89
                 Mean success rate: 56.50
                  Mean reward/step: 23.07
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 15261696
                    Iteration time: 2.57s
                        Total time: 4945.20s
                               ETA: 525943.4s

################################################################################
                    [1m Learning iteration 1863/200000 [0m

                       Computation: 3195 steps/s (collection: 0.530s, learning 2.034s)
               Value function loss: 109987.9699
                    Surrogate loss: 0.0129
             Mean action noise std: 0.88
                       Mean reward: 6757.70
               Mean episode length: 313.94
                 Mean success rate: 57.00
                  Mean reward/step: 21.97
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 15269888
                    Iteration time: 2.56s
                        Total time: 4947.77s
                               ETA: 525931.1s

################################################################################
                    [1m Learning iteration 1864/200000 [0m

                       Computation: 3166 steps/s (collection: 0.527s, learning 2.060s)
               Value function loss: 79893.2017
                    Surrogate loss: 0.0134
             Mean action noise std: 0.88
                       Mean reward: 7040.52
               Mean episode length: 322.11
                 Mean success rate: 58.50
                  Mean reward/step: 20.99
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15278080
                    Iteration time: 2.59s
                        Total time: 4950.35s
                               ETA: 525921.3s

################################################################################
                    [1m Learning iteration 1865/200000 [0m

                       Computation: 3172 steps/s (collection: 0.540s, learning 2.042s)
               Value function loss: 81966.2355
                    Surrogate loss: 0.0120
             Mean action noise std: 0.88
                       Mean reward: 7055.19
               Mean episode length: 320.30
                 Mean success rate: 59.50
                  Mean reward/step: 21.63
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15286272
                    Iteration time: 2.58s
                        Total time: 4952.94s
                               ETA: 525911.0s

################################################################################
                    [1m Learning iteration 1866/200000 [0m

                       Computation: 3257 steps/s (collection: 0.493s, learning 2.022s)
               Value function loss: 63601.6476
                    Surrogate loss: 0.0131
             Mean action noise std: 0.88
                       Mean reward: 6726.67
               Mean episode length: 307.37
                 Mean success rate: 55.00
                  Mean reward/step: 21.50
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 15294464
                    Iteration time: 2.51s
                        Total time: 4955.45s
                               ETA: 525893.6s

################################################################################
                    [1m Learning iteration 1867/200000 [0m

                       Computation: 3237 steps/s (collection: 0.492s, learning 2.039s)
               Value function loss: 119242.5485
                    Surrogate loss: 0.0136
             Mean action noise std: 0.88
                       Mean reward: 7149.56
               Mean episode length: 320.35
                 Mean success rate: 58.00
                  Mean reward/step: 21.36
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 15302656
                    Iteration time: 2.53s
                        Total time: 4957.98s
                               ETA: 525877.8s

################################################################################
                    [1m Learning iteration 1868/200000 [0m

                       Computation: 3111 steps/s (collection: 0.586s, learning 2.046s)
               Value function loss: 115013.0521
                    Surrogate loss: 0.0117
             Mean action noise std: 0.88
                       Mean reward: 7026.33
               Mean episode length: 316.54
                 Mean success rate: 56.50
                  Mean reward/step: 20.77
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 15310848
                    Iteration time: 2.63s
                        Total time: 4960.61s
                               ETA: 525872.8s

################################################################################
                    [1m Learning iteration 1869/200000 [0m

                       Computation: 3023 steps/s (collection: 0.613s, learning 2.097s)
               Value function loss: 90925.6052
                    Surrogate loss: 0.0140
             Mean action noise std: 0.88
                       Mean reward: 6921.94
               Mean episode length: 311.60
                 Mean success rate: 55.50
                  Mean reward/step: 20.04
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 15319040
                    Iteration time: 2.71s
                        Total time: 4963.32s
                               ETA: 525876.0s

################################################################################
                    [1m Learning iteration 1870/200000 [0m

                       Computation: 3234 steps/s (collection: 0.501s, learning 2.032s)
               Value function loss: 116300.3285
                    Surrogate loss: 0.0160
             Mean action noise std: 0.88
                       Mean reward: 7065.25
               Mean episode length: 316.07
                 Mean success rate: 58.00
                  Mean reward/step: 20.03
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 15327232
                    Iteration time: 2.53s
                        Total time: 4965.86s
                               ETA: 525860.5s

################################################################################
                    [1m Learning iteration 1871/200000 [0m

                       Computation: 3132 steps/s (collection: 0.528s, learning 2.087s)
               Value function loss: 102258.8330
                    Surrogate loss: 0.0161
             Mean action noise std: 0.88
                       Mean reward: 7250.24
               Mean episode length: 322.96
                 Mean success rate: 59.00
                  Mean reward/step: 20.78
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 2.61s
                        Total time: 4968.47s
                               ETA: 525853.7s

################################################################################
                    [1m Learning iteration 1872/200000 [0m

                       Computation: 3173 steps/s (collection: 0.523s, learning 2.058s)
               Value function loss: 76744.3463
                    Surrogate loss: 0.0118
             Mean action noise std: 0.88
                       Mean reward: 7050.02
               Mean episode length: 321.56
                 Mean success rate: 57.50
                  Mean reward/step: 20.96
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 15343616
                    Iteration time: 2.58s
                        Total time: 4971.05s
                               ETA: 525843.4s

################################################################################
                    [1m Learning iteration 1873/200000 [0m

                       Computation: 3197 steps/s (collection: 0.500s, learning 2.061s)
               Value function loss: 52198.3355
                    Surrogate loss: 0.0129
             Mean action noise std: 0.88
                       Mean reward: 6986.11
               Mean episode length: 319.35
                 Mean success rate: 57.00
                  Mean reward/step: 22.16
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 15351808
                    Iteration time: 2.56s
                        Total time: 4973.61s
                               ETA: 525830.9s

################################################################################
                    [1m Learning iteration 1874/200000 [0m

                       Computation: 3109 steps/s (collection: 0.514s, learning 2.120s)
               Value function loss: 63766.1131
                    Surrogate loss: 0.0138
             Mean action noise std: 0.88
                       Mean reward: 6722.42
               Mean episode length: 313.20
                 Mean success rate: 56.00
                  Mean reward/step: 23.09
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 15360000
                    Iteration time: 2.63s
                        Total time: 4976.25s
                               ETA: 525826.2s

################################################################################
                    [1m Learning iteration 1875/200000 [0m

                       Computation: 3129 steps/s (collection: 0.538s, learning 2.080s)
               Value function loss: 95200.3828
                    Surrogate loss: 0.0167
             Mean action noise std: 0.88
                       Mean reward: 6952.07
               Mean episode length: 320.35
                 Mean success rate: 57.00
                  Mean reward/step: 22.66
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15368192
                    Iteration time: 2.62s
                        Total time: 4978.87s
                               ETA: 525819.7s

################################################################################
                    [1m Learning iteration 1876/200000 [0m

                       Computation: 3139 steps/s (collection: 0.499s, learning 2.110s)
               Value function loss: 67651.0368
                    Surrogate loss: 0.0138
             Mean action noise std: 0.88
                       Mean reward: 6891.75
               Mean episode length: 316.83
                 Mean success rate: 57.00
                  Mean reward/step: 22.71
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15376384
                    Iteration time: 2.61s
                        Total time: 4981.48s
                               ETA: 525812.4s

################################################################################
                    [1m Learning iteration 1877/200000 [0m

                       Computation: 3184 steps/s (collection: 0.486s, learning 2.087s)
               Value function loss: 55570.6032
                    Surrogate loss: 0.0131
             Mean action noise std: 0.88
                       Mean reward: 6946.22
               Mean episode length: 321.41
                 Mean success rate: 58.00
                  Mean reward/step: 23.17
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 15384576
                    Iteration time: 2.57s
                        Total time: 4984.05s
                               ETA: 525801.1s

################################################################################
                    [1m Learning iteration 1878/200000 [0m

                       Computation: 3127 steps/s (collection: 0.542s, learning 2.078s)
               Value function loss: 92349.0580
                    Surrogate loss: 0.0154
             Mean action noise std: 0.88
                       Mean reward: 7067.93
               Mean episode length: 324.44
                 Mean success rate: 59.50
                  Mean reward/step: 22.93
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 15392768
                    Iteration time: 2.62s
                        Total time: 4986.67s
                               ETA: 525794.8s

################################################################################
                    [1m Learning iteration 1879/200000 [0m

                       Computation: 3206 steps/s (collection: 0.491s, learning 2.063s)
               Value function loss: 82358.2278
                    Surrogate loss: 0.0120
             Mean action noise std: 0.88
                       Mean reward: 7132.37
               Mean episode length: 331.55
                 Mean success rate: 60.00
                  Mean reward/step: 22.50
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15400960
                    Iteration time: 2.55s
                        Total time: 4989.22s
                               ETA: 525781.7s

################################################################################
                    [1m Learning iteration 1880/200000 [0m

                       Computation: 3155 steps/s (collection: 0.505s, learning 2.091s)
               Value function loss: 71399.2676
                    Surrogate loss: 0.0114
             Mean action noise std: 0.88
                       Mean reward: 7038.95
               Mean episode length: 326.03
                 Mean success rate: 59.00
                  Mean reward/step: 22.37
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15409152
                    Iteration time: 2.60s
                        Total time: 4991.82s
                               ETA: 525773.0s

################################################################################
                    [1m Learning iteration 1881/200000 [0m

                       Computation: 3131 steps/s (collection: 0.487s, learning 2.129s)
               Value function loss: 112974.2348
                    Surrogate loss: 0.0156
             Mean action noise std: 0.88
                       Mean reward: 7543.08
               Mean episode length: 344.18
                 Mean success rate: 64.00
                  Mean reward/step: 22.26
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 15417344
                    Iteration time: 2.62s
                        Total time: 4994.43s
                               ETA: 525766.3s

################################################################################
                    [1m Learning iteration 1882/200000 [0m

                       Computation: 3146 steps/s (collection: 0.504s, learning 2.099s)
               Value function loss: 52104.7458
                    Surrogate loss: 0.0177
             Mean action noise std: 0.88
                       Mean reward: 7765.20
               Mean episode length: 353.88
                 Mean success rate: 66.00
                  Mean reward/step: 21.53
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15425536
                    Iteration time: 2.60s
                        Total time: 4997.04s
                               ETA: 525758.4s

################################################################################
                    [1m Learning iteration 1883/200000 [0m

                       Computation: 3159 steps/s (collection: 0.531s, learning 2.061s)
               Value function loss: 84839.7443
                    Surrogate loss: 0.0127
             Mean action noise std: 0.88
                       Mean reward: 8163.71
               Mean episode length: 366.94
                 Mean success rate: 70.00
                  Mean reward/step: 21.74
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 2.59s
                        Total time: 4999.63s
                               ETA: 525749.3s

################################################################################
                    [1m Learning iteration 1884/200000 [0m

                       Computation: 3195 steps/s (collection: 0.469s, learning 2.094s)
               Value function loss: 99403.0764
                    Surrogate loss: 0.0108
             Mean action noise std: 0.88
                       Mean reward: 8170.47
               Mean episode length: 367.39
                 Mean success rate: 70.00
                  Mean reward/step: 21.62
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 15441920
                    Iteration time: 2.56s
                        Total time: 5002.19s
                               ETA: 525737.1s

################################################################################
                    [1m Learning iteration 1885/200000 [0m

                       Computation: 3181 steps/s (collection: 0.471s, learning 2.104s)
               Value function loss: 83035.4582
                    Surrogate loss: 0.0118
             Mean action noise std: 0.88
                       Mean reward: 8263.79
               Mean episode length: 369.38
                 Mean success rate: 71.00
                  Mean reward/step: 21.52
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15450112
                    Iteration time: 2.57s
                        Total time: 5004.77s
                               ETA: 525726.2s

################################################################################
                    [1m Learning iteration 1886/200000 [0m

                       Computation: 3153 steps/s (collection: 0.515s, learning 2.083s)
               Value function loss: 97598.6881
                    Surrogate loss: 0.0128
             Mean action noise std: 0.88
                       Mean reward: 7851.62
               Mean episode length: 358.10
                 Mean success rate: 66.50
                  Mean reward/step: 21.44
       Mean episode length/episode: 28.05
--------------------------------------------------------------------------------
                   Total timesteps: 15458304
                    Iteration time: 2.60s
                        Total time: 5007.37s
                               ETA: 525717.7s

################################################################################
                    [1m Learning iteration 1887/200000 [0m

                       Computation: 3116 steps/s (collection: 0.496s, learning 2.133s)
               Value function loss: 117218.1066
                    Surrogate loss: 0.0148
             Mean action noise std: 0.88
                       Mean reward: 7761.86
               Mean episode length: 352.43
                 Mean success rate: 65.50
                  Mean reward/step: 21.87
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 15466496
                    Iteration time: 2.63s
                        Total time: 5009.99s
                               ETA: 525712.4s

################################################################################
                    [1m Learning iteration 1888/200000 [0m

                       Computation: 3144 steps/s (collection: 0.502s, learning 2.103s)
               Value function loss: 78205.6625
                    Surrogate loss: 0.0125
             Mean action noise std: 0.88
                       Mean reward: 7894.35
               Mean episode length: 358.26
                 Mean success rate: 67.50
                  Mean reward/step: 21.68
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15474688
                    Iteration time: 2.61s
                        Total time: 5012.60s
                               ETA: 525704.7s

################################################################################
                    [1m Learning iteration 1889/200000 [0m

                       Computation: 3112 steps/s (collection: 0.516s, learning 2.116s)
               Value function loss: 88195.8461
                    Surrogate loss: 0.0142
             Mean action noise std: 0.88
                       Mean reward: 7811.23
               Mean episode length: 355.63
                 Mean success rate: 66.50
                  Mean reward/step: 22.19
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15482880
                    Iteration time: 2.63s
                        Total time: 5015.23s
                               ETA: 525699.8s

################################################################################
                    [1m Learning iteration 1890/200000 [0m

                       Computation: 3183 steps/s (collection: 0.498s, learning 2.075s)
               Value function loss: 77348.7176
                    Surrogate loss: 0.0163
             Mean action noise std: 0.88
                       Mean reward: 8174.89
               Mean episode length: 364.00
                 Mean success rate: 68.50
                  Mean reward/step: 22.28
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15491072
                    Iteration time: 2.57s
                        Total time: 5017.81s
                               ETA: 525688.7s

################################################################################
                    [1m Learning iteration 1891/200000 [0m

                       Computation: 3095 steps/s (collection: 0.554s, learning 2.092s)
               Value function loss: 97967.6175
                    Surrogate loss: 0.0117
             Mean action noise std: 0.88
                       Mean reward: 7928.67
               Mean episode length: 352.69
                 Mean success rate: 66.50
                  Mean reward/step: 21.73
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 15499264
                    Iteration time: 2.65s
                        Total time: 5020.45s
                               ETA: 525685.3s

################################################################################
                    [1m Learning iteration 1892/200000 [0m

                       Computation: 3152 steps/s (collection: 0.493s, learning 2.106s)
               Value function loss: 77002.1881
                    Surrogate loss: 0.0158
             Mean action noise std: 0.88
                       Mean reward: 7847.18
               Mean episode length: 349.90
                 Mean success rate: 67.00
                  Mean reward/step: 21.53
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15507456
                    Iteration time: 2.60s
                        Total time: 5023.05s
                               ETA: 525676.9s

################################################################################
                    [1m Learning iteration 1893/200000 [0m

                       Computation: 3180 steps/s (collection: 0.521s, learning 2.055s)
               Value function loss: 94492.0857
                    Surrogate loss: 0.0169
             Mean action noise std: 0.88
                       Mean reward: 7850.26
               Mean episode length: 354.84
                 Mean success rate: 67.00
                  Mean reward/step: 21.63
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15515648
                    Iteration time: 2.58s
                        Total time: 5025.63s
                               ETA: 525666.1s

################################################################################
                    [1m Learning iteration 1894/200000 [0m

                       Computation: 3152 steps/s (collection: 0.487s, learning 2.111s)
               Value function loss: 82961.6285
                    Surrogate loss: 0.0107
             Mean action noise std: 0.88
                       Mean reward: 7822.06
               Mean episode length: 355.23
                 Mean success rate: 68.00
                  Mean reward/step: 21.41
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 15523840
                    Iteration time: 2.60s
                        Total time: 5028.22s
                               ETA: 525657.7s

################################################################################
                    [1m Learning iteration 1895/200000 [0m

                       Computation: 3152 steps/s (collection: 0.519s, learning 2.079s)
               Value function loss: 80441.0584
                    Surrogate loss: 0.0136
             Mean action noise std: 0.88
                       Mean reward: 7907.40
               Mean episode length: 357.72
                 Mean success rate: 69.00
                  Mean reward/step: 22.01
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 2.60s
                        Total time: 5030.82s
                               ETA: 525649.2s

################################################################################
                    [1m Learning iteration 1896/200000 [0m

                       Computation: 3177 steps/s (collection: 0.501s, learning 2.077s)
               Value function loss: 55943.6258
                    Surrogate loss: 0.0142
             Mean action noise std: 0.88
                       Mean reward: 7802.44
               Mean episode length: 355.36
                 Mean success rate: 68.50
                  Mean reward/step: 22.36
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15540224
                    Iteration time: 2.58s
                        Total time: 5033.40s
                               ETA: 525638.7s

################################################################################
                    [1m Learning iteration 1897/200000 [0m

                       Computation: 3187 steps/s (collection: 0.506s, learning 2.064s)
               Value function loss: 94782.7718
                    Surrogate loss: 0.0099
             Mean action noise std: 0.88
                       Mean reward: 7905.14
               Mean episode length: 361.48
                 Mean success rate: 69.00
                  Mean reward/step: 22.09
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15548416
                    Iteration time: 2.57s
                        Total time: 5035.97s
                               ETA: 525627.4s

################################################################################
                    [1m Learning iteration 1898/200000 [0m

                       Computation: 3200 steps/s (collection: 0.512s, learning 2.048s)
               Value function loss: 82617.2247
                    Surrogate loss: 0.0147
             Mean action noise std: 0.88
                       Mean reward: 7614.69
               Mean episode length: 351.73
                 Mean success rate: 67.00
                  Mean reward/step: 22.16
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15556608
                    Iteration time: 2.56s
                        Total time: 5038.53s
                               ETA: 525615.0s

################################################################################
                    [1m Learning iteration 1899/200000 [0m

                       Computation: 3222 steps/s (collection: 0.504s, learning 2.038s)
               Value function loss: 78495.0090
                    Surrogate loss: 0.0131
             Mean action noise std: 0.88
                       Mean reward: 7272.03
               Mean episode length: 339.81
                 Mean success rate: 65.00
                  Mean reward/step: 21.98
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15564800
                    Iteration time: 2.54s
                        Total time: 5041.07s
                               ETA: 525600.7s

################################################################################
                    [1m Learning iteration 1900/200000 [0m

                       Computation: 3222 steps/s (collection: 0.507s, learning 2.035s)
               Value function loss: 98243.3357
                    Surrogate loss: 0.0127
             Mean action noise std: 0.88
                       Mean reward: 7284.19
               Mean episode length: 346.48
                 Mean success rate: 66.00
                  Mean reward/step: 21.66
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15572992
                    Iteration time: 2.54s
                        Total time: 5043.61s
                               ETA: 525586.5s

################################################################################
                    [1m Learning iteration 1901/200000 [0m

                       Computation: 3205 steps/s (collection: 0.500s, learning 2.056s)
               Value function loss: 103957.5076
                    Surrogate loss: 0.0114
             Mean action noise std: 0.88
                       Mean reward: 7698.20
               Mean episode length: 356.25
                 Mean success rate: 67.50
                  Mean reward/step: 21.35
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15581184
                    Iteration time: 2.56s
                        Total time: 5046.17s
                               ETA: 525573.7s

################################################################################
                    [1m Learning iteration 1902/200000 [0m

                       Computation: 3215 steps/s (collection: 0.527s, learning 2.021s)
               Value function loss: 85186.3662
                    Surrogate loss: 0.0144
             Mean action noise std: 0.88
                       Mean reward: 7840.20
               Mean episode length: 360.32
                 Mean success rate: 68.50
                  Mean reward/step: 20.77
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 15589376
                    Iteration time: 2.55s
                        Total time: 5048.72s
                               ETA: 525560.1s

################################################################################
                    [1m Learning iteration 1903/200000 [0m

                       Computation: 3164 steps/s (collection: 0.516s, learning 2.073s)
               Value function loss: 93161.1087
                    Surrogate loss: 0.0162
             Mean action noise std: 0.88
                       Mean reward: 8006.23
               Mean episode length: 366.37
                 Mean success rate: 70.00
                  Mean reward/step: 20.54
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 15597568
                    Iteration time: 2.59s
                        Total time: 5051.31s
                               ETA: 525550.7s

################################################################################
                    [1m Learning iteration 1904/200000 [0m

                       Computation: 3180 steps/s (collection: 0.541s, learning 2.035s)
               Value function loss: 63202.6768
                    Surrogate loss: 0.0167
             Mean action noise std: 0.88
                       Mean reward: 8127.74
               Mean episode length: 371.30
                 Mean success rate: 71.50
                  Mean reward/step: 20.77
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15605760
                    Iteration time: 2.58s
                        Total time: 5053.88s
                               ETA: 525540.0s

################################################################################
                    [1m Learning iteration 1905/200000 [0m

                       Computation: 3213 steps/s (collection: 0.486s, learning 2.063s)
               Value function loss: 82627.3951
                    Surrogate loss: 0.0151
             Mean action noise std: 0.88
                       Mean reward: 8061.40
               Mean episode length: 367.12
                 Mean success rate: 71.50
                  Mean reward/step: 21.67
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15613952
                    Iteration time: 2.55s
                        Total time: 5056.43s
                               ETA: 525526.6s

################################################################################
                    [1m Learning iteration 1906/200000 [0m

                       Computation: 3182 steps/s (collection: 0.500s, learning 2.074s)
               Value function loss: 101625.1404
                    Surrogate loss: 0.0134
             Mean action noise std: 0.88
                       Mean reward: 8263.48
               Mean episode length: 373.22
                 Mean success rate: 73.00
                  Mean reward/step: 22.18
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15622144
                    Iteration time: 2.57s
                        Total time: 5059.00s
                               ETA: 525515.7s

################################################################################
                    [1m Learning iteration 1907/200000 [0m

                       Computation: 3134 steps/s (collection: 0.521s, learning 2.093s)
               Value function loss: 79554.4197
                    Surrogate loss: 0.0153
             Mean action noise std: 0.88
                       Mean reward: 8507.90
               Mean episode length: 382.98
                 Mean success rate: 74.50
                  Mean reward/step: 21.22
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 2.61s
                        Total time: 5061.62s
                               ETA: 525509.0s

################################################################################
                    [1m Learning iteration 1908/200000 [0m

                       Computation: 3204 steps/s (collection: 0.470s, learning 2.087s)
               Value function loss: 63673.1823
                    Surrogate loss: 0.0127
             Mean action noise std: 0.88
                       Mean reward: 8378.40
               Mean episode length: 378.04
                 Mean success rate: 73.00
                  Mean reward/step: 21.86
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 15638528
                    Iteration time: 2.56s
                        Total time: 5064.18s
                               ETA: 525496.4s

################################################################################
                    [1m Learning iteration 1909/200000 [0m

                       Computation: 3158 steps/s (collection: 0.532s, learning 2.062s)
               Value function loss: 134008.4970
                    Surrogate loss: 0.0137
             Mean action noise std: 0.88
                       Mean reward: 8509.70
               Mean episode length: 379.84
                 Mean success rate: 73.00
                  Mean reward/step: 22.27
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 15646720
                    Iteration time: 2.59s
                        Total time: 5066.77s
                               ETA: 525487.6s

################################################################################
                    [1m Learning iteration 1910/200000 [0m

                       Computation: 3149 steps/s (collection: 0.532s, learning 2.069s)
               Value function loss: 71669.9111
                    Surrogate loss: 0.0161
             Mean action noise std: 0.88
                       Mean reward: 8382.72
               Mean episode length: 378.20
                 Mean success rate: 72.00
                  Mean reward/step: 21.30
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15654912
                    Iteration time: 2.60s
                        Total time: 5069.37s
                               ETA: 525479.6s

################################################################################
                    [1m Learning iteration 1911/200000 [0m

                       Computation: 3176 steps/s (collection: 0.504s, learning 2.075s)
               Value function loss: 85280.1078
                    Surrogate loss: 0.0190
             Mean action noise std: 0.88
                       Mean reward: 7867.38
               Mean episode length: 360.36
                 Mean success rate: 68.50
                  Mean reward/step: 20.73
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 15663104
                    Iteration time: 2.58s
                        Total time: 5071.95s
                               ETA: 525469.3s

################################################################################
                    [1m Learning iteration 1912/200000 [0m

                       Computation: 3178 steps/s (collection: 0.513s, learning 2.065s)
               Value function loss: 75639.0160
                    Surrogate loss: 0.0148
             Mean action noise std: 0.88
                       Mean reward: 7518.01
               Mean episode length: 348.90
                 Mean success rate: 65.50
                  Mean reward/step: 21.20
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 15671296
                    Iteration time: 2.58s
                        Total time: 5074.53s
                               ETA: 525458.8s

################################################################################
                    [1m Learning iteration 1913/200000 [0m

                       Computation: 3173 steps/s (collection: 0.490s, learning 2.092s)
               Value function loss: 71566.8561
                    Surrogate loss: 0.0118
             Mean action noise std: 0.88
                       Mean reward: 7783.37
               Mean episode length: 359.39
                 Mean success rate: 67.00
                  Mean reward/step: 21.94
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15679488
                    Iteration time: 2.58s
                        Total time: 5077.11s
                               ETA: 525448.8s

################################################################################
                    [1m Learning iteration 1914/200000 [0m

                       Computation: 3187 steps/s (collection: 0.492s, learning 2.078s)
               Value function loss: 71439.2606
                    Surrogate loss: 0.0140
             Mean action noise std: 0.88
                       Mean reward: 7841.96
               Mean episode length: 359.10
                 Mean success rate: 67.00
                  Mean reward/step: 22.64
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15687680
                    Iteration time: 2.57s
                        Total time: 5079.68s
                               ETA: 525437.6s

################################################################################
                    [1m Learning iteration 1915/200000 [0m

                       Computation: 3211 steps/s (collection: 0.483s, learning 2.068s)
               Value function loss: 117437.8524
                    Surrogate loss: 0.0137
             Mean action noise std: 0.88
                       Mean reward: 7897.67
               Mean episode length: 362.97
                 Mean success rate: 66.50
                  Mean reward/step: 22.46
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 15695872
                    Iteration time: 2.55s
                        Total time: 5082.23s
                               ETA: 525424.4s

################################################################################
                    [1m Learning iteration 1916/200000 [0m

                       Computation: 3193 steps/s (collection: 0.479s, learning 2.087s)
               Value function loss: 86104.0938
                    Surrogate loss: 0.0135
             Mean action noise std: 0.88
                       Mean reward: 8013.22
               Mean episode length: 365.91
                 Mean success rate: 67.50
                  Mean reward/step: 22.17
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15704064
                    Iteration time: 2.57s
                        Total time: 5084.79s
                               ETA: 525412.8s

################################################################################
                    [1m Learning iteration 1917/200000 [0m

                       Computation: 3276 steps/s (collection: 0.464s, learning 2.036s)
               Value function loss: 82920.8297
                    Surrogate loss: 0.0128
             Mean action noise std: 0.88
                       Mean reward: 7684.26
               Mean episode length: 350.13
                 Mean success rate: 65.00
                  Mean reward/step: 22.35
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 15712256
                    Iteration time: 2.50s
                        Total time: 5087.29s
                               ETA: 525394.4s

################################################################################
                    [1m Learning iteration 1918/200000 [0m

                       Computation: 3140 steps/s (collection: 0.541s, learning 2.068s)
               Value function loss: 91951.4899
                    Surrogate loss: 0.0133
             Mean action noise std: 0.88
                       Mean reward: 7506.65
               Mean episode length: 347.48
                 Mean success rate: 64.50
                  Mean reward/step: 21.91
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 15720448
                    Iteration time: 2.61s
                        Total time: 5089.90s
                               ETA: 525387.2s

################################################################################
                    [1m Learning iteration 1919/200000 [0m

                       Computation: 3147 steps/s (collection: 0.517s, learning 2.086s)
               Value function loss: 95408.4389
                    Surrogate loss: 0.0120
             Mean action noise std: 0.88
                       Mean reward: 7613.46
               Mean episode length: 352.98
                 Mean success rate: 66.00
                  Mean reward/step: 21.61
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 2.60s
                        Total time: 5092.51s
                               ETA: 525379.5s

################################################################################
                    [1m Learning iteration 1920/200000 [0m

                       Computation: 3220 steps/s (collection: 0.488s, learning 2.056s)
               Value function loss: 69108.3853
                    Surrogate loss: 0.0192
             Mean action noise std: 0.88
                       Mean reward: 8007.55
               Mean episode length: 365.03
                 Mean success rate: 69.00
                  Mean reward/step: 21.21
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15736832
                    Iteration time: 2.54s
                        Total time: 5095.05s
                               ETA: 525365.6s

################################################################################
                    [1m Learning iteration 1921/200000 [0m

                       Computation: 3228 steps/s (collection: 0.448s, learning 2.090s)
               Value function loss: 62754.7482
                    Surrogate loss: 0.0211
             Mean action noise std: 0.88
                       Mean reward: 8298.46
               Mean episode length: 375.40
                 Mean success rate: 71.00
                  Mean reward/step: 21.85
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15745024
                    Iteration time: 2.54s
                        Total time: 5097.59s
                               ETA: 525351.2s

################################################################################
                    [1m Learning iteration 1922/200000 [0m

                       Computation: 3190 steps/s (collection: 0.459s, learning 2.109s)
               Value function loss: 116645.3325
                    Surrogate loss: 0.0132
             Mean action noise std: 0.88
                       Mean reward: 8190.10
               Mean episode length: 370.98
                 Mean success rate: 71.50
                  Mean reward/step: 21.83
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 15753216
                    Iteration time: 2.57s
                        Total time: 5100.15s
                               ETA: 525339.8s

################################################################################
                    [1m Learning iteration 1923/200000 [0m

                       Computation: 3185 steps/s (collection: 0.491s, learning 2.081s)
               Value function loss: 65085.7524
                    Surrogate loss: 0.0128
             Mean action noise std: 0.88
                       Mean reward: 8175.16
               Mean episode length: 372.83
                 Mean success rate: 72.00
                  Mean reward/step: 21.54
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15761408
                    Iteration time: 2.57s
                        Total time: 5102.73s
                               ETA: 525328.9s

################################################################################
                    [1m Learning iteration 1924/200000 [0m

                       Computation: 3189 steps/s (collection: 0.495s, learning 2.073s)
               Value function loss: 50895.5189
                    Surrogate loss: 0.0184
             Mean action noise std: 0.88
                       Mean reward: 7934.23
               Mean episode length: 364.38
                 Mean success rate: 72.00
                  Mean reward/step: 21.97
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15769600
                    Iteration time: 2.57s
                        Total time: 5105.29s
                               ETA: 525317.6s

################################################################################
                    [1m Learning iteration 1925/200000 [0m

                       Computation: 3247 steps/s (collection: 0.455s, learning 2.067s)
               Value function loss: 69469.4886
                    Surrogate loss: 0.0132
             Mean action noise std: 0.88
                       Mean reward: 7635.06
               Mean episode length: 356.81
                 Mean success rate: 71.50
                  Mean reward/step: 22.10
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 15777792
                    Iteration time: 2.52s
                        Total time: 5107.82s
                               ETA: 525301.6s

################################################################################
                    [1m Learning iteration 1926/200000 [0m

                       Computation: 3205 steps/s (collection: 0.476s, learning 2.079s)
               Value function loss: 78559.3430
                    Surrogate loss: 0.0129
             Mean action noise std: 0.88
                       Mean reward: 7605.04
               Mean episode length: 356.24
                 Mean success rate: 71.50
                  Mean reward/step: 21.92
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 15785984
                    Iteration time: 2.56s
                        Total time: 5110.37s
                               ETA: 525289.0s

################################################################################
                    [1m Learning iteration 1927/200000 [0m

                       Computation: 3187 steps/s (collection: 0.495s, learning 2.075s)
               Value function loss: 70046.0169
                    Surrogate loss: 0.0140
             Mean action noise std: 0.88
                       Mean reward: 7656.28
               Mean episode length: 356.40
                 Mean success rate: 71.50
                  Mean reward/step: 22.08
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15794176
                    Iteration time: 2.57s
                        Total time: 5112.94s
                               ETA: 525277.9s

################################################################################
                    [1m Learning iteration 1928/200000 [0m

                       Computation: 3233 steps/s (collection: 0.477s, learning 2.057s)
               Value function loss: 110585.8218
                    Surrogate loss: 0.0136
             Mean action noise std: 0.88
                       Mean reward: 7882.48
               Mean episode length: 365.28
                 Mean success rate: 73.50
                  Mean reward/step: 22.43
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15802368
                    Iteration time: 2.53s
                        Total time: 5115.48s
                               ETA: 525263.1s

################################################################################
                    [1m Learning iteration 1929/200000 [0m

                       Computation: 3207 steps/s (collection: 0.473s, learning 2.081s)
               Value function loss: 62690.6904
                    Surrogate loss: 0.0158
             Mean action noise std: 0.88
                       Mean reward: 8015.53
               Mean episode length: 367.25
                 Mean success rate: 73.50
                  Mean reward/step: 22.71
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15810560
                    Iteration time: 2.55s
                        Total time: 5118.03s
                               ETA: 525250.5s

################################################################################
                    [1m Learning iteration 1930/200000 [0m

                       Computation: 3136 steps/s (collection: 0.525s, learning 2.086s)
               Value function loss: 87735.6806
                    Surrogate loss: 0.0155
             Mean action noise std: 0.88
                       Mean reward: 7919.96
               Mean episode length: 362.38
                 Mean success rate: 72.50
                  Mean reward/step: 23.54
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 15818752
                    Iteration time: 2.61s
                        Total time: 5120.64s
                               ETA: 525243.7s

################################################################################
                    [1m Learning iteration 1931/200000 [0m

                       Computation: 3172 steps/s (collection: 0.477s, learning 2.105s)
               Value function loss: 90563.6246
                    Surrogate loss: 0.0140
             Mean action noise std: 0.88
                       Mean reward: 8182.88
               Mean episode length: 370.88
                 Mean success rate: 74.00
                  Mean reward/step: 23.43
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 2.58s
                        Total time: 5123.22s
                               ETA: 525233.9s

################################################################################
                    [1m Learning iteration 1932/200000 [0m

                       Computation: 3264 steps/s (collection: 0.473s, learning 2.036s)
               Value function loss: 91724.8249
                    Surrogate loss: 0.0151
             Mean action noise std: 0.88
                       Mean reward: 8298.09
               Mean episode length: 370.88
                 Mean success rate: 74.00
                  Mean reward/step: 23.25
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 15835136
                    Iteration time: 2.51s
                        Total time: 5125.73s
                               ETA: 525216.7s

################################################################################
                    [1m Learning iteration 1933/200000 [0m

                       Computation: 3283 steps/s (collection: 0.471s, learning 2.024s)
               Value function loss: 92059.0646
                    Surrogate loss: 0.0138
             Mean action noise std: 0.88
                       Mean reward: 8217.11
               Mean episode length: 372.81
                 Mean success rate: 73.50
                  Mean reward/step: 22.92
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 15843328
                    Iteration time: 2.49s
                        Total time: 5128.23s
                               ETA: 525197.9s

################################################################################
                    [1m Learning iteration 1934/200000 [0m

                       Computation: 3225 steps/s (collection: 0.480s, learning 2.060s)
               Value function loss: 77073.3445
                    Surrogate loss: 0.0144
             Mean action noise std: 0.88
                       Mean reward: 8594.73
               Mean episode length: 386.06
                 Mean success rate: 75.50
                  Mean reward/step: 22.40
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 15851520
                    Iteration time: 2.54s
                        Total time: 5130.77s
                               ETA: 525183.8s

################################################################################
                    [1m Learning iteration 1935/200000 [0m

                       Computation: 3281 steps/s (collection: 0.453s, learning 2.043s)
               Value function loss: 74257.2357
                    Surrogate loss: 0.0142
             Mean action noise std: 0.88
                       Mean reward: 8788.06
               Mean episode length: 392.64
                 Mean success rate: 78.00
                  Mean reward/step: 22.09
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15859712
                    Iteration time: 2.50s
                        Total time: 5133.26s
                               ETA: 525165.3s

################################################################################
                    [1m Learning iteration 1936/200000 [0m

                       Computation: 3220 steps/s (collection: 0.513s, learning 2.031s)
               Value function loss: 86949.9260
                    Surrogate loss: 0.0152
             Mean action noise std: 0.88
                       Mean reward: 9072.98
               Mean episode length: 403.43
                 Mean success rate: 80.00
                  Mean reward/step: 23.11
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15867904
                    Iteration time: 2.54s
                        Total time: 5135.81s
                               ETA: 525151.7s

################################################################################
                    [1m Learning iteration 1937/200000 [0m

                       Computation: 3252 steps/s (collection: 0.451s, learning 2.067s)
               Value function loss: 74007.2493
                    Surrogate loss: 0.0146
             Mean action noise std: 0.88
                       Mean reward: 9089.39
               Mean episode length: 404.23
                 Mean success rate: 80.50
                  Mean reward/step: 23.79
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 15876096
                    Iteration time: 2.52s
                        Total time: 5138.33s
                               ETA: 525135.4s

################################################################################
                    [1m Learning iteration 1938/200000 [0m

                       Computation: 3238 steps/s (collection: 0.483s, learning 2.047s)
               Value function loss: 88107.6545
                    Surrogate loss: 0.0125
             Mean action noise std: 0.88
                       Mean reward: 8777.61
               Mean episode length: 392.25
                 Mean success rate: 78.50
                  Mean reward/step: 23.56
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15884288
                    Iteration time: 2.53s
                        Total time: 5140.86s
                               ETA: 525120.3s

################################################################################
                    [1m Learning iteration 1939/200000 [0m

                       Computation: 3298 steps/s (collection: 0.442s, learning 2.041s)
               Value function loss: 57705.6805
                    Surrogate loss: 0.0128
             Mean action noise std: 0.88
                       Mean reward: 8920.99
               Mean episode length: 398.52
                 Mean success rate: 80.50
                  Mean reward/step: 24.17
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 15892480
                    Iteration time: 2.48s
                        Total time: 5143.34s
                               ETA: 525100.5s

################################################################################
                    [1m Learning iteration 1940/200000 [0m

                       Computation: 3261 steps/s (collection: 0.468s, learning 2.044s)
               Value function loss: 83300.6512
                    Surrogate loss: 0.0089
             Mean action noise std: 0.88
                       Mean reward: 8830.65
               Mean episode length: 393.98
                 Mean success rate: 79.50
                  Mean reward/step: 25.04
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 15900672
                    Iteration time: 2.51s
                        Total time: 5145.85s
                               ETA: 525083.6s

################################################################################
                    [1m Learning iteration 1941/200000 [0m

                       Computation: 3250 steps/s (collection: 0.468s, learning 2.052s)
               Value function loss: 115217.5531
                    Surrogate loss: 0.0119
             Mean action noise std: 0.88
                       Mean reward: 8988.69
               Mean episode length: 396.69
                 Mean success rate: 80.00
                  Mean reward/step: 24.61
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15908864
                    Iteration time: 2.52s
                        Total time: 5148.37s
                               ETA: 525067.6s

################################################################################
                    [1m Learning iteration 1942/200000 [0m

                       Computation: 3245 steps/s (collection: 0.475s, learning 2.050s)
               Value function loss: 106232.5102
                    Surrogate loss: 0.0150
             Mean action noise std: 0.88
                       Mean reward: 9144.48
               Mean episode length: 399.26
                 Mean success rate: 80.50
                  Mean reward/step: 23.96
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15917056
                    Iteration time: 2.52s
                        Total time: 5150.90s
                               ETA: 525052.1s

################################################################################
                    [1m Learning iteration 1943/200000 [0m

                       Computation: 3239 steps/s (collection: 0.469s, learning 2.060s)
               Value function loss: 71513.1979
                    Surrogate loss: 0.0119
             Mean action noise std: 0.88
                       Mean reward: 9509.04
               Mean episode length: 410.94
                 Mean success rate: 83.00
                  Mean reward/step: 23.46
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 2.53s
                        Total time: 5153.42s
                               ETA: 525037.0s

################################################################################
                    [1m Learning iteration 1944/200000 [0m

                       Computation: 3283 steps/s (collection: 0.464s, learning 2.031s)
               Value function loss: 78550.9077
                    Surrogate loss: 0.0155
             Mean action noise std: 0.88
                       Mean reward: 9466.80
               Mean episode length: 407.46
                 Mean success rate: 81.50
                  Mean reward/step: 23.52
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15933440
                    Iteration time: 2.49s
                        Total time: 5155.92s
                               ETA: 525018.4s

################################################################################
                    [1m Learning iteration 1945/200000 [0m

                       Computation: 3306 steps/s (collection: 0.450s, learning 2.028s)
               Value function loss: 74207.7337
                    Surrogate loss: 0.0113
             Mean action noise std: 0.88
                       Mean reward: 9450.46
               Mean episode length: 405.42
                 Mean success rate: 81.00
                  Mean reward/step: 24.56
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15941632
                    Iteration time: 2.48s
                        Total time: 5158.40s
                               ETA: 524998.2s

################################################################################
                    [1m Learning iteration 1946/200000 [0m

                       Computation: 3216 steps/s (collection: 0.491s, learning 2.056s)
               Value function loss: 68104.8750
                    Surrogate loss: 0.0100
             Mean action noise std: 0.88
                       Mean reward: 9642.55
               Mean episode length: 415.30
                 Mean success rate: 83.00
                  Mean reward/step: 25.20
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 15949824
                    Iteration time: 2.55s
                        Total time: 5160.94s
                               ETA: 524985.0s

################################################################################
                    [1m Learning iteration 1947/200000 [0m

                       Computation: 3310 steps/s (collection: 0.425s, learning 2.049s)
               Value function loss: 90421.6654
                    Surrogate loss: 0.0115
             Mean action noise std: 0.88
                       Mean reward: 9735.86
               Mean episode length: 416.06
                 Mean success rate: 82.50
                  Mean reward/step: 25.41
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15958016
                    Iteration time: 2.47s
                        Total time: 5163.42s
                               ETA: 524964.4s

################################################################################
                    [1m Learning iteration 1948/200000 [0m

                       Computation: 3324 steps/s (collection: 0.448s, learning 2.016s)
               Value function loss: 122362.0182
                    Surrogate loss: 0.0101
             Mean action noise std: 0.88
                       Mean reward: 9883.98
               Mean episode length: 420.12
                 Mean success rate: 83.00
                  Mean reward/step: 25.14
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 15966208
                    Iteration time: 2.46s
                        Total time: 5165.88s
                               ETA: 524942.8s

################################################################################
                    [1m Learning iteration 1949/200000 [0m

                       Computation: 3295 steps/s (collection: 0.436s, learning 2.050s)
               Value function loss: 112623.5572
                    Surrogate loss: 0.0126
             Mean action noise std: 0.88
                       Mean reward: 10087.58
               Mean episode length: 422.82
                 Mean success rate: 83.00
                  Mean reward/step: 23.91
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 15974400
                    Iteration time: 2.49s
                        Total time: 5168.37s
                               ETA: 524923.4s

################################################################################
                    [1m Learning iteration 1950/200000 [0m

                       Computation: 3269 steps/s (collection: 0.450s, learning 2.056s)
               Value function loss: 89183.6729
                    Surrogate loss: 0.0127
             Mean action noise std: 0.88
                       Mean reward: 10662.46
               Mean episode length: 441.27
                 Mean success rate: 87.00
                  Mean reward/step: 22.81
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 15982592
                    Iteration time: 2.51s
                        Total time: 5170.87s
                               ETA: 524906.1s

################################################################################
                    [1m Learning iteration 1951/200000 [0m

                       Computation: 3301 steps/s (collection: 0.450s, learning 2.031s)
               Value function loss: 88215.7871
                    Surrogate loss: 0.0125
             Mean action noise std: 0.88
                       Mean reward: 10257.02
               Mean episode length: 428.85
                 Mean success rate: 84.00
                  Mean reward/step: 22.25
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 15990784
                    Iteration time: 2.48s
                        Total time: 5173.36s
                               ETA: 524886.3s

################################################################################
                    [1m Learning iteration 1952/200000 [0m

                       Computation: 3266 steps/s (collection: 0.437s, learning 2.071s)
               Value function loss: 75697.8347
                    Surrogate loss: 0.0141
             Mean action noise std: 0.88
                       Mean reward: 10076.18
               Mean episode length: 423.70
                 Mean success rate: 83.50
                  Mean reward/step: 22.49
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 15998976
                    Iteration time: 2.51s
                        Total time: 5175.86s
                               ETA: 524869.2s

################################################################################
                    [1m Learning iteration 1953/200000 [0m

                       Computation: 3321 steps/s (collection: 0.436s, learning 2.030s)
               Value function loss: 104567.3227
                    Surrogate loss: 0.0150
             Mean action noise std: 0.88
                       Mean reward: 10089.99
               Mean episode length: 423.73
                 Mean success rate: 83.50
                  Mean reward/step: 22.62
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16007168
                    Iteration time: 2.47s
                        Total time: 5178.33s
                               ETA: 524847.9s

################################################################################
                    [1m Learning iteration 1954/200000 [0m

                       Computation: 3293 steps/s (collection: 0.438s, learning 2.049s)
               Value function loss: 76521.7396
                    Surrogate loss: 0.0135
             Mean action noise std: 0.88
                       Mean reward: 9813.03
               Mean episode length: 410.14
                 Mean success rate: 81.00
                  Mean reward/step: 22.39
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16015360
                    Iteration time: 2.49s
                        Total time: 5180.82s
                               ETA: 524828.7s

################################################################################
                    [1m Learning iteration 1955/200000 [0m

                       Computation: 3315 steps/s (collection: 0.433s, learning 2.038s)
               Value function loss: 63996.9347
                    Surrogate loss: 0.0155
             Mean action noise std: 0.88
                       Mean reward: 9484.66
               Mean episode length: 396.71
                 Mean success rate: 78.00
                  Mean reward/step: 21.86
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 2.47s
                        Total time: 5183.29s
                               ETA: 524807.9s

################################################################################
                    [1m Learning iteration 1956/200000 [0m

                       Computation: 3170 steps/s (collection: 0.506s, learning 2.078s)
               Value function loss: 105813.8345
                    Surrogate loss: 0.0128
             Mean action noise std: 0.88
                       Mean reward: 9628.28
               Mean episode length: 399.49
                 Mean success rate: 79.00
                  Mean reward/step: 22.40
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16031744
                    Iteration time: 2.58s
                        Total time: 5185.87s
                               ETA: 524798.5s

################################################################################
                    [1m Learning iteration 1957/200000 [0m

                       Computation: 3217 steps/s (collection: 0.493s, learning 2.053s)
               Value function loss: 80010.2977
                    Surrogate loss: 0.0142
             Mean action noise std: 0.88
                       Mean reward: 9332.33
               Mean episode length: 392.26
                 Mean success rate: 77.50
                  Mean reward/step: 22.04
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16039936
                    Iteration time: 2.55s
                        Total time: 5188.42s
                               ETA: 524785.4s

################################################################################
                    [1m Learning iteration 1958/200000 [0m

                       Computation: 3142 steps/s (collection: 0.459s, learning 2.148s)
               Value function loss: 95231.3332
                    Surrogate loss: 0.0123
             Mean action noise std: 0.88
                       Mean reward: 9238.61
               Mean episode length: 389.31
                 Mean success rate: 77.50
                  Mean reward/step: 21.79
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16048128
                    Iteration time: 2.61s
                        Total time: 5191.02s
                               ETA: 524778.4s

################################################################################
                    [1m Learning iteration 1959/200000 [0m

                       Computation: 3182 steps/s (collection: 0.466s, learning 2.108s)
               Value function loss: 86987.1418
                    Surrogate loss: 0.0146
             Mean action noise std: 0.88
                       Mean reward: 8707.45
               Mean episode length: 372.69
                 Mean success rate: 74.50
                  Mean reward/step: 22.03
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 16056320
                    Iteration time: 2.57s
                        Total time: 5193.60s
                               ETA: 524768.0s

################################################################################
                    [1m Learning iteration 1960/200000 [0m

                       Computation: 3186 steps/s (collection: 0.475s, learning 2.096s)
               Value function loss: 63765.9397
                    Surrogate loss: 0.0106
             Mean action noise std: 0.88
                       Mean reward: 8795.29
               Mean episode length: 375.70
                 Mean success rate: 75.50
                  Mean reward/step: 22.43
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 16064512
                    Iteration time: 2.57s
                        Total time: 5196.17s
                               ETA: 524757.4s

################################################################################
                    [1m Learning iteration 1961/200000 [0m

                       Computation: 3266 steps/s (collection: 0.463s, learning 2.044s)
               Value function loss: 73713.5747
                    Surrogate loss: 0.0152
             Mean action noise std: 0.88
                       Mean reward: 8953.99
               Mean episode length: 380.25
                 Mean success rate: 76.00
                  Mean reward/step: 23.17
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 16072704
                    Iteration time: 2.51s
                        Total time: 5198.68s
                               ETA: 524740.4s

################################################################################
                    [1m Learning iteration 1962/200000 [0m

                       Computation: 3244 steps/s (collection: 0.454s, learning 2.071s)
               Value function loss: 71177.8637
                    Surrogate loss: 0.0212
             Mean action noise std: 0.88
                       Mean reward: 8866.31
               Mean episode length: 381.19
                 Mean success rate: 76.00
                  Mean reward/step: 23.87
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 16080896
                    Iteration time: 2.53s
                        Total time: 5201.20s
                               ETA: 524725.2s

################################################################################
                    [1m Learning iteration 1963/200000 [0m

                       Computation: 3253 steps/s (collection: 0.468s, learning 2.049s)
               Value function loss: 69689.2784
                    Surrogate loss: 0.0145
             Mean action noise std: 0.88
                       Mean reward: 8360.85
               Mean episode length: 365.61
                 Mean success rate: 72.50
                  Mean reward/step: 23.54
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 16089088
                    Iteration time: 2.52s
                        Total time: 5203.72s
                               ETA: 524709.2s

################################################################################
                    [1m Learning iteration 1964/200000 [0m

                       Computation: 3103 steps/s (collection: 0.547s, learning 2.093s)
               Value function loss: 105139.7230
                    Surrogate loss: 0.0125
             Mean action noise std: 0.88
                       Mean reward: 8732.26
               Mean episode length: 378.64
                 Mean success rate: 76.00
                  Mean reward/step: 22.78
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 16097280
                    Iteration time: 2.64s
                        Total time: 5206.36s
                               ETA: 524705.6s

################################################################################
                    [1m Learning iteration 1965/200000 [0m

                       Computation: 3297 steps/s (collection: 0.435s, learning 2.050s)
               Value function loss: 81320.9444
                    Surrogate loss: 0.0136
             Mean action noise std: 0.88
                       Mean reward: 8771.00
               Mean episode length: 387.23
                 Mean success rate: 77.50
                  Mean reward/step: 21.97
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16105472
                    Iteration time: 2.48s
                        Total time: 5208.84s
                               ETA: 524686.3s

################################################################################
                    [1m Learning iteration 1966/200000 [0m

                       Computation: 3133 steps/s (collection: 0.516s, learning 2.098s)
               Value function loss: 96576.1764
                    Surrogate loss: 0.0170
             Mean action noise std: 0.88
                       Mean reward: 8618.31
               Mean episode length: 382.05
                 Mean success rate: 77.00
                  Mean reward/step: 22.34
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16113664
                    Iteration time: 2.61s
                        Total time: 5211.46s
                               ETA: 524680.1s

################################################################################
                    [1m Learning iteration 1967/200000 [0m

                       Computation: 3234 steps/s (collection: 0.438s, learning 2.095s)
               Value function loss: 73114.0131
                    Surrogate loss: 0.0148
             Mean action noise std: 0.88
                       Mean reward: 8479.30
               Mean episode length: 378.05
                 Mean success rate: 75.50
                  Mean reward/step: 22.40
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 2.53s
                        Total time: 5213.99s
                               ETA: 524665.7s

################################################################################
                    [1m Learning iteration 1968/200000 [0m

                       Computation: 3084 steps/s (collection: 0.526s, learning 2.130s)
               Value function loss: 62923.9774
                    Surrogate loss: 0.0111
             Mean action noise std: 0.88
                       Mean reward: 8503.60
               Mean episode length: 379.05
                 Mean success rate: 76.00
                  Mean reward/step: 22.89
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16130048
                    Iteration time: 2.66s
                        Total time: 5216.65s
                               ETA: 524663.7s

################################################################################
                    [1m Learning iteration 1969/200000 [0m

                       Computation: 3131 steps/s (collection: 0.475s, learning 2.140s)
               Value function loss: 81132.3800
                    Surrogate loss: 0.0171
             Mean action noise std: 0.88
                       Mean reward: 8552.49
               Mean episode length: 381.84
                 Mean success rate: 76.00
                  Mean reward/step: 23.92
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16138240
                    Iteration time: 2.62s
                        Total time: 5219.26s
                               ETA: 524657.7s

################################################################################
                    [1m Learning iteration 1970/200000 [0m

                       Computation: 3039 steps/s (collection: 0.538s, learning 2.158s)
               Value function loss: 84166.2892
                    Surrogate loss: 0.0121
             Mean action noise std: 0.88
                       Mean reward: 8217.36
               Mean episode length: 374.50
                 Mean success rate: 74.00
                  Mean reward/step: 24.01
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 16146432
                    Iteration time: 2.70s
                        Total time: 5221.96s
                               ETA: 524659.7s

################################################################################
                    [1m Learning iteration 1971/200000 [0m

                       Computation: 3095 steps/s (collection: 0.473s, learning 2.174s)
               Value function loss: 95058.5900
                    Surrogate loss: 0.0131
             Mean action noise std: 0.88
                       Mean reward: 8142.11
               Mean episode length: 371.97
                 Mean success rate: 73.00
                  Mean reward/step: 23.92
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16154624
                    Iteration time: 2.65s
                        Total time: 5224.60s
                               ETA: 524656.7s

################################################################################
                    [1m Learning iteration 1972/200000 [0m

                       Computation: 3110 steps/s (collection: 0.500s, learning 2.134s)
               Value function loss: 96093.8257
                    Surrogate loss: 0.0144
             Mean action noise std: 0.88
                       Mean reward: 8396.42
               Mean episode length: 379.50
                 Mean success rate: 75.50
                  Mean reward/step: 23.35
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 16162816
                    Iteration time: 2.63s
                        Total time: 5227.24s
                               ETA: 524652.5s

################################################################################
                    [1m Learning iteration 1973/200000 [0m

                       Computation: 3104 steps/s (collection: 0.530s, learning 2.109s)
               Value function loss: 96024.9953
                    Surrogate loss: 0.0142
             Mean action noise std: 0.88
                       Mean reward: 8534.33
               Mean episode length: 381.67
                 Mean success rate: 75.50
                  Mean reward/step: 23.05
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16171008
                    Iteration time: 2.64s
                        Total time: 5229.88s
                               ETA: 524648.8s

################################################################################
                    [1m Learning iteration 1974/200000 [0m

                       Computation: 3071 steps/s (collection: 0.507s, learning 2.159s)
               Value function loss: 100873.7675
                    Surrogate loss: 0.0152
             Mean action noise std: 0.88
                       Mean reward: 8621.70
               Mean episode length: 376.89
                 Mean success rate: 74.50
                  Mean reward/step: 22.87
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 16179200
                    Iteration time: 2.67s
                        Total time: 5232.54s
                               ETA: 524647.9s

################################################################################
                    [1m Learning iteration 1975/200000 [0m

                       Computation: 3077 steps/s (collection: 0.519s, learning 2.143s)
               Value function loss: 70054.8511
                    Surrogate loss: 0.0154
             Mean action noise std: 0.88
                       Mean reward: 8695.89
               Mean episode length: 379.03
                 Mean success rate: 74.50
                  Mean reward/step: 22.27
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16187392
                    Iteration time: 2.66s
                        Total time: 5235.21s
                               ETA: 524646.6s

################################################################################
                    [1m Learning iteration 1976/200000 [0m

                       Computation: 3076 steps/s (collection: 0.485s, learning 2.178s)
               Value function loss: 56509.1948
                    Surrogate loss: 0.0135
             Mean action noise std: 0.88
                       Mean reward: 8673.44
               Mean episode length: 376.37
                 Mean success rate: 74.00
                  Mean reward/step: 22.41
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 16195584
                    Iteration time: 2.66s
                        Total time: 5237.87s
                               ETA: 524645.3s

################################################################################
                    [1m Learning iteration 1977/200000 [0m

                       Computation: 3008 steps/s (collection: 0.556s, learning 2.166s)
               Value function loss: 87969.9146
                    Surrogate loss: 0.0138
             Mean action noise std: 0.88
                       Mean reward: 8658.72
               Mean episode length: 373.22
                 Mean success rate: 73.00
                  Mean reward/step: 23.34
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16203776
                    Iteration time: 2.72s
                        Total time: 5240.59s
                               ETA: 524650.0s

################################################################################
                    [1m Learning iteration 1978/200000 [0m

                       Computation: 3091 steps/s (collection: 0.534s, learning 2.116s)
               Value function loss: 65345.5886
                    Surrogate loss: 0.0133
             Mean action noise std: 0.88
                       Mean reward: 8500.10
               Mean episode length: 368.99
                 Mean success rate: 72.50
                  Mean reward/step: 23.18
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 16211968
                    Iteration time: 2.65s
                        Total time: 5243.24s
                               ETA: 524647.4s

################################################################################
                    [1m Learning iteration 1979/200000 [0m

                       Computation: 3207 steps/s (collection: 0.450s, learning 2.105s)
               Value function loss: 107632.7255
                    Surrogate loss: 0.0143
             Mean action noise std: 0.88
                       Mean reward: 8763.99
               Mean episode length: 376.60
                 Mean success rate: 74.00
                  Mean reward/step: 23.13
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 2.55s
                        Total time: 5245.80s
                               ETA: 524635.2s

################################################################################
                    [1m Learning iteration 1980/200000 [0m

                       Computation: 3288 steps/s (collection: 0.451s, learning 2.040s)
               Value function loss: 119114.6842
                    Surrogate loss: 0.0160
             Mean action noise std: 0.88
                       Mean reward: 8826.16
               Mean episode length: 375.51
                 Mean success rate: 74.50
                  Mean reward/step: 22.95
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 16228352
                    Iteration time: 2.49s
                        Total time: 5248.29s
                               ETA: 524616.7s

################################################################################
                    [1m Learning iteration 1981/200000 [0m

                       Computation: 3292 steps/s (collection: 0.444s, learning 2.045s)
               Value function loss: 53968.1697
                    Surrogate loss: 0.0177
             Mean action noise std: 0.88
                       Mean reward: 8888.44
               Mean episode length: 378.60
                 Mean success rate: 75.00
                  Mean reward/step: 22.30
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 16236544
                    Iteration time: 2.49s
                        Total time: 5250.78s
                               ETA: 524598.0s

################################################################################
                    [1m Learning iteration 1982/200000 [0m

                       Computation: 3293 steps/s (collection: 0.437s, learning 2.050s)
               Value function loss: 83286.3552
                    Surrogate loss: 0.0147
             Mean action noise std: 0.88
                       Mean reward: 8795.29
               Mean episode length: 377.80
                 Mean success rate: 75.00
                  Mean reward/step: 22.83
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 16244736
                    Iteration time: 2.49s
                        Total time: 5253.26s
                               ETA: 524579.2s

################################################################################
                    [1m Learning iteration 1983/200000 [0m

                       Computation: 3262 steps/s (collection: 0.436s, learning 2.075s)
               Value function loss: 58014.7465
                    Surrogate loss: 0.0158
             Mean action noise std: 0.88
                       Mean reward: 8613.24
               Mean episode length: 370.63
                 Mean success rate: 73.50
                  Mean reward/step: 23.53
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16252928
                    Iteration time: 2.51s
                        Total time: 5255.77s
                               ETA: 524562.8s

################################################################################
                    [1m Learning iteration 1984/200000 [0m

                       Computation: 3305 steps/s (collection: 0.447s, learning 2.031s)
               Value function loss: 81015.9535
                    Surrogate loss: 0.0121
             Mean action noise std: 0.88
                       Mean reward: 8222.72
               Mean episode length: 362.01
                 Mean success rate: 71.00
                  Mean reward/step: 23.64
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 16261120
                    Iteration time: 2.48s
                        Total time: 5258.25s
                               ETA: 524543.1s

################################################################################
                    [1m Learning iteration 1985/200000 [0m

                       Computation: 3313 steps/s (collection: 0.427s, learning 2.045s)
               Value function loss: 67529.9738
                    Surrogate loss: 0.0171
             Mean action noise std: 0.88
                       Mean reward: 8369.87
               Mean episode length: 369.83
                 Mean success rate: 72.50
                  Mean reward/step: 23.94
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 16269312
                    Iteration time: 2.47s
                        Total time: 5260.72s
                               ETA: 524522.8s

################################################################################
                    [1m Learning iteration 1986/200000 [0m

                       Computation: 3281 steps/s (collection: 0.428s, learning 2.069s)
               Value function loss: 107153.0188
                    Surrogate loss: 0.0181
             Mean action noise std: 0.88
                       Mean reward: 8675.25
               Mean episode length: 379.75
                 Mean success rate: 74.50
                  Mean reward/step: 23.96
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16277504
                    Iteration time: 2.50s
                        Total time: 5263.22s
                               ETA: 524505.0s

################################################################################
                    [1m Learning iteration 1987/200000 [0m

                       Computation: 3298 steps/s (collection: 0.435s, learning 2.049s)
               Value function loss: 102663.8443
                    Surrogate loss: 0.0153
             Mean action noise std: 0.88
                       Mean reward: 9160.73
               Mean episode length: 394.05
                 Mean success rate: 77.00
                  Mean reward/step: 23.57
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16285696
                    Iteration time: 2.48s
                        Total time: 5265.70s
                               ETA: 524485.9s

################################################################################
                    [1m Learning iteration 1988/200000 [0m

                       Computation: 3277 steps/s (collection: 0.450s, learning 2.050s)
               Value function loss: 71241.5334
                    Surrogate loss: 0.0141
             Mean action noise std: 0.87
                       Mean reward: 8817.70
               Mean episode length: 384.51
                 Mean success rate: 74.50
                  Mean reward/step: 22.71
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16293888
                    Iteration time: 2.50s
                        Total time: 5268.20s
                               ETA: 524468.4s

################################################################################
                    [1m Learning iteration 1989/200000 [0m

                       Computation: 3305 steps/s (collection: 0.433s, learning 2.045s)
               Value function loss: 85519.3186
                    Surrogate loss: 0.0119
             Mean action noise std: 0.87
                       Mean reward: 8918.10
               Mean episode length: 385.80
                 Mean success rate: 75.00
                  Mean reward/step: 23.13
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16302080
                    Iteration time: 2.48s
                        Total time: 5270.68s
                               ETA: 524448.8s

################################################################################
                    [1m Learning iteration 1990/200000 [0m

                       Computation: 3268 steps/s (collection: 0.453s, learning 2.054s)
               Value function loss: 83294.7556
                    Surrogate loss: 0.0135
             Mean action noise std: 0.87
                       Mean reward: 8951.95
               Mean episode length: 388.48
                 Mean success rate: 75.50
                  Mean reward/step: 23.47
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16310272
                    Iteration time: 2.51s
                        Total time: 5273.19s
                               ETA: 524432.0s

################################################################################
                    [1m Learning iteration 1991/200000 [0m

                       Computation: 3097 steps/s (collection: 0.491s, learning 2.154s)
               Value function loss: 57296.3205
                    Surrogate loss: 0.0109
             Mean action noise std: 0.87
                       Mean reward: 9091.15
               Mean episode length: 393.25
                 Mean success rate: 76.50
                  Mean reward/step: 23.52
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 2.64s
                        Total time: 5275.83s
                               ETA: 524429.0s

################################################################################
                    [1m Learning iteration 1992/200000 [0m

                       Computation: 3133 steps/s (collection: 0.456s, learning 2.159s)
               Value function loss: 63195.5687
                    Surrogate loss: 0.0116
             Mean action noise std: 0.87
                       Mean reward: 8846.65
               Mean episode length: 385.87
                 Mean success rate: 75.50
                  Mean reward/step: 24.43
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16326656
                    Iteration time: 2.61s
                        Total time: 5278.45s
                               ETA: 524422.9s

################################################################################
                    [1m Learning iteration 1993/200000 [0m

                       Computation: 3197 steps/s (collection: 0.498s, learning 2.065s)
               Value function loss: 80509.0702
                    Surrogate loss: 0.0174
             Mean action noise std: 0.87
                       Mean reward: 8819.60
               Mean episode length: 384.68
                 Mean success rate: 75.50
                  Mean reward/step: 24.40
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16334848
                    Iteration time: 2.56s
                        Total time: 5281.01s
                               ETA: 524411.7s

################################################################################
                    [1m Learning iteration 1994/200000 [0m

                       Computation: 3152 steps/s (collection: 0.510s, learning 2.089s)
               Value function loss: 110004.3448
                    Surrogate loss: 0.0126
             Mean action noise std: 0.87
                       Mean reward: 9225.17
               Mean episode length: 391.48
                 Mean success rate: 77.50
                  Mean reward/step: 24.17
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 16343040
                    Iteration time: 2.60s
                        Total time: 5283.61s
                               ETA: 524404.1s

################################################################################
                    [1m Learning iteration 1995/200000 [0m

                       Computation: 3116 steps/s (collection: 0.499s, learning 2.129s)
               Value function loss: 87842.0036
                    Surrogate loss: 0.0125
             Mean action noise std: 0.87
                       Mean reward: 9267.86
               Mean episode length: 390.67
                 Mean success rate: 78.00
                  Mean reward/step: 23.56
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16351232
                    Iteration time: 2.63s
                        Total time: 5286.24s
                               ETA: 524399.5s

################################################################################
                    [1m Learning iteration 1996/200000 [0m

                       Computation: 3163 steps/s (collection: 0.460s, learning 2.130s)
               Value function loss: 109982.1236
                    Surrogate loss: 0.0120
             Mean action noise std: 0.87
                       Mean reward: 9386.85
               Mean episode length: 396.80
                 Mean success rate: 79.50
                  Mean reward/step: 23.28
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16359424
                    Iteration time: 2.59s
                        Total time: 5288.83s
                               ETA: 524391.0s

################################################################################
                    [1m Learning iteration 1997/200000 [0m

                       Computation: 3053 steps/s (collection: 0.515s, learning 2.167s)
               Value function loss: 99405.0369
                    Surrogate loss: 0.0134
             Mean action noise std: 0.87
                       Mean reward: 9584.15
               Mean episode length: 405.45
                 Mean success rate: 81.50
                  Mean reward/step: 24.14
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 16367616
                    Iteration time: 2.68s
                        Total time: 5291.51s
                               ETA: 524391.8s

################################################################################
                    [1m Learning iteration 1998/200000 [0m

                       Computation: 3139 steps/s (collection: 0.491s, learning 2.119s)
               Value function loss: 82693.0398
                    Surrogate loss: 0.0156
             Mean action noise std: 0.87
                       Mean reward: 9361.91
               Mean episode length: 395.43
                 Mean success rate: 80.50
                  Mean reward/step: 23.88
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 16375808
                    Iteration time: 2.61s
                        Total time: 5294.12s
                               ETA: 524385.3s

################################################################################
                    [1m Learning iteration 1999/200000 [0m

                       Computation: 3126 steps/s (collection: 0.470s, learning 2.151s)
               Value function loss: 68739.1545
                    Surrogate loss: 0.0121
             Mean action noise std: 0.87
                       Mean reward: 9383.56
               Mean episode length: 394.94
                 Mean success rate: 80.50
                  Mean reward/step: 24.08
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 16384000
                    Iteration time: 2.62s
                        Total time: 5296.74s
                               ETA: 524379.9s

################################################################################
                    [1m Learning iteration 2000/200000 [0m

                       Computation: 3143 steps/s (collection: 0.469s, learning 2.137s)
               Value function loss: 94215.6909
                    Surrogate loss: 0.0129
             Mean action noise std: 0.87
                       Mean reward: 9939.44
               Mean episode length: 412.71
                 Mean success rate: 84.00
                  Mean reward/step: 23.93
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16392192
                    Iteration time: 2.61s
                        Total time: 5299.35s
                               ETA: 524373.0s

################################################################################
                    [1m Learning iteration 2001/200000 [0m

                       Computation: 3251 steps/s (collection: 0.451s, learning 2.069s)
               Value function loss: 88016.5816
                    Surrogate loss: 0.0132
             Mean action noise std: 0.87
                       Mean reward: 9696.94
               Mean episode length: 404.53
                 Mean success rate: 82.00
                  Mean reward/step: 24.45
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16400384
                    Iteration time: 2.52s
                        Total time: 5301.86s
                               ETA: 524357.6s

################################################################################
                    [1m Learning iteration 2002/200000 [0m

                       Computation: 3152 steps/s (collection: 0.487s, learning 2.112s)
               Value function loss: 96195.8909
                    Surrogate loss: 0.0141
             Mean action noise std: 0.88
                       Mean reward: 9807.17
               Mean episode length: 407.34
                 Mean success rate: 82.00
                  Mean reward/step: 24.19
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 16408576
                    Iteration time: 2.60s
                        Total time: 5304.46s
                               ETA: 524350.0s

################################################################################
                    [1m Learning iteration 2003/200000 [0m

                       Computation: 3149 steps/s (collection: 0.476s, learning 2.124s)
               Value function loss: 105847.3290
                    Surrogate loss: 0.0145
             Mean action noise std: 0.88
                       Mean reward: 9966.94
               Mean episode length: 414.37
                 Mean success rate: 84.00
                  Mean reward/step: 23.49
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 2.60s
                        Total time: 5307.06s
                               ETA: 524342.7s

################################################################################
                    [1m Learning iteration 2004/200000 [0m

                       Computation: 3153 steps/s (collection: 0.450s, learning 2.148s)
               Value function loss: 72984.6628
                    Surrogate loss: 0.0146
             Mean action noise std: 0.88
                       Mean reward: 10009.79
               Mean episode length: 416.88
                 Mean success rate: 84.00
                  Mean reward/step: 22.90
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16424960
                    Iteration time: 2.60s
                        Total time: 5309.66s
                               ETA: 524335.1s

################################################################################
                    [1m Learning iteration 2005/200000 [0m

                       Computation: 3129 steps/s (collection: 0.485s, learning 2.133s)
               Value function loss: 100572.5965
                    Surrogate loss: 0.0153
             Mean action noise std: 0.88
                       Mean reward: 9951.32
               Mean episode length: 414.89
                 Mean success rate: 83.00
                  Mean reward/step: 23.04
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16433152
                    Iteration time: 2.62s
                        Total time: 5312.28s
                               ETA: 524329.4s

################################################################################
                    [1m Learning iteration 2006/200000 [0m

                       Computation: 3188 steps/s (collection: 0.456s, learning 2.113s)
               Value function loss: 78151.3811
                    Surrogate loss: 0.0118
             Mean action noise std: 0.88
                       Mean reward: 9934.62
               Mean episode length: 414.52
                 Mean success rate: 82.50
                  Mean reward/step: 23.85
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 16441344
                    Iteration time: 2.57s
                        Total time: 5314.85s
                               ETA: 524318.9s

################################################################################
                    [1m Learning iteration 2007/200000 [0m

                       Computation: 3139 steps/s (collection: 0.494s, learning 2.116s)
               Value function loss: 70238.0866
                    Surrogate loss: 0.0134
             Mean action noise std: 0.88
                       Mean reward: 9580.98
               Mean episode length: 401.97
                 Mean success rate: 79.50
                  Mean reward/step: 24.96
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16449536
                    Iteration time: 2.61s
                        Total time: 5317.46s
                               ETA: 524312.5s

################################################################################
                    [1m Learning iteration 2008/200000 [0m

                       Computation: 3256 steps/s (collection: 0.456s, learning 2.059s)
               Value function loss: 74607.9398
                    Surrogate loss: 0.0155
             Mean action noise std: 0.88
                       Mean reward: 9823.08
               Mean episode length: 407.29
                 Mean success rate: 80.50
                  Mean reward/step: 25.02
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16457728
                    Iteration time: 2.52s
                        Total time: 5319.97s
                               ETA: 524296.8s

################################################################################
                    [1m Learning iteration 2009/200000 [0m

                       Computation: 3187 steps/s (collection: 0.457s, learning 2.112s)
               Value function loss: 88730.0797
                    Surrogate loss: 0.0192
             Mean action noise std: 0.88
                       Mean reward: 9909.61
               Mean episode length: 412.17
                 Mean success rate: 80.50
                  Mean reward/step: 24.94
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16465920
                    Iteration time: 2.57s
                        Total time: 5322.54s
                               ETA: 524286.4s

################################################################################
                    [1m Learning iteration 2010/200000 [0m

                       Computation: 3190 steps/s (collection: 0.446s, learning 2.121s)
               Value function loss: 85601.5722
                    Surrogate loss: 0.0119
             Mean action noise std: 0.88
                       Mean reward: 9813.77
               Mean episode length: 410.04
                 Mean success rate: 80.00
                  Mean reward/step: 24.30
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16474112
                    Iteration time: 2.57s
                        Total time: 5325.11s
                               ETA: 524275.9s

################################################################################
                    [1m Learning iteration 2011/200000 [0m

                       Computation: 3160 steps/s (collection: 0.484s, learning 2.108s)
               Value function loss: 118366.2895
                    Surrogate loss: 0.0149
             Mean action noise std: 0.88
                       Mean reward: 9799.32
               Mean episode length: 412.15
                 Mean success rate: 80.00
                  Mean reward/step: 24.24
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 16482304
                    Iteration time: 2.59s
                        Total time: 5327.70s
                               ETA: 524267.7s

################################################################################
                    [1m Learning iteration 2012/200000 [0m

                       Computation: 3158 steps/s (collection: 0.465s, learning 2.129s)
               Value function loss: 54054.3606
                    Surrogate loss: 0.0152
             Mean action noise std: 0.88
                       Mean reward: 9957.36
               Mean episode length: 416.42
                 Mean success rate: 81.00
                  Mean reward/step: 24.37
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 16490496
                    Iteration time: 2.59s
                        Total time: 5330.30s
                               ETA: 524259.7s

################################################################################
                    [1m Learning iteration 2013/200000 [0m

                       Computation: 3165 steps/s (collection: 0.435s, learning 2.153s)
               Value function loss: 107426.6037
                    Surrogate loss: 0.0147
             Mean action noise std: 0.88
                       Mean reward: 10137.14
               Mean episode length: 421.61
                 Mean success rate: 82.00
                  Mean reward/step: 25.40
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16498688
                    Iteration time: 2.59s
                        Total time: 5332.88s
                               ETA: 524251.2s

################################################################################
                    [1m Learning iteration 2014/200000 [0m

                       Computation: 3129 steps/s (collection: 0.507s, learning 2.110s)
               Value function loss: 90695.0846
                    Surrogate loss: 0.0116
             Mean action noise std: 0.88
                       Mean reward: 10234.08
               Mean episode length: 425.20
                 Mean success rate: 82.50
                  Mean reward/step: 25.41
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16506880
                    Iteration time: 2.62s
                        Total time: 5335.50s
                               ETA: 524245.5s

################################################################################
                    [1m Learning iteration 2015/200000 [0m

                       Computation: 3103 steps/s (collection: 0.516s, learning 2.124s)
               Value function loss: 105470.7848
                    Surrogate loss: 0.0154
             Mean action noise std: 0.88
                       Mean reward: 10294.24
               Mean episode length: 425.60
                 Mean success rate: 83.00
                  Mean reward/step: 25.30
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 2.64s
                        Total time: 5338.14s
                               ETA: 524242.1s

################################################################################
                    [1m Learning iteration 2016/200000 [0m

                       Computation: 3195 steps/s (collection: 0.449s, learning 2.114s)
               Value function loss: 76832.5681
                    Surrogate loss: 0.0170
             Mean action noise std: 0.88
                       Mean reward: 10239.79
               Mean episode length: 425.01
                 Mean success rate: 83.00
                  Mean reward/step: 25.99
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 16523264
                    Iteration time: 2.56s
                        Total time: 5340.71s
                               ETA: 524231.2s

################################################################################
                    [1m Learning iteration 2017/200000 [0m

                       Computation: 3157 steps/s (collection: 0.498s, learning 2.096s)
               Value function loss: 74717.6193
                    Surrogate loss: 0.0161
             Mean action noise std: 0.88
                       Mean reward: 10238.12
               Mean episode length: 423.27
                 Mean success rate: 82.50
                  Mean reward/step: 26.25
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 16531456
                    Iteration time: 2.59s
                        Total time: 5343.30s
                               ETA: 524223.2s

################################################################################
                    [1m Learning iteration 2018/200000 [0m

                       Computation: 3162 steps/s (collection: 0.488s, learning 2.102s)
               Value function loss: 118443.5224
                    Surrogate loss: 0.0137
             Mean action noise std: 0.88
                       Mean reward: 10460.17
               Mean episode length: 429.12
                 Mean success rate: 84.50
                  Mean reward/step: 25.24
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 16539648
                    Iteration time: 2.59s
                        Total time: 5345.89s
                               ETA: 524214.9s

################################################################################
                    [1m Learning iteration 2019/200000 [0m

                       Computation: 3166 steps/s (collection: 0.464s, learning 2.123s)
               Value function loss: 126444.2082
                    Surrogate loss: 0.0143
             Mean action noise std: 0.88
                       Mean reward: 10727.85
               Mean episode length: 436.56
                 Mean success rate: 86.00
                  Mean reward/step: 23.52
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 16547840
                    Iteration time: 2.59s
                        Total time: 5348.48s
                               ETA: 524206.3s

################################################################################
                    [1m Learning iteration 2020/200000 [0m

                       Computation: 3251 steps/s (collection: 0.468s, learning 2.052s)
               Value function loss: 77789.7368
                    Surrogate loss: 0.0135
             Mean action noise std: 0.88
                       Mean reward: 10314.79
               Mean episode length: 422.14
                 Mean success rate: 84.00
                  Mean reward/step: 22.63
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16556032
                    Iteration time: 2.52s
                        Total time: 5351.00s
                               ETA: 524191.1s

################################################################################
                    [1m Learning iteration 2021/200000 [0m

                       Computation: 3284 steps/s (collection: 0.437s, learning 2.057s)
               Value function loss: 116096.7582
                    Surrogate loss: 0.0160
             Mean action noise std: 0.88
                       Mean reward: 10631.42
               Mean episode length: 430.73
                 Mean success rate: 85.50
                  Mean reward/step: 23.21
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16564224
                    Iteration time: 2.49s
                        Total time: 5353.49s
                               ETA: 524173.4s

################################################################################
                    [1m Learning iteration 2022/200000 [0m

                       Computation: 3266 steps/s (collection: 0.453s, learning 2.055s)
               Value function loss: 75707.9982
                    Surrogate loss: 0.0162
             Mean action noise std: 0.88
                       Mean reward: 10422.29
               Mean episode length: 423.17
                 Mean success rate: 85.00
                  Mean reward/step: 23.30
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16572416
                    Iteration time: 2.51s
                        Total time: 5356.00s
                               ETA: 524157.1s

################################################################################
                    [1m Learning iteration 2023/200000 [0m

                       Computation: 3149 steps/s (collection: 0.506s, learning 2.095s)
               Value function loss: 74910.8768
                    Surrogate loss: 0.0179
             Mean action noise std: 0.88
                       Mean reward: 10013.72
               Mean episode length: 407.77
                 Mean success rate: 82.00
                  Mean reward/step: 23.99
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 16580608
                    Iteration time: 2.60s
                        Total time: 5358.60s
                               ETA: 524150.0s

################################################################################
                    [1m Learning iteration 2024/200000 [0m

                       Computation: 3163 steps/s (collection: 0.499s, learning 2.091s)
               Value function loss: 113581.2109
                    Surrogate loss: 0.0136
             Mean action noise std: 0.88
                       Mean reward: 9693.93
               Mean episode length: 395.47
                 Mean success rate: 80.50
                  Mean reward/step: 24.00
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 16588800
                    Iteration time: 2.59s
                        Total time: 5361.19s
                               ETA: 524141.6s

################################################################################
                    [1m Learning iteration 2025/200000 [0m

                       Computation: 3204 steps/s (collection: 0.475s, learning 2.081s)
               Value function loss: 127840.9074
                    Surrogate loss: 0.0153
             Mean action noise std: 0.88
                       Mean reward: 9520.19
               Mean episode length: 389.44
                 Mean success rate: 80.50
                  Mean reward/step: 23.41
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16596992
                    Iteration time: 2.56s
                        Total time: 5363.75s
                               ETA: 524130.1s

################################################################################
                    [1m Learning iteration 2026/200000 [0m

                       Computation: 3207 steps/s (collection: 0.462s, learning 2.091s)
               Value function loss: 92687.3475
                    Surrogate loss: 0.0163
             Mean action noise std: 0.88
                       Mean reward: 9552.17
               Mean episode length: 391.05
                 Mean success rate: 81.00
                  Mean reward/step: 22.99
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16605184
                    Iteration time: 2.55s
                        Total time: 5366.30s
                               ETA: 524118.3s

################################################################################
                    [1m Learning iteration 2027/200000 [0m

                       Computation: 3246 steps/s (collection: 0.507s, learning 2.017s)
               Value function loss: 98051.1916
                    Surrogate loss: 0.0142
             Mean action noise std: 0.88
                       Mean reward: 9601.40
               Mean episode length: 395.67
                 Mean success rate: 82.00
                  Mean reward/step: 23.29
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 2.52s
                        Total time: 5368.82s
                               ETA: 524103.6s

################################################################################
                    [1m Learning iteration 2028/200000 [0m

                       Computation: 3162 steps/s (collection: 0.493s, learning 2.097s)
               Value function loss: 53120.6716
                    Surrogate loss: 0.0134
             Mean action noise std: 0.88
                       Mean reward: 9098.76
               Mean episode length: 379.29
                 Mean success rate: 78.50
                  Mean reward/step: 23.75
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 16621568
                    Iteration time: 2.59s
                        Total time: 5371.41s
                               ETA: 524095.3s

################################################################################
                    [1m Learning iteration 2029/200000 [0m

                       Computation: 3237 steps/s (collection: 0.442s, learning 2.088s)
               Value function loss: 93160.9244
                    Surrogate loss: 0.0113
             Mean action noise std: 0.88
                       Mean reward: 9381.23
               Mean episode length: 390.06
                 Mean success rate: 79.50
                  Mean reward/step: 24.38
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16629760
                    Iteration time: 2.53s
                        Total time: 5373.94s
                               ETA: 524081.3s

################################################################################
                    [1m Learning iteration 2030/200000 [0m

                       Computation: 3221 steps/s (collection: 0.443s, learning 2.100s)
               Value function loss: 61397.0564
                    Surrogate loss: 0.0121
             Mean action noise std: 0.88
                       Mean reward: 9027.51
               Mean episode length: 378.94
                 Mean success rate: 77.00
                  Mean reward/step: 24.85
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16637952
                    Iteration time: 2.54s
                        Total time: 5376.49s
                               ETA: 524068.5s

################################################################################
                    [1m Learning iteration 2031/200000 [0m

                       Computation: 3252 steps/s (collection: 0.473s, learning 2.046s)
               Value function loss: 101359.1721
                    Surrogate loss: 0.0151
             Mean action noise std: 0.88
                       Mean reward: 9147.67
               Mean episode length: 382.37
                 Mean success rate: 78.00
                  Mean reward/step: 24.22
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16646144
                    Iteration time: 2.52s
                        Total time: 5379.01s
                               ETA: 524053.3s

################################################################################
                    [1m Learning iteration 2032/200000 [0m

                       Computation: 3191 steps/s (collection: 0.497s, learning 2.070s)
               Value function loss: 44966.9989
                    Surrogate loss: 0.0159
             Mean action noise std: 0.88
                       Mean reward: 9299.15
               Mean episode length: 389.21
                 Mean success rate: 79.50
                  Mean reward/step: 23.86
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 16654336
                    Iteration time: 2.57s
                        Total time: 5381.57s
                               ETA: 524042.8s

################################################################################
                    [1m Learning iteration 2033/200000 [0m

                       Computation: 3286 steps/s (collection: 0.463s, learning 2.029s)
               Value function loss: 111674.6660
                    Surrogate loss: 0.0153
             Mean action noise std: 0.88
                       Mean reward: 9220.25
               Mean episode length: 387.58
                 Mean success rate: 79.50
                  Mean reward/step: 24.72
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 16662528
                    Iteration time: 2.49s
                        Total time: 5384.06s
                               ETA: 524025.1s

################################################################################
                    [1m Learning iteration 2034/200000 [0m

                       Computation: 3204 steps/s (collection: 0.504s, learning 2.052s)
               Value function loss: 109052.3186
                    Surrogate loss: 0.0126
             Mean action noise std: 0.88
                       Mean reward: 9244.64
               Mean episode length: 391.07
                 Mean success rate: 79.00
                  Mean reward/step: 23.95
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 16670720
                    Iteration time: 2.56s
                        Total time: 5386.62s
                               ETA: 524013.6s

################################################################################
                    [1m Learning iteration 2035/200000 [0m

                       Computation: 3204 steps/s (collection: 0.478s, learning 2.079s)
               Value function loss: 91774.0098
                    Surrogate loss: 0.0144
             Mean action noise std: 0.88
                       Mean reward: 9189.75
               Mean episode length: 386.37
                 Mean success rate: 78.00
                  Mean reward/step: 23.62
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16678912
                    Iteration time: 2.56s
                        Total time: 5389.18s
                               ETA: 524002.2s

################################################################################
                    [1m Learning iteration 2036/200000 [0m

                       Computation: 3271 steps/s (collection: 0.469s, learning 2.035s)
               Value function loss: 96036.2088
                    Surrogate loss: 0.0133
             Mean action noise std: 0.88
                       Mean reward: 9109.98
               Mean episode length: 384.35
                 Mean success rate: 77.50
                  Mean reward/step: 23.81
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 16687104
                    Iteration time: 2.50s
                        Total time: 5391.68s
                               ETA: 523985.7s

################################################################################
                    [1m Learning iteration 2037/200000 [0m

                       Computation: 3267 steps/s (collection: 0.477s, learning 2.031s)
               Value function loss: 83900.6769
                    Surrogate loss: 0.0148
             Mean action noise std: 0.88
                       Mean reward: 8940.86
               Mean episode length: 377.89
                 Mean success rate: 75.00
                  Mean reward/step: 23.30
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 16695296
                    Iteration time: 2.51s
                        Total time: 5394.19s
                               ETA: 523969.5s

################################################################################
                    [1m Learning iteration 2038/200000 [0m

                       Computation: 3324 steps/s (collection: 0.425s, learning 2.039s)
               Value function loss: 87471.3212
                    Surrogate loss: 0.0133
             Mean action noise std: 0.88
                       Mean reward: 9023.27
               Mean episode length: 378.13
                 Mean success rate: 75.50
                  Mean reward/step: 23.55
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16703488
                    Iteration time: 2.46s
                        Total time: 5396.65s
                               ETA: 523949.1s

################################################################################
                    [1m Learning iteration 2039/200000 [0m

                       Computation: 3242 steps/s (collection: 0.480s, learning 2.047s)
               Value function loss: 80627.7124
                    Surrogate loss: 0.0130
             Mean action noise std: 0.88
                       Mean reward: 8960.25
               Mean episode length: 378.60
                 Mean success rate: 76.00
                  Mean reward/step: 23.71
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 2.53s
                        Total time: 5399.18s
                               ETA: 523934.8s

################################################################################
                    [1m Learning iteration 2040/200000 [0m

                       Computation: 3249 steps/s (collection: 0.490s, learning 2.031s)
               Value function loss: 104988.6185
                    Surrogate loss: 0.0134
             Mean action noise std: 0.88
                       Mean reward: 9422.75
               Mean episode length: 393.00
                 Mean success rate: 78.50
                  Mean reward/step: 23.46
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16719872
                    Iteration time: 2.52s
                        Total time: 5401.70s
                               ETA: 523919.9s

################################################################################
                    [1m Learning iteration 2041/200000 [0m

                       Computation: 3249 steps/s (collection: 0.475s, learning 2.046s)
               Value function loss: 90789.7906
                    Surrogate loss: 0.0125
             Mean action noise std: 0.88
                       Mean reward: 9384.56
               Mean episode length: 390.29
                 Mean success rate: 77.50
                  Mean reward/step: 23.27
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16728064
                    Iteration time: 2.52s
                        Total time: 5404.22s
                               ETA: 523905.1s

################################################################################
                    [1m Learning iteration 2042/200000 [0m

                       Computation: 3210 steps/s (collection: 0.473s, learning 2.079s)
               Value function loss: 83977.6701
                    Surrogate loss: 0.0126
             Mean action noise std: 0.88
                       Mean reward: 9464.60
               Mean episode length: 393.85
                 Mean success rate: 77.50
                  Mean reward/step: 23.98
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16736256
                    Iteration time: 2.55s
                        Total time: 5406.77s
                               ETA: 523893.3s

################################################################################
                    [1m Learning iteration 2043/200000 [0m

                       Computation: 3280 steps/s (collection: 0.454s, learning 2.043s)
               Value function loss: 89870.0618
                    Surrogate loss: 0.0100
             Mean action noise std: 0.88
                       Mean reward: 9521.60
               Mean episode length: 394.22
                 Mean success rate: 76.50
                  Mean reward/step: 23.98
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16744448
                    Iteration time: 2.50s
                        Total time: 5409.27s
                               ETA: 523876.1s

################################################################################
                    [1m Learning iteration 2044/200000 [0m

                       Computation: 3278 steps/s (collection: 0.466s, learning 2.033s)
               Value function loss: 116718.8559
                    Surrogate loss: 0.0156
             Mean action noise std: 0.88
                       Mean reward: 9229.73
               Mean episode length: 384.15
                 Mean success rate: 75.00
                  Mean reward/step: 24.50
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 16752640
                    Iteration time: 2.50s
                        Total time: 5411.77s
                               ETA: 523859.1s

################################################################################
                    [1m Learning iteration 2045/200000 [0m

                       Computation: 3291 steps/s (collection: 0.452s, learning 2.037s)
               Value function loss: 108803.4357
                    Surrogate loss: 0.0124
             Mean action noise std: 0.88
                       Mean reward: 9203.38
               Mean episode length: 383.66
                 Mean success rate: 75.50
                  Mean reward/step: 24.41
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16760832
                    Iteration time: 2.49s
                        Total time: 5414.26s
                               ETA: 523841.3s

################################################################################
                    [1m Learning iteration 2046/200000 [0m

                       Computation: 3304 steps/s (collection: 0.443s, learning 2.035s)
               Value function loss: 110482.0711
                    Surrogate loss: 0.0139
             Mean action noise std: 0.88
                       Mean reward: 9372.99
               Mean episode length: 389.47
                 Mean success rate: 77.00
                  Mean reward/step: 24.20
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16769024
                    Iteration time: 2.48s
                        Total time: 5416.74s
                               ETA: 523822.4s

################################################################################
                    [1m Learning iteration 2047/200000 [0m

                       Computation: 3259 steps/s (collection: 0.483s, learning 2.030s)
               Value function loss: 72184.6255
                    Surrogate loss: 0.0130
             Mean action noise std: 0.88
                       Mean reward: 9050.31
               Mean episode length: 378.70
                 Mean success rate: 75.00
                  Mean reward/step: 23.94
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16777216
                    Iteration time: 2.51s
                        Total time: 5419.25s
                               ETA: 523807.0s

################################################################################
                    [1m Learning iteration 2048/200000 [0m

                       Computation: 3236 steps/s (collection: 0.489s, learning 2.043s)
               Value function loss: 112357.8785
                    Surrogate loss: 0.0183
             Mean action noise std: 0.88
                       Mean reward: 9383.59
               Mean episode length: 388.80
                 Mean success rate: 77.00
                  Mean reward/step: 24.93
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16785408
                    Iteration time: 2.53s
                        Total time: 5421.78s
                               ETA: 523793.2s

################################################################################
                    [1m Learning iteration 2049/200000 [0m

                       Computation: 3273 steps/s (collection: 0.486s, learning 2.016s)
               Value function loss: 91894.8396
                    Surrogate loss: 0.0111
             Mean action noise std: 0.88
                       Mean reward: 9111.60
               Mean episode length: 378.41
                 Mean success rate: 75.00
                  Mean reward/step: 24.96
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16793600
                    Iteration time: 2.50s
                        Total time: 5424.28s
                               ETA: 523776.7s

################################################################################
                    [1m Learning iteration 2050/200000 [0m

                       Computation: 3305 steps/s (collection: 0.431s, learning 2.047s)
               Value function loss: 100865.2551
                    Surrogate loss: 0.0133
             Mean action noise std: 0.88
                       Mean reward: 9128.40
               Mean episode length: 378.92
                 Mean success rate: 75.00
                  Mean reward/step: 24.49
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 16801792
                    Iteration time: 2.48s
                        Total time: 5426.76s
                               ETA: 523757.9s

################################################################################
                    [1m Learning iteration 2051/200000 [0m

                       Computation: 3343 steps/s (collection: 0.442s, learning 2.008s)
               Value function loss: 73826.5422
                    Surrogate loss: 0.0215
             Mean action noise std: 0.88
                       Mean reward: 9420.52
               Mean episode length: 390.22
                 Mean success rate: 77.00
                  Mean reward/step: 24.59
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 2.45s
                        Total time: 5429.21s
                               ETA: 523736.4s

################################################################################
                    [1m Learning iteration 2052/200000 [0m

                       Computation: 3199 steps/s (collection: 0.499s, learning 2.061s)
               Value function loss: 85620.4757
                    Surrogate loss: 0.0141
             Mean action noise std: 0.88
                       Mean reward: 9602.75
               Mean episode length: 397.15
                 Mean success rate: 78.50
                  Mean reward/step: 25.08
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 16818176
                    Iteration time: 2.56s
                        Total time: 5431.77s
                               ETA: 523725.5s

################################################################################
                    [1m Learning iteration 2053/200000 [0m

                       Computation: 3262 steps/s (collection: 0.474s, learning 2.036s)
               Value function loss: 94474.0587
                    Surrogate loss: 0.0155
             Mean action noise std: 0.88
                       Mean reward: 9613.91
               Mean episode length: 398.95
                 Mean success rate: 79.00
                  Mean reward/step: 25.10
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16826368
                    Iteration time: 2.51s
                        Total time: 5434.28s
                               ETA: 523709.8s

################################################################################
                    [1m Learning iteration 2054/200000 [0m

                       Computation: 3253 steps/s (collection: 0.465s, learning 2.053s)
               Value function loss: 83155.0343
                    Surrogate loss: 0.0141
             Mean action noise std: 0.88
                       Mean reward: 10022.06
               Mean episode length: 412.80
                 Mean success rate: 81.50
                  Mean reward/step: 24.59
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 16834560
                    Iteration time: 2.52s
                        Total time: 5436.80s
                               ETA: 523694.9s

################################################################################
                    [1m Learning iteration 2055/200000 [0m

                       Computation: 3206 steps/s (collection: 0.498s, learning 2.057s)
               Value function loss: 108640.1849
                    Surrogate loss: 0.0125
             Mean action noise std: 0.88
                       Mean reward: 9985.20
               Mean episode length: 412.41
                 Mean success rate: 80.00
                  Mean reward/step: 24.12
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 16842752
                    Iteration time: 2.55s
                        Total time: 5439.36s
                               ETA: 523683.5s

################################################################################
                    [1m Learning iteration 2056/200000 [0m

                       Computation: 3263 steps/s (collection: 0.491s, learning 2.019s)
               Value function loss: 108649.6078
                    Surrogate loss: 0.0117
             Mean action noise std: 0.88
                       Mean reward: 9500.49
               Mean episode length: 398.17
                 Mean success rate: 77.50
                  Mean reward/step: 23.16
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 16850944
                    Iteration time: 2.51s
                        Total time: 5441.87s
                               ETA: 523667.8s

################################################################################
                    [1m Learning iteration 2057/200000 [0m

                       Computation: 3261 steps/s (collection: 0.467s, learning 2.045s)
               Value function loss: 99207.8305
                    Surrogate loss: 0.0157
             Mean action noise std: 0.88
                       Mean reward: 9938.63
               Mean episode length: 413.33
                 Mean success rate: 80.50
                  Mean reward/step: 22.74
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16859136
                    Iteration time: 2.51s
                        Total time: 5444.38s
                               ETA: 523652.2s

################################################################################
                    [1m Learning iteration 2058/200000 [0m

                       Computation: 3277 steps/s (collection: 0.452s, learning 2.048s)
               Value function loss: 86481.5183
                    Surrogate loss: 0.0118
             Mean action noise std: 0.88
                       Mean reward: 9722.34
               Mean episode length: 404.25
                 Mean success rate: 79.50
                  Mean reward/step: 22.85
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 16867328
                    Iteration time: 2.50s
                        Total time: 5446.88s
                               ETA: 523635.6s

################################################################################
                    [1m Learning iteration 2059/200000 [0m

                       Computation: 3249 steps/s (collection: 0.473s, learning 2.049s)
               Value function loss: 61031.7551
                    Surrogate loss: 0.0131
             Mean action noise std: 0.88
                       Mean reward: 9319.59
               Mean episode length: 394.38
                 Mean success rate: 77.50
                  Mean reward/step: 23.05
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 16875520
                    Iteration time: 2.52s
                        Total time: 5449.40s
                               ETA: 523621.0s

################################################################################
                    [1m Learning iteration 2060/200000 [0m

                       Computation: 3286 steps/s (collection: 0.457s, learning 2.036s)
               Value function loss: 90756.4757
                    Surrogate loss: 0.0126
             Mean action noise std: 0.88
                       Mean reward: 9076.59
               Mean episode length: 383.64
                 Mean success rate: 76.00
                  Mean reward/step: 23.42
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 16883712
                    Iteration time: 2.49s
                        Total time: 5451.89s
                               ETA: 523603.7s

################################################################################
                    [1m Learning iteration 2061/200000 [0m

                       Computation: 3124 steps/s (collection: 0.520s, learning 2.101s)
               Value function loss: 99830.7519
                    Surrogate loss: 0.0092
             Mean action noise std: 0.88
                       Mean reward: 8645.03
               Mean episode length: 366.88
                 Mean success rate: 72.00
                  Mean reward/step: 22.97
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 16891904
                    Iteration time: 2.62s
                        Total time: 5454.51s
                               ETA: 523598.8s

################################################################################
                    [1m Learning iteration 2062/200000 [0m

                       Computation: 3249 steps/s (collection: 0.467s, learning 2.054s)
               Value function loss: 87873.8907
                    Surrogate loss: 0.0116
             Mean action noise std: 0.88
                       Mean reward: 8523.16
               Mean episode length: 361.91
                 Mean success rate: 71.00
                  Mean reward/step: 22.51
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16900096
                    Iteration time: 2.52s
                        Total time: 5457.03s
                               ETA: 523584.2s

################################################################################
                    [1m Learning iteration 2063/200000 [0m

                       Computation: 3219 steps/s (collection: 0.466s, learning 2.079s)
               Value function loss: 68343.0283
                    Surrogate loss: 0.0122
             Mean action noise std: 0.88
                       Mean reward: 8380.71
               Mean episode length: 353.91
                 Mean success rate: 70.00
                  Mean reward/step: 23.11
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 2.54s
                        Total time: 5459.58s
                               ETA: 523571.9s

################################################################################
                    [1m Learning iteration 2064/200000 [0m

                       Computation: 3241 steps/s (collection: 0.447s, learning 2.081s)
               Value function loss: 104992.8388
                    Surrogate loss: 0.0100
             Mean action noise std: 0.88
                       Mean reward: 8548.54
               Mean episode length: 358.17
                 Mean success rate: 70.50
                  Mean reward/step: 23.99
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16916480
                    Iteration time: 2.53s
                        Total time: 5462.10s
                               ETA: 523558.0s

################################################################################
                    [1m Learning iteration 2065/200000 [0m

                       Computation: 3258 steps/s (collection: 0.490s, learning 2.024s)
               Value function loss: 116771.2835
                    Surrogate loss: 0.0106
             Mean action noise std: 0.88
                       Mean reward: 8520.00
               Mean episode length: 354.97
                 Mean success rate: 70.00
                  Mean reward/step: 24.02
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 16924672
                    Iteration time: 2.51s
                        Total time: 5464.62s
                               ETA: 523542.8s

################################################################################
                    [1m Learning iteration 2066/200000 [0m

                       Computation: 3271 steps/s (collection: 0.457s, learning 2.046s)
               Value function loss: 75919.2792
                    Surrogate loss: 0.0127
             Mean action noise std: 0.88
                       Mean reward: 8782.57
               Mean episode length: 364.51
                 Mean success rate: 72.00
                  Mean reward/step: 22.98
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 16932864
                    Iteration time: 2.50s
                        Total time: 5467.12s
                               ETA: 523526.6s

################################################################################
                    [1m Learning iteration 2067/200000 [0m

                       Computation: 3273 steps/s (collection: 0.463s, learning 2.039s)
               Value function loss: 74612.4904
                    Surrogate loss: 0.0128
             Mean action noise std: 0.88
                       Mean reward: 8125.78
               Mean episode length: 342.95
                 Mean success rate: 67.00
                  Mean reward/step: 23.33
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 16941056
                    Iteration time: 2.50s
                        Total time: 5469.63s
                               ETA: 523510.3s

################################################################################
                    [1m Learning iteration 2068/200000 [0m

                       Computation: 3241 steps/s (collection: 0.480s, learning 2.047s)
               Value function loss: 66394.1259
                    Surrogate loss: 0.0150
             Mean action noise std: 0.88
                       Mean reward: 7799.63
               Mean episode length: 333.19
                 Mean success rate: 64.00
                  Mean reward/step: 23.88
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 16949248
                    Iteration time: 2.53s
                        Total time: 5472.15s
                               ETA: 523496.4s

################################################################################
                    [1m Learning iteration 2069/200000 [0m

                       Computation: 3127 steps/s (collection: 0.532s, learning 2.088s)
               Value function loss: 89897.5915
                    Surrogate loss: 0.0108
             Mean action noise std: 0.88
                       Mean reward: 7885.57
               Mean episode length: 337.19
                 Mean success rate: 65.50
                  Mean reward/step: 24.40
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 16957440
                    Iteration time: 2.62s
                        Total time: 5474.77s
                               ETA: 523491.3s

################################################################################
                    [1m Learning iteration 2070/200000 [0m

                       Computation: 3171 steps/s (collection: 0.483s, learning 2.100s)
               Value function loss: 82438.9342
                    Surrogate loss: 0.0126
             Mean action noise std: 0.88
                       Mean reward: 7856.42
               Mean episode length: 338.23
                 Mean success rate: 66.00
                  Mean reward/step: 24.85
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 16965632
                    Iteration time: 2.58s
                        Total time: 5477.35s
                               ETA: 523482.7s

################################################################################
                    [1m Learning iteration 2071/200000 [0m

                       Computation: 3132 steps/s (collection: 0.567s, learning 2.049s)
               Value function loss: 101306.3864
                    Surrogate loss: 0.0114
             Mean action noise std: 0.88
                       Mean reward: 8227.91
               Mean episode length: 351.88
                 Mean success rate: 68.50
                  Mean reward/step: 24.80
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 16973824
                    Iteration time: 2.62s
                        Total time: 5479.97s
                               ETA: 523477.3s

################################################################################
                    [1m Learning iteration 2072/200000 [0m

                       Computation: 3080 steps/s (collection: 0.542s, learning 2.117s)
               Value function loss: 92778.5367
                    Surrogate loss: 0.0098
             Mean action noise std: 0.88
                       Mean reward: 8412.59
               Mean episode length: 363.46
                 Mean success rate: 70.50
                  Mean reward/step: 24.61
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 16982016
                    Iteration time: 2.66s
                        Total time: 5482.63s
                               ETA: 523476.1s

################################################################################
                    [1m Learning iteration 2073/200000 [0m

                       Computation: 3218 steps/s (collection: 0.481s, learning 2.065s)
               Value function loss: 100356.3582
                    Surrogate loss: 0.0122
             Mean action noise std: 0.88
                       Mean reward: 8502.53
               Mean episode length: 363.34
                 Mean success rate: 71.00
                  Mean reward/step: 24.85
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16990208
                    Iteration time: 2.55s
                        Total time: 5485.18s
                               ETA: 523464.0s

################################################################################
                    [1m Learning iteration 2074/200000 [0m

                       Computation: 3299 steps/s (collection: 0.469s, learning 2.013s)
               Value function loss: 76345.5260
                    Surrogate loss: 0.0105
             Mean action noise std: 0.88
                       Mean reward: 8332.32
               Mean episode length: 356.84
                 Mean success rate: 69.50
                  Mean reward/step: 24.97
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 16998400
                    Iteration time: 2.48s
                        Total time: 5487.66s
                               ETA: 523445.9s

################################################################################
                    [1m Learning iteration 2075/200000 [0m

                       Computation: 3243 steps/s (collection: 0.516s, learning 2.009s)
               Value function loss: 96019.9487
                    Surrogate loss: 0.0159
             Mean action noise std: 0.88
                       Mean reward: 8302.67
               Mean episode length: 355.83
                 Mean success rate: 68.50
                  Mean reward/step: 24.98
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 2.53s
                        Total time: 5490.18s
                               ETA: 523431.9s

################################################################################
                    [1m Learning iteration 2076/200000 [0m

                       Computation: 3223 steps/s (collection: 0.511s, learning 2.030s)
               Value function loss: 115593.1225
                    Surrogate loss: 0.0105
             Mean action noise std: 0.88
                       Mean reward: 8869.90
               Mean episode length: 372.62
                 Mean success rate: 72.00
                  Mean reward/step: 24.43
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 17014784
                    Iteration time: 2.54s
                        Total time: 5492.72s
                               ETA: 523419.4s

################################################################################
                    [1m Learning iteration 2077/200000 [0m

                       Computation: 3147 steps/s (collection: 0.546s, learning 2.057s)
               Value function loss: 105932.0827
                    Surrogate loss: 0.0122
             Mean action noise std: 0.88
                       Mean reward: 9408.52
               Mean episode length: 387.88
                 Mean success rate: 76.50
                  Mean reward/step: 23.91
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 17022976
                    Iteration time: 2.60s
                        Total time: 5495.33s
                               ETA: 523412.8s

################################################################################
                    [1m Learning iteration 2078/200000 [0m

                       Computation: 3264 steps/s (collection: 0.477s, learning 2.033s)
               Value function loss: 98631.8214
                    Surrogate loss: 0.0131
             Mean action noise std: 0.88
                       Mean reward: 9480.63
               Mean episode length: 388.64
                 Mean success rate: 77.00
                  Mean reward/step: 24.02
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 17031168
                    Iteration time: 2.51s
                        Total time: 5497.84s
                               ETA: 523397.3s

################################################################################
                    [1m Learning iteration 2079/200000 [0m

                       Computation: 3196 steps/s (collection: 0.512s, learning 2.051s)
               Value function loss: 44073.6844
                    Surrogate loss: 0.0171
             Mean action noise std: 0.88
                       Mean reward: 9295.22
               Mean episode length: 379.20
                 Mean success rate: 75.50
                  Mean reward/step: 24.85
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 17039360
                    Iteration time: 2.56s
                        Total time: 5500.40s
                               ETA: 523386.9s

################################################################################
                    [1m Learning iteration 2080/200000 [0m

                       Computation: 3297 steps/s (collection: 0.444s, learning 2.040s)
               Value function loss: 75042.9291
                    Surrogate loss: 0.0107
             Mean action noise std: 0.88
                       Mean reward: 9309.21
               Mean episode length: 383.33
                 Mean success rate: 76.50
                  Mean reward/step: 25.60
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 17047552
                    Iteration time: 2.48s
                        Total time: 5502.88s
                               ETA: 523369.0s

################################################################################
                    [1m Learning iteration 2081/200000 [0m

                       Computation: 3288 steps/s (collection: 0.466s, learning 2.025s)
               Value function loss: 116437.2085
                    Surrogate loss: 0.0131
             Mean action noise std: 0.88
                       Mean reward: 9456.43
               Mean episode length: 384.24
                 Mean success rate: 77.50
                  Mean reward/step: 25.76
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 17055744
                    Iteration time: 2.49s
                        Total time: 5505.38s
                               ETA: 523351.7s

################################################################################
                    [1m Learning iteration 2082/200000 [0m

                       Computation: 3305 steps/s (collection: 0.450s, learning 2.028s)
               Value function loss: 83005.6630
                    Surrogate loss: 0.0134
             Mean action noise std: 0.88
                       Mean reward: 9652.26
               Mean episode length: 387.43
                 Mean success rate: 78.50
                  Mean reward/step: 25.26
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 17063936
                    Iteration time: 2.48s
                        Total time: 5507.85s
                               ETA: 523333.3s

################################################################################
                    [1m Learning iteration 2083/200000 [0m

                       Computation: 3243 steps/s (collection: 0.468s, learning 2.058s)
               Value function loss: 100590.5571
                    Surrogate loss: 0.0138
             Mean action noise std: 0.87
                       Mean reward: 9874.45
               Mean episode length: 395.98
                 Mean success rate: 80.00
                  Mean reward/step: 25.06
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 17072128
                    Iteration time: 2.53s
                        Total time: 5510.38s
                               ETA: 523319.4s

################################################################################
                    [1m Learning iteration 2084/200000 [0m

                       Computation: 3308 steps/s (collection: 0.446s, learning 2.030s)
               Value function loss: 90955.7637
                    Surrogate loss: 0.0120
             Mean action noise std: 0.87
                       Mean reward: 9984.74
               Mean episode length: 399.44
                 Mean success rate: 81.00
                  Mean reward/step: 25.13
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 17080320
                    Iteration time: 2.48s
                        Total time: 5512.86s
                               ETA: 523300.9s

################################################################################
                    [1m Learning iteration 2085/200000 [0m

                       Computation: 3290 steps/s (collection: 0.451s, learning 2.038s)
               Value function loss: 85246.1988
                    Surrogate loss: 0.0130
             Mean action noise std: 0.87
                       Mean reward: 9989.64
               Mean episode length: 400.97
                 Mean success rate: 80.00
                  Mean reward/step: 24.25
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 17088512
                    Iteration time: 2.49s
                        Total time: 5515.34s
                               ETA: 523283.5s

################################################################################
                    [1m Learning iteration 2086/200000 [0m

                       Computation: 3260 steps/s (collection: 0.479s, learning 2.034s)
               Value function loss: 95027.1933
                    Surrogate loss: 0.0143
             Mean action noise std: 0.87
                       Mean reward: 9765.85
               Mean episode length: 392.17
                 Mean success rate: 78.50
                  Mean reward/step: 24.05
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 17096704
                    Iteration time: 2.51s
                        Total time: 5517.86s
                               ETA: 523268.5s

################################################################################
                    [1m Learning iteration 2087/200000 [0m

                       Computation: 3291 steps/s (collection: 0.460s, learning 2.029s)
               Value function loss: 98761.9807
                    Surrogate loss: 0.0131
             Mean action noise std: 0.87
                       Mean reward: 9869.83
               Mean episode length: 394.51
                 Mean success rate: 79.00
                  Mean reward/step: 24.12
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 2.49s
                        Total time: 5520.35s
                               ETA: 523251.1s

################################################################################
                    [1m Learning iteration 2088/200000 [0m

                       Computation: 3345 steps/s (collection: 0.423s, learning 2.025s)
               Value function loss: 71790.7215
                    Surrogate loss: 0.0129
             Mean action noise std: 0.87
                       Mean reward: 9877.80
               Mean episode length: 396.80
                 Mean success rate: 79.00
                  Mean reward/step: 24.87
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 17113088
                    Iteration time: 2.45s
                        Total time: 5522.80s
                               ETA: 523230.0s

################################################################################
                    [1m Learning iteration 2089/200000 [0m

                       Computation: 3301 steps/s (collection: 0.470s, learning 2.011s)
               Value function loss: 82734.3615
                    Surrogate loss: 0.0103
             Mean action noise std: 0.88
                       Mean reward: 9971.75
               Mean episode length: 400.64
                 Mean success rate: 78.00
                  Mean reward/step: 24.96
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 17121280
                    Iteration time: 2.48s
                        Total time: 5525.28s
                               ETA: 523212.0s

################################################################################
                    [1m Learning iteration 2090/200000 [0m

                       Computation: 3262 steps/s (collection: 0.496s, learning 2.015s)
               Value function loss: 85143.3174
                    Surrogate loss: 0.0114
             Mean action noise std: 0.88
                       Mean reward: 9934.45
               Mean episode length: 398.75
                 Mean success rate: 77.50
                  Mean reward/step: 24.34
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 17129472
                    Iteration time: 2.51s
                        Total time: 5527.79s
                               ETA: 523196.8s

################################################################################
                    [1m Learning iteration 2091/200000 [0m

                       Computation: 3243 steps/s (collection: 0.489s, learning 2.038s)
               Value function loss: 128111.9531
                    Surrogate loss: 0.0118
             Mean action noise std: 0.88
                       Mean reward: 9196.58
               Mean episode length: 375.48
                 Mean success rate: 72.50
                  Mean reward/step: 23.80
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 17137664
                    Iteration time: 2.53s
                        Total time: 5530.31s
                               ETA: 523183.0s

################################################################################
                    [1m Learning iteration 2092/200000 [0m

                       Computation: 3279 steps/s (collection: 0.490s, learning 2.008s)
               Value function loss: 113069.0605
                    Surrogate loss: 0.0142
             Mean action noise std: 0.88
                       Mean reward: 9158.57
               Mean episode length: 372.81
                 Mean success rate: 72.00
                  Mean reward/step: 22.53
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 17145856
                    Iteration time: 2.50s
                        Total time: 5532.81s
                               ETA: 523166.6s

################################################################################
                    [1m Learning iteration 2093/200000 [0m

                       Computation: 3258 steps/s (collection: 0.485s, learning 2.030s)
               Value function loss: 101447.6987
                    Surrogate loss: 0.0141
             Mean action noise std: 0.88
                       Mean reward: 9108.71
               Mean episode length: 369.37
                 Mean success rate: 71.50
                  Mean reward/step: 22.92
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 17154048
                    Iteration time: 2.51s
                        Total time: 5535.33s
                               ETA: 523151.7s

################################################################################
                    [1m Learning iteration 2094/200000 [0m

                       Computation: 3245 steps/s (collection: 0.502s, learning 2.022s)
               Value function loss: 90168.5454
                    Surrogate loss: 0.0196
             Mean action noise std: 0.88
                       Mean reward: 8953.93
               Mean episode length: 365.43
                 Mean success rate: 72.00
                  Mean reward/step: 22.91
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 17162240
                    Iteration time: 2.52s
                        Total time: 5537.85s
                               ETA: 523137.8s

################################################################################
                    [1m Learning iteration 2095/200000 [0m

                       Computation: 3288 steps/s (collection: 0.460s, learning 2.031s)
               Value function loss: 107990.1348
                    Surrogate loss: 0.0134
             Mean action noise std: 0.88
                       Mean reward: 8693.36
               Mean episode length: 360.03
                 Mean success rate: 70.50
                  Mean reward/step: 23.24
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 17170432
                    Iteration time: 2.49s
                        Total time: 5540.34s
                               ETA: 523120.8s

################################################################################
                    [1m Learning iteration 2096/200000 [0m

                       Computation: 3337 steps/s (collection: 0.432s, learning 2.022s)
               Value function loss: 78984.2899
                    Surrogate loss: 0.0125
             Mean action noise std: 0.88
                       Mean reward: 8632.24
               Mean episode length: 354.76
                 Mean success rate: 70.50
                  Mean reward/step: 22.95
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 17178624
                    Iteration time: 2.45s
                        Total time: 5542.80s
                               ETA: 523100.3s

################################################################################
                    [1m Learning iteration 2097/200000 [0m

                       Computation: 3242 steps/s (collection: 0.458s, learning 2.069s)
               Value function loss: 52538.7146
                    Surrogate loss: 0.0123
             Mean action noise std: 0.88
                       Mean reward: 8432.57
               Mean episode length: 349.27
                 Mean success rate: 69.00
                  Mean reward/step: 23.34
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 17186816
                    Iteration time: 2.53s
                        Total time: 5545.32s
                               ETA: 523086.7s

################################################################################
                    [1m Learning iteration 2098/200000 [0m

                       Computation: 3280 steps/s (collection: 0.453s, learning 2.045s)
               Value function loss: 80187.6911
                    Surrogate loss: 0.0163
             Mean action noise std: 0.88
                       Mean reward: 8420.59
               Mean episode length: 348.30
                 Mean success rate: 68.50
                  Mean reward/step: 24.73
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 17195008
                    Iteration time: 2.50s
                        Total time: 5547.82s
                               ETA: 523070.2s

################################################################################
                    [1m Learning iteration 2099/200000 [0m

                       Computation: 3267 steps/s (collection: 0.445s, learning 2.062s)
               Value function loss: 73863.1918
                    Surrogate loss: 0.0130
             Mean action noise std: 0.88
                       Mean reward: 8733.58
               Mean episode length: 358.23
                 Mean success rate: 71.00
                  Mean reward/step: 25.11
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 2.51s
                        Total time: 5550.33s
                               ETA: 523054.8s

################################################################################
                    [1m Learning iteration 2100/200000 [0m

                       Computation: 3255 steps/s (collection: 0.473s, learning 2.044s)
               Value function loss: 91937.0117
                    Surrogate loss: 0.0179
             Mean action noise std: 0.88
                       Mean reward: 9258.59
               Mean episode length: 375.43
                 Mean success rate: 74.50
                  Mean reward/step: 25.04
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 17211392
                    Iteration time: 2.52s
                        Total time: 5552.84s
                               ETA: 523040.2s

################################################################################
                    [1m Learning iteration 2101/200000 [0m

                       Computation: 3307 steps/s (collection: 0.455s, learning 2.021s)
               Value function loss: 85644.2117
                    Surrogate loss: 0.0157
             Mean action noise std: 0.88
                       Mean reward: 8854.68
               Mean episode length: 366.01
                 Mean success rate: 72.50
                  Mean reward/step: 24.60
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 17219584
                    Iteration time: 2.48s
                        Total time: 5555.32s
                               ETA: 523021.9s

################################################################################
                    [1m Learning iteration 2102/200000 [0m

                       Computation: 3289 steps/s (collection: 0.457s, learning 2.033s)
               Value function loss: 98449.0390
                    Surrogate loss: 0.0123
             Mean action noise std: 0.88
                       Mean reward: 8417.63
               Mean episode length: 350.78
                 Mean success rate: 69.50
                  Mean reward/step: 24.33
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 17227776
                    Iteration time: 2.49s
                        Total time: 5557.81s
                               ETA: 523005.0s

################################################################################
                    [1m Learning iteration 2103/200000 [0m

                       Computation: 3252 steps/s (collection: 0.468s, learning 2.051s)
               Value function loss: 84748.8366
                    Surrogate loss: 0.0123
             Mean action noise std: 0.88
                       Mean reward: 8663.66
               Mean episode length: 362.85
                 Mean success rate: 71.50
                  Mean reward/step: 24.18
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 17235968
                    Iteration time: 2.52s
                        Total time: 5560.33s
                               ETA: 522990.6s

################################################################################
                    [1m Learning iteration 2104/200000 [0m

                       Computation: 3322 steps/s (collection: 0.433s, learning 2.033s)
               Value function loss: 99151.1410
                    Surrogate loss: 0.0158
             Mean action noise std: 0.88
                       Mean reward: 8971.56
               Mean episode length: 372.67
                 Mean success rate: 72.50
                  Mean reward/step: 24.24
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 17244160
                    Iteration time: 2.47s
                        Total time: 5562.79s
                               ETA: 522971.4s

################################################################################
                    [1m Learning iteration 2105/200000 [0m

                       Computation: 3289 steps/s (collection: 0.474s, learning 2.016s)
               Value function loss: 97111.0531
                    Surrogate loss: 0.0124
             Mean action noise std: 0.88
                       Mean reward: 9164.32
               Mean episode length: 378.70
                 Mean success rate: 74.00
                  Mean reward/step: 24.43
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 17252352
                    Iteration time: 2.49s
                        Total time: 5565.28s
                               ETA: 522954.4s

################################################################################
                    [1m Learning iteration 2106/200000 [0m

                       Computation: 3243 steps/s (collection: 0.473s, learning 2.052s)
               Value function loss: 83372.1223
                    Surrogate loss: 0.0188
             Mean action noise std: 0.88
                       Mean reward: 9085.62
               Mean episode length: 373.76
                 Mean success rate: 73.00
                  Mean reward/step: 24.09
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 17260544
                    Iteration time: 2.53s
                        Total time: 5567.81s
                               ETA: 522940.8s

################################################################################
                    [1m Learning iteration 2107/200000 [0m

                       Computation: 3216 steps/s (collection: 0.458s, learning 2.089s)
               Value function loss: 118230.4330
                    Surrogate loss: 0.0150
             Mean action noise std: 0.88
                       Mean reward: 8997.03
               Mean episode length: 372.68
                 Mean success rate: 73.00
                  Mean reward/step: 22.91
       Mean episode length/episode: 28.15
--------------------------------------------------------------------------------
                   Total timesteps: 17268736
                    Iteration time: 2.55s
                        Total time: 5570.36s
                               ETA: 522929.2s

################################################################################
                    [1m Learning iteration 2108/200000 [0m

                       Computation: 3312 steps/s (collection: 0.425s, learning 2.048s)
               Value function loss: 105671.9970
                    Surrogate loss: 0.0160
             Mean action noise std: 0.88
                       Mean reward: 8738.70
               Mean episode length: 366.06
                 Mean success rate: 71.00
                  Mean reward/step: 22.51
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 17276928
                    Iteration time: 2.47s
                        Total time: 5572.83s
                               ETA: 522910.6s

################################################################################
                    [1m Learning iteration 2109/200000 [0m

                       Computation: 3151 steps/s (collection: 0.517s, learning 2.082s)
               Value function loss: 82551.7390
                    Surrogate loss: 0.0166
             Mean action noise std: 0.88
                       Mean reward: 8657.32
               Mean episode length: 364.93
                 Mean success rate: 71.00
                  Mean reward/step: 22.75
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 17285120
                    Iteration time: 2.60s
                        Total time: 5575.43s
                               ETA: 522903.9s

################################################################################
                    [1m Learning iteration 2110/200000 [0m

                       Computation: 3146 steps/s (collection: 0.475s, learning 2.128s)
               Value function loss: 73745.3326
                    Surrogate loss: 0.0150
             Mean action noise std: 0.88
                       Mean reward: 9137.86
               Mean episode length: 381.98
                 Mean success rate: 74.50
                  Mean reward/step: 23.27
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 17293312
                    Iteration time: 2.60s
                        Total time: 5578.03s
                               ETA: 522897.7s

################################################################################
                    [1m Learning iteration 2111/200000 [0m

                       Computation: 3161 steps/s (collection: 0.513s, learning 2.078s)
               Value function loss: 95441.7214
                    Surrogate loss: 0.0124
             Mean action noise std: 0.88
                       Mean reward: 9418.12
               Mean episode length: 393.71
                 Mean success rate: 76.50
                  Mean reward/step: 23.32
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 2.59s
                        Total time: 5580.62s
                               ETA: 522890.2s

################################################################################
                    [1m Learning iteration 2112/200000 [0m

                       Computation: 3164 steps/s (collection: 0.493s, learning 2.096s)
               Value function loss: 107576.4152
                    Surrogate loss: 0.0113
             Mean action noise std: 0.88
                       Mean reward: 9332.19
               Mean episode length: 388.79
                 Mean success rate: 76.00
                  Mean reward/step: 23.20
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 17309696
                    Iteration time: 2.59s
                        Total time: 5583.21s
                               ETA: 522882.6s

################################################################################
                    [1m Learning iteration 2113/200000 [0m

                       Computation: 3227 steps/s (collection: 0.485s, learning 2.053s)
               Value function loss: 63991.3123
                    Surrogate loss: 0.0145
             Mean action noise std: 0.88
                       Mean reward: 9304.89
               Mean episode length: 387.92
                 Mean success rate: 76.00
                  Mean reward/step: 22.94
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 17317888
                    Iteration time: 2.54s
                        Total time: 5585.75s
                               ETA: 522870.2s

################################################################################
                    [1m Learning iteration 2114/200000 [0m

                       Computation: 3175 steps/s (collection: 0.490s, learning 2.090s)
               Value function loss: 79004.5517
                    Surrogate loss: 0.0139
             Mean action noise std: 0.88
                       Mean reward: 9028.74
               Mean episode length: 380.37
                 Mean success rate: 74.50
                  Mean reward/step: 23.66
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 17326080
                    Iteration time: 2.58s
                        Total time: 5588.33s
                               ETA: 522861.7s

################################################################################
                    [1m Learning iteration 2115/200000 [0m

                       Computation: 3199 steps/s (collection: 0.472s, learning 2.088s)
               Value function loss: 74980.1826
                    Surrogate loss: 0.0143
             Mean action noise std: 0.88
                       Mean reward: 8917.33
               Mean episode length: 377.42
                 Mean success rate: 73.50
                  Mean reward/step: 24.66
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 17334272
                    Iteration time: 2.56s
                        Total time: 5590.89s
                               ETA: 522851.4s

################################################################################
                    [1m Learning iteration 2116/200000 [0m

                       Computation: 3256 steps/s (collection: 0.460s, learning 2.055s)
               Value function loss: 77341.6830
                    Surrogate loss: 0.0142
             Mean action noise std: 0.88
                       Mean reward: 8930.96
               Mean episode length: 379.08
                 Mean success rate: 74.00
                  Mean reward/step: 25.18
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 17342464
                    Iteration time: 2.52s
                        Total time: 5593.41s
                               ETA: 522836.9s

################################################################################
                    [1m Learning iteration 2117/200000 [0m

                       Computation: 3200 steps/s (collection: 0.500s, learning 2.059s)
               Value function loss: 100844.5311
                    Surrogate loss: 0.0121
             Mean action noise std: 0.88
                       Mean reward: 8870.46
               Mean episode length: 373.47
                 Mean success rate: 74.00
                  Mean reward/step: 25.28
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 17350656
                    Iteration time: 2.56s
                        Total time: 5595.97s
                               ETA: 522826.5s

################################################################################
                    [1m Learning iteration 2118/200000 [0m

                       Computation: 3193 steps/s (collection: 0.466s, learning 2.099s)
               Value function loss: 89291.6128
                    Surrogate loss: 0.0146
             Mean action noise std: 0.88
                       Mean reward: 8808.50
               Mean episode length: 372.79
                 Mean success rate: 74.50
                  Mean reward/step: 24.26
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 17358848
                    Iteration time: 2.57s
                        Total time: 5598.53s
                               ETA: 522816.7s

################################################################################
                    [1m Learning iteration 2119/200000 [0m

                       Computation: 3178 steps/s (collection: 0.449s, learning 2.128s)
               Value function loss: 90310.6081
                    Surrogate loss: 0.0140
             Mean action noise std: 0.88
                       Mean reward: 8928.21
               Mean episode length: 377.54
                 Mean success rate: 75.50
                  Mean reward/step: 23.54
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 17367040
                    Iteration time: 2.58s
                        Total time: 5601.11s
                               ETA: 522808.0s

################################################################################
                    [1m Learning iteration 2120/200000 [0m

                       Computation: 3242 steps/s (collection: 0.453s, learning 2.074s)
               Value function loss: 108313.8552
                    Surrogate loss: 0.0134
             Mean action noise std: 0.88
                       Mean reward: 9074.62
               Mean episode length: 377.55
                 Mean success rate: 76.00
                  Mean reward/step: 23.70
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 17375232
                    Iteration time: 2.53s
                        Total time: 5603.64s
                               ETA: 522794.6s

################################################################################
                    [1m Learning iteration 2121/200000 [0m

                       Computation: 3251 steps/s (collection: 0.469s, learning 2.050s)
               Value function loss: 101582.4245
                    Surrogate loss: 0.0140
             Mean action noise std: 0.88
                       Mean reward: 9222.35
               Mean episode length: 383.92
                 Mean success rate: 77.50
                  Mean reward/step: 23.19
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 17383424
                    Iteration time: 2.52s
                        Total time: 5606.16s
                               ETA: 522780.6s

################################################################################
                    [1m Learning iteration 2122/200000 [0m

                       Computation: 3220 steps/s (collection: 0.502s, learning 2.042s)
               Value function loss: 113756.4312
                    Surrogate loss: 0.0138
             Mean action noise std: 0.88
                       Mean reward: 9055.44
               Mean episode length: 382.37
                 Mean success rate: 76.00
                  Mean reward/step: 23.31
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 17391616
                    Iteration time: 2.54s
                        Total time: 5608.70s
                               ETA: 522768.8s

################################################################################
                    [1m Learning iteration 2123/200000 [0m

                       Computation: 3295 steps/s (collection: 0.469s, learning 2.017s)
               Value function loss: 105177.8846
                    Surrogate loss: 0.0145
             Mean action noise std: 0.88
                       Mean reward: 9329.39
               Mean episode length: 391.12
                 Mean success rate: 78.50
                  Mean reward/step: 23.05
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 2.49s
                        Total time: 5611.18s
                               ETA: 522751.6s

################################################################################
                    [1m Learning iteration 2124/200000 [0m

                       Computation: 3287 steps/s (collection: 0.454s, learning 2.038s)
               Value function loss: 100521.6891
                    Surrogate loss: 0.0131
             Mean action noise std: 0.88
                       Mean reward: 9536.31
               Mean episode length: 398.06
                 Mean success rate: 79.50
                  Mean reward/step: 21.74
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 17408000
                    Iteration time: 2.49s
                        Total time: 5613.68s
                               ETA: 522735.0s

################################################################################
                    [1m Learning iteration 2125/200000 [0m

                       Computation: 3275 steps/s (collection: 0.469s, learning 2.032s)
               Value function loss: 87333.2982
                    Surrogate loss: 0.0149
             Mean action noise std: 0.88
                       Mean reward: 9722.37
               Mean episode length: 405.71
                 Mean success rate: 80.50
                  Mean reward/step: 22.38
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 17416192
                    Iteration time: 2.50s
                        Total time: 5616.18s
                               ETA: 522719.2s

################################################################################
                    [1m Learning iteration 2126/200000 [0m

                       Computation: 3281 steps/s (collection: 0.462s, learning 2.034s)
               Value function loss: 86022.1739
                    Surrogate loss: 0.0124
             Mean action noise std: 0.88
                       Mean reward: 9441.56
               Mean episode length: 398.93
                 Mean success rate: 78.00
                  Mean reward/step: 23.62
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 17424384
                    Iteration time: 2.50s
                        Total time: 5618.67s
                               ETA: 522703.0s

################################################################################
                    [1m Learning iteration 2127/200000 [0m

                       Computation: 3294 steps/s (collection: 0.476s, learning 2.010s)
               Value function loss: 70424.3713
                    Surrogate loss: 0.0129
             Mean action noise std: 0.88
                       Mean reward: 9328.59
               Mean episode length: 398.52
                 Mean success rate: 77.00
                  Mean reward/step: 24.50
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 17432576
                    Iteration time: 2.49s
                        Total time: 5621.16s
                               ETA: 522686.0s

################################################################################
                    [1m Learning iteration 2128/200000 [0m

                       Computation: 3256 steps/s (collection: 0.451s, learning 2.065s)
               Value function loss: 103983.0106
                    Surrogate loss: 0.0157
             Mean action noise std: 0.88
                       Mean reward: 9270.05
               Mean episode length: 393.80
                 Mean success rate: 75.50
                  Mean reward/step: 24.10
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 17440768
                    Iteration time: 2.52s
                        Total time: 5623.68s
                               ETA: 522671.6s

################################################################################
                    [1m Learning iteration 2129/200000 [0m

                       Computation: 3291 steps/s (collection: 0.464s, learning 2.025s)
               Value function loss: 70069.3186
                    Surrogate loss: 0.0128
             Mean action noise std: 0.88
                       Mean reward: 9155.99
               Mean episode length: 390.14
                 Mean success rate: 74.50
                  Mean reward/step: 24.22
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 17448960
                    Iteration time: 2.49s
                        Total time: 5626.16s
                               ETA: 522654.8s

################################################################################
                    [1m Learning iteration 2130/200000 [0m

                       Computation: 3262 steps/s (collection: 0.465s, learning 2.046s)
               Value function loss: 102070.0083
                    Surrogate loss: 0.0154
             Mean action noise std: 0.88
                       Mean reward: 8958.98
               Mean episode length: 383.75
                 Mean success rate: 72.50
                  Mean reward/step: 24.39
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 17457152
                    Iteration time: 2.51s
                        Total time: 5628.67s
                               ETA: 522640.0s

################################################################################
                    [1m Learning iteration 2131/200000 [0m

                       Computation: 3254 steps/s (collection: 0.468s, learning 2.049s)
               Value function loss: 81773.6892
                    Surrogate loss: 0.0121
             Mean action noise std: 0.88
                       Mean reward: 8883.66
               Mean episode length: 381.75
                 Mean success rate: 72.00
                  Mean reward/step: 24.60
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 17465344
                    Iteration time: 2.52s
                        Total time: 5631.19s
                               ETA: 522625.8s

################################################################################
                    [1m Learning iteration 2132/200000 [0m

                       Computation: 3287 steps/s (collection: 0.436s, learning 2.056s)
               Value function loss: 61962.4730
                    Surrogate loss: 0.0187
             Mean action noise std: 0.88
                       Mean reward: 8873.06
               Mean episode length: 377.44
                 Mean success rate: 71.50
                  Mean reward/step: 25.02
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 17473536
                    Iteration time: 2.49s
                        Total time: 5633.68s
                               ETA: 522609.3s

################################################################################
                    [1m Learning iteration 2133/200000 [0m

                       Computation: 3235 steps/s (collection: 0.453s, learning 2.079s)
               Value function loss: 124201.5145
                    Surrogate loss: 0.0203
             Mean action noise std: 0.88
                       Mean reward: 9207.97
               Mean episode length: 389.31
                 Mean success rate: 74.00
                  Mean reward/step: 24.39
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 17481728
                    Iteration time: 2.53s
                        Total time: 5636.22s
                               ETA: 522596.5s

################################################################################
                    [1m Learning iteration 2134/200000 [0m

                       Computation: 3144 steps/s (collection: 0.502s, learning 2.103s)
               Value function loss: 97362.5877
                    Surrogate loss: 0.0142
             Mean action noise std: 0.88
                       Mean reward: 9326.53
               Mean episode length: 394.24
                 Mean success rate: 74.50
                  Mean reward/step: 23.44
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 17489920
                    Iteration time: 2.61s
                        Total time: 5638.82s
                               ETA: 522590.6s

################################################################################
                    [1m Learning iteration 2135/200000 [0m

                       Computation: 3197 steps/s (collection: 0.474s, learning 2.088s)
               Value function loss: 88973.1186
                    Surrogate loss: 0.0140
             Mean action noise std: 0.88
                       Mean reward: 9297.78
               Mean episode length: 391.47
                 Mean success rate: 74.50
                  Mean reward/step: 23.11
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 2.56s
                        Total time: 5641.38s
                               ETA: 522580.6s

################################################################################
                    [1m Learning iteration 2136/200000 [0m

                       Computation: 3294 steps/s (collection: 0.459s, learning 2.027s)
               Value function loss: 78464.3221
                    Surrogate loss: 0.0116
             Mean action noise std: 0.88
                       Mean reward: 9397.69
               Mean episode length: 390.67
                 Mean success rate: 76.00
                  Mean reward/step: 23.36
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 17506304
                    Iteration time: 2.49s
                        Total time: 5643.87s
                               ETA: 522563.6s

################################################################################
                    [1m Learning iteration 2137/200000 [0m

                       Computation: 3216 steps/s (collection: 0.478s, learning 2.069s)
               Value function loss: 101330.7217
                    Surrogate loss: 0.0137
             Mean action noise std: 0.88
                       Mean reward: 9278.49
               Mean episode length: 387.25
                 Mean success rate: 75.00
                  Mean reward/step: 23.51
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 17514496
                    Iteration time: 2.55s
                        Total time: 5646.42s
                               ETA: 522552.2s

################################################################################
                    [1m Learning iteration 2138/200000 [0m

                       Computation: 3159 steps/s (collection: 0.490s, learning 2.103s)
               Value function loss: 113683.9893
                    Surrogate loss: 0.0119
             Mean action noise std: 0.88
                       Mean reward: 9624.59
               Mean episode length: 398.26
                 Mean success rate: 77.50
                  Mean reward/step: 23.33
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 17522688
                    Iteration time: 2.59s
                        Total time: 5649.01s
                               ETA: 522545.2s

################################################################################
                    [1m Learning iteration 2139/200000 [0m

                       Computation: 3129 steps/s (collection: 0.489s, learning 2.128s)
               Value function loss: 131723.6787
                    Surrogate loss: 0.0141
             Mean action noise std: 0.88
                       Mean reward: 9620.49
               Mean episode length: 398.59
                 Mean success rate: 78.50
                  Mean reward/step: 23.42
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 17530880
                    Iteration time: 2.62s
                        Total time: 5651.63s
                               ETA: 522540.3s

################################################################################
                    [1m Learning iteration 2140/200000 [0m

                       Computation: 3252 steps/s (collection: 0.479s, learning 2.040s)
               Value function loss: 77411.1644
                    Surrogate loss: 0.0151
             Mean action noise std: 0.88
                       Mean reward: 9695.57
               Mean episode length: 398.34
                 Mean success rate: 79.00
                  Mean reward/step: 23.10
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 17539072
                    Iteration time: 2.52s
                        Total time: 5654.14s
                               ETA: 522526.4s

################################################################################
                    [1m Learning iteration 2141/200000 [0m

                       Computation: 3244 steps/s (collection: 0.457s, learning 2.068s)
               Value function loss: 83211.0810
                    Surrogate loss: 0.0218
             Mean action noise std: 0.88
                       Mean reward: 9774.43
               Mean episode length: 401.29
                 Mean success rate: 80.00
                  Mean reward/step: 23.19
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 17547264
                    Iteration time: 2.52s
                        Total time: 5656.67s
                               ETA: 522513.0s

################################################################################
                    [1m Learning iteration 2142/200000 [0m

                       Computation: 3173 steps/s (collection: 0.478s, learning 2.103s)
               Value function loss: 95171.8205
                    Surrogate loss: 0.0195
             Mean action noise std: 0.88
                       Mean reward: 9382.37
               Mean episode length: 389.45
                 Mean success rate: 77.50
                  Mean reward/step: 23.91
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 17555456
                    Iteration time: 2.58s
                        Total time: 5659.25s
                               ETA: 522504.9s

################################################################################
                    [1m Learning iteration 2143/200000 [0m

                       Computation: 3110 steps/s (collection: 0.516s, learning 2.117s)
               Value function loss: 112694.1256
                    Surrogate loss: 0.0172
             Mean action noise std: 0.88
                       Mean reward: 9060.03
               Mean episode length: 381.60
                 Mean success rate: 76.00
                  Mean reward/step: 24.22
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 17563648
                    Iteration time: 2.63s
                        Total time: 5661.88s
                               ETA: 522501.6s

################################################################################
                    [1m Learning iteration 2144/200000 [0m

                       Computation: 3255 steps/s (collection: 0.457s, learning 2.059s)
               Value function loss: 71943.3905
                    Surrogate loss: 0.0157
             Mean action noise std: 0.88
                       Mean reward: 9179.59
               Mean episode length: 386.42
                 Mean success rate: 76.00
                  Mean reward/step: 23.82
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 17571840
                    Iteration time: 2.52s
                        Total time: 5664.40s
                               ETA: 522487.5s

################################################################################
                    [1m Learning iteration 2145/200000 [0m

                       Computation: 3229 steps/s (collection: 0.462s, learning 2.075s)
               Value function loss: 69033.8396
                    Surrogate loss: 0.0117
             Mean action noise std: 0.88
                       Mean reward: 9236.98
               Mean episode length: 389.92
                 Mean success rate: 76.50
                  Mean reward/step: 24.72
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 17580032
                    Iteration time: 2.54s
                        Total time: 5666.94s
                               ETA: 522475.3s

################################################################################
                    [1m Learning iteration 2146/200000 [0m

                       Computation: 3197 steps/s (collection: 0.509s, learning 2.053s)
               Value function loss: 87951.4937
                    Surrogate loss: 0.0113
             Mean action noise std: 0.88
                       Mean reward: 9219.64
               Mean episode length: 387.82
                 Mean success rate: 76.50
                  Mean reward/step: 24.99
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 17588224
                    Iteration time: 2.56s
                        Total time: 5669.50s
                               ETA: 522465.4s

################################################################################
                    [1m Learning iteration 2147/200000 [0m

                       Computation: 3331 steps/s (collection: 0.438s, learning 2.021s)
               Value function loss: 64283.5288
                    Surrogate loss: 0.0151
             Mean action noise std: 0.88
                       Mean reward: 8939.92
               Mean episode length: 379.70
                 Mean success rate: 75.00
                  Mean reward/step: 25.20
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 2.46s
                        Total time: 5671.96s
                               ETA: 522446.0s

################################################################################
                    [1m Learning iteration 2148/200000 [0m

                       Computation: 3292 steps/s (collection: 0.452s, learning 2.036s)
               Value function loss: 72603.3766
                    Surrogate loss: 0.0118
             Mean action noise std: 0.88
                       Mean reward: 8706.32
               Mean episode length: 370.56
                 Mean success rate: 73.00
                  Mean reward/step: 25.57
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 17604608
                    Iteration time: 2.49s
                        Total time: 5674.45s
                               ETA: 522429.4s

################################################################################
                    [1m Learning iteration 2149/200000 [0m

                       Computation: 3264 steps/s (collection: 0.470s, learning 2.040s)
               Value function loss: 104665.8094
                    Surrogate loss: 0.0141
             Mean action noise std: 0.88
                       Mean reward: 8992.20
               Mean episode length: 379.84
                 Mean success rate: 75.00
                  Mean reward/step: 25.22
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 17612800
                    Iteration time: 2.51s
                        Total time: 5676.96s
                               ETA: 522414.7s

################################################################################
                    [1m Learning iteration 2150/200000 [0m

                       Computation: 3306 steps/s (collection: 0.431s, learning 2.046s)
               Value function loss: 99474.3760
                    Surrogate loss: 0.0146
             Mean action noise std: 0.88
                       Mean reward: 9109.05
               Mean episode length: 382.49
                 Mean success rate: 76.00
                  Mean reward/step: 24.45
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 17620992
                    Iteration time: 2.48s
                        Total time: 5679.43s
                               ETA: 522397.0s

################################################################################
                    [1m Learning iteration 2151/200000 [0m

                       Computation: 3212 steps/s (collection: 0.469s, learning 2.081s)
               Value function loss: 88787.4059
                    Surrogate loss: 0.0139
             Mean action noise std: 0.88
                       Mean reward: 9076.13
               Mean episode length: 380.32
                 Mean success rate: 75.50
                  Mean reward/step: 24.23
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 17629184
                    Iteration time: 2.55s
                        Total time: 5681.98s
                               ETA: 522386.1s

################################################################################
                    [1m Learning iteration 2152/200000 [0m

                       Computation: 3305 steps/s (collection: 0.460s, learning 2.019s)
               Value function loss: 84770.6539
                    Surrogate loss: 0.0168
             Mean action noise std: 0.88
                       Mean reward: 9035.72
               Mean episode length: 381.06
                 Mean success rate: 74.50
                  Mean reward/step: 24.06
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 17637376
                    Iteration time: 2.48s
                        Total time: 5684.46s
                               ETA: 522368.6s

################################################################################
                    [1m Learning iteration 2153/200000 [0m

                       Computation: 3226 steps/s (collection: 0.494s, learning 2.045s)
               Value function loss: 106933.7672
                    Surrogate loss: 0.0149
             Mean action noise std: 0.88
                       Mean reward: 9397.00
               Mean episode length: 389.64
                 Mean success rate: 76.50
                  Mean reward/step: 23.69
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 17645568
                    Iteration time: 2.54s
                        Total time: 5687.00s
                               ETA: 522356.6s

################################################################################
                    [1m Learning iteration 2154/200000 [0m

                       Computation: 3216 steps/s (collection: 0.479s, learning 2.068s)
               Value function loss: 110356.0357
                    Surrogate loss: 0.0126
             Mean action noise std: 0.88
                       Mean reward: 9320.08
               Mean episode length: 390.50
                 Mean success rate: 76.00
                  Mean reward/step: 23.54
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 17653760
                    Iteration time: 2.55s
                        Total time: 5689.55s
                               ETA: 522345.4s

################################################################################
                    [1m Learning iteration 2155/200000 [0m

                       Computation: 3200 steps/s (collection: 0.469s, learning 2.091s)
               Value function loss: 159536.2264
                    Surrogate loss: 0.0152
             Mean action noise std: 0.88
                       Mean reward: 9762.32
               Mean episode length: 402.51
                 Mean success rate: 78.50
                  Mean reward/step: 22.58
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 17661952
                    Iteration time: 2.56s
                        Total time: 5692.11s
                               ETA: 522335.4s

################################################################################
                    [1m Learning iteration 2156/200000 [0m

                       Computation: 3275 steps/s (collection: 0.468s, learning 2.033s)
               Value function loss: 75985.0779
                    Surrogate loss: 0.0175
             Mean action noise std: 0.88
                       Mean reward: 9600.96
               Mean episode length: 396.84
                 Mean success rate: 77.50
                  Mean reward/step: 22.10
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 17670144
                    Iteration time: 2.50s
                        Total time: 5694.61s
                               ETA: 522320.0s

################################################################################
                    [1m Learning iteration 2157/200000 [0m

                       Computation: 3214 steps/s (collection: 0.463s, learning 2.085s)
               Value function loss: 94596.9621
                    Surrogate loss: 0.0129
             Mean action noise std: 0.88
                       Mean reward: 9649.21
               Mean episode length: 398.62
                 Mean success rate: 77.00
                  Mean reward/step: 22.86
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 17678336
                    Iteration time: 2.55s
                        Total time: 5697.16s
                               ETA: 522309.0s

################################################################################
                    [1m Learning iteration 2158/200000 [0m

                       Computation: 3183 steps/s (collection: 0.483s, learning 2.090s)
               Value function loss: 86998.1410
                    Surrogate loss: 0.0154
             Mean action noise std: 0.88
                       Mean reward: 9032.49
               Mean episode length: 380.37
                 Mean success rate: 73.50
                  Mean reward/step: 23.70
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 17686528
                    Iteration time: 2.57s
                        Total time: 5699.73s
                               ETA: 522300.3s

################################################################################
                    [1m Learning iteration 2159/200000 [0m

                       Computation: 3189 steps/s (collection: 0.497s, learning 2.071s)
               Value function loss: 110336.8458
                    Surrogate loss: 0.0133
             Mean action noise std: 0.88
                       Mean reward: 9164.95
               Mean episode length: 385.10
                 Mean success rate: 74.00
                  Mean reward/step: 23.85
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 2.57s
                        Total time: 5702.30s
                               ETA: 522291.0s

################################################################################
                    [1m Learning iteration 2160/200000 [0m

                       Computation: 3198 steps/s (collection: 0.528s, learning 2.033s)
               Value function loss: 55495.2240
                    Surrogate loss: 0.0137
             Mean action noise std: 0.88
                       Mean reward: 9073.07
               Mean episode length: 381.00
                 Mean success rate: 73.50
                  Mean reward/step: 24.01
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 17702912
                    Iteration time: 2.56s
                        Total time: 5704.86s
                               ETA: 522281.1s

################################################################################
                    [1m Learning iteration 2161/200000 [0m

                       Computation: 3225 steps/s (collection: 0.479s, learning 2.060s)
               Value function loss: 83902.1167
                    Surrogate loss: 0.0126
             Mean action noise std: 0.88
                       Mean reward: 8759.04
               Mean episode length: 369.54
                 Mean success rate: 71.50
                  Mean reward/step: 24.66
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 17711104
                    Iteration time: 2.54s
                        Total time: 5707.40s
                               ETA: 522269.3s

################################################################################
                    [1m Learning iteration 2162/200000 [0m

                       Computation: 3249 steps/s (collection: 0.464s, learning 2.057s)
               Value function loss: 78140.5035
                    Surrogate loss: 0.0125
             Mean action noise std: 0.88
                       Mean reward: 8555.39
               Mean episode length: 360.61
                 Mean success rate: 69.50
                  Mean reward/step: 24.13
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 17719296
                    Iteration time: 2.52s
                        Total time: 5709.92s
                               ETA: 522255.8s

################################################################################
                    [1m Learning iteration 2163/200000 [0m

                       Computation: 3235 steps/s (collection: 0.478s, learning 2.054s)
               Value function loss: 62263.7550
                    Surrogate loss: 0.0131
             Mean action noise std: 0.88
                       Mean reward: 7638.28
               Mean episode length: 334.88
                 Mean success rate: 63.50
                  Mean reward/step: 24.20
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 17727488
                    Iteration time: 2.53s
                        Total time: 5712.45s
                               ETA: 522243.3s

################################################################################
                    [1m Learning iteration 2164/200000 [0m

                       Computation: 3275 steps/s (collection: 0.472s, learning 2.028s)
               Value function loss: 91181.8344
                    Surrogate loss: 0.0158
             Mean action noise std: 0.88
                       Mean reward: 7969.04
               Mean episode length: 343.74
                 Mean success rate: 65.50
                  Mean reward/step: 25.25
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 17735680
                    Iteration time: 2.50s
                        Total time: 5714.95s
                               ETA: 522228.0s

################################################################################
                    [1m Learning iteration 2165/200000 [0m

                       Computation: 3197 steps/s (collection: 0.505s, learning 2.057s)
               Value function loss: 88769.0372
                    Surrogate loss: 0.0124
             Mean action noise std: 0.88
                       Mean reward: 8271.36
               Mean episode length: 353.08
                 Mean success rate: 67.00
                  Mean reward/step: 25.23
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 17743872
                    Iteration time: 2.56s
                        Total time: 5717.52s
                               ETA: 522218.3s

################################################################################
                    [1m Learning iteration 2166/200000 [0m

                       Computation: 3245 steps/s (collection: 0.450s, learning 2.075s)
               Value function loss: 66695.5310
                    Surrogate loss: 0.0156
             Mean action noise std: 0.88
                       Mean reward: 8044.73
               Mean episode length: 344.80
                 Mean success rate: 65.50
                  Mean reward/step: 25.23
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 17752064
                    Iteration time: 2.52s
                        Total time: 5720.04s
                               ETA: 522205.1s

################################################################################
                    [1m Learning iteration 2167/200000 [0m

                       Computation: 3201 steps/s (collection: 0.458s, learning 2.101s)
               Value function loss: 84137.4764
                    Surrogate loss: 0.0130
             Mean action noise std: 0.88
                       Mean reward: 8232.69
               Mean episode length: 350.75
                 Mean success rate: 66.00
                  Mean reward/step: 25.58
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 17760256
                    Iteration time: 2.56s
                        Total time: 5722.60s
                               ETA: 522195.1s

################################################################################
                    [1m Learning iteration 2168/200000 [0m

                       Computation: 3245 steps/s (collection: 0.480s, learning 2.044s)
               Value function loss: 122252.7725
                    Surrogate loss: 0.0110
             Mean action noise std: 0.88
                       Mean reward: 8507.11
               Mean episode length: 359.00
                 Mean success rate: 68.00
                  Mean reward/step: 25.57
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 17768448
                    Iteration time: 2.52s
                        Total time: 5725.12s
                               ETA: 522181.9s

################################################################################
                    [1m Learning iteration 2169/200000 [0m

                       Computation: 3235 steps/s (collection: 0.482s, learning 2.050s)
               Value function loss: 121812.2166
                    Surrogate loss: 0.0146
             Mean action noise std: 0.88
                       Mean reward: 8536.43
               Mean episode length: 360.27
                 Mean success rate: 68.00
                  Mean reward/step: 25.19
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 17776640
                    Iteration time: 2.53s
                        Total time: 5727.66s
                               ETA: 522169.5s

################################################################################
                    [1m Learning iteration 2170/200000 [0m

                       Computation: 3222 steps/s (collection: 0.459s, learning 2.083s)
               Value function loss: 100161.9674
                    Surrogate loss: 0.0162
             Mean action noise std: 0.88
                       Mean reward: 8990.19
               Mean episode length: 374.74
                 Mean success rate: 70.50
                  Mean reward/step: 24.34
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 17784832
                    Iteration time: 2.54s
                        Total time: 5730.20s
                               ETA: 522158.0s

################################################################################
                    [1m Learning iteration 2171/200000 [0m

                       Computation: 3236 steps/s (collection: 0.474s, learning 2.057s)
               Value function loss: 110610.3037
                    Surrogate loss: 0.0115
             Mean action noise std: 0.88
                       Mean reward: 9234.89
               Mean episode length: 381.58
                 Mean success rate: 72.00
                  Mean reward/step: 22.96
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 2.53s
                        Total time: 5732.73s
                               ETA: 522145.5s

################################################################################
                    [1m Learning iteration 2172/200000 [0m

                       Computation: 3271 steps/s (collection: 0.490s, learning 2.014s)
               Value function loss: 122606.1955
                    Surrogate loss: 0.0104
             Mean action noise std: 0.88
                       Mean reward: 9701.87
               Mean episode length: 391.87
                 Mean success rate: 75.50
                  Mean reward/step: 22.62
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 17801216
                    Iteration time: 2.50s
                        Total time: 5735.23s
                               ETA: 522130.5s

################################################################################
                    [1m Learning iteration 2173/200000 [0m

                       Computation: 3238 steps/s (collection: 0.478s, learning 2.052s)
               Value function loss: 95313.1844
                    Surrogate loss: 0.0130
             Mean action noise std: 0.88
                       Mean reward: 9702.97
               Mean episode length: 391.51
                 Mean success rate: 75.00
                  Mean reward/step: 22.75
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 17809408
                    Iteration time: 2.53s
                        Total time: 5737.76s
                               ETA: 522117.9s

################################################################################
                    [1m Learning iteration 2174/200000 [0m

                       Computation: 3196 steps/s (collection: 0.509s, learning 2.054s)
               Value function loss: 95974.9432
                    Surrogate loss: 0.0131
             Mean action noise std: 0.88
                       Mean reward: 9805.64
               Mean episode length: 395.89
                 Mean success rate: 75.50
                  Mean reward/step: 23.59
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 17817600
                    Iteration time: 2.56s
                        Total time: 5740.32s
                               ETA: 522108.2s

################################################################################
                    [1m Learning iteration 2175/200000 [0m

                       Computation: 3228 steps/s (collection: 0.486s, learning 2.052s)
               Value function loss: 69301.1941
                    Surrogate loss: 0.0133
             Mean action noise std: 0.88
                       Mean reward: 10030.44
               Mean episode length: 405.12
                 Mean success rate: 77.50
                  Mean reward/step: 22.71
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 17825792
                    Iteration time: 2.54s
                        Total time: 5742.86s
                               ETA: 522096.3s

################################################################################
                    [1m Learning iteration 2176/200000 [0m

                       Computation: 3278 steps/s (collection: 0.455s, learning 2.043s)
               Value function loss: 49889.0563
                    Surrogate loss: 0.0125
             Mean action noise std: 0.88
                       Mean reward: 9632.99
               Mean episode length: 391.79
                 Mean success rate: 74.50
                  Mean reward/step: 23.34
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 17833984
                    Iteration time: 2.50s
                        Total time: 5745.36s
                               ETA: 522080.9s

################################################################################
                    [1m Learning iteration 2177/200000 [0m

                       Computation: 3227 steps/s (collection: 0.500s, learning 2.038s)
               Value function loss: 113833.9940
                    Surrogate loss: 0.0118
             Mean action noise std: 0.88
                       Mean reward: 9588.90
               Mean episode length: 391.51
                 Mean success rate: 74.00
                  Mean reward/step: 23.48
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 17842176
                    Iteration time: 2.54s
                        Total time: 5747.90s
                               ETA: 522069.1s

################################################################################
                    [1m Learning iteration 2178/200000 [0m

                       Computation: 3235 steps/s (collection: 0.504s, learning 2.028s)
               Value function loss: 93720.6985
                    Surrogate loss: 0.0104
             Mean action noise std: 0.88
                       Mean reward: 9351.95
               Mean episode length: 382.49
                 Mean success rate: 72.00
                  Mean reward/step: 23.36
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 17850368
                    Iteration time: 2.53s
                        Total time: 5750.43s
                               ETA: 522056.7s

################################################################################
                    [1m Learning iteration 2179/200000 [0m

                       Computation: 3254 steps/s (collection: 0.488s, learning 2.030s)
               Value function loss: 87744.1162
                    Surrogate loss: 0.0113
             Mean action noise std: 0.88
                       Mean reward: 9174.42
               Mean episode length: 377.19
                 Mean success rate: 70.50
                  Mean reward/step: 22.92
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 17858560
                    Iteration time: 2.52s
                        Total time: 5752.95s
                               ETA: 522043.1s

################################################################################
                    [1m Learning iteration 2180/200000 [0m

                       Computation: 3346 steps/s (collection: 0.447s, learning 2.001s)
               Value function loss: 79537.4627
                    Surrogate loss: 0.0125
             Mean action noise std: 0.88
                       Mean reward: 8952.59
               Mean episode length: 371.62
                 Mean success rate: 69.50
                  Mean reward/step: 23.16
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 17866752
                    Iteration time: 2.45s
                        Total time: 5755.40s
                               ETA: 522023.1s

################################################################################
                    [1m Learning iteration 2181/200000 [0m

                       Computation: 3244 steps/s (collection: 0.492s, learning 2.033s)
               Value function loss: 92090.0896
                    Surrogate loss: 0.0115
             Mean action noise std: 0.88
                       Mean reward: 9328.98
               Mean episode length: 383.63
                 Mean success rate: 71.50
                  Mean reward/step: 23.07
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 17874944
                    Iteration time: 2.52s
                        Total time: 5757.92s
                               ETA: 522010.1s

################################################################################
                    [1m Learning iteration 2182/200000 [0m

                       Computation: 3283 steps/s (collection: 0.473s, learning 2.022s)
               Value function loss: 91775.3648
                    Surrogate loss: 0.0136
             Mean action noise std: 0.88
                       Mean reward: 9100.19
               Mean episode length: 379.26
                 Mean success rate: 70.00
                  Mean reward/step: 22.32
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 17883136
                    Iteration time: 2.49s
                        Total time: 5760.41s
                               ETA: 521994.4s

################################################################################
                    [1m Learning iteration 2183/200000 [0m

                       Computation: 3289 steps/s (collection: 0.469s, learning 2.021s)
               Value function loss: 79984.9595
                    Surrogate loss: 0.0107
             Mean action noise std: 0.88
                       Mean reward: 8048.97
               Mean episode length: 347.60
                 Mean success rate: 63.00
                  Mean reward/step: 22.02
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 2.49s
                        Total time: 5762.91s
                               ETA: 521978.3s

################################################################################
                    [1m Learning iteration 2184/200000 [0m

                       Computation: 3224 steps/s (collection: 0.504s, learning 2.037s)
               Value function loss: 96295.0478
                    Surrogate loss: 0.0118
             Mean action noise std: 0.88
                       Mean reward: 7741.40
               Mean episode length: 340.38
                 Mean success rate: 60.50
                  Mean reward/step: 21.42
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 17899520
                    Iteration time: 2.54s
                        Total time: 5765.45s
                               ETA: 521966.8s

################################################################################
                    [1m Learning iteration 2185/200000 [0m

                       Computation: 3228 steps/s (collection: 0.476s, learning 2.061s)
               Value function loss: 90443.8172
                    Surrogate loss: 0.0124
             Mean action noise std: 0.88
                       Mean reward: 7693.05
               Mean episode length: 339.43
                 Mean success rate: 61.00
                  Mean reward/step: 22.15
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 17907712
                    Iteration time: 2.54s
                        Total time: 5767.98s
                               ETA: 521955.0s

################################################################################
                    [1m Learning iteration 2186/200000 [0m

                       Computation: 3351 steps/s (collection: 0.424s, learning 2.021s)
               Value function loss: 122442.6576
                    Surrogate loss: 0.0138
             Mean action noise std: 0.88
                       Mean reward: 7861.12
               Mean episode length: 346.54
                 Mean success rate: 62.50
                  Mean reward/step: 22.30
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 17915904
                    Iteration time: 2.44s
                        Total time: 5770.43s
                               ETA: 521934.8s

################################################################################
                    [1m Learning iteration 2187/200000 [0m

                       Computation: 3273 steps/s (collection: 0.478s, learning 2.024s)
               Value function loss: 86275.3055
                    Surrogate loss: 0.0164
             Mean action noise std: 0.88
                       Mean reward: 7928.93
               Mean episode length: 352.03
                 Mean success rate: 64.00
                  Mean reward/step: 21.92
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 17924096
                    Iteration time: 2.50s
                        Total time: 5772.93s
                               ETA: 521919.9s

################################################################################
                    [1m Learning iteration 2188/200000 [0m

                       Computation: 3266 steps/s (collection: 0.492s, learning 2.016s)
               Value function loss: 90243.6221
                    Surrogate loss: 0.0123
             Mean action noise std: 0.88
                       Mean reward: 7804.66
               Mean episode length: 347.12
                 Mean success rate: 62.50
                  Mean reward/step: 22.31
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 17932288
                    Iteration time: 2.51s
                        Total time: 5775.44s
                               ETA: 521905.4s

################################################################################
                    [1m Learning iteration 2189/200000 [0m

                       Computation: 3149 steps/s (collection: 0.541s, learning 2.060s)
               Value function loss: 73563.1605
                    Surrogate loss: 0.0116
             Mean action noise std: 0.88
                       Mean reward: 7717.91
               Mean episode length: 343.41
                 Mean success rate: 63.00
                  Mean reward/step: 23.04
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 17940480
                    Iteration time: 2.60s
                        Total time: 5778.04s
                               ETA: 521899.4s

################################################################################
                    [1m Learning iteration 2190/200000 [0m

                       Computation: 3211 steps/s (collection: 0.493s, learning 2.057s)
               Value function loss: 108017.1146
                    Surrogate loss: 0.0129
             Mean action noise std: 0.88
                       Mean reward: 8025.62
               Mean episode length: 350.62
                 Mean success rate: 65.00
                  Mean reward/step: 24.09
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 17948672
                    Iteration time: 2.55s
                        Total time: 5780.59s
                               ETA: 521888.9s

################################################################################
                    [1m Learning iteration 2191/200000 [0m

                       Computation: 3307 steps/s (collection: 0.460s, learning 2.017s)
               Value function loss: 38624.4347
                    Surrogate loss: 0.0121
             Mean action noise std: 0.88
                       Mean reward: 8168.06
               Mean episode length: 355.76
                 Mean success rate: 66.00
                  Mean reward/step: 24.07
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 17956864
                    Iteration time: 2.48s
                        Total time: 5783.07s
                               ETA: 521871.7s

################################################################################
                    [1m Learning iteration 2192/200000 [0m

                       Computation: 3286 steps/s (collection: 0.472s, learning 2.021s)
               Value function loss: 113032.0066
                    Surrogate loss: 0.0119
             Mean action noise std: 0.88
                       Mean reward: 8614.64
               Mean episode length: 370.37
                 Mean success rate: 69.50
                  Mean reward/step: 25.19
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 17965056
                    Iteration time: 2.49s
                        Total time: 5785.56s
                               ETA: 521855.9s

################################################################################
                    [1m Learning iteration 2193/200000 [0m

                       Computation: 3297 steps/s (collection: 0.471s, learning 2.013s)
               Value function loss: 57833.3161
                    Surrogate loss: 0.0139
             Mean action noise std: 0.88
                       Mean reward: 8601.65
               Mean episode length: 370.58
                 Mean success rate: 70.00
                  Mean reward/step: 24.45
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 17973248
                    Iteration time: 2.48s
                        Total time: 5788.04s
                               ETA: 521839.4s

################################################################################
                    [1m Learning iteration 2194/200000 [0m

                       Computation: 3310 steps/s (collection: 0.458s, learning 2.016s)
               Value function loss: 82146.3098
                    Surrogate loss: 0.0162
             Mean action noise std: 0.88
                       Mean reward: 8341.61
               Mean episode length: 359.25
                 Mean success rate: 67.50
                  Mean reward/step: 24.24
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 17981440
                    Iteration time: 2.47s
                        Total time: 5790.52s
                               ETA: 521822.0s

################################################################################
                    [1m Learning iteration 2195/200000 [0m

                       Computation: 3360 steps/s (collection: 0.424s, learning 2.014s)
               Value function loss: 105761.2594
                    Surrogate loss: 0.0137
             Mean action noise std: 0.88
                       Mean reward: 8391.36
               Mean episode length: 363.01
                 Mean success rate: 68.00
                  Mean reward/step: 23.91
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 2.44s
                        Total time: 5792.96s
                               ETA: 521801.3s

################################################################################
                    [1m Learning iteration 2196/200000 [0m

                       Computation: 3279 steps/s (collection: 0.483s, learning 2.015s)
               Value function loss: 81655.4686
                    Surrogate loss: 0.0136
             Mean action noise std: 0.88
                       Mean reward: 8063.87
               Mean episode length: 352.07
                 Mean success rate: 66.00
                  Mean reward/step: 24.74
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 17997824
                    Iteration time: 2.50s
                        Total time: 5795.45s
                               ETA: 521786.0s

################################################################################
                    [1m Learning iteration 2197/200000 [0m

                       Computation: 3294 steps/s (collection: 0.474s, learning 2.013s)
               Value function loss: 102347.4649
                    Surrogate loss: 0.0113
             Mean action noise std: 0.88
                       Mean reward: 8342.49
               Mean episode length: 360.30
                 Mean success rate: 68.00
                  Mean reward/step: 24.92
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 18006016
                    Iteration time: 2.49s
                        Total time: 5797.94s
                               ETA: 521769.8s

################################################################################
                    [1m Learning iteration 2198/200000 [0m

                       Computation: 3305 steps/s (collection: 0.458s, learning 2.020s)
               Value function loss: 122326.2142
                    Surrogate loss: 0.0140
             Mean action noise std: 0.88
                       Mean reward: 8427.87
               Mean episode length: 363.92
                 Mean success rate: 69.00
                  Mean reward/step: 24.20
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 18014208
                    Iteration time: 2.48s
                        Total time: 5800.42s
                               ETA: 521752.8s

################################################################################
                    [1m Learning iteration 2199/200000 [0m

                       Computation: 3263 steps/s (collection: 0.471s, learning 2.039s)
               Value function loss: 79592.6501
                    Surrogate loss: 0.0155
             Mean action noise std: 0.88
                       Mean reward: 8270.35
               Mean episode length: 358.93
                 Mean success rate: 67.50
                  Mean reward/step: 23.42
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 18022400
                    Iteration time: 2.51s
                        Total time: 5802.93s
                               ETA: 521738.7s

################################################################################
                    [1m Learning iteration 2200/200000 [0m

                       Computation: 3231 steps/s (collection: 0.501s, learning 2.035s)
               Value function loss: 122436.8664
                    Surrogate loss: 0.0135
             Mean action noise std: 0.88
                       Mean reward: 8812.15
               Mean episode length: 377.06
                 Mean success rate: 71.50
                  Mean reward/step: 22.60
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 18030592
                    Iteration time: 2.54s
                        Total time: 5805.46s
                               ETA: 521726.9s

################################################################################
                    [1m Learning iteration 2201/200000 [0m

                       Computation: 3183 steps/s (collection: 0.517s, learning 2.056s)
               Value function loss: 87372.3435
                    Surrogate loss: 0.0124
             Mean action noise std: 0.88
                       Mean reward: 8889.12
               Mean episode length: 375.20
                 Mean success rate: 71.50
                  Mean reward/step: 22.20
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 18038784
                    Iteration time: 2.57s
                        Total time: 5808.04s
                               ETA: 521718.4s

################################################################################
                    [1m Learning iteration 2202/200000 [0m

                       Computation: 3182 steps/s (collection: 0.517s, learning 2.057s)
               Value function loss: 112238.6389
                    Surrogate loss: 0.0172
             Mean action noise std: 0.88
                       Mean reward: 9153.42
               Mean episode length: 383.00
                 Mean success rate: 73.00
                  Mean reward/step: 22.41
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 18046976
                    Iteration time: 2.57s
                        Total time: 5810.61s
                               ETA: 521710.1s

################################################################################
                    [1m Learning iteration 2203/200000 [0m

                       Computation: 3185 steps/s (collection: 0.545s, learning 2.027s)
               Value function loss: 88754.2712
                    Surrogate loss: 0.0156
             Mean action noise std: 0.88
                       Mean reward: 9158.76
               Mean episode length: 377.72
                 Mean success rate: 73.00
                  Mean reward/step: 22.51
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 18055168
                    Iteration time: 2.57s
                        Total time: 5813.18s
                               ETA: 521701.5s

################################################################################
                    [1m Learning iteration 2204/200000 [0m

                       Computation: 3071 steps/s (collection: 0.591s, learning 2.076s)
               Value function loss: 84361.0723
                    Surrogate loss: 0.0135
             Mean action noise std: 0.88
                       Mean reward: 9156.84
               Mean episode length: 378.25
                 Mean success rate: 73.00
                  Mean reward/step: 22.61
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 18063360
                    Iteration time: 2.67s
                        Total time: 5815.85s
                               ETA: 521701.6s

################################################################################
                    [1m Learning iteration 2205/200000 [0m

                       Computation: 3168 steps/s (collection: 0.506s, learning 2.079s)
               Value function loss: 57833.7089
                    Surrogate loss: 0.0150
             Mean action noise std: 0.88
                       Mean reward: 8939.73
               Mean episode length: 372.97
                 Mean success rate: 72.00
                  Mean reward/step: 23.40
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 18071552
                    Iteration time: 2.59s
                        Total time: 5818.44s
                               ETA: 521694.3s

################################################################################
                    [1m Learning iteration 2206/200000 [0m

                       Computation: 3151 steps/s (collection: 0.500s, learning 2.099s)
               Value function loss: 91046.0054
                    Surrogate loss: 0.0118
             Mean action noise std: 0.88
                       Mean reward: 9299.23
               Mean episode length: 386.35
                 Mean success rate: 75.00
                  Mean reward/step: 23.56
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 18079744
                    Iteration time: 2.60s
                        Total time: 5821.04s
                               ETA: 521688.2s

################################################################################
                    [1m Learning iteration 2207/200000 [0m

                       Computation: 3166 steps/s (collection: 0.506s, learning 2.080s)
               Value function loss: 52688.3481
                    Surrogate loss: 0.0137
             Mean action noise std: 0.88
                       Mean reward: 8996.84
               Mean episode length: 376.64
                 Mean success rate: 72.50
                  Mean reward/step: 23.45
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 2.59s
                        Total time: 5823.62s
                               ETA: 521681.0s

################################################################################
                    [1m Learning iteration 2208/200000 [0m

                       Computation: 3073 steps/s (collection: 0.569s, learning 2.096s)
               Value function loss: 122302.8448
                    Surrogate loss: 0.0138
             Mean action noise std: 0.88
                       Mean reward: 8718.80
               Mean episode length: 368.78
                 Mean success rate: 71.00
                  Mean reward/step: 23.20
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 18096128
                    Iteration time: 2.67s
                        Total time: 5826.29s
                               ETA: 521680.9s

################################################################################
                    [1m Learning iteration 2209/200000 [0m

                       Computation: 3134 steps/s (collection: 0.559s, learning 2.055s)
               Value function loss: 67244.6156
                    Surrogate loss: 0.0139
             Mean action noise std: 0.88
                       Mean reward: 8507.27
               Mean episode length: 360.44
                 Mean success rate: 69.00
                  Mean reward/step: 23.01
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 18104320
                    Iteration time: 2.61s
                        Total time: 5828.90s
                               ETA: 521676.1s

################################################################################
                    [1m Learning iteration 2210/200000 [0m

                       Computation: 3188 steps/s (collection: 0.507s, learning 2.062s)
               Value function loss: 80571.0888
                    Surrogate loss: 0.0119
             Mean action noise std: 0.88
                       Mean reward: 8454.00
               Mean episode length: 359.08
                 Mean success rate: 69.00
                  Mean reward/step: 22.23
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 18112512
                    Iteration time: 2.57s
                        Total time: 5831.47s
                               ETA: 521667.4s

################################################################################
                    [1m Learning iteration 2211/200000 [0m

                       Computation: 3178 steps/s (collection: 0.525s, learning 2.052s)
               Value function loss: 91181.2525
                    Surrogate loss: 0.0118
             Mean action noise std: 0.88
                       Mean reward: 8275.44
               Mean episode length: 354.37
                 Mean success rate: 68.00
                  Mean reward/step: 22.38
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 18120704
                    Iteration time: 2.58s
                        Total time: 5834.05s
                               ETA: 521659.3s

################################################################################
                    [1m Learning iteration 2212/200000 [0m

                       Computation: 3123 steps/s (collection: 0.551s, learning 2.071s)
               Value function loss: 68892.2596
                    Surrogate loss: 0.0139
             Mean action noise std: 0.88
                       Mean reward: 8298.29
               Mean episode length: 354.50
                 Mean success rate: 68.00
                  Mean reward/step: 22.96
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 18128896
                    Iteration time: 2.62s
                        Total time: 5836.67s
                               ETA: 521655.4s

################################################################################
                    [1m Learning iteration 2213/200000 [0m

                       Computation: 3143 steps/s (collection: 0.536s, learning 2.070s)
               Value function loss: 120372.2383
                    Surrogate loss: 0.0140
             Mean action noise std: 0.88
                       Mean reward: 8261.52
               Mean episode length: 355.20
                 Mean success rate: 68.00
                  Mean reward/step: 23.44
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 18137088
                    Iteration time: 2.61s
                        Total time: 5839.28s
                               ETA: 521650.0s

################################################################################
                    [1m Learning iteration 2214/200000 [0m

                       Computation: 3116 steps/s (collection: 0.520s, learning 2.109s)
               Value function loss: 104490.6849
                    Surrogate loss: 0.0107
             Mean action noise std: 0.88
                       Mean reward: 8219.96
               Mean episode length: 354.94
                 Mean success rate: 66.50
                  Mean reward/step: 23.25
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 18145280
                    Iteration time: 2.63s
                        Total time: 5841.91s
                               ETA: 521646.6s

################################################################################
                    [1m Learning iteration 2215/200000 [0m

                       Computation: 3192 steps/s (collection: 0.518s, learning 2.047s)
               Value function loss: 106313.4285
                    Surrogate loss: 0.0123
             Mean action noise std: 0.88
                       Mean reward: 8069.45
               Mean episode length: 349.48
                 Mean success rate: 65.50
                  Mean reward/step: 23.57
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 18153472
                    Iteration time: 2.57s
                        Total time: 5844.47s
                               ETA: 521637.5s

################################################################################
                    [1m Learning iteration 2216/200000 [0m

                       Computation: 3194 steps/s (collection: 0.499s, learning 2.066s)
               Value function loss: 96492.9117
                    Surrogate loss: 0.0136
             Mean action noise std: 0.88
                       Mean reward: 8586.53
               Mean episode length: 363.32
                 Mean success rate: 68.50
                  Mean reward/step: 23.75
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 18161664
                    Iteration time: 2.56s
                        Total time: 5847.04s
                               ETA: 521628.4s

################################################################################
                    [1m Learning iteration 2217/200000 [0m

                       Computation: 3278 steps/s (collection: 0.471s, learning 2.027s)
               Value function loss: 98877.6230
                    Surrogate loss: 0.0121
             Mean action noise std: 0.88
                       Mean reward: 8605.67
               Mean episode length: 367.44
                 Mean success rate: 70.00
                  Mean reward/step: 24.76
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 18169856
                    Iteration time: 2.50s
                        Total time: 5849.53s
                               ETA: 521613.4s

################################################################################
                    [1m Learning iteration 2218/200000 [0m

                       Computation: 3304 steps/s (collection: 0.445s, learning 2.034s)
               Value function loss: 93071.1048
                    Surrogate loss: 0.0127
             Mean action noise std: 0.88
                       Mean reward: 8808.04
               Mean episode length: 373.54
                 Mean success rate: 72.00
                  Mean reward/step: 25.31
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 18178048
                    Iteration time: 2.48s
                        Total time: 5852.01s
                               ETA: 521596.6s

################################################################################
                    [1m Learning iteration 2219/200000 [0m

                       Computation: 3302 steps/s (collection: 0.444s, learning 2.037s)
               Value function loss: 87856.7740
                    Surrogate loss: 0.0136
             Mean action noise std: 0.88
                       Mean reward: 8788.67
               Mean episode length: 376.01
                 Mean success rate: 72.00
                  Mean reward/step: 25.22
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 2.48s
                        Total time: 5854.49s
                               ETA: 521580.0s

################################################################################
                    [1m Learning iteration 2220/200000 [0m

                       Computation: 3274 steps/s (collection: 0.457s, learning 2.045s)
               Value function loss: 94491.2688
                    Surrogate loss: 0.0121
             Mean action noise std: 0.88
                       Mean reward: 8816.08
               Mean episode length: 378.69
                 Mean success rate: 73.00
                  Mean reward/step: 25.37
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 18194432
                    Iteration time: 2.50s
                        Total time: 5857.00s
                               ETA: 521565.3s

################################################################################
                    [1m Learning iteration 2221/200000 [0m

                       Computation: 3313 steps/s (collection: 0.449s, learning 2.023s)
               Value function loss: 83488.0068
                    Surrogate loss: 0.0136
             Mean action noise std: 0.88
                       Mean reward: 9005.21
               Mean episode length: 389.20
                 Mean success rate: 75.00
                  Mean reward/step: 26.16
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 18202624
                    Iteration time: 2.47s
                        Total time: 5859.47s
                               ETA: 521548.0s

################################################################################
                    [1m Learning iteration 2222/200000 [0m

                       Computation: 3277 steps/s (collection: 0.452s, learning 2.047s)
               Value function loss: 55556.0079
                    Surrogate loss: 0.0125
             Mean action noise std: 0.88
                       Mean reward: 9096.36
               Mean episode length: 393.20
                 Mean success rate: 75.50
                  Mean reward/step: 26.34
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 18210816
                    Iteration time: 2.50s
                        Total time: 5861.97s
                               ETA: 521533.2s

################################################################################
                    [1m Learning iteration 2223/200000 [0m

                       Computation: 3292 steps/s (collection: 0.450s, learning 2.038s)
               Value function loss: 112543.9990
                    Surrogate loss: 0.0135
             Mean action noise std: 0.88
                       Mean reward: 9349.19
               Mean episode length: 400.06
                 Mean success rate: 77.50
                  Mean reward/step: 26.55
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 18219008
                    Iteration time: 2.49s
                        Total time: 5864.46s
                               ETA: 521517.3s

################################################################################
                    [1m Learning iteration 2224/200000 [0m

                       Computation: 3216 steps/s (collection: 0.492s, learning 2.055s)
               Value function loss: 105548.3429
                    Surrogate loss: 0.0141
             Mean action noise std: 0.88
                       Mean reward: 9708.93
               Mean episode length: 410.25
                 Mean success rate: 80.50
                  Mean reward/step: 25.66
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 18227200
                    Iteration time: 2.55s
                        Total time: 5867.00s
                               ETA: 521506.7s

################################################################################
                    [1m Learning iteration 2225/200000 [0m

                       Computation: 3305 steps/s (collection: 0.457s, learning 2.022s)
               Value function loss: 95768.8313
                    Surrogate loss: 0.0145
             Mean action noise std: 0.88
                       Mean reward: 10055.33
               Mean episode length: 419.54
                 Mean success rate: 83.00
                  Mean reward/step: 24.44
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 18235392
                    Iteration time: 2.48s
                        Total time: 5869.48s
                               ETA: 521490.0s

################################################################################
                    [1m Learning iteration 2226/200000 [0m

                       Computation: 3341 steps/s (collection: 0.421s, learning 2.031s)
               Value function loss: 87089.1264
                    Surrogate loss: 0.0127
             Mean action noise std: 0.88
                       Mean reward: 10175.66
               Mean episode length: 423.91
                 Mean success rate: 84.00
                  Mean reward/step: 23.98
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 18243584
                    Iteration time: 2.45s
                        Total time: 5871.93s
                               ETA: 521470.9s

################################################################################
                    [1m Learning iteration 2227/200000 [0m

                       Computation: 3290 steps/s (collection: 0.443s, learning 2.047s)
               Value function loss: 114317.8250
                    Surrogate loss: 0.0137
             Mean action noise std: 0.88
                       Mean reward: 9858.44
               Mean episode length: 414.31
                 Mean success rate: 81.50
                  Mean reward/step: 24.63
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 18251776
                    Iteration time: 2.49s
                        Total time: 5874.42s
                               ETA: 521455.2s

################################################################################
                    [1m Learning iteration 2228/200000 [0m

                       Computation: 3297 steps/s (collection: 0.449s, learning 2.036s)
               Value function loss: 102945.2376
                    Surrogate loss: 0.0129
             Mean action noise std: 0.88
                       Mean reward: 10123.79
               Mean episode length: 418.36
                 Mean success rate: 82.50
                  Mean reward/step: 24.21
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 18259968
                    Iteration time: 2.48s
                        Total time: 5876.91s
                               ETA: 521439.1s

################################################################################
                    [1m Learning iteration 2229/200000 [0m

                       Computation: 3263 steps/s (collection: 0.493s, learning 2.017s)
               Value function loss: 99258.3611
                    Surrogate loss: 0.0162
             Mean action noise std: 0.88
                       Mean reward: 10322.26
               Mean episode length: 422.68
                 Mean success rate: 83.00
                  Mean reward/step: 23.53
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 18268160
                    Iteration time: 2.51s
                        Total time: 5879.42s
                               ETA: 521425.2s

################################################################################
                    [1m Learning iteration 2230/200000 [0m

                       Computation: 3289 steps/s (collection: 0.447s, learning 2.043s)
               Value function loss: 121228.2662
                    Surrogate loss: 0.0177
             Mean action noise std: 0.88
                       Mean reward: 10887.04
               Mean episode length: 438.78
                 Mean success rate: 87.00
                  Mean reward/step: 23.42
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 18276352
                    Iteration time: 2.49s
                        Total time: 5881.91s
                               ETA: 521409.7s

################################################################################
                    [1m Learning iteration 2231/200000 [0m

                       Computation: 3218 steps/s (collection: 0.467s, learning 2.078s)
               Value function loss: 102737.4670
                    Surrogate loss: 0.0116
             Mean action noise std: 0.88
                       Mean reward: 11064.73
               Mean episode length: 441.24
                 Mean success rate: 88.00
                  Mean reward/step: 23.38
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 2.55s
                        Total time: 5884.45s
                               ETA: 521398.9s

################################################################################
                    [1m Learning iteration 2232/200000 [0m

                       Computation: 3273 steps/s (collection: 0.477s, learning 2.025s)
               Value function loss: 87622.1604
                    Surrogate loss: 0.0144
             Mean action noise std: 0.88
                       Mean reward: 10986.36
               Mean episode length: 432.23
                 Mean success rate: 86.00
                  Mean reward/step: 23.59
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 18292736
                    Iteration time: 2.50s
                        Total time: 5886.96s
                               ETA: 521384.4s

################################################################################
                    [1m Learning iteration 2233/200000 [0m

                       Computation: 3309 steps/s (collection: 0.437s, learning 2.038s)
               Value function loss: 84213.9989
                    Surrogate loss: 0.0130
             Mean action noise std: 0.88
                       Mean reward: 10824.24
               Mean episode length: 426.69
                 Mean success rate: 84.50
                  Mean reward/step: 23.86
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 18300928
                    Iteration time: 2.48s
                        Total time: 5889.43s
                               ETA: 521367.5s

################################################################################
                    [1m Learning iteration 2234/200000 [0m

                       Computation: 3298 steps/s (collection: 0.469s, learning 2.015s)
               Value function loss: 102564.9854
                    Surrogate loss: 0.0145
             Mean action noise std: 0.88
                       Mean reward: 10714.64
               Mean episode length: 425.65
                 Mean success rate: 84.00
                  Mean reward/step: 24.16
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 18309120
                    Iteration time: 2.48s
                        Total time: 5891.91s
                               ETA: 521351.4s

################################################################################
                    [1m Learning iteration 2235/200000 [0m

                       Computation: 3262 steps/s (collection: 0.487s, learning 2.024s)
               Value function loss: 81743.9699
                    Surrogate loss: 0.0126
             Mean action noise std: 0.88
                       Mean reward: 10541.97
               Mean episode length: 422.89
                 Mean success rate: 83.00
                  Mean reward/step: 24.45
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 18317312
                    Iteration time: 2.51s
                        Total time: 5894.43s
                               ETA: 521337.6s

################################################################################
                    [1m Learning iteration 2236/200000 [0m

                       Computation: 3260 steps/s (collection: 0.473s, learning 2.040s)
               Value function loss: 86655.0897
                    Surrogate loss: 0.0178
             Mean action noise std: 0.88
                       Mean reward: 10570.23
               Mean episode length: 423.23
                 Mean success rate: 83.50
                  Mean reward/step: 24.22
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 18325504
                    Iteration time: 2.51s
                        Total time: 5896.94s
                               ETA: 521324.1s

################################################################################
                    [1m Learning iteration 2237/200000 [0m

                       Computation: 3335 steps/s (collection: 0.431s, learning 2.025s)
               Value function loss: 68623.6793
                    Surrogate loss: 0.0156
             Mean action noise std: 0.88
                       Mean reward: 10540.79
               Mean episode length: 425.00
                 Mean success rate: 83.50
                  Mean reward/step: 25.05
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 18333696
                    Iteration time: 2.46s
                        Total time: 5899.39s
                               ETA: 521305.5s

################################################################################
                    [1m Learning iteration 2238/200000 [0m

                       Computation: 3337 steps/s (collection: 0.447s, learning 2.008s)
               Value function loss: 62691.2654
                    Surrogate loss: 0.0147
             Mean action noise std: 0.88
                       Mean reward: 10071.89
               Mean episode length: 410.12
                 Mean success rate: 80.50
                  Mean reward/step: 25.65
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 18341888
                    Iteration time: 2.45s
                        Total time: 5901.85s
                               ETA: 521286.9s

################################################################################
                    [1m Learning iteration 2239/200000 [0m

                       Computation: 3209 steps/s (collection: 0.451s, learning 2.101s)
               Value function loss: 106432.1131
                    Surrogate loss: 0.0126
             Mean action noise std: 0.88
                       Mean reward: 9877.72
               Mean episode length: 408.07
                 Mean success rate: 80.00
                  Mean reward/step: 25.65
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 18350080
                    Iteration time: 2.55s
                        Total time: 5904.40s
                               ETA: 521276.9s

################################################################################
                    [1m Learning iteration 2240/200000 [0m

                       Computation: 3246 steps/s (collection: 0.455s, learning 2.069s)
               Value function loss: 97279.0437
                    Surrogate loss: 0.0115
             Mean action noise std: 0.88
                       Mean reward: 9410.24
               Mean episode length: 397.00
                 Mean success rate: 77.00
                  Mean reward/step: 24.82
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 18358272
                    Iteration time: 2.52s
                        Total time: 5906.92s
                               ETA: 521264.3s

################################################################################
                    [1m Learning iteration 2241/200000 [0m

                       Computation: 3226 steps/s (collection: 0.475s, learning 2.064s)
               Value function loss: 89596.0264
                    Surrogate loss: 0.0098
             Mean action noise std: 0.88
                       Mean reward: 9116.62
               Mean episode length: 390.87
                 Mean success rate: 75.00
                  Mean reward/step: 24.91
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 18366464
                    Iteration time: 2.54s
                        Total time: 5909.46s
                               ETA: 521253.1s

################################################################################
                    [1m Learning iteration 2242/200000 [0m

                       Computation: 3270 steps/s (collection: 0.465s, learning 2.040s)
               Value function loss: 92889.3381
                    Surrogate loss: 0.0142
             Mean action noise std: 0.88
                       Mean reward: 9199.75
               Mean episode length: 392.51
                 Mean success rate: 75.50
                  Mean reward/step: 25.24
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 18374656
                    Iteration time: 2.50s
                        Total time: 5911.97s
                               ETA: 521238.9s

################################################################################
                    [1m Learning iteration 2243/200000 [0m

                       Computation: 3261 steps/s (collection: 0.473s, learning 2.039s)
               Value function loss: 90255.1301
                    Surrogate loss: 0.0101
             Mean action noise std: 0.88
                       Mean reward: 9265.38
               Mean episode length: 392.29
                 Mean success rate: 76.00
                  Mean reward/step: 25.13
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 2.51s
                        Total time: 5914.48s
                               ETA: 521225.4s

################################################################################
                    [1m Learning iteration 2244/200000 [0m

                       Computation: 3269 steps/s (collection: 0.454s, learning 2.052s)
               Value function loss: 87634.9223
                    Surrogate loss: 0.0115
             Mean action noise std: 0.88
                       Mean reward: 9250.63
               Mean episode length: 389.54
                 Mean success rate: 76.00
                  Mean reward/step: 25.16
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 18391040
                    Iteration time: 2.51s
                        Total time: 5916.99s
                               ETA: 521211.3s

################################################################################
                    [1m Learning iteration 2245/200000 [0m

                       Computation: 3309 steps/s (collection: 0.450s, learning 2.025s)
               Value function loss: 85322.8641
                    Surrogate loss: 0.0128
             Mean action noise std: 0.88
                       Mean reward: 9534.93
               Mean episode length: 397.88
                 Mean success rate: 78.00
                  Mean reward/step: 25.29
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 18399232
                    Iteration time: 2.48s
                        Total time: 5919.46s
                               ETA: 521194.5s

################################################################################
                    [1m Learning iteration 2246/200000 [0m

                       Computation: 3280 steps/s (collection: 0.463s, learning 2.034s)
               Value function loss: 112124.7393
                    Surrogate loss: 0.0137
             Mean action noise std: 0.88
                       Mean reward: 9588.48
               Mean episode length: 397.56
                 Mean success rate: 77.50
                  Mean reward/step: 25.28
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 18407424
                    Iteration time: 2.50s
                        Total time: 5921.96s
                               ETA: 521179.8s

################################################################################
                    [1m Learning iteration 2247/200000 [0m

                       Computation: 3211 steps/s (collection: 0.503s, learning 2.048s)
               Value function loss: 102201.9139
                    Surrogate loss: 0.0124
             Mean action noise std: 0.88
                       Mean reward: 9798.60
               Mean episode length: 404.11
                 Mean success rate: 79.00
                  Mean reward/step: 25.35
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 18415616
                    Iteration time: 2.55s
                        Total time: 5924.51s
                               ETA: 521169.6s

################################################################################
                    [1m Learning iteration 2248/200000 [0m

                       Computation: 3144 steps/s (collection: 0.526s, learning 2.079s)
               Value function loss: 117687.0949
                    Surrogate loss: 0.0144
             Mean action noise std: 0.88
                       Mean reward: 9849.28
               Mean episode length: 400.19
                 Mean success rate: 79.00
                  Mean reward/step: 25.26
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 18423808
                    Iteration time: 2.60s
                        Total time: 5927.11s
                               ETA: 521164.3s

################################################################################
                    [1m Learning iteration 2249/200000 [0m

                       Computation: 3189 steps/s (collection: 0.487s, learning 2.082s)
               Value function loss: 115242.9555
                    Surrogate loss: 0.0103
             Mean action noise std: 0.88
                       Mean reward: 10154.86
               Mean episode length: 408.31
                 Mean success rate: 81.00
                  Mean reward/step: 24.45
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 18432000
                    Iteration time: 2.57s
                        Total time: 5929.68s
                               ETA: 521155.8s

################################################################################
                    [1m Learning iteration 2250/200000 [0m

                       Computation: 3119 steps/s (collection: 0.539s, learning 2.087s)
               Value function loss: 105177.6214
                    Surrogate loss: 0.0155
             Mean action noise std: 0.88
                       Mean reward: 10381.22
               Mean episode length: 411.37
                 Mean success rate: 81.00
                  Mean reward/step: 23.72
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 18440192
                    Iteration time: 2.63s
                        Total time: 5932.31s
                               ETA: 521152.3s

################################################################################
                    [1m Learning iteration 2251/200000 [0m

                       Computation: 3121 steps/s (collection: 0.524s, learning 2.100s)
               Value function loss: 104981.3555
                    Surrogate loss: 0.0151
             Mean action noise std: 0.88
                       Mean reward: 10329.02
               Mean episode length: 409.80
                 Mean success rate: 81.00
                  Mean reward/step: 23.94
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 18448384
                    Iteration time: 2.62s
                        Total time: 5934.93s
                               ETA: 521148.7s

################################################################################
                    [1m Learning iteration 2252/200000 [0m

                       Computation: 3063 steps/s (collection: 0.581s, learning 2.093s)
               Value function loss: 79376.1728
                    Surrogate loss: 0.0112
             Mean action noise std: 0.88
                       Mean reward: 10228.29
               Mean episode length: 405.84
                 Mean success rate: 80.00
                  Mean reward/step: 23.53
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 18456576
                    Iteration time: 2.67s
                        Total time: 5937.61s
                               ETA: 521149.4s

################################################################################
                    [1m Learning iteration 2253/200000 [0m

                       Computation: 3143 steps/s (collection: 0.471s, learning 2.135s)
               Value function loss: 63907.2987
                    Surrogate loss: 0.0137
             Mean action noise std: 0.88
                       Mean reward: 10100.99
               Mean episode length: 401.61
                 Mean success rate: 79.50
                  Mean reward/step: 24.24
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 18464768
                    Iteration time: 2.61s
                        Total time: 5940.21s
                               ETA: 521144.2s

################################################################################
                    [1m Learning iteration 2254/200000 [0m

                       Computation: 3110 steps/s (collection: 0.516s, learning 2.118s)
               Value function loss: 89249.9420
                    Surrogate loss: 0.0124
             Mean action noise std: 0.88
                       Mean reward: 9677.79
               Mean episode length: 387.80
                 Mean success rate: 76.00
                  Mean reward/step: 24.51
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 18472960
                    Iteration time: 2.63s
                        Total time: 5942.85s
                               ETA: 521141.5s

################################################################################
                    [1m Learning iteration 2255/200000 [0m

                       Computation: 3086 steps/s (collection: 0.563s, learning 2.092s)
               Value function loss: 117238.8922
                    Surrogate loss: 0.0128
             Mean action noise std: 0.88
                       Mean reward: 9513.54
               Mean episode length: 381.80
                 Mean success rate: 75.00
                  Mean reward/step: 24.75
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 2.65s
                        Total time: 5945.50s
                               ETA: 521140.5s

################################################################################
                    [1m Learning iteration 2256/200000 [0m

                       Computation: 3106 steps/s (collection: 0.531s, learning 2.106s)
               Value function loss: 101263.5559
                    Surrogate loss: 0.0116
             Mean action noise std: 0.88
                       Mean reward: 9225.68
               Mean episode length: 373.85
                 Mean success rate: 72.50
                  Mean reward/step: 23.99
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 18489344
                    Iteration time: 2.64s
                        Total time: 5948.14s
                               ETA: 521138.0s

################################################################################
                    [1m Learning iteration 2257/200000 [0m

                       Computation: 3108 steps/s (collection: 0.529s, learning 2.107s)
               Value function loss: 86899.1211
                    Surrogate loss: 0.0117
             Mean action noise std: 0.88
                       Mean reward: 9482.17
               Mean episode length: 384.71
                 Mean success rate: 74.50
                  Mean reward/step: 24.17
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 18497536
                    Iteration time: 2.64s
                        Total time: 5950.77s
                               ETA: 521135.4s

################################################################################
                    [1m Learning iteration 2258/200000 [0m

                       Computation: 3203 steps/s (collection: 0.453s, learning 2.104s)
               Value function loss: 101518.7795
                    Surrogate loss: 0.0118
             Mean action noise std: 0.88
                       Mean reward: 9448.93
               Mean episode length: 385.15
                 Mean success rate: 75.00
                  Mean reward/step: 24.81
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 18505728
                    Iteration time: 2.56s
                        Total time: 5953.33s
                               ETA: 521125.9s

################################################################################
                    [1m Learning iteration 2259/200000 [0m

                       Computation: 3297 steps/s (collection: 0.460s, learning 2.025s)
               Value function loss: 61953.5397
                    Surrogate loss: 0.0143
             Mean action noise std: 0.88
                       Mean reward: 9481.01
               Mean episode length: 384.86
                 Mean success rate: 75.50
                  Mean reward/step: 24.24
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 18513920
                    Iteration time: 2.48s
                        Total time: 5955.82s
                               ETA: 521110.1s

################################################################################
                    [1m Learning iteration 2260/200000 [0m

                       Computation: 3149 steps/s (collection: 0.507s, learning 2.094s)
               Value function loss: 108550.0092
                    Surrogate loss: 0.0152
             Mean action noise std: 0.88
                       Mean reward: 9474.61
               Mean episode length: 385.79
                 Mean success rate: 76.00
                  Mean reward/step: 23.88
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 18522112
                    Iteration time: 2.60s
                        Total time: 5958.42s
                               ETA: 521104.5s

################################################################################
                    [1m Learning iteration 2261/200000 [0m

                       Computation: 3128 steps/s (collection: 0.495s, learning 2.123s)
               Value function loss: 97589.7541
                    Surrogate loss: 0.0127
             Mean action noise std: 0.88
                       Mean reward: 9642.51
               Mean episode length: 392.00
                 Mean success rate: 77.00
                  Mean reward/step: 23.42
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 18530304
                    Iteration time: 2.62s
                        Total time: 5961.03s
                               ETA: 521100.3s

################################################################################
                    [1m Learning iteration 2262/200000 [0m

                       Computation: 3222 steps/s (collection: 0.485s, learning 2.057s)
               Value function loss: 109459.6338
                    Surrogate loss: 0.0140
             Mean action noise std: 0.88
                       Mean reward: 9690.34
               Mean episode length: 396.47
                 Mean success rate: 77.50
                  Mean reward/step: 23.65
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 18538496
                    Iteration time: 2.54s
                        Total time: 5963.58s
                               ETA: 521089.5s

################################################################################
                    [1m Learning iteration 2263/200000 [0m

                       Computation: 3236 steps/s (collection: 0.504s, learning 2.028s)
               Value function loss: 81004.9367
                    Surrogate loss: 0.0099
             Mean action noise std: 0.88
                       Mean reward: 9923.59
               Mean episode length: 405.04
                 Mean success rate: 79.00
                  Mean reward/step: 23.11
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 18546688
                    Iteration time: 2.53s
                        Total time: 5966.11s
                               ETA: 521077.8s

################################################################################
                    [1m Learning iteration 2264/200000 [0m

                       Computation: 3286 steps/s (collection: 0.457s, learning 2.035s)
               Value function loss: 110158.8809
                    Surrogate loss: 0.0116
             Mean action noise std: 0.88
                       Mean reward: 10219.62
               Mean episode length: 416.44
                 Mean success rate: 81.00
                  Mean reward/step: 23.01
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 18554880
                    Iteration time: 2.49s
                        Total time: 5968.60s
                               ETA: 521062.7s

################################################################################
                    [1m Learning iteration 2265/200000 [0m

                       Computation: 3232 steps/s (collection: 0.518s, learning 2.017s)
               Value function loss: 90968.3059
                    Surrogate loss: 0.0162
             Mean action noise std: 0.88
                       Mean reward: 10310.49
               Mean episode length: 420.29
                 Mean success rate: 81.50
                  Mean reward/step: 22.95
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 18563072
                    Iteration time: 2.53s
                        Total time: 5971.13s
                               ETA: 521051.3s

################################################################################
                    [1m Learning iteration 2266/200000 [0m

                       Computation: 3267 steps/s (collection: 0.496s, learning 2.012s)
               Value function loss: 66001.8930
                    Surrogate loss: 0.0136
             Mean action noise std: 0.88
                       Mean reward: 10096.45
               Mean episode length: 409.08
                 Mean success rate: 79.00
                  Mean reward/step: 23.09
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 18571264
                    Iteration time: 2.51s
                        Total time: 5973.64s
                               ETA: 521037.5s

################################################################################
                    [1m Learning iteration 2267/200000 [0m

                       Computation: 3270 steps/s (collection: 0.483s, learning 2.021s)
               Value function loss: 105180.5831
                    Surrogate loss: 0.0126
             Mean action noise std: 0.88
                       Mean reward: 9393.43
               Mean episode length: 388.62
                 Mean success rate: 74.50
                  Mean reward/step: 23.67
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 2.50s
                        Total time: 5976.15s
                               ETA: 521023.6s

################################################################################
                    [1m Learning iteration 2268/200000 [0m

                       Computation: 3254 steps/s (collection: 0.503s, learning 2.014s)
               Value function loss: 77287.8999
                    Surrogate loss: 0.0118
             Mean action noise std: 0.88
                       Mean reward: 9487.86
               Mean episode length: 392.96
                 Mean success rate: 75.00
                  Mean reward/step: 23.44
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 18587648
                    Iteration time: 2.52s
                        Total time: 5978.66s
                               ETA: 521010.6s

################################################################################
                    [1m Learning iteration 2269/200000 [0m

                       Computation: 3285 steps/s (collection: 0.478s, learning 2.016s)
               Value function loss: 76568.0060
                    Surrogate loss: 0.0150
             Mean action noise std: 0.88
                       Mean reward: 9402.39
               Mean episode length: 388.63
                 Mean success rate: 74.00
                  Mean reward/step: 24.42
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 18595840
                    Iteration time: 2.49s
                        Total time: 5981.16s
                               ETA: 520995.7s

################################################################################
                    [1m Learning iteration 2270/200000 [0m

                       Computation: 3260 steps/s (collection: 0.480s, learning 2.032s)
               Value function loss: 125901.5538
                    Surrogate loss: 0.0109
             Mean action noise std: 0.88
                       Mean reward: 9625.54
               Mean episode length: 394.60
                 Mean success rate: 75.50
                  Mean reward/step: 23.98
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 18604032
                    Iteration time: 2.51s
                        Total time: 5983.67s
                               ETA: 520982.4s

################################################################################
                    [1m Learning iteration 2271/200000 [0m

                       Computation: 3269 steps/s (collection: 0.471s, learning 2.035s)
               Value function loss: 117622.9879
                    Surrogate loss: 0.0116
             Mean action noise std: 0.88
                       Mean reward: 9306.32
               Mean episode length: 389.12
                 Mean success rate: 75.00
                  Mean reward/step: 23.35
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 18612224
                    Iteration time: 2.51s
                        Total time: 5986.18s
                               ETA: 520968.5s

################################################################################
                    [1m Learning iteration 2272/200000 [0m

                       Computation: 3266 steps/s (collection: 0.486s, learning 2.021s)
               Value function loss: 95646.0656
                    Surrogate loss: 0.0132
             Mean action noise std: 0.88
                       Mean reward: 9260.98
               Mean episode length: 388.32
                 Mean success rate: 74.00
                  Mean reward/step: 22.38
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 18620416
                    Iteration time: 2.51s
                        Total time: 5988.68s
                               ETA: 520954.8s

################################################################################
                    [1m Learning iteration 2273/200000 [0m

                       Computation: 3281 steps/s (collection: 0.475s, learning 2.022s)
               Value function loss: 87642.2746
                    Surrogate loss: 0.0156
             Mean action noise std: 0.88
                       Mean reward: 8929.38
               Mean episode length: 379.75
                 Mean success rate: 72.00
                  Mean reward/step: 22.71
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 18628608
                    Iteration time: 2.50s
                        Total time: 5991.18s
                               ETA: 520940.2s

################################################################################
                    [1m Learning iteration 2274/200000 [0m

                       Computation: 3216 steps/s (collection: 0.470s, learning 2.077s)
               Value function loss: 94466.9468
                    Surrogate loss: 0.0108
             Mean action noise std: 0.88
                       Mean reward: 8565.82
               Mean episode length: 369.73
                 Mean success rate: 70.50
                  Mean reward/step: 23.02
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 18636800
                    Iteration time: 2.55s
                        Total time: 5993.73s
                               ETA: 520929.9s

################################################################################
                    [1m Learning iteration 2275/200000 [0m

                       Computation: 3217 steps/s (collection: 0.468s, learning 2.078s)
               Value function loss: 66535.7778
                    Surrogate loss: 0.0154
             Mean action noise std: 0.88
                       Mean reward: 8750.25
               Mean episode length: 377.62
                 Mean success rate: 72.00
                  Mean reward/step: 23.69
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 18644992
                    Iteration time: 2.55s
                        Total time: 5996.27s
                               ETA: 520919.6s

################################################################################
                    [1m Learning iteration 2276/200000 [0m

                       Computation: 3172 steps/s (collection: 0.508s, learning 2.074s)
               Value function loss: 83296.3835
                    Surrogate loss: 0.0131
             Mean action noise std: 0.88
                       Mean reward: 9107.40
               Mean episode length: 392.39
                 Mean success rate: 75.00
                  Mean reward/step: 23.58
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 18653184
                    Iteration time: 2.58s
                        Total time: 5998.85s
                               ETA: 520912.4s

################################################################################
                    [1m Learning iteration 2277/200000 [0m

                       Computation: 3218 steps/s (collection: 0.442s, learning 2.103s)
               Value function loss: 87859.2027
                    Surrogate loss: 0.0142
             Mean action noise std: 0.88
                       Mean reward: 9082.55
               Mean episode length: 391.18
                 Mean success rate: 74.50
                  Mean reward/step: 23.14
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 18661376
                    Iteration time: 2.55s
                        Total time: 6001.40s
                               ETA: 520902.0s

################################################################################
                    [1m Learning iteration 2278/200000 [0m

                       Computation: 3146 steps/s (collection: 0.508s, learning 2.096s)
               Value function loss: 67124.5764
                    Surrogate loss: 0.0143
             Mean action noise std: 0.88
                       Mean reward: 8787.70
               Mean episode length: 382.00
                 Mean success rate: 72.00
                  Mean reward/step: 23.02
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 18669568
                    Iteration time: 2.60s
                        Total time: 6004.00s
                               ETA: 520896.7s

################################################################################
                    [1m Learning iteration 2279/200000 [0m

                       Computation: 3296 steps/s (collection: 0.462s, learning 2.023s)
               Value function loss: 93935.5367
                    Surrogate loss: 0.0125
             Mean action noise std: 0.88
                       Mean reward: 8790.01
               Mean episode length: 382.60
                 Mean success rate: 72.00
                  Mean reward/step: 22.76
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 2.48s
                        Total time: 6006.49s
                               ETA: 520881.1s

################################################################################
                    [1m Learning iteration 2280/200000 [0m

                       Computation: 3160 steps/s (collection: 0.447s, learning 2.145s)
               Value function loss: 94431.6557
                    Surrogate loss: 0.0097
             Mean action noise std: 0.88
                       Mean reward: 8786.94
               Mean episode length: 383.20
                 Mean success rate: 72.00
                  Mean reward/step: 23.26
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 18685952
                    Iteration time: 2.59s
                        Total time: 6009.08s
                               ETA: 520874.7s

################################################################################
                    [1m Learning iteration 2281/200000 [0m

                       Computation: 3191 steps/s (collection: 0.503s, learning 2.063s)
               Value function loss: 106738.2311
                    Surrogate loss: 0.0107
             Mean action noise std: 0.88
                       Mean reward: 8983.27
               Mean episode length: 382.80
                 Mean success rate: 72.00
                  Mean reward/step: 23.63
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 18694144
                    Iteration time: 2.57s
                        Total time: 6011.65s
                               ETA: 520866.2s

################################################################################
                    [1m Learning iteration 2282/200000 [0m

                       Computation: 3230 steps/s (collection: 0.461s, learning 2.074s)
               Value function loss: 92626.2670
                    Surrogate loss: 0.0079
             Mean action noise std: 0.88
                       Mean reward: 9144.74
               Mean episode length: 386.44
                 Mean success rate: 73.00
                  Mean reward/step: 23.60
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 18702336
                    Iteration time: 2.54s
                        Total time: 6014.18s
                               ETA: 520855.0s

################################################################################
                    [1m Learning iteration 2283/200000 [0m

                       Computation: 3192 steps/s (collection: 0.510s, learning 2.056s)
               Value function loss: 92781.0629
                    Surrogate loss: 0.0132
             Mean action noise std: 0.88
                       Mean reward: 9345.79
               Mean episode length: 396.14
                 Mean success rate: 75.50
                  Mean reward/step: 22.88
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 18710528
                    Iteration time: 2.57s
                        Total time: 6016.75s
                               ETA: 520846.5s

################################################################################
                    [1m Learning iteration 2284/200000 [0m

                       Computation: 3178 steps/s (collection: 0.483s, learning 2.094s)
               Value function loss: 68987.1178
                    Surrogate loss: 0.0136
             Mean action noise std: 0.88
                       Mean reward: 9178.51
               Mean episode length: 391.97
                 Mean success rate: 73.50
                  Mean reward/step: 23.68
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 18718720
                    Iteration time: 2.58s
                        Total time: 6019.33s
                               ETA: 520838.9s

################################################################################
                    [1m Learning iteration 2285/200000 [0m

                       Computation: 3140 steps/s (collection: 0.499s, learning 2.109s)
               Value function loss: 66364.6817
                    Surrogate loss: 0.0136
             Mean action noise std: 0.88
                       Mean reward: 9439.75
               Mean episode length: 400.30
                 Mean success rate: 75.50
                  Mean reward/step: 24.92
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 18726912
                    Iteration time: 2.61s
                        Total time: 6021.93s
                               ETA: 520834.1s

################################################################################
                    [1m Learning iteration 2286/200000 [0m

                       Computation: 3189 steps/s (collection: 0.493s, learning 2.075s)
               Value function loss: 82698.4784
                    Surrogate loss: 0.0111
             Mean action noise std: 0.88
                       Mean reward: 9681.24
               Mean episode length: 408.33
                 Mean success rate: 77.00
                  Mean reward/step: 25.34
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 18735104
                    Iteration time: 2.57s
                        Total time: 6024.50s
                               ETA: 520825.8s

################################################################################
                    [1m Learning iteration 2287/200000 [0m

                       Computation: 3281 steps/s (collection: 0.460s, learning 2.037s)
               Value function loss: 120921.5709
                    Surrogate loss: 0.0114
             Mean action noise std: 0.88
                       Mean reward: 10007.72
               Mean episode length: 417.05
                 Mean success rate: 79.00
                  Mean reward/step: 24.96
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 18743296
                    Iteration time: 2.50s
                        Total time: 6027.00s
                               ETA: 520811.2s

################################################################################
                    [1m Learning iteration 2288/200000 [0m

                       Computation: 3297 steps/s (collection: 0.472s, learning 2.013s)
               Value function loss: 109118.9891
                    Surrogate loss: 0.0132
             Mean action noise std: 0.88
                       Mean reward: 10271.36
               Mean episode length: 426.40
                 Mean success rate: 81.50
                  Mean reward/step: 24.52
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 18751488
                    Iteration time: 2.48s
                        Total time: 6029.48s
                               ETA: 520795.6s

################################################################################
                    [1m Learning iteration 2289/200000 [0m

                       Computation: 3284 steps/s (collection: 0.470s, learning 2.024s)
               Value function loss: 91615.5943
                    Surrogate loss: 0.0118
             Mean action noise std: 0.88
                       Mean reward: 10343.34
               Mean episode length: 433.02
                 Mean success rate: 83.00
                  Mean reward/step: 23.97
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 18759680
                    Iteration time: 2.49s
                        Total time: 6031.98s
                               ETA: 520781.0s

################################################################################
                    [1m Learning iteration 2290/200000 [0m

                       Computation: 3227 steps/s (collection: 0.490s, learning 2.048s)
               Value function loss: 78079.3410
                    Surrogate loss: 0.0113
             Mean action noise std: 0.88
                       Mean reward: 10336.86
               Mean episode length: 433.75
                 Mean success rate: 83.00
                  Mean reward/step: 24.35
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 18767872
                    Iteration time: 2.54s
                        Total time: 6034.52s
                               ETA: 520770.0s

################################################################################
                    [1m Learning iteration 2291/200000 [0m

                       Computation: 3086 steps/s (collection: 0.543s, learning 2.111s)
               Value function loss: 83500.4668
                    Surrogate loss: 0.0124
             Mean action noise std: 0.88
                       Mean reward: 10406.86
               Mean episode length: 437.15
                 Mean success rate: 83.50
                  Mean reward/step: 25.18
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 2.65s
                        Total time: 6037.17s
                               ETA: 520769.1s

################################################################################
                    [1m Learning iteration 2292/200000 [0m

                       Computation: 3239 steps/s (collection: 0.476s, learning 2.052s)
               Value function loss: 90026.8646
                    Surrogate loss: 0.0119
             Mean action noise std: 0.88
                       Mean reward: 10288.25
               Mean episode length: 437.92
                 Mean success rate: 84.00
                  Mean reward/step: 26.06
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 18784256
                    Iteration time: 2.53s
                        Total time: 6039.70s
                               ETA: 520757.4s

################################################################################
                    [1m Learning iteration 2293/200000 [0m

                       Computation: 3233 steps/s (collection: 0.514s, learning 2.020s)
               Value function loss: 91356.4366
                    Surrogate loss: 0.0217
             Mean action noise std: 0.88
                       Mean reward: 10459.66
               Mean episode length: 442.11
                 Mean success rate: 85.50
                  Mean reward/step: 25.39
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 18792448
                    Iteration time: 2.53s
                        Total time: 6042.23s
                               ETA: 520746.1s

################################################################################
                    [1m Learning iteration 2294/200000 [0m

                       Computation: 3211 steps/s (collection: 0.466s, learning 2.085s)
               Value function loss: 94080.7283
                    Surrogate loss: 0.0132
             Mean action noise std: 0.88
                       Mean reward: 10385.33
               Mean episode length: 434.14
                 Mean success rate: 84.50
                  Mean reward/step: 25.50
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 18800640
                    Iteration time: 2.55s
                        Total time: 6044.78s
                               ETA: 520736.3s

################################################################################
                    [1m Learning iteration 2295/200000 [0m

                       Computation: 3164 steps/s (collection: 0.520s, learning 2.069s)
               Value function loss: 80719.4596
                    Surrogate loss: 0.0129
             Mean action noise std: 0.88
                       Mean reward: 10724.44
               Mean episode length: 443.77
                 Mean success rate: 87.50
                  Mean reward/step: 25.74
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 18808832
                    Iteration time: 2.59s
                        Total time: 6047.37s
                               ETA: 520729.8s

################################################################################
                    [1m Learning iteration 2296/200000 [0m

                       Computation: 3287 steps/s (collection: 0.457s, learning 2.035s)
               Value function loss: 99305.2038
                    Surrogate loss: 0.0117
             Mean action noise std: 0.88
                       Mean reward: 10604.54
               Mean episode length: 439.86
                 Mean success rate: 86.50
                  Mean reward/step: 25.52
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 18817024
                    Iteration time: 2.49s
                        Total time: 6049.86s
                               ETA: 520715.0s

################################################################################
                    [1m Learning iteration 2297/200000 [0m

                       Computation: 3187 steps/s (collection: 0.497s, learning 2.073s)
               Value function loss: 101357.1415
                    Surrogate loss: 0.0136
             Mean action noise std: 0.88
                       Mean reward: 10339.91
               Mean episode length: 429.11
                 Mean success rate: 84.50
                  Mean reward/step: 25.07
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 18825216
                    Iteration time: 2.57s
                        Total time: 6052.43s
                               ETA: 520706.8s

################################################################################
                    [1m Learning iteration 2298/200000 [0m

                       Computation: 3171 steps/s (collection: 0.497s, learning 2.087s)
               Value function loss: 152430.0770
                    Surrogate loss: 0.0161
             Mean action noise std: 0.88
                       Mean reward: 10447.19
               Mean episode length: 429.56
                 Mean success rate: 84.50
                  Mean reward/step: 25.13
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 18833408
                    Iteration time: 2.58s
                        Total time: 6055.02s
                               ETA: 520699.8s

################################################################################
                    [1m Learning iteration 2299/200000 [0m

                       Computation: 3120 steps/s (collection: 0.567s, learning 2.059s)
               Value function loss: 93530.5743
                    Surrogate loss: 0.0123
             Mean action noise std: 0.88
                       Mean reward: 10233.76
               Mean episode length: 418.76
                 Mean success rate: 82.50
                  Mean reward/step: 23.64
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 18841600
                    Iteration time: 2.63s
                        Total time: 6057.64s
                               ETA: 520696.5s

################################################################################
                    [1m Learning iteration 2300/200000 [0m

                       Computation: 3213 steps/s (collection: 0.462s, learning 2.088s)
               Value function loss: 57099.9235
                    Surrogate loss: 0.0130
             Mean action noise std: 0.88
                       Mean reward: 10237.83
               Mean episode length: 415.71
                 Mean success rate: 82.50
                  Mean reward/step: 23.61
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 18849792
                    Iteration time: 2.55s
                        Total time: 6060.19s
                               ETA: 520686.6s

################################################################################
                    [1m Learning iteration 2301/200000 [0m

                       Computation: 3226 steps/s (collection: 0.469s, learning 2.070s)
               Value function loss: 90313.6083
                    Surrogate loss: 0.0120
             Mean action noise std: 0.88
                       Mean reward: 9919.83
               Mean episode length: 399.70
                 Mean success rate: 79.50
                  Mean reward/step: 25.12
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 18857984
                    Iteration time: 2.54s
                        Total time: 6062.73s
                               ETA: 520675.9s

################################################################################
                    [1m Learning iteration 2302/200000 [0m

                       Computation: 3270 steps/s (collection: 0.453s, learning 2.052s)
               Value function loss: 115986.2203
                    Surrogate loss: 0.0118
             Mean action noise std: 0.88
                       Mean reward: 10144.02
               Mean episode length: 405.46
                 Mean success rate: 80.50
                  Mean reward/step: 25.53
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 18866176
                    Iteration time: 2.50s
                        Total time: 6065.24s
                               ETA: 520662.1s

################################################################################
                    [1m Learning iteration 2303/200000 [0m

                       Computation: 3202 steps/s (collection: 0.488s, learning 2.070s)
               Value function loss: 98612.0514
                    Surrogate loss: 0.0146
             Mean action noise std: 0.88
                       Mean reward: 10397.74
               Mean episode length: 412.40
                 Mean success rate: 81.50
                  Mean reward/step: 24.70
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 2.56s
                        Total time: 6067.79s
                               ETA: 520653.0s

################################################################################
                    [1m Learning iteration 2304/200000 [0m

                       Computation: 3190 steps/s (collection: 0.498s, learning 2.070s)
               Value function loss: 83324.5696
                    Surrogate loss: 0.0147
             Mean action noise std: 0.88
                       Mean reward: 10560.32
               Mean episode length: 416.60
                 Mean success rate: 83.00
                  Mean reward/step: 24.41
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 18882560
                    Iteration time: 2.57s
                        Total time: 6070.36s
                               ETA: 520644.7s

################################################################################
                    [1m Learning iteration 2305/200000 [0m

                       Computation: 3256 steps/s (collection: 0.466s, learning 2.049s)
               Value function loss: 83369.1518
                    Surrogate loss: 0.0166
             Mean action noise std: 0.88
                       Mean reward: 10320.03
               Mean episode length: 409.17
                 Mean success rate: 81.00
                  Mean reward/step: 24.66
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 18890752
                    Iteration time: 2.52s
                        Total time: 6072.88s
                               ETA: 520631.9s

################################################################################
                    [1m Learning iteration 2306/200000 [0m

                       Computation: 3280 steps/s (collection: 0.478s, learning 2.019s)
               Value function loss: 67811.1139
                    Surrogate loss: 0.0120
             Mean action noise std: 0.88
                       Mean reward: 10360.23
               Mean episode length: 410.67
                 Mean success rate: 81.50
                  Mean reward/step: 24.95
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 18898944
                    Iteration time: 2.50s
                        Total time: 6075.37s
                               ETA: 520617.6s

################################################################################
                    [1m Learning iteration 2307/200000 [0m

                       Computation: 3222 steps/s (collection: 0.489s, learning 2.053s)
               Value function loss: 81969.4688
                    Surrogate loss: 0.0128
             Mean action noise std: 0.88
                       Mean reward: 10434.44
               Mean episode length: 413.74
                 Mean success rate: 82.00
                  Mean reward/step: 25.41
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 18907136
                    Iteration time: 2.54s
                        Total time: 6077.92s
                               ETA: 520607.2s

################################################################################
                    [1m Learning iteration 2308/200000 [0m

                       Computation: 3200 steps/s (collection: 0.502s, learning 2.057s)
               Value function loss: 113895.7899
                    Surrogate loss: 0.0119
             Mean action noise std: 0.88
                       Mean reward: 10007.71
               Mean episode length: 399.17
                 Mean success rate: 79.00
                  Mean reward/step: 24.92
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 18915328
                    Iteration time: 2.56s
                        Total time: 6080.47s
                               ETA: 520598.2s

################################################################################
                    [1m Learning iteration 2309/200000 [0m

                       Computation: 3240 steps/s (collection: 0.445s, learning 2.084s)
               Value function loss: 56875.9719
                    Surrogate loss: 0.0106
             Mean action noise std: 0.88
                       Mean reward: 9726.82
               Mean episode length: 394.44
                 Mean success rate: 76.50
                  Mean reward/step: 24.11
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 18923520
                    Iteration time: 2.53s
                        Total time: 6083.00s
                               ETA: 520586.6s

################################################################################
                    [1m Learning iteration 2310/200000 [0m

                       Computation: 3250 steps/s (collection: 0.462s, learning 2.058s)
               Value function loss: 95180.6964
                    Surrogate loss: 0.0122
             Mean action noise std: 0.88
                       Mean reward: 10047.05
               Mean episode length: 406.10
                 Mean success rate: 78.50
                  Mean reward/step: 24.29
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 18931712
                    Iteration time: 2.52s
                        Total time: 6085.52s
                               ETA: 520574.3s

################################################################################
                    [1m Learning iteration 2311/200000 [0m

                       Computation: 3221 steps/s (collection: 0.470s, learning 2.073s)
               Value function loss: 98689.6121
                    Surrogate loss: 0.0139
             Mean action noise std: 0.88
                       Mean reward: 9843.71
               Mean episode length: 398.57
                 Mean success rate: 76.50
                  Mean reward/step: 24.84
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 18939904
                    Iteration time: 2.54s
                        Total time: 6088.07s
                               ETA: 520563.9s

################################################################################
                    [1m Learning iteration 2312/200000 [0m

                       Computation: 3281 steps/s (collection: 0.464s, learning 2.032s)
               Value function loss: 105657.5248
                    Surrogate loss: 0.0155
             Mean action noise std: 0.88
                       Mean reward: 9795.27
               Mean episode length: 399.78
                 Mean success rate: 76.50
                  Mean reward/step: 24.86
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 18948096
                    Iteration time: 2.50s
                        Total time: 6090.56s
                               ETA: 520549.6s

################################################################################
                    [1m Learning iteration 2313/200000 [0m

                       Computation: 3218 steps/s (collection: 0.472s, learning 2.074s)
               Value function loss: 102474.3867
                    Surrogate loss: 0.0118
             Mean action noise std: 0.88
                       Mean reward: 8965.83
               Mean episode length: 375.13
                 Mean success rate: 71.00
                  Mean reward/step: 24.59
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 18956288
                    Iteration time: 2.55s
                        Total time: 6093.11s
                               ETA: 520539.5s

################################################################################
                    [1m Learning iteration 2314/200000 [0m

                       Computation: 3218 steps/s (collection: 0.511s, learning 2.034s)
               Value function loss: 111783.7741
                    Surrogate loss: 0.0130
             Mean action noise std: 0.88
                       Mean reward: 9159.70
               Mean episode length: 379.67
                 Mean success rate: 71.50
                  Mean reward/step: 23.96
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 18964480
                    Iteration time: 2.55s
                        Total time: 6095.65s
                               ETA: 520529.4s

################################################################################
                    [1m Learning iteration 2315/200000 [0m

                       Computation: 3198 steps/s (collection: 0.477s, learning 2.084s)
               Value function loss: 88473.9814
                    Surrogate loss: 0.0117
             Mean action noise std: 0.88
                       Mean reward: 8963.23
               Mean episode length: 371.01
                 Mean success rate: 70.50
                  Mean reward/step: 23.91
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 2.56s
                        Total time: 6098.22s
                               ETA: 520520.6s

################################################################################
                    [1m Learning iteration 2316/200000 [0m

                       Computation: 3250 steps/s (collection: 0.447s, learning 2.073s)
               Value function loss: 84001.3244
                    Surrogate loss: 0.0105
             Mean action noise std: 0.88
                       Mean reward: 8987.16
               Mean episode length: 372.35
                 Mean success rate: 71.00
                  Mean reward/step: 24.52
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 18980864
                    Iteration time: 2.52s
                        Total time: 6100.74s
                               ETA: 520508.4s

################################################################################
                    [1m Learning iteration 2317/200000 [0m

                       Computation: 3256 steps/s (collection: 0.465s, learning 2.051s)
               Value function loss: 109650.3897
                    Surrogate loss: 0.0122
             Mean action noise std: 0.88
                       Mean reward: 9116.62
               Mean episode length: 374.60
                 Mean success rate: 71.50
                  Mean reward/step: 25.19
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 18989056
                    Iteration time: 2.52s
                        Total time: 6103.25s
                               ETA: 520495.7s

################################################################################
                    [1m Learning iteration 2318/200000 [0m

                       Computation: 3312 steps/s (collection: 0.436s, learning 2.037s)
               Value function loss: 86641.7225
                    Surrogate loss: 0.0131
             Mean action noise std: 0.88
                       Mean reward: 9530.18
               Mean episode length: 386.63
                 Mean success rate: 75.00
                  Mean reward/step: 25.06
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 18997248
                    Iteration time: 2.47s
                        Total time: 6105.72s
                               ETA: 520479.4s

################################################################################
                    [1m Learning iteration 2319/200000 [0m

                       Computation: 3214 steps/s (collection: 0.498s, learning 2.051s)
               Value function loss: 93406.0425
                    Surrogate loss: 0.0151
             Mean action noise std: 0.88
                       Mean reward: 9704.26
               Mean episode length: 392.60
                 Mean success rate: 77.00
                  Mean reward/step: 25.45
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 19005440
                    Iteration time: 2.55s
                        Total time: 6108.27s
                               ETA: 520469.6s

################################################################################
                    [1m Learning iteration 2320/200000 [0m

                       Computation: 3197 steps/s (collection: 0.477s, learning 2.085s)
               Value function loss: 84107.5621
                    Surrogate loss: 0.0134
             Mean action noise std: 0.88
                       Mean reward: 9974.29
               Mean episode length: 402.34
                 Mean success rate: 79.50
                  Mean reward/step: 25.23
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 19013632
                    Iteration time: 2.56s
                        Total time: 6110.83s
                               ETA: 520460.9s

################################################################################
                    [1m Learning iteration 2321/200000 [0m

                       Computation: 3229 steps/s (collection: 0.468s, learning 2.069s)
               Value function loss: 78976.6068
                    Surrogate loss: 0.0198
             Mean action noise std: 0.88
                       Mean reward: 9692.68
               Mean episode length: 394.21
                 Mean success rate: 78.00
                  Mean reward/step: 25.01
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 19021824
                    Iteration time: 2.54s
                        Total time: 6113.37s
                               ETA: 520450.0s

################################################################################
                    [1m Learning iteration 2322/200000 [0m

                       Computation: 3247 steps/s (collection: 0.461s, learning 2.062s)
               Value function loss: 57071.3940
                    Surrogate loss: 0.0172
             Mean action noise std: 0.88
                       Mean reward: 9732.31
               Mean episode length: 395.76
                 Mean success rate: 78.50
                  Mean reward/step: 25.41
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 19030016
                    Iteration time: 2.52s
                        Total time: 6115.89s
                               ETA: 520438.0s

################################################################################
                    [1m Learning iteration 2323/200000 [0m

                       Computation: 3278 steps/s (collection: 0.474s, learning 2.025s)
               Value function loss: 86930.6424
                    Surrogate loss: 0.0121
             Mean action noise std: 0.88
                       Mean reward: 10014.65
               Mean episode length: 406.02
                 Mean success rate: 81.00
                  Mean reward/step: 26.32
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 19038208
                    Iteration time: 2.50s
                        Total time: 6118.39s
                               ETA: 520423.9s

################################################################################
                    [1m Learning iteration 2324/200000 [0m

                       Computation: 3246 steps/s (collection: 0.474s, learning 2.049s)
               Value function loss: 85979.9219
                    Surrogate loss: 0.0112
             Mean action noise std: 0.88
                       Mean reward: 10092.87
               Mean episode length: 408.61
                 Mean success rate: 81.00
                  Mean reward/step: 26.11
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 19046400
                    Iteration time: 2.52s
                        Total time: 6120.91s
                               ETA: 520412.0s

################################################################################
                    [1m Learning iteration 2325/200000 [0m

                       Computation: 3246 steps/s (collection: 0.447s, learning 2.076s)
               Value function loss: 99662.0724
                    Surrogate loss: 0.0124
             Mean action noise std: 0.88
                       Mean reward: 10308.83
               Mean episode length: 416.45
                 Mean success rate: 83.00
                  Mean reward/step: 25.39
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 19054592
                    Iteration time: 2.52s
                        Total time: 6123.44s
                               ETA: 520400.1s

################################################################################
                    [1m Learning iteration 2326/200000 [0m

                       Computation: 3340 steps/s (collection: 0.441s, learning 2.011s)
               Value function loss: 78927.0374
                    Surrogate loss: 0.0158
             Mean action noise std: 0.88
                       Mean reward: 10566.86
               Mean episode length: 424.37
                 Mean success rate: 84.50
                  Mean reward/step: 25.21
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 19062784
                    Iteration time: 2.45s
                        Total time: 6125.89s
                               ETA: 520382.2s

################################################################################
                    [1m Learning iteration 2327/200000 [0m

                       Computation: 3202 steps/s (collection: 0.474s, learning 2.084s)
               Value function loss: 104176.9229
                    Surrogate loss: 0.0102
             Mean action noise std: 0.88
                       Mean reward: 10507.66
               Mean episode length: 424.70
                 Mean success rate: 83.50
                  Mean reward/step: 24.34
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 2.56s
                        Total time: 6128.45s
                               ETA: 520373.2s

################################################################################
                    [1m Learning iteration 2328/200000 [0m

                       Computation: 3300 steps/s (collection: 0.441s, learning 2.041s)
               Value function loss: 108371.0682
                    Surrogate loss: 0.0109
             Mean action noise std: 0.88
                       Mean reward: 10769.72
               Mean episode length: 434.05
                 Mean success rate: 85.50
                  Mean reward/step: 24.36
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 19079168
                    Iteration time: 2.48s
                        Total time: 6130.93s
                               ETA: 520357.8s

################################################################################
                    [1m Learning iteration 2329/200000 [0m

                       Computation: 3293 steps/s (collection: 0.467s, learning 2.021s)
               Value function loss: 119103.0590
                    Surrogate loss: 0.0136
             Mean action noise std: 0.88
                       Mean reward: 10707.52
               Mean episode length: 430.58
                 Mean success rate: 84.50
                  Mean reward/step: 24.09
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 19087360
                    Iteration time: 2.49s
                        Total time: 6133.42s
                               ETA: 520342.9s

################################################################################
                    [1m Learning iteration 2330/200000 [0m

                       Computation: 3268 steps/s (collection: 0.475s, learning 2.032s)
               Value function loss: 102203.5391
                    Surrogate loss: 0.0137
             Mean action noise std: 0.88
                       Mean reward: 10647.53
               Mean episode length: 429.69
                 Mean success rate: 84.00
                  Mean reward/step: 23.38
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 19095552
                    Iteration time: 2.51s
                        Total time: 6135.92s
                               ETA: 520329.6s

################################################################################
                    [1m Learning iteration 2331/200000 [0m

                       Computation: 3304 steps/s (collection: 0.458s, learning 2.021s)
               Value function loss: 62738.1527
                    Surrogate loss: 0.0144
             Mean action noise std: 0.88
                       Mean reward: 10625.60
               Mean episode length: 427.92
                 Mean success rate: 83.00
                  Mean reward/step: 23.62
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 19103744
                    Iteration time: 2.48s
                        Total time: 6138.40s
                               ETA: 520314.0s

################################################################################
                    [1m Learning iteration 2332/200000 [0m

                       Computation: 3283 steps/s (collection: 0.486s, learning 2.009s)
               Value function loss: 90511.3283
                    Surrogate loss: 0.0125
             Mean action noise std: 0.88
                       Mean reward: 10557.51
               Mean episode length: 421.58
                 Mean success rate: 81.50
                  Mean reward/step: 24.93
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 19111936
                    Iteration time: 2.49s
                        Total time: 6140.90s
                               ETA: 520299.7s

################################################################################
                    [1m Learning iteration 2333/200000 [0m

                       Computation: 3275 steps/s (collection: 0.458s, learning 2.043s)
               Value function loss: 86433.4195
                    Surrogate loss: 0.0121
             Mean action noise std: 0.88
                       Mean reward: 10773.79
               Mean episode length: 425.17
                 Mean success rate: 82.00
                  Mean reward/step: 24.65
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 19120128
                    Iteration time: 2.50s
                        Total time: 6143.40s
                               ETA: 520285.9s

################################################################################
                    [1m Learning iteration 2334/200000 [0m

                       Computation: 3219 steps/s (collection: 0.513s, learning 2.032s)
               Value function loss: 97768.7376
                    Surrogate loss: 0.0141
             Mean action noise std: 0.88
                       Mean reward: 10903.50
               Mean episode length: 427.94
                 Mean success rate: 83.00
                  Mean reward/step: 24.33
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 19128320
                    Iteration time: 2.54s
                        Total time: 6145.94s
                               ETA: 520275.9s

################################################################################
                    [1m Learning iteration 2335/200000 [0m

                       Computation: 3220 steps/s (collection: 0.497s, learning 2.046s)
               Value function loss: 104695.2453
                    Surrogate loss: 0.0142
             Mean action noise std: 0.88
                       Mean reward: 10778.83
               Mean episode length: 423.95
                 Mean success rate: 82.00
                  Mean reward/step: 24.81
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 19136512
                    Iteration time: 2.54s
                        Total time: 6148.49s
                               ETA: 520265.8s

################################################################################
                    [1m Learning iteration 2336/200000 [0m

                       Computation: 3221 steps/s (collection: 0.476s, learning 2.067s)
               Value function loss: 108480.1933
                    Surrogate loss: 0.0129
             Mean action noise std: 0.88
                       Mean reward: 10412.20
               Mean episode length: 413.66
                 Mean success rate: 80.00
                  Mean reward/step: 24.12
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 19144704
                    Iteration time: 2.54s
                        Total time: 6151.03s
                               ETA: 520255.6s

################################################################################
                    [1m Learning iteration 2337/200000 [0m

                       Computation: 3227 steps/s (collection: 0.482s, learning 2.056s)
               Value function loss: 67989.3939
                    Surrogate loss: 0.0134
             Mean action noise std: 0.88
                       Mean reward: 10366.81
               Mean episode length: 410.13
                 Mean success rate: 79.50
                  Mean reward/step: 24.52
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 19152896
                    Iteration time: 2.54s
                        Total time: 6153.57s
                               ETA: 520245.0s

################################################################################
                    [1m Learning iteration 2338/200000 [0m

                       Computation: 3253 steps/s (collection: 0.422s, learning 2.096s)
               Value function loss: 79815.0669
                    Surrogate loss: 0.0129
             Mean action noise std: 0.88
                       Mean reward: 10366.68
               Mean episode length: 410.13
                 Mean success rate: 79.50
                  Mean reward/step: 25.56
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 19161088
                    Iteration time: 2.52s
                        Total time: 6156.09s
                               ETA: 520232.8s

################################################################################
                    [1m Learning iteration 2339/200000 [0m

                       Computation: 3174 steps/s (collection: 0.503s, learning 2.077s)
               Value function loss: 112696.6793
                    Surrogate loss: 0.0102
             Mean action noise std: 0.88
                       Mean reward: 10172.14
               Mean episode length: 406.19
                 Mean success rate: 78.50
                  Mean reward/step: 26.76
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 2.58s
                        Total time: 6158.67s
                               ETA: 520225.8s

################################################################################
                    [1m Learning iteration 2340/200000 [0m

                       Computation: 3217 steps/s (collection: 0.503s, learning 2.043s)
               Value function loss: 100995.0273
                    Surrogate loss: 0.0132
             Mean action noise std: 0.88
                       Mean reward: 10294.38
               Mean episode length: 411.23
                 Mean success rate: 80.00
                  Mean reward/step: 26.34
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 19177472
                    Iteration time: 2.55s
                        Total time: 6161.21s
                               ETA: 520215.9s

################################################################################
                    [1m Learning iteration 2341/200000 [0m

                       Computation: 3132 steps/s (collection: 0.562s, learning 2.053s)
               Value function loss: 100841.6225
                    Surrogate loss: 0.0110
             Mean action noise std: 0.88
                       Mean reward: 10696.62
               Mean episode length: 420.98
                 Mean success rate: 82.50
                  Mean reward/step: 25.57
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 19185664
                    Iteration time: 2.61s
                        Total time: 6163.83s
                               ETA: 520211.8s

################################################################################
                    [1m Learning iteration 2342/200000 [0m

                       Computation: 3191 steps/s (collection: 0.500s, learning 2.067s)
               Value function loss: 113984.2383
                    Surrogate loss: 0.0141
             Mean action noise std: 0.88
                       Mean reward: 10674.69
               Mean episode length: 423.74
                 Mean success rate: 83.00
                  Mean reward/step: 25.19
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 19193856
                    Iteration time: 2.57s
                        Total time: 6166.39s
                               ETA: 520203.7s

################################################################################
                    [1m Learning iteration 2343/200000 [0m

                       Computation: 3246 steps/s (collection: 0.482s, learning 2.041s)
               Value function loss: 70945.3806
                    Surrogate loss: 0.0093
             Mean action noise std: 0.88
                       Mean reward: 10686.92
               Mean episode length: 426.56
                 Mean success rate: 83.50
                  Mean reward/step: 24.72
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 19202048
                    Iteration time: 2.52s
                        Total time: 6168.92s
                               ETA: 520191.9s

################################################################################
                    [1m Learning iteration 2344/200000 [0m

                       Computation: 3284 steps/s (collection: 0.463s, learning 2.031s)
               Value function loss: 93845.8719
                    Surrogate loss: 0.0125
             Mean action noise std: 0.88
                       Mean reward: 10510.66
               Mean episode length: 423.41
                 Mean success rate: 83.50
                  Mean reward/step: 24.78
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 19210240
                    Iteration time: 2.49s
                        Total time: 6171.41s
                               ETA: 520177.7s

################################################################################
                    [1m Learning iteration 2345/200000 [0m

                       Computation: 3238 steps/s (collection: 0.484s, learning 2.045s)
               Value function loss: 104644.6875
                    Surrogate loss: 0.0168
             Mean action noise std: 0.88
                       Mean reward: 10656.74
               Mean episode length: 429.58
                 Mean success rate: 85.00
                  Mean reward/step: 24.74
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 19218432
                    Iteration time: 2.53s
                        Total time: 6173.94s
                               ETA: 520166.4s

################################################################################
                    [1m Learning iteration 2346/200000 [0m

                       Computation: 3249 steps/s (collection: 0.473s, learning 2.048s)
               Value function loss: 88293.0019
                    Surrogate loss: 0.0133
             Mean action noise std: 0.88
                       Mean reward: 10526.40
               Mean episode length: 426.13
                 Mean success rate: 84.50
                  Mean reward/step: 24.54
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 19226624
                    Iteration time: 2.52s
                        Total time: 6176.46s
                               ETA: 520154.4s

################################################################################
                    [1m Learning iteration 2347/200000 [0m

                       Computation: 3221 steps/s (collection: 0.474s, learning 2.069s)
               Value function loss: 68531.5014
                    Surrogate loss: 0.0130
             Mean action noise std: 0.88
                       Mean reward: 10635.25
               Mean episode length: 429.30
                 Mean success rate: 85.00
                  Mean reward/step: 25.04
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 19234816
                    Iteration time: 2.54s
                        Total time: 6179.00s
                               ETA: 520144.3s

################################################################################
                    [1m Learning iteration 2348/200000 [0m

                       Computation: 3256 steps/s (collection: 0.473s, learning 2.043s)
               Value function loss: 92585.7630
                    Surrogate loss: 0.0112
             Mean action noise std: 0.88
                       Mean reward: 10510.48
               Mean episode length: 428.68
                 Mean success rate: 85.00
                  Mean reward/step: 25.19
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 19243008
                    Iteration time: 2.52s
                        Total time: 6181.52s
                               ETA: 520131.9s

################################################################################
                    [1m Learning iteration 2349/200000 [0m

                       Computation: 3253 steps/s (collection: 0.467s, learning 2.051s)
               Value function loss: 109033.7536
                    Surrogate loss: 0.0143
             Mean action noise std: 0.88
                       Mean reward: 10717.52
               Mean episode length: 433.53
                 Mean success rate: 86.00
                  Mean reward/step: 25.82
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 19251200
                    Iteration time: 2.52s
                        Total time: 6184.04s
                               ETA: 520119.7s

################################################################################
                    [1m Learning iteration 2350/200000 [0m

                       Computation: 3245 steps/s (collection: 0.483s, learning 2.041s)
               Value function loss: 96601.7912
                    Surrogate loss: 0.0133
             Mean action noise std: 0.88
                       Mean reward: 10344.18
               Mean episode length: 419.52
                 Mean success rate: 83.50
                  Mean reward/step: 25.37
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 19259392
                    Iteration time: 2.52s
                        Total time: 6186.56s
                               ETA: 520108.0s

################################################################################
                    [1m Learning iteration 2351/200000 [0m

                       Computation: 3215 steps/s (collection: 0.455s, learning 2.093s)
               Value function loss: 128101.2392
                    Surrogate loss: 0.0118
             Mean action noise std: 0.88
                       Mean reward: 10355.50
               Mean episode length: 420.43
                 Mean success rate: 83.50
                  Mean reward/step: 24.75
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 2.55s
                        Total time: 6189.11s
                               ETA: 520098.3s

################################################################################
                    [1m Learning iteration 2352/200000 [0m

                       Computation: 3290 steps/s (collection: 0.457s, learning 2.033s)
               Value function loss: 92682.9038
                    Surrogate loss: 0.0127
             Mean action noise std: 0.88
                       Mean reward: 10441.04
               Mean episode length: 421.31
                 Mean success rate: 84.50
                  Mean reward/step: 23.97
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 19275776
                    Iteration time: 2.49s
                        Total time: 6191.60s
                               ETA: 520083.8s

################################################################################
                    [1m Learning iteration 2353/200000 [0m

                       Computation: 3288 steps/s (collection: 0.451s, learning 2.040s)
               Value function loss: 59482.2019
                    Surrogate loss: 0.0120
             Mean action noise std: 0.88
                       Mean reward: 10230.44
               Mean episode length: 413.50
                 Mean success rate: 83.00
                  Mean reward/step: 24.67
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 19283968
                    Iteration time: 2.49s
                        Total time: 6194.09s
                               ETA: 520069.4s

################################################################################
                    [1m Learning iteration 2354/200000 [0m

                       Computation: 3188 steps/s (collection: 0.498s, learning 2.072s)
               Value function loss: 70959.3297
                    Surrogate loss: 0.0101
             Mean action noise std: 0.88
                       Mean reward: 10404.34
               Mean episode length: 416.74
                 Mean success rate: 83.50
                  Mean reward/step: 25.17
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 19292160
                    Iteration time: 2.57s
                        Total time: 6196.66s
                               ETA: 520061.5s

################################################################################
                    [1m Learning iteration 2355/200000 [0m

                       Computation: 3222 steps/s (collection: 0.473s, learning 2.069s)
               Value function loss: 115890.4139
                    Surrogate loss: 0.0121
             Mean action noise std: 0.88
                       Mean reward: 10424.68
               Mean episode length: 415.12
                 Mean success rate: 83.00
                  Mean reward/step: 25.55
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 19300352
                    Iteration time: 2.54s
                        Total time: 6199.20s
                               ETA: 520051.4s

################################################################################
                    [1m Learning iteration 2356/200000 [0m

                       Computation: 3265 steps/s (collection: 0.462s, learning 2.047s)
               Value function loss: 87354.2191
                    Surrogate loss: 0.0109
             Mean action noise std: 0.89
                       Mean reward: 10353.13
               Mean episode length: 413.93
                 Mean success rate: 82.00
                  Mean reward/step: 25.29
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 19308544
                    Iteration time: 2.51s
                        Total time: 6201.71s
                               ETA: 520038.5s

################################################################################
                    [1m Learning iteration 2357/200000 [0m

                       Computation: 3307 steps/s (collection: 0.436s, learning 2.041s)
               Value function loss: 98727.9803
                    Surrogate loss: 0.0114
             Mean action noise std: 0.88
                       Mean reward: 10557.28
               Mean episode length: 420.00
                 Mean success rate: 83.50
                  Mean reward/step: 25.05
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 19316736
                    Iteration time: 2.48s
                        Total time: 6204.19s
                               ETA: 520022.9s

################################################################################
                    [1m Learning iteration 2358/200000 [0m

                       Computation: 3243 steps/s (collection: 0.489s, learning 2.036s)
               Value function loss: 103306.9200
                    Surrogate loss: 0.0101
             Mean action noise std: 0.89
                       Mean reward: 10332.01
               Mean episode length: 409.83
                 Mean success rate: 82.00
                  Mean reward/step: 25.02
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 19324928
                    Iteration time: 2.53s
                        Total time: 6206.71s
                               ETA: 520011.4s

################################################################################
                    [1m Learning iteration 2359/200000 [0m

                       Computation: 3233 steps/s (collection: 0.490s, learning 2.044s)
               Value function loss: 94517.1775
                    Surrogate loss: 0.0128
             Mean action noise std: 0.89
                       Mean reward: 10479.22
               Mean episode length: 412.41
                 Mean success rate: 82.00
                  Mean reward/step: 24.49
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 19333120
                    Iteration time: 2.53s
                        Total time: 6209.25s
                               ETA: 520000.6s

################################################################################
                    [1m Learning iteration 2360/200000 [0m

                       Computation: 3299 steps/s (collection: 0.449s, learning 2.034s)
               Value function loss: 95698.9767
                    Surrogate loss: 0.0124
             Mean action noise std: 0.89
                       Mean reward: 10686.33
               Mean episode length: 418.14
                 Mean success rate: 83.00
                  Mean reward/step: 24.67
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 19341312
                    Iteration time: 2.48s
                        Total time: 6211.73s
                               ETA: 519985.5s

################################################################################
                    [1m Learning iteration 2361/200000 [0m

                       Computation: 3311 steps/s (collection: 0.448s, learning 2.026s)
               Value function loss: 86003.8002
                    Surrogate loss: 0.0133
             Mean action noise std: 0.89
                       Mean reward: 10684.69
               Mean episode length: 417.33
                 Mean success rate: 83.00
                  Mean reward/step: 24.57
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 19349504
                    Iteration time: 2.47s
                        Total time: 6214.20s
                               ETA: 519969.8s

################################################################################
                    [1m Learning iteration 2362/200000 [0m

                       Computation: 3356 steps/s (collection: 0.435s, learning 2.005s)
               Value function loss: 87487.3567
                    Surrogate loss: 0.0114
             Mean action noise std: 0.89
                       Mean reward: 10313.55
               Mean episode length: 409.04
                 Mean success rate: 80.50
                  Mean reward/step: 23.92
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 19357696
                    Iteration time: 2.44s
                        Total time: 6216.64s
                               ETA: 519951.2s

################################################################################
                    [1m Learning iteration 2363/200000 [0m

                       Computation: 3361 steps/s (collection: 0.430s, learning 2.008s)
               Value function loss: 90534.9204
                    Surrogate loss: 0.0107
             Mean action noise std: 0.89
                       Mean reward: 10397.80
               Mean episode length: 415.44
                 Mean success rate: 81.50
                  Mean reward/step: 24.62
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 2.44s
                        Total time: 6219.08s
                               ETA: 519932.4s

################################################################################
                    [1m Learning iteration 2364/200000 [0m

                       Computation: 3296 steps/s (collection: 0.441s, learning 2.044s)
               Value function loss: 83868.6955
                    Surrogate loss: 0.0096
             Mean action noise std: 0.89
                       Mean reward: 10390.58
               Mean episode length: 418.43
                 Mean success rate: 81.50
                  Mean reward/step: 25.31
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 19374080
                    Iteration time: 2.49s
                        Total time: 6221.56s
                               ETA: 519917.6s

################################################################################
                    [1m Learning iteration 2365/200000 [0m

                       Computation: 3297 steps/s (collection: 0.465s, learning 2.019s)
               Value function loss: 117473.5852
                    Surrogate loss: 0.0141
             Mean action noise std: 0.89
                       Mean reward: 9888.26
               Mean episode length: 403.24
                 Mean success rate: 78.50
                  Mean reward/step: 25.28
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 19382272
                    Iteration time: 2.48s
                        Total time: 6224.05s
                               ETA: 519902.7s

################################################################################
                    [1m Learning iteration 2366/200000 [0m

                       Computation: 3240 steps/s (collection: 0.489s, learning 2.039s)
               Value function loss: 98442.3117
                    Surrogate loss: 0.0136
             Mean action noise std: 0.89
                       Mean reward: 9816.92
               Mean episode length: 397.42
                 Mean success rate: 78.50
                  Mean reward/step: 24.15
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 19390464
                    Iteration time: 2.53s
                        Total time: 6226.58s
                               ETA: 519891.5s

################################################################################
                    [1m Learning iteration 2367/200000 [0m

                       Computation: 3254 steps/s (collection: 0.499s, learning 2.017s)
               Value function loss: 124847.0398
                    Surrogate loss: 0.0106
             Mean action noise std: 0.89
                       Mean reward: 9704.81
               Mean episode length: 396.78
                 Mean success rate: 78.00
                  Mean reward/step: 23.80
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 19398656
                    Iteration time: 2.52s
                        Total time: 6229.09s
                               ETA: 519879.4s

################################################################################
                    [1m Learning iteration 2368/200000 [0m

                       Computation: 3227 steps/s (collection: 0.488s, learning 2.050s)
               Value function loss: 90920.1130
                    Surrogate loss: 0.0108
             Mean action noise std: 0.89
                       Mean reward: 9677.29
               Mean episode length: 397.56
                 Mean success rate: 78.00
                  Mean reward/step: 23.78
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 19406848
                    Iteration time: 2.54s
                        Total time: 6231.63s
                               ETA: 519869.1s

################################################################################
                    [1m Learning iteration 2369/200000 [0m

                       Computation: 3271 steps/s (collection: 0.465s, learning 2.039s)
               Value function loss: 82023.8313
                    Surrogate loss: 0.0131
             Mean action noise std: 0.89
                       Mean reward: 9666.94
               Mean episode length: 396.18
                 Mean success rate: 77.50
                  Mean reward/step: 24.06
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 19415040
                    Iteration time: 2.50s
                        Total time: 6234.14s
                               ETA: 519855.9s

################################################################################
                    [1m Learning iteration 2370/200000 [0m

                       Computation: 3214 steps/s (collection: 0.499s, learning 2.049s)
               Value function loss: 79807.4938
                    Surrogate loss: 0.0121
             Mean action noise std: 0.89
                       Mean reward: 9374.96
               Mean episode length: 387.75
                 Mean success rate: 75.50
                  Mean reward/step: 24.78
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 19423232
                    Iteration time: 2.55s
                        Total time: 6236.68s
                               ETA: 519846.4s

################################################################################
                    [1m Learning iteration 2371/200000 [0m

                       Computation: 3171 steps/s (collection: 0.537s, learning 2.045s)
               Value function loss: 73792.1655
                    Surrogate loss: 0.0120
             Mean action noise std: 0.89
                       Mean reward: 9049.55
               Mean episode length: 374.71
                 Mean success rate: 73.00
                  Mean reward/step: 25.23
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 19431424
                    Iteration time: 2.58s
                        Total time: 6239.27s
                               ETA: 519839.8s

################################################################################
                    [1m Learning iteration 2372/200000 [0m

                       Computation: 3196 steps/s (collection: 0.530s, learning 2.033s)
               Value function loss: 85144.9268
                    Surrogate loss: 0.0149
             Mean action noise std: 0.89
                       Mean reward: 9342.30
               Mean episode length: 382.25
                 Mean success rate: 75.50
                  Mean reward/step: 24.99
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 19439616
                    Iteration time: 2.56s
                        Total time: 6241.83s
                               ETA: 519831.6s

################################################################################
                    [1m Learning iteration 2373/200000 [0m

                       Computation: 3297 steps/s (collection: 0.459s, learning 2.025s)
               Value function loss: 66906.4875
                    Surrogate loss: 0.0148
             Mean action noise std: 0.89
                       Mean reward: 9176.20
               Mean episode length: 375.98
                 Mean success rate: 74.00
                  Mean reward/step: 24.94
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 19447808
                    Iteration time: 2.48s
                        Total time: 6244.31s
                               ETA: 519816.8s

################################################################################
                    [1m Learning iteration 2374/200000 [0m

                       Computation: 3138 steps/s (collection: 0.563s, learning 2.047s)
               Value function loss: 118852.0898
                    Surrogate loss: 0.0118
             Mean action noise std: 0.89
                       Mean reward: 9369.64
               Mean episode length: 382.37
                 Mean success rate: 75.50
                  Mean reward/step: 24.19
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 19456000
                    Iteration time: 2.61s
                        Total time: 6246.92s
                               ETA: 519812.5s

################################################################################
                    [1m Learning iteration 2375/200000 [0m

                       Computation: 3166 steps/s (collection: 0.533s, learning 2.054s)
               Value function loss: 82546.1600
                    Surrogate loss: 0.0139
             Mean action noise std: 0.89
                       Mean reward: 9018.00
               Mean episode length: 373.92
                 Mean success rate: 73.50
                  Mean reward/step: 23.57
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 2.59s
                        Total time: 6249.51s
                               ETA: 519806.3s

################################################################################
                    [1m Learning iteration 2376/200000 [0m

                       Computation: 3235 steps/s (collection: 0.492s, learning 2.039s)
               Value function loss: 106413.6767
                    Surrogate loss: 0.0135
             Mean action noise std: 0.89
                       Mean reward: 9418.25
               Mean episode length: 385.12
                 Mean success rate: 75.00
                  Mean reward/step: 23.35
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 19472384
                    Iteration time: 2.53s
                        Total time: 6252.04s
                               ETA: 519795.4s

################################################################################
                    [1m Learning iteration 2377/200000 [0m

                       Computation: 3250 steps/s (collection: 0.474s, learning 2.046s)
               Value function loss: 107267.0746
                    Surrogate loss: 0.0112
             Mean action noise std: 0.89
                       Mean reward: 9348.00
               Mean episode length: 380.69
                 Mean success rate: 74.00
                  Mean reward/step: 23.66
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 19480576
                    Iteration time: 2.52s
                        Total time: 6254.56s
                               ETA: 519783.6s

################################################################################
                    [1m Learning iteration 2378/200000 [0m

                       Computation: 3107 steps/s (collection: 0.563s, learning 2.073s)
               Value function loss: 85062.1763
                    Surrogate loss: 0.0120
             Mean action noise std: 0.89
                       Mean reward: 9188.84
               Mean episode length: 376.67
                 Mean success rate: 74.00
                  Mean reward/step: 24.19
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 19488768
                    Iteration time: 2.64s
                        Total time: 6257.20s
                               ETA: 519781.5s

################################################################################
                    [1m Learning iteration 2379/200000 [0m

                       Computation: 3176 steps/s (collection: 0.557s, learning 2.022s)
               Value function loss: 76879.8368
                    Surrogate loss: 0.0149
             Mean action noise std: 0.89
                       Mean reward: 8962.81
               Mean episode length: 370.39
                 Mean success rate: 73.00
                  Mean reward/step: 24.07
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 19496960
                    Iteration time: 2.58s
                        Total time: 6259.78s
                               ETA: 519774.6s

################################################################################
                    [1m Learning iteration 2380/200000 [0m

                       Computation: 3222 steps/s (collection: 0.492s, learning 2.050s)
               Value function loss: 112163.0055
                    Surrogate loss: 0.0110
             Mean action noise std: 0.89
                       Mean reward: 8900.46
               Mean episode length: 369.13
                 Mean success rate: 73.00
                  Mean reward/step: 24.25
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 19505152
                    Iteration time: 2.54s
                        Total time: 6262.32s
                               ETA: 519764.7s

################################################################################
                    [1m Learning iteration 2381/200000 [0m

                       Computation: 3213 steps/s (collection: 0.472s, learning 2.077s)
               Value function loss: 87531.5637
                    Surrogate loss: 0.0146
             Mean action noise std: 0.89
                       Mean reward: 8834.69
               Mean episode length: 366.17
                 Mean success rate: 72.00
                  Mean reward/step: 24.33
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 19513344
                    Iteration time: 2.55s
                        Total time: 6264.87s
                               ETA: 519755.3s

################################################################################
                    [1m Learning iteration 2382/200000 [0m

                       Computation: 3077 steps/s (collection: 0.556s, learning 2.106s)
               Value function loss: 112645.8131
                    Surrogate loss: 0.0235
             Mean action noise std: 0.89
                       Mean reward: 8944.68
               Mean episode length: 370.46
                 Mean success rate: 73.50
                  Mean reward/step: 24.14
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 19521536
                    Iteration time: 2.66s
                        Total time: 6267.53s
                               ETA: 519755.3s

################################################################################
                    [1m Learning iteration 2383/200000 [0m

                       Computation: 3122 steps/s (collection: 0.562s, learning 2.062s)
               Value function loss: 106792.4154
                    Surrogate loss: 0.0173
             Mean action noise std: 0.89
                       Mean reward: 9131.57
               Mean episode length: 374.69
                 Mean success rate: 73.50
                  Mean reward/step: 23.14
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 19529728
                    Iteration time: 2.62s
                        Total time: 6270.15s
                               ETA: 519752.1s

################################################################################
                    [1m Learning iteration 2384/200000 [0m

                       Computation: 3195 steps/s (collection: 0.494s, learning 2.070s)
               Value function loss: 87510.2593
                    Surrogate loss: 0.0161
             Mean action noise std: 0.89
                       Mean reward: 9162.17
               Mean episode length: 377.43
                 Mean success rate: 73.50
                  Mean reward/step: 22.81
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 19537920
                    Iteration time: 2.56s
                        Total time: 6272.72s
                               ETA: 519744.0s

################################################################################
                    [1m Learning iteration 2385/200000 [0m

                       Computation: 3198 steps/s (collection: 0.502s, learning 2.059s)
               Value function loss: 60886.8432
                    Surrogate loss: 0.0175
             Mean action noise std: 0.89
                       Mean reward: 8786.72
               Mean episode length: 364.96
                 Mean success rate: 72.00
                  Mean reward/step: 23.37
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 19546112
                    Iteration time: 2.56s
                        Total time: 6275.28s
                               ETA: 519735.7s

################################################################################
                    [1m Learning iteration 2386/200000 [0m

                       Computation: 3191 steps/s (collection: 0.508s, learning 2.058s)
               Value function loss: 98609.9729
                    Surrogate loss: 0.0121
             Mean action noise std: 0.89
                       Mean reward: 8827.04
               Mean episode length: 365.09
                 Mean success rate: 72.50
                  Mean reward/step: 24.41
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 19554304
                    Iteration time: 2.57s
                        Total time: 6277.85s
                               ETA: 519727.8s

################################################################################
                    [1m Learning iteration 2387/200000 [0m

                       Computation: 3199 steps/s (collection: 0.508s, learning 2.052s)
               Value function loss: 86057.3319
                    Surrogate loss: 0.0108
             Mean action noise std: 0.89
                       Mean reward: 8984.33
               Mean episode length: 371.54
                 Mean success rate: 73.50
                  Mean reward/step: 24.36
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 2.56s
                        Total time: 6280.41s
                               ETA: 519719.4s

################################################################################
                    [1m Learning iteration 2388/200000 [0m

                       Computation: 3270 steps/s (collection: 0.468s, learning 2.037s)
               Value function loss: 55476.7280
                    Surrogate loss: 0.0106
             Mean action noise std: 0.89
                       Mean reward: 8953.70
               Mean episode length: 370.49
                 Mean success rate: 74.00
                  Mean reward/step: 24.58
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 19570688
                    Iteration time: 2.50s
                        Total time: 6282.91s
                               ETA: 519706.4s

################################################################################
                    [1m Learning iteration 2389/200000 [0m

                       Computation: 3201 steps/s (collection: 0.482s, learning 2.077s)
               Value function loss: 93775.9584
                    Surrogate loss: 0.0125
             Mean action noise std: 0.89
                       Mean reward: 8690.94
               Mean episode length: 360.86
                 Mean success rate: 71.00
                  Mean reward/step: 24.68
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 19578880
                    Iteration time: 2.56s
                        Total time: 6285.47s
                               ETA: 519697.9s

################################################################################
                    [1m Learning iteration 2390/200000 [0m

                       Computation: 3088 steps/s (collection: 0.577s, learning 2.076s)
               Value function loss: 95101.8400
                    Surrogate loss: 0.0117
             Mean action noise std: 0.89
                       Mean reward: 8739.82
               Mean episode length: 361.24
                 Mean success rate: 71.50
                  Mean reward/step: 24.60
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 19587072
                    Iteration time: 2.65s
                        Total time: 6288.12s
                               ETA: 519697.2s

################################################################################
                    [1m Learning iteration 2391/200000 [0m

                       Computation: 3213 steps/s (collection: 0.500s, learning 2.050s)
               Value function loss: 73739.5628
                    Surrogate loss: 0.0127
             Mean action noise std: 0.89
                       Mean reward: 8329.79
               Mean episode length: 349.33
                 Mean success rate: 68.50
                  Mean reward/step: 24.27
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 19595264
                    Iteration time: 2.55s
                        Total time: 6290.67s
                               ETA: 519687.9s

################################################################################
                    [1m Learning iteration 2392/200000 [0m

                       Computation: 3190 steps/s (collection: 0.497s, learning 2.070s)
               Value function loss: 81890.6409
                    Surrogate loss: 0.0128
             Mean action noise std: 0.89
                       Mean reward: 8137.41
               Mean episode length: 342.82
                 Mean success rate: 67.00
                  Mean reward/step: 24.48
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 19603456
                    Iteration time: 2.57s
                        Total time: 6293.24s
                               ETA: 519680.1s

################################################################################
                    [1m Learning iteration 2393/200000 [0m

                       Computation: 3079 steps/s (collection: 0.520s, learning 2.140s)
               Value function loss: 100421.7652
                    Surrogate loss: 0.0105
             Mean action noise std: 0.89
                       Mean reward: 8248.89
               Mean episode length: 344.74
                 Mean success rate: 68.50
                  Mean reward/step: 24.70
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 19611648
                    Iteration time: 2.66s
                        Total time: 6295.90s
                               ETA: 519679.9s

################################################################################
                    [1m Learning iteration 2394/200000 [0m

                       Computation: 3042 steps/s (collection: 0.550s, learning 2.143s)
               Value function loss: 97638.9898
                    Surrogate loss: 0.0110
             Mean action noise std: 0.89
                       Mean reward: 8663.20
               Mean episode length: 357.61
                 Mean success rate: 70.50
                  Mean reward/step: 23.85
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 19619840
                    Iteration time: 2.69s
                        Total time: 6298.59s
                               ETA: 519682.5s

################################################################################
                    [1m Learning iteration 2395/200000 [0m

                       Computation: 3046 steps/s (collection: 0.541s, learning 2.148s)
               Value function loss: 105368.6515
                    Surrogate loss: 0.0137
             Mean action noise std: 0.89
                       Mean reward: 9042.34
               Mean episode length: 369.54
                 Mean success rate: 72.50
                  Mean reward/step: 24.04
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 19628032
                    Iteration time: 2.69s
                        Total time: 6301.28s
                               ETA: 519684.8s

################################################################################
                    [1m Learning iteration 2396/200000 [0m

                       Computation: 3074 steps/s (collection: 0.548s, learning 2.117s)
               Value function loss: 104360.4174
                    Surrogate loss: 0.0123
             Mean action noise std: 0.89
                       Mean reward: 8914.65
               Mean episode length: 368.97
                 Mean success rate: 71.50
                  Mean reward/step: 24.33
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 19636224
                    Iteration time: 2.66s
                        Total time: 6303.95s
                               ETA: 519685.0s

################################################################################
                    [1m Learning iteration 2397/200000 [0m

                       Computation: 3200 steps/s (collection: 0.483s, learning 2.077s)
               Value function loss: 91347.8555
                    Surrogate loss: 0.0115
             Mean action noise std: 0.89
                       Mean reward: 9187.48
               Mean episode length: 381.05
                 Mean success rate: 73.50
                  Mean reward/step: 24.28
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 19644416
                    Iteration time: 2.56s
                        Total time: 6306.51s
                               ETA: 519676.6s

################################################################################
                    [1m Learning iteration 2398/200000 [0m

                       Computation: 3110 steps/s (collection: 0.509s, learning 2.125s)
               Value function loss: 130296.5608
                    Surrogate loss: 0.0108
             Mean action noise std: 0.88
                       Mean reward: 9822.68
               Mean episode length: 401.63
                 Mean success rate: 78.50
                  Mean reward/step: 23.65
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 19652608
                    Iteration time: 2.63s
                        Total time: 6309.14s
                               ETA: 519674.2s

################################################################################
                    [1m Learning iteration 2399/200000 [0m

                       Computation: 3083 steps/s (collection: 0.537s, learning 2.120s)
               Value function loss: 67230.1620
                    Surrogate loss: 0.0122
             Mean action noise std: 0.88
                       Mean reward: 9664.86
               Mean episode length: 397.88
                 Mean success rate: 77.00
                  Mean reward/step: 23.66
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 2.66s
                        Total time: 6311.80s
                               ETA: 519673.8s

################################################################################
                    [1m Learning iteration 2400/200000 [0m

                       Computation: 3146 steps/s (collection: 0.471s, learning 2.133s)
               Value function loss: 108566.8501
                    Surrogate loss: 0.0144
             Mean action noise std: 0.88
                       Mean reward: 9817.14
               Mean episode length: 403.31
                 Mean success rate: 78.00
                  Mean reward/step: 24.22
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 19668992
                    Iteration time: 2.60s
                        Total time: 6314.40s
                               ETA: 519669.0s

################################################################################
                    [1m Learning iteration 2401/200000 [0m

                       Computation: 3189 steps/s (collection: 0.487s, learning 2.081s)
               Value function loss: 66280.6499
                    Surrogate loss: 0.0101
             Mean action noise std: 0.88
                       Mean reward: 9977.00
               Mean episode length: 406.44
                 Mean success rate: 79.00
                  Mean reward/step: 24.20
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 19677184
                    Iteration time: 2.57s
                        Total time: 6316.97s
                               ETA: 519661.3s

################################################################################
                    [1m Learning iteration 2402/200000 [0m

                       Computation: 3133 steps/s (collection: 0.545s, learning 2.069s)
               Value function loss: 70728.7716
                    Surrogate loss: 0.0148
             Mean action noise std: 0.88
                       Mean reward: 9956.73
               Mean episode length: 406.20
                 Mean success rate: 79.00
                  Mean reward/step: 23.92
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 19685376
                    Iteration time: 2.61s
                        Total time: 6319.58s
                               ETA: 519657.4s

################################################################################
                    [1m Learning iteration 2403/200000 [0m

                       Computation: 3117 steps/s (collection: 0.525s, learning 2.103s)
               Value function loss: 113427.3854
                    Surrogate loss: 0.0153
             Mean action noise std: 0.89
                       Mean reward: 10046.58
               Mean episode length: 409.95
                 Mean success rate: 79.50
                  Mean reward/step: 23.71
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 19693568
                    Iteration time: 2.63s
                        Total time: 6322.21s
                               ETA: 519654.6s

################################################################################
                    [1m Learning iteration 2404/200000 [0m

                       Computation: 3222 steps/s (collection: 0.461s, learning 2.081s)
               Value function loss: 72095.0277
                    Surrogate loss: 0.0128
             Mean action noise std: 0.88
                       Mean reward: 9305.50
               Mean episode length: 391.25
                 Mean success rate: 73.50
                  Mean reward/step: 23.69
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 19701760
                    Iteration time: 2.54s
                        Total time: 6324.75s
                               ETA: 519644.8s

################################################################################
                    [1m Learning iteration 2405/200000 [0m

                       Computation: 3208 steps/s (collection: 0.535s, learning 2.018s)
               Value function loss: 119154.5238
                    Surrogate loss: 0.0121
             Mean action noise std: 0.88
                       Mean reward: 9690.82
               Mean episode length: 399.19
                 Mean success rate: 75.50
                  Mean reward/step: 23.37
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 19709952
                    Iteration time: 2.55s
                        Total time: 6327.31s
                               ETA: 519635.9s

################################################################################
                    [1m Learning iteration 2406/200000 [0m

                       Computation: 3273 steps/s (collection: 0.487s, learning 2.015s)
               Value function loss: 112166.9099
                    Surrogate loss: 0.0134
             Mean action noise std: 0.88
                       Mean reward: 9763.46
               Mean episode length: 400.90
                 Mean success rate: 76.50
                  Mean reward/step: 23.16
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 19718144
                    Iteration time: 2.50s
                        Total time: 6329.81s
                               ETA: 519622.8s

################################################################################
                    [1m Learning iteration 2407/200000 [0m

                       Computation: 3307 steps/s (collection: 0.451s, learning 2.026s)
               Value function loss: 102204.5875
                    Surrogate loss: 0.0146
             Mean action noise std: 0.88
                       Mean reward: 9256.96
               Mean episode length: 381.81
                 Mean success rate: 73.00
                  Mean reward/step: 22.64
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 19726336
                    Iteration time: 2.48s
                        Total time: 6332.28s
                               ETA: 519607.6s

################################################################################
                    [1m Learning iteration 2408/200000 [0m

                       Computation: 3314 steps/s (collection: 0.433s, learning 2.039s)
               Value function loss: 76525.6221
                    Surrogate loss: 0.0116
             Mean action noise std: 0.88
                       Mean reward: 9365.92
               Mean episode length: 385.25
                 Mean success rate: 74.00
                  Mean reward/step: 23.10
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 19734528
                    Iteration time: 2.47s
                        Total time: 6334.76s
                               ETA: 519592.0s

################################################################################
                    [1m Learning iteration 2409/200000 [0m

                       Computation: 3246 steps/s (collection: 0.504s, learning 2.019s)
               Value function loss: 128563.0500
                    Surrogate loss: 0.0099
             Mean action noise std: 0.88
                       Mean reward: 9512.12
               Mean episode length: 392.08
                 Mean success rate: 74.50
                  Mean reward/step: 23.16
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 19742720
                    Iteration time: 2.52s
                        Total time: 6337.28s
                               ETA: 519580.6s

################################################################################
                    [1m Learning iteration 2410/200000 [0m

                       Computation: 3203 steps/s (collection: 0.502s, learning 2.055s)
               Value function loss: 71069.4503
                    Surrogate loss: 0.0098
             Mean action noise std: 0.89
                       Mean reward: 9032.75
               Mean episode length: 382.87
                 Mean success rate: 71.50
                  Mean reward/step: 23.28
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 19750912
                    Iteration time: 2.56s
                        Total time: 6339.84s
                               ETA: 519572.1s

################################################################################
                    [1m Learning iteration 2411/200000 [0m

                       Computation: 3322 steps/s (collection: 0.437s, learning 2.028s)
               Value function loss: 63653.5890
                    Surrogate loss: 0.0124
             Mean action noise std: 0.89
                       Mean reward: 8809.78
               Mean episode length: 374.42
                 Mean success rate: 70.00
                  Mean reward/step: 23.59
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 2.47s
                        Total time: 6342.30s
                               ETA: 519556.0s

################################################################################
                    [1m Learning iteration 2412/200000 [0m

                       Computation: 3267 steps/s (collection: 0.457s, learning 2.050s)
               Value function loss: 115486.1489
                    Surrogate loss: 0.0119
             Mean action noise std: 0.89
                       Mean reward: 8806.49
               Mean episode length: 373.71
                 Mean success rate: 70.50
                  Mean reward/step: 22.46
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 19767296
                    Iteration time: 2.51s
                        Total time: 6344.81s
                               ETA: 519543.3s

################################################################################
                    [1m Learning iteration 2413/200000 [0m

                       Computation: 3150 steps/s (collection: 0.517s, learning 2.083s)
               Value function loss: 100220.8003
                    Surrogate loss: 0.0138
             Mean action noise std: 0.89
                       Mean reward: 8956.53
               Mean episode length: 379.74
                 Mean success rate: 72.00
                  Mean reward/step: 22.09
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 19775488
                    Iteration time: 2.60s
                        Total time: 6347.41s
                               ETA: 519538.3s

################################################################################
                    [1m Learning iteration 2414/200000 [0m

                       Computation: 3158 steps/s (collection: 0.527s, learning 2.067s)
               Value function loss: 76067.7846
                    Surrogate loss: 0.0124
             Mean action noise std: 0.89
                       Mean reward: 8679.15
               Mean episode length: 372.30
                 Mean success rate: 70.00
                  Mean reward/step: 22.13
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 19783680
                    Iteration time: 2.59s
                        Total time: 6350.00s
                               ETA: 519532.7s

################################################################################
                    [1m Learning iteration 2415/200000 [0m

                       Computation: 3152 steps/s (collection: 0.519s, learning 2.080s)
               Value function loss: 80277.0012
                    Surrogate loss: 0.0141
             Mean action noise std: 0.88
                       Mean reward: 8613.21
               Mean episode length: 370.23
                 Mean success rate: 69.50
                  Mean reward/step: 22.96
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 19791872
                    Iteration time: 2.60s
                        Total time: 6352.60s
                               ETA: 519527.6s

################################################################################
                    [1m Learning iteration 2416/200000 [0m

                       Computation: 3101 steps/s (collection: 0.528s, learning 2.113s)
               Value function loss: 92709.3120
                    Surrogate loss: 0.0131
             Mean action noise std: 0.88
                       Mean reward: 8826.89
               Mean episode length: 380.94
                 Mean success rate: 71.00
                  Mean reward/step: 23.75
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 19800064
                    Iteration time: 2.64s
                        Total time: 6355.24s
                               ETA: 519525.9s

################################################################################
                    [1m Learning iteration 2417/200000 [0m

                       Computation: 3107 steps/s (collection: 0.536s, learning 2.100s)
               Value function loss: 75064.9171
                    Surrogate loss: 0.0106
             Mean action noise std: 0.88
                       Mean reward: 8799.95
               Mean episode length: 377.74
                 Mean success rate: 70.50
                  Mean reward/step: 23.79
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 19808256
                    Iteration time: 2.64s
                        Total time: 6357.88s
                               ETA: 519523.8s

################################################################################
                    [1m Learning iteration 2418/200000 [0m

                       Computation: 3148 steps/s (collection: 0.489s, learning 2.112s)
               Value function loss: 59142.2124
                    Surrogate loss: 0.0125
             Mean action noise std: 0.88
                       Mean reward: 8574.60
               Mean episode length: 371.01
                 Mean success rate: 70.00
                  Mean reward/step: 24.50
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 19816448
                    Iteration time: 2.60s
                        Total time: 6360.48s
                               ETA: 519518.9s

################################################################################
                    [1m Learning iteration 2419/200000 [0m

                       Computation: 3094 steps/s (collection: 0.545s, learning 2.102s)
               Value function loss: 83539.1004
                    Surrogate loss: 0.0131
             Mean action noise std: 0.88
                       Mean reward: 8641.08
               Mean episode length: 372.19
                 Mean success rate: 71.00
                  Mean reward/step: 24.83
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 19824640
                    Iteration time: 2.65s
                        Total time: 6363.13s
                               ETA: 519517.8s

################################################################################
                    [1m Learning iteration 2420/200000 [0m

                       Computation: 3131 steps/s (collection: 0.512s, learning 2.103s)
               Value function loss: 96107.3661
                    Surrogate loss: 0.0136
             Mean action noise std: 0.88
                       Mean reward: 8737.49
               Mean episode length: 373.68
                 Mean success rate: 72.00
                  Mean reward/step: 24.84
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 19832832
                    Iteration time: 2.62s
                        Total time: 6365.74s
                               ETA: 519514.0s

################################################################################
                    [1m Learning iteration 2421/200000 [0m

                       Computation: 3134 steps/s (collection: 0.510s, learning 2.104s)
               Value function loss: 90059.0552
                    Surrogate loss: 0.0123
             Mean action noise std: 0.88
                       Mean reward: 8984.26
               Mean episode length: 383.80
                 Mean success rate: 74.00
                  Mean reward/step: 24.92
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 19841024
                    Iteration time: 2.61s
                        Total time: 6368.36s
                               ETA: 519510.2s

################################################################################
                    [1m Learning iteration 2422/200000 [0m

                       Computation: 3091 steps/s (collection: 0.538s, learning 2.113s)
               Value function loss: 119938.5934
                    Surrogate loss: 0.0147
             Mean action noise std: 0.88
                       Mean reward: 9094.53
               Mean episode length: 389.00
                 Mean success rate: 75.50
                  Mean reward/step: 25.14
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 19849216
                    Iteration time: 2.65s
                        Total time: 6371.01s
                               ETA: 519509.2s

################################################################################
                    [1m Learning iteration 2423/200000 [0m

                       Computation: 3132 steps/s (collection: 0.517s, learning 2.098s)
               Value function loss: 81668.1529
                    Surrogate loss: 0.0169
             Mean action noise std: 0.88
                       Mean reward: 9339.27
               Mean episode length: 395.93
                 Mean success rate: 78.00
                  Mean reward/step: 24.12
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 2.62s
                        Total time: 6373.62s
                               ETA: 519505.4s

################################################################################
                    [1m Learning iteration 2424/200000 [0m

                       Computation: 3157 steps/s (collection: 0.509s, learning 2.086s)
               Value function loss: 103090.5307
                    Surrogate loss: 0.0143
             Mean action noise std: 0.88
                       Mean reward: 9428.37
               Mean episode length: 396.97
                 Mean success rate: 79.00
                  Mean reward/step: 23.70
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 19865600
                    Iteration time: 2.59s
                        Total time: 6376.22s
                               ETA: 519500.0s

################################################################################
                    [1m Learning iteration 2425/200000 [0m

                       Computation: 3099 steps/s (collection: 0.556s, learning 2.087s)
               Value function loss: 83848.6078
                    Surrogate loss: 0.0086
             Mean action noise std: 0.88
                       Mean reward: 9365.50
               Mean episode length: 396.50
                 Mean success rate: 79.00
                  Mean reward/step: 22.84
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 19873792
                    Iteration time: 2.64s
                        Total time: 6378.86s
                               ETA: 519498.5s

################################################################################
                    [1m Learning iteration 2426/200000 [0m

                       Computation: 3108 steps/s (collection: 0.491s, learning 2.144s)
               Value function loss: 92413.7472
                    Surrogate loss: 0.0136
             Mean action noise std: 0.88
                       Mean reward: 9227.65
               Mean episode length: 391.19
                 Mean success rate: 78.00
                  Mean reward/step: 22.59
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 19881984
                    Iteration time: 2.64s
                        Total time: 6381.50s
                               ETA: 519496.3s

################################################################################
                    [1m Learning iteration 2427/200000 [0m

                       Computation: 3224 steps/s (collection: 0.516s, learning 2.024s)
               Value function loss: 108544.4701
                    Surrogate loss: 0.0172
             Mean action noise std: 0.88
                       Mean reward: 9581.82
               Mean episode length: 403.89
                 Mean success rate: 80.50
                  Mean reward/step: 22.92
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 19890176
                    Iteration time: 2.54s
                        Total time: 6384.04s
                               ETA: 519486.5s

################################################################################
                    [1m Learning iteration 2428/200000 [0m

                       Computation: 3187 steps/s (collection: 0.499s, learning 2.071s)
               Value function loss: 91440.6021
                    Surrogate loss: 0.0144
             Mean action noise std: 0.88
                       Mean reward: 9753.99
               Mean episode length: 406.15
                 Mean success rate: 81.00
                  Mean reward/step: 22.99
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 19898368
                    Iteration time: 2.57s
                        Total time: 6386.61s
                               ETA: 519479.0s

################################################################################
                    [1m Learning iteration 2429/200000 [0m

                       Computation: 3106 steps/s (collection: 0.561s, learning 2.076s)
               Value function loss: 106141.1227
                    Surrogate loss: 0.0148
             Mean action noise std: 0.88
                       Mean reward: 9727.43
               Mean episode length: 406.32
                 Mean success rate: 81.50
                  Mean reward/step: 22.70
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 19906560
                    Iteration time: 2.64s
                        Total time: 6389.24s
                               ETA: 519477.0s

################################################################################
                    [1m Learning iteration 2430/200000 [0m

                       Computation: 3264 steps/s (collection: 0.486s, learning 2.023s)
               Value function loss: 72211.1874
                    Surrogate loss: 0.0150
             Mean action noise std: 0.88
                       Mean reward: 9618.01
               Mean episode length: 400.92
                 Mean success rate: 81.00
                  Mean reward/step: 21.82
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 19914752
                    Iteration time: 2.51s
                        Total time: 6391.75s
                               ETA: 519464.6s

################################################################################
                    [1m Learning iteration 2431/200000 [0m

                       Computation: 3212 steps/s (collection: 0.488s, learning 2.062s)
               Value function loss: 91945.0951
                    Surrogate loss: 0.0128
             Mean action noise std: 0.88
                       Mean reward: 9371.08
               Mean episode length: 392.93
                 Mean success rate: 79.50
                  Mean reward/step: 22.40
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 19922944
                    Iteration time: 2.55s
                        Total time: 6394.30s
                               ETA: 519455.5s

################################################################################
                    [1m Learning iteration 2432/200000 [0m

                       Computation: 3194 steps/s (collection: 0.480s, learning 2.084s)
               Value function loss: 73799.2224
                    Surrogate loss: 0.0127
             Mean action noise std: 0.88
                       Mean reward: 9367.46
               Mean episode length: 393.08
                 Mean success rate: 79.00
                  Mean reward/step: 22.95
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 19931136
                    Iteration time: 2.56s
                        Total time: 6396.87s
                               ETA: 519447.6s

################################################################################
                    [1m Learning iteration 2433/200000 [0m

                       Computation: 3158 steps/s (collection: 0.562s, learning 2.032s)
               Value function loss: 99812.7333
                    Surrogate loss: 0.0149
             Mean action noise std: 0.88
                       Mean reward: 9279.14
               Mean episode length: 394.31
                 Mean success rate: 77.50
                  Mean reward/step: 24.24
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 19939328
                    Iteration time: 2.59s
                        Total time: 6399.46s
                               ETA: 519442.1s

################################################################################
                    [1m Learning iteration 2434/200000 [0m

                       Computation: 3253 steps/s (collection: 0.481s, learning 2.037s)
               Value function loss: 105142.3993
                    Surrogate loss: 0.0149
             Mean action noise std: 0.88
                       Mean reward: 9008.49
               Mean episode length: 388.69
                 Mean success rate: 77.00
                  Mean reward/step: 24.65
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 19947520
                    Iteration time: 2.52s
                        Total time: 6401.98s
                               ETA: 519430.4s

################################################################################
                    [1m Learning iteration 2435/200000 [0m

                       Computation: 3256 steps/s (collection: 0.484s, learning 2.032s)
               Value function loss: 69276.5302
                    Surrogate loss: 0.0121
             Mean action noise std: 0.88
                       Mean reward: 9074.52
               Mean episode length: 388.51
                 Mean success rate: 77.50
                  Mean reward/step: 24.78
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 2.52s
                        Total time: 6404.49s
                               ETA: 519418.6s

################################################################################
                    [1m Learning iteration 2436/200000 [0m

                       Computation: 3231 steps/s (collection: 0.471s, learning 2.064s)
               Value function loss: 68958.9119
                    Surrogate loss: 0.0139
             Mean action noise std: 0.88
                       Mean reward: 9100.66
               Mean episode length: 389.39
                 Mean success rate: 77.50
                  Mean reward/step: 24.95
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 19963904
                    Iteration time: 2.53s
                        Total time: 6407.03s
                               ETA: 519408.3s

################################################################################
                    [1m Learning iteration 2437/200000 [0m

                       Computation: 3157 steps/s (collection: 0.547s, learning 2.047s)
               Value function loss: 95904.7045
                    Surrogate loss: 0.0120
             Mean action noise std: 0.88
                       Mean reward: 9127.79
               Mean episode length: 389.10
                 Mean success rate: 77.50
                  Mean reward/step: 25.42
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 19972096
                    Iteration time: 2.59s
                        Total time: 6409.62s
                               ETA: 519402.9s

################################################################################
                    [1m Learning iteration 2438/200000 [0m

                       Computation: 3191 steps/s (collection: 0.511s, learning 2.055s)
               Value function loss: 87517.3395
                    Surrogate loss: 0.0146
             Mean action noise std: 0.88
                       Mean reward: 8937.31
               Mean episode length: 389.46
                 Mean success rate: 77.00
                  Mean reward/step: 25.57
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 19980288
                    Iteration time: 2.57s
                        Total time: 6412.19s
                               ETA: 519395.2s

################################################################################
                    [1m Learning iteration 2439/200000 [0m

                       Computation: 3219 steps/s (collection: 0.486s, learning 2.058s)
               Value function loss: 65316.9079
                    Surrogate loss: 0.0134
             Mean action noise std: 0.88
                       Mean reward: 9062.75
               Mean episode length: 394.64
                 Mean success rate: 77.50
                  Mean reward/step: 25.36
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 19988480
                    Iteration time: 2.54s
                        Total time: 6414.73s
                               ETA: 519385.7s

################################################################################
                    [1m Learning iteration 2440/200000 [0m

                       Computation: 3199 steps/s (collection: 0.500s, learning 2.060s)
               Value function loss: 107890.7486
                    Surrogate loss: 0.0145
             Mean action noise std: 0.88
                       Mean reward: 9436.37
               Mean episode length: 403.81
                 Mean success rate: 79.00
                  Mean reward/step: 25.59
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 19996672
                    Iteration time: 2.56s
                        Total time: 6417.29s
                               ETA: 519377.5s

################################################################################
                    [1m Learning iteration 2441/200000 [0m

                       Computation: 3134 steps/s (collection: 0.539s, learning 2.075s)
               Value function loss: 94077.6766
                    Surrogate loss: 0.0115
             Mean action noise std: 0.88
                       Mean reward: 9648.41
               Mean episode length: 410.19
                 Mean success rate: 79.50
                  Mean reward/step: 25.25
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 20004864
                    Iteration time: 2.61s
                        Total time: 6419.91s
                               ETA: 519373.7s

################################################################################
                    [1m Learning iteration 2442/200000 [0m

                       Computation: 3198 steps/s (collection: 0.483s, learning 2.078s)
               Value function loss: 83489.0035
                    Surrogate loss: 0.0096
             Mean action noise std: 0.88
                       Mean reward: 9827.96
               Mean episode length: 414.52
                 Mean success rate: 80.50
                  Mean reward/step: 24.82
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 20013056
                    Iteration time: 2.56s
                        Total time: 6422.47s
                               ETA: 519365.6s

################################################################################
                    [1m Learning iteration 2443/200000 [0m

                       Computation: 3237 steps/s (collection: 0.475s, learning 2.056s)
               Value function loss: 97156.1050
                    Surrogate loss: 0.0088
             Mean action noise std: 0.88
                       Mean reward: 9759.20
               Mean episode length: 417.25
                 Mean success rate: 81.00
                  Mean reward/step: 25.58
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 20021248
                    Iteration time: 2.53s
                        Total time: 6425.00s
                               ETA: 519355.0s

################################################################################
                    [1m Learning iteration 2444/200000 [0m

                       Computation: 3105 steps/s (collection: 0.553s, learning 2.084s)
               Value function loss: 92296.5081
                    Surrogate loss: 0.0088
             Mean action noise std: 0.88
                       Mean reward: 10100.97
               Mean episode length: 421.96
                 Mean success rate: 82.50
                  Mean reward/step: 25.39
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 20029440
                    Iteration time: 2.64s
                        Total time: 6427.64s
                               ETA: 519353.0s

################################################################################
                    [1m Learning iteration 2445/200000 [0m

                       Computation: 3185 steps/s (collection: 0.486s, learning 2.085s)
               Value function loss: 106941.9674
                    Surrogate loss: 0.0101
             Mean action noise std: 0.88
                       Mean reward: 10522.57
               Mean episode length: 434.81
                 Mean success rate: 85.00
                  Mean reward/step: 25.55
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 20037632
                    Iteration time: 2.57s
                        Total time: 6430.21s
                               ETA: 519345.8s

################################################################################
                    [1m Learning iteration 2446/200000 [0m

                       Computation: 3207 steps/s (collection: 0.500s, learning 2.054s)
               Value function loss: 100663.7646
                    Surrogate loss: 0.0160
             Mean action noise std: 0.88
                       Mean reward: 10686.17
               Mean episode length: 436.80
                 Mean success rate: 85.50
                  Mean reward/step: 24.78
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 20045824
                    Iteration time: 2.55s
                        Total time: 6432.76s
                               ETA: 519337.1s

################################################################################
                    [1m Learning iteration 2447/200000 [0m

                       Computation: 3189 steps/s (collection: 0.501s, learning 2.067s)
               Value function loss: 105891.2288
                    Surrogate loss: 0.0150
             Mean action noise std: 0.88
                       Mean reward: 10794.66
               Mean episode length: 437.77
                 Mean success rate: 86.00
                  Mean reward/step: 24.01
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 2.57s
                        Total time: 6435.33s
                               ETA: 519329.6s

################################################################################
                    [1m Learning iteration 2448/200000 [0m

                       Computation: 3137 steps/s (collection: 0.532s, learning 2.080s)
               Value function loss: 70279.9634
                    Surrogate loss: 0.0139
             Mean action noise std: 0.88
                       Mean reward: 10583.06
               Mean episode length: 428.63
                 Mean success rate: 84.50
                  Mean reward/step: 24.37
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 20062208
                    Iteration time: 2.61s
                        Total time: 6437.94s
                               ETA: 519325.5s

################################################################################
                    [1m Learning iteration 2449/200000 [0m

                       Computation: 3156 steps/s (collection: 0.541s, learning 2.054s)
               Value function loss: 72664.0898
                    Surrogate loss: 0.0110
             Mean action noise std: 0.88
                       Mean reward: 10887.22
               Mean episode length: 433.94
                 Mean success rate: 86.00
                  Mean reward/step: 25.13
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 20070400
                    Iteration time: 2.60s
                        Total time: 6440.54s
                               ETA: 519320.2s

################################################################################
                    [1m Learning iteration 2450/200000 [0m

                       Computation: 3229 steps/s (collection: 0.503s, learning 2.034s)
               Value function loss: 107205.3554
                    Surrogate loss: 0.0109
             Mean action noise std: 0.88
                       Mean reward: 10710.05
               Mean episode length: 431.00
                 Mean success rate: 85.00
                  Mean reward/step: 24.52
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 20078592
                    Iteration time: 2.54s
                        Total time: 6443.07s
                               ETA: 519310.1s

################################################################################
                    [1m Learning iteration 2451/200000 [0m

                       Computation: 3122 steps/s (collection: 0.533s, learning 2.091s)
               Value function loss: 79213.4363
                    Surrogate loss: 0.0133
             Mean action noise std: 0.88
                       Mean reward: 10798.59
               Mean episode length: 433.14
                 Mean success rate: 86.00
                  Mean reward/step: 24.65
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 20086784
                    Iteration time: 2.62s
                        Total time: 6445.70s
                               ETA: 519307.1s

################################################################################
                    [1m Learning iteration 2452/200000 [0m

                       Computation: 3123 steps/s (collection: 0.539s, learning 2.084s)
               Value function loss: 60118.8780
                    Surrogate loss: 0.0123
             Mean action noise std: 0.88
                       Mean reward: 10755.69
               Mean episode length: 431.57
                 Mean success rate: 85.50
                  Mean reward/step: 25.40
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 20094976
                    Iteration time: 2.62s
                        Total time: 6448.32s
                               ETA: 519304.0s

################################################################################
                    [1m Learning iteration 2453/200000 [0m

                       Computation: 3266 steps/s (collection: 0.471s, learning 2.038s)
               Value function loss: 112731.8773
                    Surrogate loss: 0.0121
             Mean action noise std: 0.88
                       Mean reward: 10709.11
               Mean episode length: 423.73
                 Mean success rate: 84.50
                  Mean reward/step: 25.65
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 20103168
                    Iteration time: 2.51s
                        Total time: 6450.83s
                               ETA: 519291.6s

################################################################################
                    [1m Learning iteration 2454/200000 [0m

                       Computation: 3246 steps/s (collection: 0.498s, learning 2.025s)
               Value function loss: 91599.4116
                    Surrogate loss: 0.0093
             Mean action noise std: 0.88
                       Mean reward: 10273.64
               Mean episode length: 409.80
                 Mean success rate: 82.00
                  Mean reward/step: 24.78
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 20111360
                    Iteration time: 2.52s
                        Total time: 6453.35s
                               ETA: 519280.5s

################################################################################
                    [1m Learning iteration 2455/200000 [0m

                       Computation: 3180 steps/s (collection: 0.491s, learning 2.085s)
               Value function loss: 66210.2897
                    Surrogate loss: 0.0122
             Mean action noise std: 0.88
                       Mean reward: 9837.85
               Mean episode length: 394.45
                 Mean success rate: 78.50
                  Mean reward/step: 24.85
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 20119552
                    Iteration time: 2.58s
                        Total time: 6455.93s
                               ETA: 519273.6s

################################################################################
                    [1m Learning iteration 2456/200000 [0m

                       Computation: 3142 steps/s (collection: 0.553s, learning 2.054s)
               Value function loss: 122207.7276
                    Surrogate loss: 0.0107
             Mean action noise std: 0.88
                       Mean reward: 10082.58
               Mean episode length: 402.37
                 Mean success rate: 80.00
                  Mean reward/step: 24.46
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 20127744
                    Iteration time: 2.61s
                        Total time: 6458.53s
                               ETA: 519269.3s

################################################################################
                    [1m Learning iteration 2457/200000 [0m

                       Computation: 3148 steps/s (collection: 0.533s, learning 2.069s)
               Value function loss: 102182.9703
                    Surrogate loss: 0.0114
             Mean action noise std: 0.88
                       Mean reward: 10108.91
               Mean episode length: 402.25
                 Mean success rate: 79.50
                  Mean reward/step: 23.56
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 20135936
                    Iteration time: 2.60s
                        Total time: 6461.14s
                               ETA: 519264.5s

################################################################################
                    [1m Learning iteration 2458/200000 [0m

                       Computation: 3151 steps/s (collection: 0.515s, learning 2.085s)
               Value function loss: 68737.1442
                    Surrogate loss: 0.0145
             Mean action noise std: 0.88
                       Mean reward: 10123.77
               Mean episode length: 400.86
                 Mean success rate: 79.50
                  Mean reward/step: 23.82
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 20144128
                    Iteration time: 2.60s
                        Total time: 6463.74s
                               ETA: 519259.5s

################################################################################
                    [1m Learning iteration 2459/200000 [0m

                       Computation: 3167 steps/s (collection: 0.519s, learning 2.068s)
               Value function loss: 90616.3479
                    Surrogate loss: 0.0131
             Mean action noise std: 0.88
                       Mean reward: 9892.64
               Mean episode length: 396.35
                 Mean success rate: 79.00
                  Mean reward/step: 23.80
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 2.59s
                        Total time: 6466.32s
                               ETA: 519253.5s

################################################################################
                    [1m Learning iteration 2460/200000 [0m

                       Computation: 3134 steps/s (collection: 0.527s, learning 2.087s)
               Value function loss: 91212.4505
                    Surrogate loss: 0.0155
             Mean action noise std: 0.88
                       Mean reward: 9579.82
               Mean episode length: 385.15
                 Mean success rate: 76.50
                  Mean reward/step: 23.64
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 20160512
                    Iteration time: 2.61s
                        Total time: 6468.93s
                               ETA: 519249.7s

################################################################################
                    [1m Learning iteration 2461/200000 [0m

                       Computation: 3233 steps/s (collection: 0.483s, learning 2.051s)
               Value function loss: 109806.6358
                    Surrogate loss: 0.0103
             Mean action noise std: 0.88
                       Mean reward: 9379.43
               Mean episode length: 379.67
                 Mean success rate: 76.50
                  Mean reward/step: 23.67
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 20168704
                    Iteration time: 2.53s
                        Total time: 6471.47s
                               ETA: 519239.4s

################################################################################
                    [1m Learning iteration 2462/200000 [0m

                       Computation: 3204 steps/s (collection: 0.498s, learning 2.058s)
               Value function loss: 104268.7695
                    Surrogate loss: 0.0137
             Mean action noise std: 0.88
                       Mean reward: 9552.11
               Mean episode length: 384.80
                 Mean success rate: 77.00
                  Mean reward/step: 24.08
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 20176896
                    Iteration time: 2.56s
                        Total time: 6474.03s
                               ETA: 519231.0s

################################################################################
                    [1m Learning iteration 2463/200000 [0m

                       Computation: 3190 steps/s (collection: 0.502s, learning 2.066s)
               Value function loss: 85724.8383
                    Surrogate loss: 0.0112
             Mean action noise std: 0.88
                       Mean reward: 9570.22
               Mean episode length: 385.88
                 Mean success rate: 77.00
                  Mean reward/step: 23.83
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 20185088
                    Iteration time: 2.57s
                        Total time: 6476.59s
                               ETA: 519223.5s

################################################################################
                    [1m Learning iteration 2464/200000 [0m

                       Computation: 3159 steps/s (collection: 0.531s, learning 2.062s)
               Value function loss: 81710.1692
                    Surrogate loss: 0.0116
             Mean action noise std: 0.88
                       Mean reward: 9625.05
               Mean episode length: 386.91
                 Mean success rate: 77.50
                  Mean reward/step: 24.98
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 20193280
                    Iteration time: 2.59s
                        Total time: 6479.19s
                               ETA: 519218.0s

################################################################################
                    [1m Learning iteration 2465/200000 [0m

                       Computation: 3249 steps/s (collection: 0.506s, learning 2.015s)
               Value function loss: 98936.6684
                    Surrogate loss: 0.0111
             Mean action noise std: 0.88
                       Mean reward: 9464.60
               Mean episode length: 383.71
                 Mean success rate: 76.50
                  Mean reward/step: 25.10
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 20201472
                    Iteration time: 2.52s
                        Total time: 6481.71s
                               ETA: 519206.7s

################################################################################
                    [1m Learning iteration 2466/200000 [0m

                       Computation: 3218 steps/s (collection: 0.489s, learning 2.056s)
               Value function loss: 95338.7231
                    Surrogate loss: 0.0143
             Mean action noise std: 0.88
                       Mean reward: 9214.13
               Mean episode length: 378.05
                 Mean success rate: 75.50
                  Mean reward/step: 24.06
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 20209664
                    Iteration time: 2.54s
                        Total time: 6484.25s
                               ETA: 519197.4s

################################################################################
                    [1m Learning iteration 2467/200000 [0m

                       Computation: 3227 steps/s (collection: 0.467s, learning 2.071s)
               Value function loss: 75696.0181
                    Surrogate loss: 0.0147
             Mean action noise std: 0.88
                       Mean reward: 8915.28
               Mean episode length: 372.97
                 Mean success rate: 73.50
                  Mean reward/step: 24.37
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 20217856
                    Iteration time: 2.54s
                        Total time: 6486.79s
                               ETA: 519187.6s

################################################################################
                    [1m Learning iteration 2468/200000 [0m

                       Computation: 3208 steps/s (collection: 0.507s, learning 2.046s)
               Value function loss: 89345.7052
                    Surrogate loss: 0.0128
             Mean action noise std: 0.88
                       Mean reward: 9148.13
               Mean episode length: 376.75
                 Mean success rate: 74.50
                  Mean reward/step: 24.79
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 20226048
                    Iteration time: 2.55s
                        Total time: 6489.34s
                               ETA: 519178.9s

################################################################################
                    [1m Learning iteration 2469/200000 [0m

                       Computation: 3214 steps/s (collection: 0.486s, learning 2.063s)
               Value function loss: 88491.1652
                    Surrogate loss: 0.0112
             Mean action noise std: 0.88
                       Mean reward: 9015.25
               Mean episode length: 373.75
                 Mean success rate: 73.50
                  Mean reward/step: 24.24
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 20234240
                    Iteration time: 2.55s
                        Total time: 6491.89s
                               ETA: 519169.9s

################################################################################
                    [1m Learning iteration 2470/200000 [0m

                       Computation: 3257 steps/s (collection: 0.454s, learning 2.061s)
               Value function loss: 97378.5827
                    Surrogate loss: 0.0101
             Mean action noise std: 0.88
                       Mean reward: 9370.58
               Mean episode length: 383.45
                 Mean success rate: 75.50
                  Mean reward/step: 23.94
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 20242432
                    Iteration time: 2.52s
                        Total time: 6494.41s
                               ETA: 519158.2s

################################################################################
                    [1m Learning iteration 2471/200000 [0m

                       Computation: 3194 steps/s (collection: 0.491s, learning 2.074s)
               Value function loss: 97605.7329
                    Surrogate loss: 0.0115
             Mean action noise std: 0.88
                       Mean reward: 9197.20
               Mean episode length: 379.37
                 Mean success rate: 74.00
                  Mean reward/step: 24.27
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 2.56s
                        Total time: 6496.97s
                               ETA: 519150.5s

################################################################################
                    [1m Learning iteration 2472/200000 [0m

                       Computation: 3119 steps/s (collection: 0.528s, learning 2.098s)
               Value function loss: 119434.1428
                    Surrogate loss: 0.0127
             Mean action noise std: 0.88
                       Mean reward: 9216.80
               Mean episode length: 377.40
                 Mean success rate: 74.00
                  Mean reward/step: 24.22
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 20258816
                    Iteration time: 2.63s
                        Total time: 6499.60s
                               ETA: 519147.8s

################################################################################
                    [1m Learning iteration 2473/200000 [0m

                       Computation: 3286 steps/s (collection: 0.441s, learning 2.052s)
               Value function loss: 94358.7188
                    Surrogate loss: 0.0134
             Mean action noise std: 0.88
                       Mean reward: 9645.73
               Mean episode length: 392.02
                 Mean success rate: 77.00
                  Mean reward/step: 23.72
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 20267008
                    Iteration time: 2.49s
                        Total time: 6502.09s
                               ETA: 519134.3s

################################################################################
                    [1m Learning iteration 2474/200000 [0m

                       Computation: 3236 steps/s (collection: 0.467s, learning 2.065s)
               Value function loss: 88707.7815
                    Surrogate loss: 0.0151
             Mean action noise std: 0.88
                       Mean reward: 9780.04
               Mean episode length: 399.17
                 Mean success rate: 78.50
                  Mean reward/step: 23.94
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 20275200
                    Iteration time: 2.53s
                        Total time: 6504.62s
                               ETA: 519123.9s

################################################################################
                    [1m Learning iteration 2475/200000 [0m

                       Computation: 3246 steps/s (collection: 0.447s, learning 2.076s)
               Value function loss: 77578.5496
                    Surrogate loss: 0.0116
             Mean action noise std: 0.88
                       Mean reward: 9388.42
               Mean episode length: 389.69
                 Mean success rate: 76.50
                  Mean reward/step: 24.40
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 20283392
                    Iteration time: 2.52s
                        Total time: 6507.14s
                               ETA: 519113.0s

################################################################################
                    [1m Learning iteration 2476/200000 [0m

                       Computation: 3157 steps/s (collection: 0.502s, learning 2.092s)
               Value function loss: 114428.1699
                    Surrogate loss: 0.0122
             Mean action noise std: 0.88
                       Mean reward: 9449.53
               Mean episode length: 390.71
                 Mean success rate: 77.00
                  Mean reward/step: 24.39
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 20291584
                    Iteration time: 2.59s
                        Total time: 6509.74s
                               ETA: 519107.6s

################################################################################
                    [1m Learning iteration 2477/200000 [0m

                       Computation: 3265 steps/s (collection: 0.474s, learning 2.035s)
               Value function loss: 79451.0565
                    Surrogate loss: 0.0121
             Mean action noise std: 0.88
                       Mean reward: 9585.70
               Mean episode length: 397.48
                 Mean success rate: 78.50
                  Mean reward/step: 24.11
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 20299776
                    Iteration time: 2.51s
                        Total time: 6512.25s
                               ETA: 519095.5s

################################################################################
                    [1m Learning iteration 2478/200000 [0m

                       Computation: 3165 steps/s (collection: 0.530s, learning 2.058s)
               Value function loss: 104302.8351
                    Surrogate loss: 0.0100
             Mean action noise std: 0.88
                       Mean reward: 9469.83
               Mean episode length: 392.74
                 Mean success rate: 77.00
                  Mean reward/step: 23.51
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 20307968
                    Iteration time: 2.59s
                        Total time: 6514.83s
                               ETA: 519089.6s

################################################################################
                    [1m Learning iteration 2479/200000 [0m

                       Computation: 3125 steps/s (collection: 0.553s, learning 2.069s)
               Value function loss: 66434.2949
                    Surrogate loss: 0.0095
             Mean action noise std: 0.88
                       Mean reward: 9210.32
               Mean episode length: 386.27
                 Mean success rate: 75.50
                  Mean reward/step: 23.93
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 20316160
                    Iteration time: 2.62s
                        Total time: 6517.46s
                               ETA: 519086.5s

################################################################################
                    [1m Learning iteration 2480/200000 [0m

                       Computation: 3178 steps/s (collection: 0.491s, learning 2.086s)
               Value function loss: 91679.2248
                    Surrogate loss: 0.0124
             Mean action noise std: 0.88
                       Mean reward: 9313.37
               Mean episode length: 388.18
                 Mean success rate: 76.50
                  Mean reward/step: 24.49
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 20324352
                    Iteration time: 2.58s
                        Total time: 6520.03s
                               ETA: 519079.8s

################################################################################
                    [1m Learning iteration 2481/200000 [0m

                       Computation: 3257 steps/s (collection: 0.473s, learning 2.042s)
               Value function loss: 121611.2588
                    Surrogate loss: 0.0106
             Mean action noise std: 0.88
                       Mean reward: 9390.98
               Mean episode length: 390.43
                 Mean success rate: 76.50
                  Mean reward/step: 24.72
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 20332544
                    Iteration time: 2.51s
                        Total time: 6522.55s
                               ETA: 519068.1s

################################################################################
                    [1m Learning iteration 2482/200000 [0m

                       Computation: 3343 steps/s (collection: 0.422s, learning 2.028s)
               Value function loss: 61885.7588
                    Surrogate loss: 0.0128
             Mean action noise std: 0.88
                       Mean reward: 9077.96
               Mean episode length: 381.53
                 Mean success rate: 74.00
                  Mean reward/step: 25.21
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 20340736
                    Iteration time: 2.45s
                        Total time: 6525.00s
                               ETA: 519051.4s

################################################################################
                    [1m Learning iteration 2483/200000 [0m

                       Computation: 3261 steps/s (collection: 0.469s, learning 2.043s)
               Value function loss: 82532.2107
                    Surrogate loss: 0.0128
             Mean action noise std: 0.88
                       Mean reward: 8943.19
               Mean episode length: 376.32
                 Mean success rate: 72.50
                  Mean reward/step: 25.67
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 2.51s
                        Total time: 6527.51s
                               ETA: 519039.5s

################################################################################
                    [1m Learning iteration 2484/200000 [0m

                       Computation: 3175 steps/s (collection: 0.528s, learning 2.052s)
               Value function loss: 110139.7969
                    Surrogate loss: 0.0132
             Mean action noise std: 0.88
                       Mean reward: 8721.02
               Mean episode length: 365.30
                 Mean success rate: 71.00
                  Mean reward/step: 25.59
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 20357120
                    Iteration time: 2.58s
                        Total time: 6530.09s
                               ETA: 519033.1s

################################################################################
                    [1m Learning iteration 2485/200000 [0m

                       Computation: 3268 steps/s (collection: 0.455s, learning 2.051s)
               Value function loss: 104663.1418
                    Surrogate loss: 0.0125
             Mean action noise std: 0.88
                       Mean reward: 9158.48
               Mean episode length: 377.15
                 Mean success rate: 73.00
                  Mean reward/step: 25.23
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 20365312
                    Iteration time: 2.51s
                        Total time: 6532.60s
                               ETA: 519020.8s

################################################################################
                    [1m Learning iteration 2486/200000 [0m

                       Computation: 3235 steps/s (collection: 0.460s, learning 2.071s)
               Value function loss: 100702.7521
                    Surrogate loss: 0.0128
             Mean action noise std: 0.88
                       Mean reward: 9521.11
               Mean episode length: 387.77
                 Mean success rate: 75.00
                  Mean reward/step: 25.11
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 20373504
                    Iteration time: 2.53s
                        Total time: 6535.13s
                               ETA: 519010.6s

################################################################################
                    [1m Learning iteration 2487/200000 [0m

                       Computation: 3240 steps/s (collection: 0.491s, learning 2.037s)
               Value function loss: 132278.2322
                    Surrogate loss: 0.0140
             Mean action noise std: 0.88
                       Mean reward: 9711.56
               Mean episode length: 391.21
                 Mean success rate: 76.50
                  Mean reward/step: 24.53
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 20381696
                    Iteration time: 2.53s
                        Total time: 6537.66s
                               ETA: 519000.0s

################################################################################
                    [1m Learning iteration 2488/200000 [0m

                       Computation: 3199 steps/s (collection: 0.472s, learning 2.089s)
               Value function loss: 83537.6695
                    Surrogate loss: 0.0138
             Mean action noise std: 0.88
                       Mean reward: 9614.34
               Mean episode length: 386.94
                 Mean success rate: 76.50
                  Mean reward/step: 23.53
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 20389888
                    Iteration time: 2.56s
                        Total time: 6540.22s
                               ETA: 518992.0s

################################################################################
                    [1m Learning iteration 2489/200000 [0m

                       Computation: 3257 steps/s (collection: 0.480s, learning 2.035s)
               Value function loss: 74900.0601
                    Surrogate loss: 0.0127
             Mean action noise std: 0.88
                       Mean reward: 9591.55
               Mean episode length: 383.42
                 Mean success rate: 75.50
                  Mean reward/step: 24.17
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 20398080
                    Iteration time: 2.52s
                        Total time: 6542.73s
                               ETA: 518980.5s

################################################################################
                    [1m Learning iteration 2490/200000 [0m

                       Computation: 3208 steps/s (collection: 0.473s, learning 2.080s)
               Value function loss: 104974.1806
                    Surrogate loss: 0.0168
             Mean action noise std: 0.88
                       Mean reward: 9432.19
               Mean episode length: 382.18
                 Mean success rate: 75.00
                  Mean reward/step: 24.31
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 20406272
                    Iteration time: 2.55s
                        Total time: 6545.28s
                               ETA: 518972.0s

################################################################################
                    [1m Learning iteration 2491/200000 [0m

                       Computation: 3222 steps/s (collection: 0.493s, learning 2.049s)
               Value function loss: 71826.7003
                    Surrogate loss: 0.0124
             Mean action noise std: 0.88
                       Mean reward: 9406.72
               Mean episode length: 381.36
                 Mean success rate: 75.00
                  Mean reward/step: 24.26
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 20414464
                    Iteration time: 2.54s
                        Total time: 6547.83s
                               ETA: 518962.6s

################################################################################
                    [1m Learning iteration 2492/200000 [0m

                       Computation: 3239 steps/s (collection: 0.487s, learning 2.042s)
               Value function loss: 125098.7280
                    Surrogate loss: 0.0129
             Mean action noise std: 0.88
                       Mean reward: 9727.94
               Mean episode length: 391.20
                 Mean success rate: 77.50
                  Mean reward/step: 23.92
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 20422656
                    Iteration time: 2.53s
                        Total time: 6550.36s
                               ETA: 518952.2s

################################################################################
                    [1m Learning iteration 2493/200000 [0m

                       Computation: 3263 steps/s (collection: 0.443s, learning 2.067s)
               Value function loss: 109367.7885
                    Surrogate loss: 0.0170
             Mean action noise std: 0.88
                       Mean reward: 10069.30
               Mean episode length: 401.34
                 Mean success rate: 79.50
                  Mean reward/step: 23.29
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 20430848
                    Iteration time: 2.51s
                        Total time: 6552.87s
                               ETA: 518940.2s

################################################################################
                    [1m Learning iteration 2494/200000 [0m

                       Computation: 3319 steps/s (collection: 0.441s, learning 2.027s)
               Value function loss: 73508.8071
                    Surrogate loss: 0.0168
             Mean action noise std: 0.88
                       Mean reward: 10034.32
               Mean episode length: 402.45
                 Mean success rate: 80.00
                  Mean reward/step: 22.90
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 20439040
                    Iteration time: 2.47s
                        Total time: 6555.33s
                               ETA: 518925.0s

################################################################################
                    [1m Learning iteration 2495/200000 [0m

                       Computation: 3205 steps/s (collection: 0.480s, learning 2.076s)
               Value function loss: 87826.0412
                    Surrogate loss: 0.0139
             Mean action noise std: 0.88
                       Mean reward: 9457.10
               Mean episode length: 386.23
                 Mean success rate: 76.50
                  Mean reward/step: 24.51
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 2.56s
                        Total time: 6557.89s
                               ETA: 518916.7s

################################################################################
                    [1m Learning iteration 2496/200000 [0m

                       Computation: 3145 steps/s (collection: 0.542s, learning 2.062s)
               Value function loss: 79036.3438
                    Surrogate loss: 0.0155
             Mean action noise std: 0.88
                       Mean reward: 9251.92
               Mean episode length: 380.17
                 Mean success rate: 74.50
                  Mean reward/step: 24.36
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 20455424
                    Iteration time: 2.60s
                        Total time: 6560.49s
                               ETA: 518912.2s

################################################################################
                    [1m Learning iteration 2497/200000 [0m

                       Computation: 3253 steps/s (collection: 0.478s, learning 2.040s)
               Value function loss: 134315.9314
                    Surrogate loss: 0.0124
             Mean action noise std: 0.88
                       Mean reward: 8947.12
               Mean episode length: 373.12
                 Mean success rate: 72.50
                  Mean reward/step: 23.47
       Mean episode length/episode: 27.68
--------------------------------------------------------------------------------
                   Total timesteps: 20463616
                    Iteration time: 2.52s
                        Total time: 6563.01s
                               ETA: 518900.9s

################################################################################
                    [1m Learning iteration 2498/200000 [0m

                       Computation: 3214 steps/s (collection: 0.482s, learning 2.067s)
               Value function loss: 87333.7073
                    Surrogate loss: 0.0153
             Mean action noise std: 0.88
                       Mean reward: 9061.17
               Mean episode length: 377.57
                 Mean success rate: 74.00
                  Mean reward/step: 23.30
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 20471808
                    Iteration time: 2.55s
                        Total time: 6565.56s
                               ETA: 518892.1s

################################################################################
                    [1m Learning iteration 2499/200000 [0m

                       Computation: 3138 steps/s (collection: 0.555s, learning 2.054s)
               Value function loss: 71411.0495
                    Surrogate loss: 0.0105
             Mean action noise std: 0.88
                       Mean reward: 8977.00
               Mean episode length: 372.01
                 Mean success rate: 74.00
                  Mean reward/step: 24.28
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 20480000
                    Iteration time: 2.61s
                        Total time: 6568.17s
                               ETA: 518888.1s

################################################################################
                    [1m Learning iteration 2500/200000 [0m

                       Computation: 3148 steps/s (collection: 0.533s, learning 2.069s)
               Value function loss: 92998.7402
                    Surrogate loss: 0.0143
             Mean action noise std: 0.88
                       Mean reward: 8875.91
               Mean episode length: 371.10
                 Mean success rate: 73.00
                  Mean reward/step: 24.60
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 20488192
                    Iteration time: 2.60s
                        Total time: 6570.77s
                               ETA: 518883.5s

################################################################################
                    [1m Learning iteration 2501/200000 [0m

                       Computation: 3198 steps/s (collection: 0.507s, learning 2.054s)
               Value function loss: 85511.3030
                    Surrogate loss: 0.0146
             Mean action noise std: 0.88
                       Mean reward: 8758.52
               Mean episode length: 369.98
                 Mean success rate: 73.00
                  Mean reward/step: 25.13
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 20496384
                    Iteration time: 2.56s
                        Total time: 6573.33s
                               ETA: 518875.6s

################################################################################
                    [1m Learning iteration 2502/200000 [0m

                       Computation: 3212 steps/s (collection: 0.524s, learning 2.026s)
               Value function loss: 97900.2797
                    Surrogate loss: 0.0147
             Mean action noise std: 0.88
                       Mean reward: 8510.24
               Mean episode length: 361.58
                 Mean success rate: 71.00
                  Mean reward/step: 25.16
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 20504576
                    Iteration time: 2.55s
                        Total time: 6575.88s
                               ETA: 518866.9s

################################################################################
                    [1m Learning iteration 2503/200000 [0m

                       Computation: 3189 steps/s (collection: 0.515s, learning 2.053s)
               Value function loss: 106680.3348
                    Surrogate loss: 0.0131
             Mean action noise std: 0.88
                       Mean reward: 8582.48
               Mean episode length: 360.64
                 Mean success rate: 70.50
                  Mean reward/step: 24.56
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 20512768
                    Iteration time: 2.57s
                        Total time: 6578.45s
                               ETA: 518859.7s

################################################################################
                    [1m Learning iteration 2504/200000 [0m

                       Computation: 3237 steps/s (collection: 0.486s, learning 2.044s)
               Value function loss: 74138.5579
                    Surrogate loss: 0.0133
             Mean action noise std: 0.88
                       Mean reward: 8691.13
               Mean episode length: 366.06
                 Mean success rate: 71.50
                  Mean reward/step: 23.76
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 20520960
                    Iteration time: 2.53s
                        Total time: 6580.98s
                               ETA: 518849.5s

################################################################################
                    [1m Learning iteration 2505/200000 [0m

                       Computation: 3205 steps/s (collection: 0.481s, learning 2.075s)
               Value function loss: 126440.3836
                    Surrogate loss: 0.0101
             Mean action noise std: 0.88
                       Mean reward: 9268.46
               Mean episode length: 383.52
                 Mean success rate: 75.50
                  Mean reward/step: 23.85
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 20529152
                    Iteration time: 2.56s
                        Total time: 6583.54s
                               ETA: 518841.2s

################################################################################
                    [1m Learning iteration 2506/200000 [0m

                       Computation: 3274 steps/s (collection: 0.443s, learning 2.058s)
               Value function loss: 70538.0846
                    Surrogate loss: 0.0120
             Mean action noise std: 0.88
                       Mean reward: 9172.66
               Mean episode length: 380.19
                 Mean success rate: 74.50
                  Mean reward/step: 23.13
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 20537344
                    Iteration time: 2.50s
                        Total time: 6586.04s
                               ETA: 518828.7s

################################################################################
                    [1m Learning iteration 2507/200000 [0m

                       Computation: 3161 steps/s (collection: 0.516s, learning 2.075s)
               Value function loss: 74524.2453
                    Surrogate loss: 0.0122
             Mean action noise std: 0.88
                       Mean reward: 9380.14
               Mean episode length: 389.35
                 Mean success rate: 76.50
                  Mean reward/step: 24.65
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 2.59s
                        Total time: 6588.63s
                               ETA: 518823.2s

################################################################################
                    [1m Learning iteration 2508/200000 [0m

                       Computation: 3216 steps/s (collection: 0.515s, learning 2.032s)
               Value function loss: 96370.7407
                    Surrogate loss: 0.0131
             Mean action noise std: 0.88
                       Mean reward: 9543.19
               Mean episode length: 396.05
                 Mean success rate: 77.00
                  Mean reward/step: 25.51
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 20553728
                    Iteration time: 2.55s
                        Total time: 6591.18s
                               ETA: 518814.3s

################################################################################
                    [1m Learning iteration 2509/200000 [0m

                       Computation: 3239 steps/s (collection: 0.468s, learning 2.061s)
               Value function loss: 78255.6152
                    Surrogate loss: 0.0112
             Mean action noise std: 0.88
                       Mean reward: 9611.68
               Mean episode length: 398.00
                 Mean success rate: 77.00
                  Mean reward/step: 25.76
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 20561920
                    Iteration time: 2.53s
                        Total time: 6593.71s
                               ETA: 518804.0s

################################################################################
                    [1m Learning iteration 2510/200000 [0m

                       Computation: 3300 steps/s (collection: 0.448s, learning 2.034s)
               Value function loss: 63825.8261
                    Surrogate loss: 0.0125
             Mean action noise std: 0.88
                       Mean reward: 9807.15
               Mean episode length: 402.69
                 Mean success rate: 78.50
                  Mean reward/step: 25.96
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 20570112
                    Iteration time: 2.48s
                        Total time: 6596.19s
                               ETA: 518789.9s

################################################################################
                    [1m Learning iteration 2511/200000 [0m

                       Computation: 3291 steps/s (collection: 0.465s, learning 2.023s)
               Value function loss: 106405.7596
                    Surrogate loss: 0.0129
             Mean action noise std: 0.88
                       Mean reward: 10153.93
               Mean episode length: 412.85
                 Mean success rate: 80.50
                  Mean reward/step: 26.04
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 20578304
                    Iteration time: 2.49s
                        Total time: 6598.68s
                               ETA: 518776.4s

################################################################################
                    [1m Learning iteration 2512/200000 [0m

                       Computation: 3233 steps/s (collection: 0.530s, learning 2.004s)
               Value function loss: 146383.3326
                    Surrogate loss: 0.0142
             Mean action noise std: 0.88
                       Mean reward: 9854.39
               Mean episode length: 403.50
                 Mean success rate: 79.00
                  Mean reward/step: 25.41
       Mean episode length/episode: 27.58
--------------------------------------------------------------------------------
                   Total timesteps: 20586496
                    Iteration time: 2.53s
                        Total time: 6601.21s
                               ETA: 518766.5s

################################################################################
                    [1m Learning iteration 2513/200000 [0m

                       Computation: 3315 steps/s (collection: 0.443s, learning 2.028s)
               Value function loss: 72710.4156
                    Surrogate loss: 0.0157
             Mean action noise std: 0.88
                       Mean reward: 10089.39
               Mean episode length: 409.63
                 Mean success rate: 80.50
                  Mean reward/step: 24.37
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 20594688
                    Iteration time: 2.47s
                        Total time: 6603.68s
                               ETA: 518751.6s

################################################################################
                    [1m Learning iteration 2514/200000 [0m

                       Computation: 3354 steps/s (collection: 0.422s, learning 2.021s)
               Value function loss: 77421.0900
                    Surrogate loss: 0.0129
             Mean action noise std: 0.88
                       Mean reward: 10054.69
               Mean episode length: 405.31
                 Mean success rate: 79.50
                  Mean reward/step: 24.78
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 20602880
                    Iteration time: 2.44s
                        Total time: 6606.13s
                               ETA: 518734.5s

################################################################################
                    [1m Learning iteration 2515/200000 [0m

                       Computation: 3288 steps/s (collection: 0.466s, learning 2.025s)
               Value function loss: 78170.2298
                    Surrogate loss: 0.0168
             Mean action noise std: 0.88
                       Mean reward: 10175.13
               Mean episode length: 408.81
                 Mean success rate: 80.50
                  Mean reward/step: 26.13
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 20611072
                    Iteration time: 2.49s
                        Total time: 6608.62s
                               ETA: 518721.2s

################################################################################
                    [1m Learning iteration 2516/200000 [0m

                       Computation: 3328 steps/s (collection: 0.444s, learning 2.017s)
               Value function loss: 93425.4686
                    Surrogate loss: 0.0121
             Mean action noise std: 0.88
                       Mean reward: 10356.32
               Mean episode length: 418.72
                 Mean success rate: 82.50
                  Mean reward/step: 26.13
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 20619264
                    Iteration time: 2.46s
                        Total time: 6611.08s
                               ETA: 518705.6s

################################################################################
                    [1m Learning iteration 2517/200000 [0m

                       Computation: 3325 steps/s (collection: 0.450s, learning 2.014s)
               Value function loss: 66928.5059
                    Surrogate loss: 0.0147
             Mean action noise std: 0.88
                       Mean reward: 10647.46
               Mean episode length: 427.73
                 Mean success rate: 84.00
                  Mean reward/step: 25.05
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 20627456
                    Iteration time: 2.46s
                        Total time: 6613.54s
                               ETA: 518690.2s

################################################################################
                    [1m Learning iteration 2518/200000 [0m

                       Computation: 3368 steps/s (collection: 0.419s, learning 2.013s)
               Value function loss: 100486.4449
                    Surrogate loss: 0.0142
             Mean action noise std: 0.88
                       Mean reward: 10596.13
               Mean episode length: 424.65
                 Mean success rate: 83.50
                  Mean reward/step: 24.36
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 20635648
                    Iteration time: 2.43s
                        Total time: 6615.97s
                               ETA: 518672.3s

################################################################################
                    [1m Learning iteration 2519/200000 [0m

                       Computation: 3214 steps/s (collection: 0.506s, learning 2.042s)
               Value function loss: 116240.7892
                    Surrogate loss: 0.0148
             Mean action noise std: 0.88
                       Mean reward: 10477.51
               Mean episode length: 422.97
                 Mean success rate: 82.50
                  Mean reward/step: 24.38
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 2.55s
                        Total time: 6618.52s
                               ETA: 518663.5s

################################################################################
                    [1m Learning iteration 2520/200000 [0m

                       Computation: 3312 steps/s (collection: 0.456s, learning 2.017s)
               Value function loss: 85923.0518
                    Surrogate loss: 0.0147
             Mean action noise std: 0.88
                       Mean reward: 10167.44
               Mean episode length: 409.97
                 Mean success rate: 79.50
                  Mean reward/step: 24.00
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 20652032
                    Iteration time: 2.47s
                        Total time: 6620.99s
                               ETA: 518648.9s

################################################################################
                    [1m Learning iteration 2521/200000 [0m

                       Computation: 3305 steps/s (collection: 0.431s, learning 2.047s)
               Value function loss: 124179.3317
                    Surrogate loss: 0.0122
             Mean action noise std: 0.88
                       Mean reward: 10165.81
               Mean episode length: 409.92
                 Mean success rate: 79.50
                  Mean reward/step: 23.89
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 20660224
                    Iteration time: 2.48s
                        Total time: 6623.47s
                               ETA: 518634.7s

################################################################################
                    [1m Learning iteration 2522/200000 [0m

                       Computation: 3211 steps/s (collection: 0.475s, learning 2.075s)
               Value function loss: 67934.5123
                    Surrogate loss: 0.0121
             Mean action noise std: 0.88
                       Mean reward: 10363.25
               Mean episode length: 417.08
                 Mean success rate: 81.00
                  Mean reward/step: 24.01
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 20668416
                    Iteration time: 2.55s
                        Total time: 6626.02s
                               ETA: 518626.2s

################################################################################
                    [1m Learning iteration 2523/200000 [0m

                       Computation: 3275 steps/s (collection: 0.481s, learning 2.020s)
               Value function loss: 92985.6375
                    Surrogate loss: 0.0156
             Mean action noise std: 0.88
                       Mean reward: 10244.83
               Mean episode length: 414.59
                 Mean success rate: 80.50
                  Mean reward/step: 25.12
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 20676608
                    Iteration time: 2.50s
                        Total time: 6628.52s
                               ETA: 518613.7s

################################################################################
                    [1m Learning iteration 2524/200000 [0m

                       Computation: 3254 steps/s (collection: 0.462s, learning 2.056s)
               Value function loss: 75299.3026
                    Surrogate loss: 0.0162
             Mean action noise std: 0.88
                       Mean reward: 10140.31
               Mean episode length: 411.54
                 Mean success rate: 80.00
                  Mean reward/step: 25.29
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 20684800
                    Iteration time: 2.52s
                        Total time: 6631.04s
                               ETA: 518602.6s

################################################################################
                    [1m Learning iteration 2525/200000 [0m

                       Computation: 3216 steps/s (collection: 0.474s, learning 2.073s)
               Value function loss: 83719.2535
                    Surrogate loss: 0.0110
             Mean action noise std: 0.88
                       Mean reward: 9822.36
               Mean episode length: 401.89
                 Mean success rate: 78.00
                  Mean reward/step: 24.83
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 20692992
                    Iteration time: 2.55s
                        Total time: 6633.59s
                               ETA: 518593.8s

################################################################################
                    [1m Learning iteration 2526/200000 [0m

                       Computation: 3229 steps/s (collection: 0.491s, learning 2.046s)
               Value function loss: 85867.6370
                    Surrogate loss: 0.0113
             Mean action noise std: 0.88
                       Mean reward: 9308.22
               Mean episode length: 382.61
                 Mean success rate: 74.00
                  Mean reward/step: 24.92
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 20701184
                    Iteration time: 2.54s
                        Total time: 6636.13s
                               ETA: 518584.2s

################################################################################
                    [1m Learning iteration 2527/200000 [0m

                       Computation: 3176 steps/s (collection: 0.472s, learning 2.107s)
               Value function loss: 80644.6152
                    Surrogate loss: 0.0107
             Mean action noise std: 0.88
                       Mean reward: 9429.02
               Mean episode length: 382.80
                 Mean success rate: 74.50
                  Mean reward/step: 25.18
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 20709376
                    Iteration time: 2.58s
                        Total time: 6638.70s
                               ETA: 518577.8s

################################################################################
                    [1m Learning iteration 2528/200000 [0m

                       Computation: 3237 steps/s (collection: 0.495s, learning 2.035s)
               Value function loss: 169786.6284
                    Surrogate loss: 0.0124
             Mean action noise std: 0.88
                       Mean reward: 9767.72
               Mean episode length: 390.29
                 Mean success rate: 76.50
                  Mean reward/step: 24.87
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 20717568
                    Iteration time: 2.53s
                        Total time: 6641.23s
                               ETA: 518567.7s

################################################################################
                    [1m Learning iteration 2529/200000 [0m

                       Computation: 3272 steps/s (collection: 0.450s, learning 2.053s)
               Value function loss: 89587.1222
                    Surrogate loss: 0.0089
             Mean action noise std: 0.88
                       Mean reward: 10166.73
               Mean episode length: 400.42
                 Mean success rate: 79.00
                  Mean reward/step: 24.05
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 20725760
                    Iteration time: 2.50s
                        Total time: 6643.74s
                               ETA: 518555.5s

################################################################################
                    [1m Learning iteration 2530/200000 [0m

                       Computation: 3242 steps/s (collection: 0.484s, learning 2.042s)
               Value function loss: 70508.2312
                    Surrogate loss: 0.0129
             Mean action noise std: 0.88
                       Mean reward: 10225.69
               Mean episode length: 403.76
                 Mean success rate: 80.00
                  Mean reward/step: 24.62
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 20733952
                    Iteration time: 2.53s
                        Total time: 6646.26s
                               ETA: 518545.1s

################################################################################
                    [1m Learning iteration 2531/200000 [0m

                       Computation: 3256 steps/s (collection: 0.483s, learning 2.032s)
               Value function loss: 79542.5477
                    Surrogate loss: 0.0133
             Mean action noise std: 0.88
                       Mean reward: 10377.45
               Mean episode length: 408.24
                 Mean success rate: 81.00
                  Mean reward/step: 25.26
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 2.52s
                        Total time: 6648.78s
                               ETA: 518533.9s

################################################################################
                    [1m Learning iteration 2532/200000 [0m

                       Computation: 3248 steps/s (collection: 0.454s, learning 2.068s)
               Value function loss: 123838.9561
                    Surrogate loss: 0.0134
             Mean action noise std: 0.88
                       Mean reward: 10313.68
               Mean episode length: 407.70
                 Mean success rate: 80.50
                  Mean reward/step: 25.39
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 20750336
                    Iteration time: 2.52s
                        Total time: 6651.30s
                               ETA: 518523.1s

################################################################################
                    [1m Learning iteration 2533/200000 [0m

                       Computation: 3236 steps/s (collection: 0.473s, learning 2.058s)
               Value function loss: 72295.2263
                    Surrogate loss: 0.0133
             Mean action noise std: 0.88
                       Mean reward: 10030.97
               Mean episode length: 398.95
                 Mean success rate: 78.00
                  Mean reward/step: 25.38
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 20758528
                    Iteration time: 2.53s
                        Total time: 6653.83s
                               ETA: 518513.1s

################################################################################
                    [1m Learning iteration 2534/200000 [0m

                       Computation: 3116 steps/s (collection: 0.516s, learning 2.113s)
               Value function loss: 111650.1297
                    Surrogate loss: 0.0162
             Mean action noise std: 0.88
                       Mean reward: 9782.22
               Mean episode length: 392.73
                 Mean success rate: 76.50
                  Mean reward/step: 24.97
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 20766720
                    Iteration time: 2.63s
                        Total time: 6656.46s
                               ETA: 518510.7s

################################################################################
                    [1m Learning iteration 2535/200000 [0m

                       Computation: 3259 steps/s (collection: 0.467s, learning 2.046s)
               Value function loss: 124929.9510
                    Surrogate loss: 0.0132
             Mean action noise std: 0.88
                       Mean reward: 10326.81
               Mean episode length: 410.80
                 Mean success rate: 80.50
                  Mean reward/step: 24.58
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 20774912
                    Iteration time: 2.51s
                        Total time: 6658.97s
                               ETA: 518499.4s

################################################################################
                    [1m Learning iteration 2536/200000 [0m

                       Computation: 3159 steps/s (collection: 0.485s, learning 2.108s)
               Value function loss: 94137.6596
                    Surrogate loss: 0.0104
             Mean action noise std: 0.88
                       Mean reward: 10439.42
               Mean episode length: 415.47
                 Mean success rate: 81.50
                  Mean reward/step: 24.76
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 20783104
                    Iteration time: 2.59s
                        Total time: 6661.57s
                               ETA: 518494.2s

################################################################################
                    [1m Learning iteration 2537/200000 [0m

                       Computation: 3150 steps/s (collection: 0.504s, learning 2.096s)
               Value function loss: 124496.7594
                    Surrogate loss: 0.0143
             Mean action noise std: 0.88
                       Mean reward: 10212.28
               Mean episode length: 409.10
                 Mean success rate: 80.50
                  Mean reward/step: 24.16
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 20791296
                    Iteration time: 2.60s
                        Total time: 6664.17s
                               ETA: 518489.5s

################################################################################
                    [1m Learning iteration 2538/200000 [0m

                       Computation: 3174 steps/s (collection: 0.467s, learning 2.114s)
               Value function loss: 53532.5942
                    Surrogate loss: 0.0109
             Mean action noise std: 0.88
                       Mean reward: 10228.38
               Mean episode length: 410.92
                 Mean success rate: 80.50
                  Mean reward/step: 24.42
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 20799488
                    Iteration time: 2.58s
                        Total time: 6666.75s
                               ETA: 518483.4s

################################################################################
                    [1m Learning iteration 2539/200000 [0m

                       Computation: 3040 steps/s (collection: 0.567s, learning 2.128s)
               Value function loss: 74918.0520
                    Surrogate loss: 0.0111
             Mean action noise std: 0.88
                       Mean reward: 10040.05
               Mean episode length: 406.73
                 Mean success rate: 79.50
                  Mean reward/step: 24.52
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 20807680
                    Iteration time: 2.69s
                        Total time: 6669.44s
                               ETA: 518486.2s

################################################################################
                    [1m Learning iteration 2540/200000 [0m

                       Computation: 2998 steps/s (collection: 0.603s, learning 2.129s)
               Value function loss: 101248.3954
                    Surrogate loss: 0.0123
             Mean action noise std: 0.88
                       Mean reward: 9617.89
               Mean episode length: 392.42
                 Mean success rate: 76.00
                  Mean reward/step: 25.61
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 20815872
                    Iteration time: 2.73s
                        Total time: 6672.17s
                               ETA: 518491.8s

################################################################################
                    [1m Learning iteration 2541/200000 [0m

                       Computation: 3192 steps/s (collection: 0.466s, learning 2.100s)
               Value function loss: 96139.6902
                    Surrogate loss: 0.0116
             Mean action noise std: 0.88
                       Mean reward: 9451.60
               Mean episode length: 386.44
                 Mean success rate: 74.50
                  Mean reward/step: 25.70
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 20824064
                    Iteration time: 2.57s
                        Total time: 6674.74s
                               ETA: 518484.5s

################################################################################
                    [1m Learning iteration 2542/200000 [0m

                       Computation: 3050 steps/s (collection: 0.557s, learning 2.129s)
               Value function loss: 85906.9869
                    Surrogate loss: 0.0108
             Mean action noise std: 0.88
                       Mean reward: 9799.22
               Mean episode length: 394.70
                 Mean success rate: 76.50
                  Mean reward/step: 25.81
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 20832256
                    Iteration time: 2.69s
                        Total time: 6677.43s
                               ETA: 518486.5s

################################################################################
                    [1m Learning iteration 2543/200000 [0m

                       Computation: 3129 steps/s (collection: 0.512s, learning 2.106s)
               Value function loss: 115256.6621
                    Surrogate loss: 0.0122
             Mean action noise std: 0.88
                       Mean reward: 10146.74
               Mean episode length: 406.89
                 Mean success rate: 79.00
                  Mean reward/step: 26.46
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 2.62s
                        Total time: 6680.04s
                               ETA: 518483.3s

################################################################################
                    [1m Learning iteration 2544/200000 [0m

                       Computation: 3134 steps/s (collection: 0.547s, learning 2.067s)
               Value function loss: 106902.9614
                    Surrogate loss: 0.0120
             Mean action noise std: 0.88
                       Mean reward: 10325.10
               Mean episode length: 408.90
                 Mean success rate: 79.50
                  Mean reward/step: 25.39
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 20848640
                    Iteration time: 2.61s
                        Total time: 6682.66s
                               ETA: 518479.7s

################################################################################
                    [1m Learning iteration 2545/200000 [0m

                       Computation: 3287 steps/s (collection: 0.459s, learning 2.032s)
               Value function loss: 80876.6840
                    Surrogate loss: 0.0133
             Mean action noise std: 0.88
                       Mean reward: 10079.65
               Mean episode length: 400.79
                 Mean success rate: 77.50
                  Mean reward/step: 25.30
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 20856832
                    Iteration time: 2.49s
                        Total time: 6685.15s
                               ETA: 518466.7s

################################################################################
                    [1m Learning iteration 2546/200000 [0m

                       Computation: 3301 steps/s (collection: 0.440s, learning 2.041s)
               Value function loss: 90746.3431
                    Surrogate loss: 0.0093
             Mean action noise std: 0.88
                       Mean reward: 10120.12
               Mean episode length: 399.87
                 Mean success rate: 77.50
                  Mean reward/step: 26.03
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 20865024
                    Iteration time: 2.48s
                        Total time: 6687.63s
                               ETA: 518452.9s

################################################################################
                    [1m Learning iteration 2547/200000 [0m

                       Computation: 3299 steps/s (collection: 0.454s, learning 2.029s)
               Value function loss: 79124.0190
                    Surrogate loss: 0.0128
             Mean action noise std: 0.88
                       Mean reward: 9702.16
               Mean episode length: 387.21
                 Mean success rate: 74.50
                  Mean reward/step: 26.06
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 20873216
                    Iteration time: 2.48s
                        Total time: 6690.11s
                               ETA: 518439.2s

################################################################################
                    [1m Learning iteration 2548/200000 [0m

                       Computation: 3308 steps/s (collection: 0.433s, learning 2.043s)
               Value function loss: 102671.1352
                    Surrogate loss: 0.0117
             Mean action noise std: 0.88
                       Mean reward: 9818.98
               Mean episode length: 389.83
                 Mean success rate: 74.50
                  Mean reward/step: 25.49
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 20881408
                    Iteration time: 2.48s
                        Total time: 6692.59s
                               ETA: 518424.9s

################################################################################
                    [1m Learning iteration 2549/200000 [0m

                       Computation: 3296 steps/s (collection: 0.454s, learning 2.031s)
               Value function loss: 127480.4687
                    Surrogate loss: 0.0133
             Mean action noise std: 0.88
                       Mean reward: 10244.06
               Mean episode length: 404.11
                 Mean success rate: 77.50
                  Mean reward/step: 25.07
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 20889600
                    Iteration time: 2.48s
                        Total time: 6695.07s
                               ETA: 518411.4s

################################################################################
                    [1m Learning iteration 2550/200000 [0m

                       Computation: 3302 steps/s (collection: 0.432s, learning 2.048s)
               Value function loss: 123828.4504
                    Surrogate loss: 0.0095
             Mean action noise std: 0.88
                       Mean reward: 10524.62
               Mean episode length: 413.78
                 Mean success rate: 80.00
                  Mean reward/step: 24.44
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 20897792
                    Iteration time: 2.48s
                        Total time: 6697.56s
                               ETA: 518397.6s

################################################################################
                    [1m Learning iteration 2551/200000 [0m

                       Computation: 3330 steps/s (collection: 0.446s, learning 2.014s)
               Value function loss: 95963.2240
                    Surrogate loss: 0.0117
             Mean action noise std: 0.88
                       Mean reward: 10470.32
               Mean episode length: 414.78
                 Mean success rate: 80.00
                  Mean reward/step: 24.02
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 20905984
                    Iteration time: 2.46s
                        Total time: 6700.01s
                               ETA: 518382.1s

################################################################################
                    [1m Learning iteration 2552/200000 [0m

                       Computation: 3292 steps/s (collection: 0.431s, learning 2.057s)
               Value function loss: 136455.2875
                    Surrogate loss: 0.0143
             Mean action noise std: 0.88
                       Mean reward: 10524.66
               Mean episode length: 414.78
                 Mean success rate: 80.00
                  Mean reward/step: 24.06
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 20914176
                    Iteration time: 2.49s
                        Total time: 6702.50s
                               ETA: 518368.9s

################################################################################
                    [1m Learning iteration 2553/200000 [0m

                       Computation: 3317 steps/s (collection: 0.446s, learning 2.023s)
               Value function loss: 72296.2717
                    Surrogate loss: 0.0125
             Mean action noise std: 0.88
                       Mean reward: 10404.42
               Mean episode length: 411.01
                 Mean success rate: 79.50
                  Mean reward/step: 24.18
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 20922368
                    Iteration time: 2.47s
                        Total time: 6704.97s
                               ETA: 518354.2s

################################################################################
                    [1m Learning iteration 2554/200000 [0m

                       Computation: 3348 steps/s (collection: 0.430s, learning 2.017s)
               Value function loss: 80105.2261
                    Surrogate loss: 0.0100
             Mean action noise std: 0.88
                       Mean reward: 10132.34
               Mean episode length: 401.69
                 Mean success rate: 78.00
                  Mean reward/step: 25.25
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 20930560
                    Iteration time: 2.45s
                        Total time: 6707.42s
                               ETA: 518337.7s

################################################################################
                    [1m Learning iteration 2555/200000 [0m

                       Computation: 3274 steps/s (collection: 0.490s, learning 2.011s)
               Value function loss: 87144.9689
                    Surrogate loss: 0.0097
             Mean action noise std: 0.88
                       Mean reward: 9849.72
               Mean episode length: 396.75
                 Mean success rate: 77.00
                  Mean reward/step: 25.24
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 2.50s
                        Total time: 6709.92s
                               ETA: 518325.6s

################################################################################
                    [1m Learning iteration 2556/200000 [0m

                       Computation: 3294 steps/s (collection: 0.438s, learning 2.049s)
               Value function loss: 78308.9328
                    Surrogate loss: 0.0116
             Mean action noise std: 0.88
                       Mean reward: 9992.55
               Mean episode length: 401.31
                 Mean success rate: 78.00
                  Mean reward/step: 24.92
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 20946944
                    Iteration time: 2.49s
                        Total time: 6712.41s
                               ETA: 518312.3s

################################################################################
                    [1m Learning iteration 2557/200000 [0m

                       Computation: 3273 steps/s (collection: 0.448s, learning 2.054s)
               Value function loss: 60561.4830
                    Surrogate loss: 0.0116
             Mean action noise std: 0.88
                       Mean reward: 10093.71
               Mean episode length: 403.64
                 Mean success rate: 78.50
                  Mean reward/step: 25.08
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 20955136
                    Iteration time: 2.50s
                        Total time: 6714.91s
                               ETA: 518300.2s

################################################################################
                    [1m Learning iteration 2558/200000 [0m

                       Computation: 3142 steps/s (collection: 0.519s, learning 2.088s)
               Value function loss: 101132.9357
                    Surrogate loss: 0.0102
             Mean action noise std: 0.88
                       Mean reward: 9826.31
               Mean episode length: 395.80
                 Mean success rate: 77.00
                  Mean reward/step: 25.58
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 20963328
                    Iteration time: 2.61s
                        Total time: 6717.52s
                               ETA: 518296.1s

################################################################################
                    [1m Learning iteration 2559/200000 [0m

                       Computation: 3225 steps/s (collection: 0.486s, learning 2.054s)
               Value function loss: 111091.0436
                    Surrogate loss: 0.0102
             Mean action noise std: 0.88
                       Mean reward: 9766.96
               Mean episode length: 393.12
                 Mean success rate: 76.00
                  Mean reward/step: 25.85
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 20971520
                    Iteration time: 2.54s
                        Total time: 6720.06s
                               ETA: 518286.9s

################################################################################
                    [1m Learning iteration 2560/200000 [0m

                       Computation: 3173 steps/s (collection: 0.470s, learning 2.112s)
               Value function loss: 78202.4315
                    Surrogate loss: 0.0126
             Mean action noise std: 0.88
                       Mean reward: 10080.18
               Mean episode length: 401.10
                 Mean success rate: 78.50
                  Mean reward/step: 25.35
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 20979712
                    Iteration time: 2.58s
                        Total time: 6722.64s
                               ETA: 518281.0s

################################################################################
                    [1m Learning iteration 2561/200000 [0m

                       Computation: 3108 steps/s (collection: 0.491s, learning 2.145s)
               Value function loss: 60740.0108
                    Surrogate loss: 0.0104
             Mean action noise std: 0.88
                       Mean reward: 10130.01
               Mean episode length: 399.84
                 Mean success rate: 79.50
                  Mean reward/step: 25.50
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 20987904
                    Iteration time: 2.64s
                        Total time: 6725.27s
                               ETA: 518279.2s

################################################################################
                    [1m Learning iteration 2562/200000 [0m

                       Computation: 3184 steps/s (collection: 0.488s, learning 2.085s)
               Value function loss: 88383.1068
                    Surrogate loss: 0.0104
             Mean action noise std: 0.88
                       Mean reward: 10222.77
               Mean episode length: 404.27
                 Mean success rate: 80.00
                  Mean reward/step: 25.84
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 20996096
                    Iteration time: 2.57s
                        Total time: 6727.85s
                               ETA: 518272.5s

################################################################################
                    [1m Learning iteration 2563/200000 [0m

                       Computation: 3233 steps/s (collection: 0.460s, learning 2.074s)
               Value function loss: 88301.6053
                    Surrogate loss: 0.0106
             Mean action noise std: 0.88
                       Mean reward: 9965.00
               Mean episode length: 395.69
                 Mean success rate: 78.50
                  Mean reward/step: 25.56
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 21004288
                    Iteration time: 2.53s
                        Total time: 6730.38s
                               ETA: 518262.8s

################################################################################
                    [1m Learning iteration 2564/200000 [0m

                       Computation: 3186 steps/s (collection: 0.452s, learning 2.119s)
               Value function loss: 104918.3698
                    Surrogate loss: 0.0087
             Mean action noise std: 0.88
                       Mean reward: 10016.80
               Mean episode length: 400.95
                 Mean success rate: 78.50
                  Mean reward/step: 24.23
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 21012480
                    Iteration time: 2.57s
                        Total time: 6732.95s
                               ETA: 518256.0s

################################################################################
                    [1m Learning iteration 2565/200000 [0m

                       Computation: 3271 steps/s (collection: 0.446s, learning 2.058s)
               Value function loss: 71697.7603
                    Surrogate loss: 0.0101
             Mean action noise std: 0.88
                       Mean reward: 10257.23
               Mean episode length: 409.31
                 Mean success rate: 80.00
                  Mean reward/step: 23.66
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 21020672
                    Iteration time: 2.50s
                        Total time: 6735.45s
                               ETA: 518244.1s

################################################################################
                    [1m Learning iteration 2566/200000 [0m

                       Computation: 3234 steps/s (collection: 0.469s, learning 2.063s)
               Value function loss: 126697.3074
                    Surrogate loss: 0.0128
             Mean action noise std: 0.88
                       Mean reward: 10396.53
               Mean episode length: 410.37
                 Mean success rate: 80.00
                  Mean reward/step: 23.28
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 21028864
                    Iteration time: 2.53s
                        Total time: 6737.99s
                               ETA: 518234.4s

################################################################################
                    [1m Learning iteration 2567/200000 [0m

                       Computation: 3195 steps/s (collection: 0.488s, learning 2.075s)
               Value function loss: 120196.5437
                    Surrogate loss: 0.0137
             Mean action noise std: 0.88
                       Mean reward: 10645.93
               Mean episode length: 418.10
                 Mean success rate: 81.50
                  Mean reward/step: 22.92
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 2.56s
                        Total time: 6740.55s
                               ETA: 518227.1s

################################################################################
                    [1m Learning iteration 2568/200000 [0m

                       Computation: 3124 steps/s (collection: 0.518s, learning 2.105s)
               Value function loss: 106763.0584
                    Surrogate loss: 0.0111
             Mean action noise std: 0.88
                       Mean reward: 10734.39
               Mean episode length: 419.38
                 Mean success rate: 82.50
                  Mean reward/step: 22.36
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 21045248
                    Iteration time: 2.62s
                        Total time: 6743.17s
                               ETA: 518224.2s

################################################################################
                    [1m Learning iteration 2569/200000 [0m

                       Computation: 3185 steps/s (collection: 0.462s, learning 2.110s)
               Value function loss: 83570.7277
                    Surrogate loss: 0.0091
             Mean action noise std: 0.88
                       Mean reward: 10549.28
               Mean episode length: 415.94
                 Mean success rate: 81.00
                  Mean reward/step: 22.83
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 21053440
                    Iteration time: 2.57s
                        Total time: 6745.74s
                               ETA: 518217.5s

################################################################################
                    [1m Learning iteration 2570/200000 [0m

                       Computation: 3185 steps/s (collection: 0.505s, learning 2.067s)
               Value function loss: 102666.3723
                    Surrogate loss: 0.0106
             Mean action noise std: 0.88
                       Mean reward: 9944.00
               Mean episode length: 400.69
                 Mean success rate: 76.50
                  Mean reward/step: 23.66
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 21061632
                    Iteration time: 2.57s
                        Total time: 6748.32s
                               ETA: 518210.8s

################################################################################
                    [1m Learning iteration 2571/200000 [0m

                       Computation: 3269 steps/s (collection: 0.465s, learning 2.041s)
               Value function loss: 112484.4033
                    Surrogate loss: 0.0086
             Mean action noise std: 0.88
                       Mean reward: 9842.07
               Mean episode length: 399.44
                 Mean success rate: 76.50
                  Mean reward/step: 23.59
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 21069824
                    Iteration time: 2.51s
                        Total time: 6750.82s
                               ETA: 518199.1s

################################################################################
                    [1m Learning iteration 2572/200000 [0m

                       Computation: 3172 steps/s (collection: 0.505s, learning 2.077s)
               Value function loss: 80643.4678
                    Surrogate loss: 0.0098
             Mean action noise std: 0.88
                       Mean reward: 9777.56
               Mean episode length: 397.71
                 Mean success rate: 76.00
                  Mean reward/step: 23.71
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 21078016
                    Iteration time: 2.58s
                        Total time: 6753.40s
                               ETA: 518193.1s

################################################################################
                    [1m Learning iteration 2573/200000 [0m

                       Computation: 3176 steps/s (collection: 0.484s, learning 2.095s)
               Value function loss: 83774.3156
                    Surrogate loss: 0.0084
             Mean action noise std: 0.88
                       Mean reward: 9590.58
               Mean episode length: 390.85
                 Mean success rate: 74.50
                  Mean reward/step: 24.57
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 21086208
                    Iteration time: 2.58s
                        Total time: 6755.98s
                               ETA: 518187.0s

################################################################################
                    [1m Learning iteration 2574/200000 [0m

                       Computation: 3175 steps/s (collection: 0.458s, learning 2.122s)
               Value function loss: 66189.0256
                    Surrogate loss: 0.0124
             Mean action noise std: 0.88
                       Mean reward: 9398.27
               Mean episode length: 387.03
                 Mean success rate: 74.00
                  Mean reward/step: 24.79
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 21094400
                    Iteration time: 2.58s
                        Total time: 6758.56s
                               ETA: 518180.9s

################################################################################
                    [1m Learning iteration 2575/200000 [0m

                       Computation: 3196 steps/s (collection: 0.459s, learning 2.104s)
               Value function loss: 84811.5932
                    Surrogate loss: 0.0084
             Mean action noise std: 0.88
                       Mean reward: 9356.19
               Mean episode length: 386.35
                 Mean success rate: 74.00
                  Mean reward/step: 24.31
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 21102592
                    Iteration time: 2.56s
                        Total time: 6761.13s
                               ETA: 518173.6s

################################################################################
                    [1m Learning iteration 2576/200000 [0m

                       Computation: 3138 steps/s (collection: 0.458s, learning 2.151s)
               Value function loss: 51563.6922
                    Surrogate loss: 0.0093
             Mean action noise std: 0.88
                       Mean reward: 9411.19
               Mean episode length: 390.85
                 Mean success rate: 74.50
                  Mean reward/step: 25.10
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 21110784
                    Iteration time: 2.61s
                        Total time: 6763.74s
                               ETA: 518169.8s

################################################################################
                    [1m Learning iteration 2577/200000 [0m

                       Computation: 3132 steps/s (collection: 0.487s, learning 2.128s)
               Value function loss: 64526.9133
                    Surrogate loss: 0.0126
             Mean action noise std: 0.88
                       Mean reward: 9229.64
               Mean episode length: 387.05
                 Mean success rate: 74.00
                  Mean reward/step: 26.04
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 21118976
                    Iteration time: 2.62s
                        Total time: 6766.35s
                               ETA: 518166.5s

################################################################################
                    [1m Learning iteration 2578/200000 [0m

                       Computation: 3258 steps/s (collection: 0.476s, learning 2.038s)
               Value function loss: 74563.1822
                    Surrogate loss: 0.0086
             Mean action noise std: 0.88
                       Mean reward: 9116.98
               Mean episode length: 383.95
                 Mean success rate: 73.00
                  Mean reward/step: 26.43
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 21127168
                    Iteration time: 2.51s
                        Total time: 6768.86s
                               ETA: 518155.4s

################################################################################
                    [1m Learning iteration 2579/200000 [0m

                       Computation: 3244 steps/s (collection: 0.473s, learning 2.052s)
               Value function loss: 120961.9563
                    Surrogate loss: 0.0125
             Mean action noise std: 0.88
                       Mean reward: 9427.58
               Mean episode length: 393.58
                 Mean success rate: 76.00
                  Mean reward/step: 26.21
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 2.52s
                        Total time: 6771.39s
                               ETA: 518145.1s

################################################################################
                    [1m Learning iteration 2580/200000 [0m

                       Computation: 3322 steps/s (collection: 0.448s, learning 2.018s)
               Value function loss: 124012.6391
                    Surrogate loss: 0.0090
             Mean action noise std: 0.89
                       Mean reward: 9715.94
               Mean episode length: 402.31
                 Mean success rate: 78.50
                  Mean reward/step: 25.07
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 21143552
                    Iteration time: 2.47s
                        Total time: 6773.85s
                               ETA: 518130.3s

################################################################################
                    [1m Learning iteration 2581/200000 [0m

                       Computation: 3205 steps/s (collection: 0.485s, learning 2.070s)
               Value function loss: 98052.3584
                    Surrogate loss: 0.0116
             Mean action noise std: 0.89
                       Mean reward: 9846.01
               Mean episode length: 404.29
                 Mean success rate: 79.50
                  Mean reward/step: 24.84
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 21151744
                    Iteration time: 2.56s
                        Total time: 6776.41s
                               ETA: 518122.4s

################################################################################
                    [1m Learning iteration 2582/200000 [0m

                       Computation: 3249 steps/s (collection: 0.487s, learning 2.034s)
               Value function loss: 91113.9958
                    Surrogate loss: 0.0114
             Mean action noise std: 0.89
                       Mean reward: 9996.25
               Mean episode length: 408.35
                 Mean success rate: 80.00
                  Mean reward/step: 24.34
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 21159936
                    Iteration time: 2.52s
                        Total time: 6778.93s
                               ETA: 518111.9s

################################################################################
                    [1m Learning iteration 2583/200000 [0m

                       Computation: 3257 steps/s (collection: 0.455s, learning 2.060s)
               Value function loss: 109110.1166
                    Surrogate loss: 0.0115
             Mean action noise std: 0.89
                       Mean reward: 10214.39
               Mean episode length: 414.38
                 Mean success rate: 81.00
                  Mean reward/step: 24.01
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 21168128
                    Iteration time: 2.51s
                        Total time: 6781.45s
                               ETA: 518100.9s

################################################################################
                    [1m Learning iteration 2584/200000 [0m

                       Computation: 3174 steps/s (collection: 0.499s, learning 2.081s)
               Value function loss: 99593.1613
                    Surrogate loss: 0.0103
             Mean action noise std: 0.89
                       Mean reward: 10657.91
               Mean episode length: 429.87
                 Mean success rate: 84.50
                  Mean reward/step: 23.97
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 21176320
                    Iteration time: 2.58s
                        Total time: 6784.03s
                               ETA: 518094.9s

################################################################################
                    [1m Learning iteration 2585/200000 [0m

                       Computation: 3194 steps/s (collection: 0.503s, learning 2.062s)
               Value function loss: 83673.0535
                    Surrogate loss: 0.0108
             Mean action noise std: 0.89
                       Mean reward: 10546.58
               Mean episode length: 427.44
                 Mean success rate: 84.00
                  Mean reward/step: 23.90
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 21184512
                    Iteration time: 2.56s
                        Total time: 6786.59s
                               ETA: 518087.8s

################################################################################
                    [1m Learning iteration 2586/200000 [0m

                       Computation: 3220 steps/s (collection: 0.454s, learning 2.090s)
               Value function loss: 112460.9729
                    Surrogate loss: 0.0128
             Mean action noise std: 0.89
                       Mean reward: 10374.16
               Mean episode length: 419.39
                 Mean success rate: 82.50
                  Mean reward/step: 24.21
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 21192704
                    Iteration time: 2.54s
                        Total time: 6789.14s
                               ETA: 518079.0s

################################################################################
                    [1m Learning iteration 2587/200000 [0m

                       Computation: 3179 steps/s (collection: 0.495s, learning 2.081s)
               Value function loss: 104425.8288
                    Surrogate loss: 0.0103
             Mean action noise std: 0.89
                       Mean reward: 10514.23
               Mean episode length: 424.19
                 Mean success rate: 83.50
                  Mean reward/step: 23.91
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 21200896
                    Iteration time: 2.58s
                        Total time: 6791.71s
                               ETA: 518072.7s

################################################################################
                    [1m Learning iteration 2588/200000 [0m

                       Computation: 3117 steps/s (collection: 0.518s, learning 2.110s)
               Value function loss: 77595.7240
                    Surrogate loss: 0.0135
             Mean action noise std: 0.89
                       Mean reward: 10482.97
               Mean episode length: 423.00
                 Mean success rate: 83.00
                  Mean reward/step: 24.40
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 21209088
                    Iteration time: 2.63s
                        Total time: 6794.34s
                               ETA: 518070.4s

################################################################################
                    [1m Learning iteration 2589/200000 [0m

                       Computation: 3255 steps/s (collection: 0.463s, learning 2.053s)
               Value function loss: 97830.6912
                    Surrogate loss: 0.0126
             Mean action noise std: 0.89
                       Mean reward: 10044.77
               Mean episode length: 408.26
                 Mean success rate: 79.50
                  Mean reward/step: 24.74
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 21217280
                    Iteration time: 2.52s
                        Total time: 6796.86s
                               ETA: 518059.5s

################################################################################
                    [1m Learning iteration 2590/200000 [0m

                       Computation: 3305 steps/s (collection: 0.469s, learning 2.009s)
               Value function loss: 101799.8729
                    Surrogate loss: 0.0132
             Mean action noise std: 0.89
                       Mean reward: 9771.77
               Mean episode length: 397.90
                 Mean success rate: 77.00
                  Mean reward/step: 24.92
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 21225472
                    Iteration time: 2.48s
                        Total time: 6799.33s
                               ETA: 518045.8s

################################################################################
                    [1m Learning iteration 2591/200000 [0m

                       Computation: 3169 steps/s (collection: 0.491s, learning 2.093s)
               Value function loss: 59063.2740
                    Surrogate loss: 0.0126
             Mean action noise std: 0.89
                       Mean reward: 9753.37
               Mean episode length: 397.22
                 Mean success rate: 77.00
                  Mean reward/step: 24.56
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 2.58s
                        Total time: 6801.92s
                               ETA: 518040.1s

################################################################################
                    [1m Learning iteration 2592/200000 [0m

                       Computation: 3274 steps/s (collection: 0.448s, learning 2.053s)
               Value function loss: 57667.6053
                    Surrogate loss: 0.0128
             Mean action noise std: 0.89
                       Mean reward: 9595.51
               Mean episode length: 393.27
                 Mean success rate: 76.00
                  Mean reward/step: 25.29
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 21241856
                    Iteration time: 2.50s
                        Total time: 6804.42s
                               ETA: 518028.2s

################################################################################
                    [1m Learning iteration 2593/200000 [0m

                       Computation: 3263 steps/s (collection: 0.458s, learning 2.053s)
               Value function loss: 81418.4408
                    Surrogate loss: 0.0108
             Mean action noise std: 0.89
                       Mean reward: 9580.65
               Mean episode length: 393.79
                 Mean success rate: 76.00
                  Mean reward/step: 25.89
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 21250048
                    Iteration time: 2.51s
                        Total time: 6806.93s
                               ETA: 518016.9s

################################################################################
                    [1m Learning iteration 2594/200000 [0m

                       Computation: 3267 steps/s (collection: 0.472s, learning 2.036s)
               Value function loss: 70434.2624
                    Surrogate loss: 0.0099
             Mean action noise std: 0.89
                       Mean reward: 9604.22
               Mean episode length: 393.19
                 Mean success rate: 76.50
                  Mean reward/step: 25.90
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 21258240
                    Iteration time: 2.51s
                        Total time: 6809.44s
                               ETA: 518005.4s

################################################################################
                    [1m Learning iteration 2595/200000 [0m

                       Computation: 3281 steps/s (collection: 0.467s, learning 2.030s)
               Value function loss: 104716.2117
                    Surrogate loss: 0.0082
             Mean action noise std: 0.89
                       Mean reward: 9896.22
               Mean episode length: 399.33
                 Mean success rate: 78.00
                  Mean reward/step: 25.83
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 21266432
                    Iteration time: 2.50s
                        Total time: 6811.93s
                               ETA: 517993.0s

################################################################################
                    [1m Learning iteration 2596/200000 [0m

                       Computation: 3048 steps/s (collection: 0.482s, learning 2.205s)
               Value function loss: 98504.1037
                    Surrogate loss: 0.0083
             Mean action noise std: 0.89
                       Mean reward: 9763.54
               Mean episode length: 396.35
                 Mean success rate: 78.00
                  Mean reward/step: 25.20
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 21274624
                    Iteration time: 2.69s
                        Total time: 6814.62s
                               ETA: 517995.2s

################################################################################
                    [1m Learning iteration 2597/200000 [0m

                       Computation: 3279 steps/s (collection: 0.459s, learning 2.039s)
               Value function loss: 95154.5475
                    Surrogate loss: 0.0105
             Mean action noise std: 0.89
                       Mean reward: 9936.87
               Mean episode length: 400.74
                 Mean success rate: 79.00
                  Mean reward/step: 25.19
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 21282816
                    Iteration time: 2.50s
                        Total time: 6817.12s
                               ETA: 517983.0s

################################################################################
                    [1m Learning iteration 2598/200000 [0m

                       Computation: 3129 steps/s (collection: 0.512s, learning 2.106s)
               Value function loss: 102044.1064
                    Surrogate loss: 0.0111
             Mean action noise std: 0.89
                       Mean reward: 9782.24
               Mean episode length: 397.40
                 Mean success rate: 78.50
                  Mean reward/step: 25.13
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 21291008
                    Iteration time: 2.62s
                        Total time: 6819.74s
                               ETA: 517980.0s

################################################################################
                    [1m Learning iteration 2599/200000 [0m

                       Computation: 3189 steps/s (collection: 0.499s, learning 2.069s)
               Value function loss: 96542.9645
                    Surrogate loss: 0.0112
             Mean action noise std: 0.89
                       Mean reward: 9671.32
               Mean episode length: 392.98
                 Mean success rate: 77.00
                  Mean reward/step: 24.99
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 21299200
                    Iteration time: 2.57s
                        Total time: 6822.31s
                               ETA: 517973.1s

################################################################################
                    [1m Learning iteration 2600/200000 [0m

                       Computation: 3185 steps/s (collection: 0.490s, learning 2.081s)
               Value function loss: 115422.5547
                    Surrogate loss: 0.0120
             Mean action noise std: 0.89
                       Mean reward: 10164.67
               Mean episode length: 411.04
                 Mean success rate: 80.50
                  Mean reward/step: 24.70
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 21307392
                    Iteration time: 2.57s
                        Total time: 6824.88s
                               ETA: 517966.5s

################################################################################
                    [1m Learning iteration 2601/200000 [0m

                       Computation: 3164 steps/s (collection: 0.489s, learning 2.100s)
               Value function loss: 101681.1643
                    Surrogate loss: 0.0122
             Mean action noise std: 0.89
                       Mean reward: 10355.51
               Mean episode length: 414.89
                 Mean success rate: 81.50
                  Mean reward/step: 24.81
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 21315584
                    Iteration time: 2.59s
                        Total time: 6827.47s
                               ETA: 517961.2s

################################################################################
                    [1m Learning iteration 2602/200000 [0m

                       Computation: 3198 steps/s (collection: 0.467s, learning 2.094s)
               Value function loss: 102586.9056
                    Surrogate loss: 0.0114
             Mean action noise std: 0.89
                       Mean reward: 10285.90
               Mean episode length: 412.29
                 Mean success rate: 81.50
                  Mean reward/step: 24.81
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 21323776
                    Iteration time: 2.56s
                        Total time: 6830.03s
                               ETA: 517953.8s

################################################################################
                    [1m Learning iteration 2603/200000 [0m

                       Computation: 3218 steps/s (collection: 0.466s, learning 2.079s)
               Value function loss: 101404.2291
                    Surrogate loss: 0.0092
             Mean action noise std: 0.89
                       Mean reward: 10679.29
               Mean episode length: 424.65
                 Mean success rate: 84.00
                  Mean reward/step: 24.51
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 2.54s
                        Total time: 6832.57s
                               ETA: 517945.2s

################################################################################
                    [1m Learning iteration 2604/200000 [0m

                       Computation: 3216 steps/s (collection: 0.483s, learning 2.064s)
               Value function loss: 78623.6874
                    Surrogate loss: 0.0106
             Mean action noise std: 0.89
                       Mean reward: 10733.56
               Mean episode length: 425.19
                 Mean success rate: 83.50
                  Mean reward/step: 25.13
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 21340160
                    Iteration time: 2.55s
                        Total time: 6835.12s
                               ETA: 517936.8s

################################################################################
                    [1m Learning iteration 2605/200000 [0m

                       Computation: 3276 steps/s (collection: 0.471s, learning 2.029s)
               Value function loss: 109533.8102
                    Surrogate loss: 0.0092
             Mean action noise std: 0.89
                       Mean reward: 10956.00
               Mean episode length: 432.70
                 Mean success rate: 85.00
                  Mean reward/step: 25.27
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 21348352
                    Iteration time: 2.50s
                        Total time: 6837.62s
                               ETA: 517924.8s

################################################################################
                    [1m Learning iteration 2606/200000 [0m

                       Computation: 3204 steps/s (collection: 0.477s, learning 2.079s)
               Value function loss: 116083.9944
                    Surrogate loss: 0.0104
             Mean action noise std: 0.89
                       Mean reward: 10834.81
               Mean episode length: 429.44
                 Mean success rate: 84.50
                  Mean reward/step: 24.40
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 21356544
                    Iteration time: 2.56s
                        Total time: 6840.18s
                               ETA: 517917.1s

################################################################################
                    [1m Learning iteration 2607/200000 [0m

                       Computation: 3208 steps/s (collection: 0.466s, learning 2.087s)
               Value function loss: 48921.4828
                    Surrogate loss: 0.0103
             Mean action noise std: 0.89
                       Mean reward: 10827.73
               Mean episode length: 426.19
                 Mean success rate: 84.50
                  Mean reward/step: 24.83
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 21364736
                    Iteration time: 2.55s
                        Total time: 6842.73s
                               ETA: 517909.1s

################################################################################
                    [1m Learning iteration 2608/200000 [0m

                       Computation: 3225 steps/s (collection: 0.480s, learning 2.060s)
               Value function loss: 65080.6038
                    Surrogate loss: 0.0102
             Mean action noise std: 0.89
                       Mean reward: 10552.87
               Mean episode length: 415.95
                 Mean success rate: 83.00
                  Mean reward/step: 26.43
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 21372928
                    Iteration time: 2.54s
                        Total time: 6845.27s
                               ETA: 517900.2s

################################################################################
                    [1m Learning iteration 2609/200000 [0m

                       Computation: 3239 steps/s (collection: 0.463s, learning 2.066s)
               Value function loss: 81166.8457
                    Surrogate loss: 0.0114
             Mean action noise std: 0.89
                       Mean reward: 10896.64
               Mean episode length: 425.49
                 Mean success rate: 85.50
                  Mean reward/step: 26.72
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 21381120
                    Iteration time: 2.53s
                        Total time: 6847.80s
                               ETA: 517890.4s

################################################################################
                    [1m Learning iteration 2610/200000 [0m

                       Computation: 3247 steps/s (collection: 0.447s, learning 2.075s)
               Value function loss: 70633.7939
                    Surrogate loss: 0.0103
             Mean action noise std: 0.89
                       Mean reward: 10897.52
               Mean episode length: 425.62
                 Mean success rate: 86.00
                  Mean reward/step: 26.86
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 21389312
                    Iteration time: 2.52s
                        Total time: 6850.32s
                               ETA: 517880.1s

################################################################################
                    [1m Learning iteration 2611/200000 [0m

                       Computation: 3140 steps/s (collection: 0.525s, learning 2.084s)
               Value function loss: 121280.4184
                    Surrogate loss: 0.0113
             Mean action noise std: 0.89
                       Mean reward: 10601.22
               Mean episode length: 421.43
                 Mean success rate: 85.00
                  Mean reward/step: 26.42
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 21397504
                    Iteration time: 2.61s
                        Total time: 6852.93s
                               ETA: 517876.3s

################################################################################
                    [1m Learning iteration 2612/200000 [0m

                       Computation: 3169 steps/s (collection: 0.500s, learning 2.085s)
               Value function loss: 81419.4298
                    Surrogate loss: 0.0121
             Mean action noise std: 0.89
                       Mean reward: 10516.57
               Mean episode length: 420.28
                 Mean success rate: 85.00
                  Mean reward/step: 25.24
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 21405696
                    Iteration time: 2.58s
                        Total time: 6855.51s
                               ETA: 517870.8s

################################################################################
                    [1m Learning iteration 2613/200000 [0m

                       Computation: 3269 steps/s (collection: 0.443s, learning 2.062s)
               Value function loss: 87133.1558
                    Surrogate loss: 0.0119
             Mean action noise std: 0.89
                       Mean reward: 10862.49
               Mean episode length: 431.13
                 Mean success rate: 87.00
                  Mean reward/step: 25.03
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 21413888
                    Iteration time: 2.51s
                        Total time: 6858.02s
                               ETA: 517859.3s

################################################################################
                    [1m Learning iteration 2614/200000 [0m

                       Computation: 3221 steps/s (collection: 0.474s, learning 2.068s)
               Value function loss: 106315.5250
                    Surrogate loss: 0.0129
             Mean action noise std: 0.89
                       Mean reward: 10519.20
               Mean episode length: 420.31
                 Mean success rate: 84.50
                  Mean reward/step: 25.16
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 21422080
                    Iteration time: 2.54s
                        Total time: 6860.56s
                               ETA: 517850.5s

################################################################################
                    [1m Learning iteration 2615/200000 [0m

                       Computation: 3261 steps/s (collection: 0.473s, learning 2.039s)
               Value function loss: 106364.5840
                    Surrogate loss: 0.0135
             Mean action noise std: 0.89
                       Mean reward: 10293.00
               Mean episode length: 413.08
                 Mean success rate: 83.00
                  Mean reward/step: 25.08
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 2.51s
                        Total time: 6863.07s
                               ETA: 517839.5s

################################################################################
                    [1m Learning iteration 2616/200000 [0m

                       Computation: 3177 steps/s (collection: 0.474s, learning 2.105s)
               Value function loss: 106313.3480
                    Surrogate loss: 0.0093
             Mean action noise std: 0.89
                       Mean reward: 10227.12
               Mean episode length: 409.26
                 Mean success rate: 82.50
                  Mean reward/step: 24.30
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 21438464
                    Iteration time: 2.58s
                        Total time: 6865.65s
                               ETA: 517833.4s

################################################################################
                    [1m Learning iteration 2617/200000 [0m

                       Computation: 3244 steps/s (collection: 0.468s, learning 2.057s)
               Value function loss: 117005.9402
                    Surrogate loss: 0.0109
             Mean action noise std: 0.89
                       Mean reward: 10444.29
               Mean episode length: 417.69
                 Mean success rate: 83.00
                  Mean reward/step: 23.75
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 21446656
                    Iteration time: 2.52s
                        Total time: 6868.18s
                               ETA: 517823.4s

################################################################################
                    [1m Learning iteration 2618/200000 [0m

                       Computation: 3210 steps/s (collection: 0.478s, learning 2.073s)
               Value function loss: 107274.2883
                    Surrogate loss: 0.0112
             Mean action noise std: 0.89
                       Mean reward: 10549.50
               Mean episode length: 420.47
                 Mean success rate: 83.50
                  Mean reward/step: 23.36
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 21454848
                    Iteration time: 2.55s
                        Total time: 6870.73s
                               ETA: 517815.3s

################################################################################
                    [1m Learning iteration 2619/200000 [0m

                       Computation: 3162 steps/s (collection: 0.498s, learning 2.092s)
               Value function loss: 87295.9847
                    Surrogate loss: 0.0153
             Mean action noise std: 0.89
                       Mean reward: 10406.86
               Mean episode length: 414.60
                 Mean success rate: 82.50
                  Mean reward/step: 23.73
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 21463040
                    Iteration time: 2.59s
                        Total time: 6873.32s
                               ETA: 517810.2s

################################################################################
                    [1m Learning iteration 2620/200000 [0m

                       Computation: 3259 steps/s (collection: 0.467s, learning 2.046s)
               Value function loss: 112428.1438
                    Surrogate loss: 0.0130
             Mean action noise std: 0.89
                       Mean reward: 10325.92
               Mean episode length: 406.74
                 Mean success rate: 80.50
                  Mean reward/step: 24.16
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 21471232
                    Iteration time: 2.51s
                        Total time: 6875.83s
                               ETA: 517799.3s

################################################################################
                    [1m Learning iteration 2621/200000 [0m

                       Computation: 3307 steps/s (collection: 0.457s, learning 2.020s)
               Value function loss: 98218.7029
                    Surrogate loss: 0.0098
             Mean action noise std: 0.89
                       Mean reward: 10470.30
               Mean episode length: 408.26
                 Mean success rate: 81.00
                  Mean reward/step: 24.38
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 21479424
                    Iteration time: 2.48s
                        Total time: 6878.31s
                               ETA: 517785.6s

################################################################################
                    [1m Learning iteration 2622/200000 [0m

                       Computation: 3321 steps/s (collection: 0.430s, learning 2.036s)
               Value function loss: 91735.6169
                    Surrogate loss: 0.0121
             Mean action noise std: 0.89
                       Mean reward: 10215.22
               Mean episode length: 400.56
                 Mean success rate: 79.50
                  Mean reward/step: 24.53
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 21487616
                    Iteration time: 2.47s
                        Total time: 6880.78s
                               ETA: 517771.2s

################################################################################
                    [1m Learning iteration 2623/200000 [0m

                       Computation: 3247 steps/s (collection: 0.470s, learning 2.053s)
               Value function loss: 79562.3292
                    Surrogate loss: 0.0110
             Mean action noise std: 0.89
                       Mean reward: 10010.07
               Mean episode length: 393.72
                 Mean success rate: 78.50
                  Mean reward/step: 24.99
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 21495808
                    Iteration time: 2.52s
                        Total time: 6883.30s
                               ETA: 517761.0s

################################################################################
                    [1m Learning iteration 2624/200000 [0m

                       Computation: 3244 steps/s (collection: 0.460s, learning 2.065s)
               Value function loss: 89975.3738
                    Surrogate loss: 0.0100
             Mean action noise std: 0.89
                       Mean reward: 10061.78
               Mean episode length: 397.04
                 Mean success rate: 79.00
                  Mean reward/step: 25.70
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 21504000
                    Iteration time: 2.52s
                        Total time: 6885.82s
                               ETA: 517751.0s

################################################################################
                    [1m Learning iteration 2625/200000 [0m

                       Computation: 3259 steps/s (collection: 0.432s, learning 2.081s)
               Value function loss: 87827.4330
                    Surrogate loss: 0.0111
             Mean action noise std: 0.89
                       Mean reward: 10010.28
               Mean episode length: 397.81
                 Mean success rate: 79.00
                  Mean reward/step: 25.41
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 21512192
                    Iteration time: 2.51s
                        Total time: 6888.34s
                               ETA: 517740.1s

################################################################################
                    [1m Learning iteration 2626/200000 [0m

                       Computation: 3224 steps/s (collection: 0.465s, learning 2.076s)
               Value function loss: 93645.0379
                    Surrogate loss: 0.0104
             Mean action noise std: 0.89
                       Mean reward: 9818.07
               Mean episode length: 392.83
                 Mean success rate: 77.50
                  Mean reward/step: 25.21
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 21520384
                    Iteration time: 2.54s
                        Total time: 6890.88s
                               ETA: 517731.3s

################################################################################
                    [1m Learning iteration 2627/200000 [0m

                       Computation: 3242 steps/s (collection: 0.455s, learning 2.071s)
               Value function loss: 99392.5328
                    Surrogate loss: 0.0110
             Mean action noise std: 0.89
                       Mean reward: 9629.66
               Mean episode length: 389.06
                 Mean success rate: 76.50
                  Mean reward/step: 24.49
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 2.53s
                        Total time: 6893.40s
                               ETA: 517721.4s

################################################################################
                    [1m Learning iteration 2628/200000 [0m

                       Computation: 3228 steps/s (collection: 0.461s, learning 2.077s)
               Value function loss: 97503.3508
                    Surrogate loss: 0.0140
             Mean action noise std: 0.89
                       Mean reward: 9686.25
               Mean episode length: 390.76
                 Mean success rate: 76.50
                  Mean reward/step: 24.47
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 21536768
                    Iteration time: 2.54s
                        Total time: 6895.94s
                               ETA: 517712.4s

################################################################################
                    [1m Learning iteration 2629/200000 [0m

                       Computation: 3245 steps/s (collection: 0.472s, learning 2.052s)
               Value function loss: 59131.9230
                    Surrogate loss: 0.0093
             Mean action noise std: 0.89
                       Mean reward: 9392.55
               Mean episode length: 385.37
                 Mean success rate: 74.50
                  Mean reward/step: 24.96
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 21544960
                    Iteration time: 2.52s
                        Total time: 6898.47s
                               ETA: 517702.4s

################################################################################
                    [1m Learning iteration 2630/200000 [0m

                       Computation: 3127 steps/s (collection: 0.501s, learning 2.119s)
               Value function loss: 116041.6468
                    Surrogate loss: 0.0095
             Mean action noise std: 0.89
                       Mean reward: 9934.75
               Mean episode length: 403.19
                 Mean success rate: 78.00
                  Mean reward/step: 25.24
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 21553152
                    Iteration time: 2.62s
                        Total time: 6901.09s
                               ETA: 517699.5s

################################################################################
                    [1m Learning iteration 2631/200000 [0m

                       Computation: 3232 steps/s (collection: 0.455s, learning 2.080s)
               Value function loss: 101077.9517
                    Surrogate loss: 0.0142
             Mean action noise std: 0.89
                       Mean reward: 9863.83
               Mean episode length: 401.74
                 Mean success rate: 78.00
                  Mean reward/step: 25.18
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 21561344
                    Iteration time: 2.53s
                        Total time: 6903.62s
                               ETA: 517690.2s

################################################################################
                    [1m Learning iteration 2632/200000 [0m

                       Computation: 3192 steps/s (collection: 0.469s, learning 2.097s)
               Value function loss: 111428.0730
                    Surrogate loss: 0.0125
             Mean action noise std: 0.89
                       Mean reward: 9844.24
               Mean episode length: 401.19
                 Mean success rate: 77.50
                  Mean reward/step: 24.97
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 21569536
                    Iteration time: 2.57s
                        Total time: 6906.19s
                               ETA: 517683.3s

################################################################################
                    [1m Learning iteration 2633/200000 [0m

                       Computation: 3223 steps/s (collection: 0.484s, learning 2.058s)
               Value function loss: 132396.5960
                    Surrogate loss: 0.0115
             Mean action noise std: 0.89
                       Mean reward: 10274.77
               Mean episode length: 416.85
                 Mean success rate: 80.50
                  Mean reward/step: 24.85
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 21577728
                    Iteration time: 2.54s
                        Total time: 6908.73s
                               ETA: 517674.6s

################################################################################
                    [1m Learning iteration 2634/200000 [0m

                       Computation: 3194 steps/s (collection: 0.478s, learning 2.086s)
               Value function loss: 82149.6512
                    Surrogate loss: 0.0151
             Mean action noise std: 0.89
                       Mean reward: 10256.32
               Mean episode length: 413.82
                 Mean success rate: 80.50
                  Mean reward/step: 24.57
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 21585920
                    Iteration time: 2.56s
                        Total time: 6911.29s
                               ETA: 517667.6s

################################################################################
                    [1m Learning iteration 2635/200000 [0m

                       Computation: 3190 steps/s (collection: 0.465s, learning 2.103s)
               Value function loss: 97609.6420
                    Surrogate loss: 0.0129
             Mean action noise std: 0.89
                       Mean reward: 10343.91
               Mean episode length: 416.67
                 Mean success rate: 81.50
                  Mean reward/step: 24.37
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 21594112
                    Iteration time: 2.57s
                        Total time: 6913.86s
                               ETA: 517660.8s

################################################################################
                    [1m Learning iteration 2636/200000 [0m

                       Computation: 3219 steps/s (collection: 0.447s, learning 2.097s)
               Value function loss: 107555.0494
                    Surrogate loss: 0.0115
             Mean action noise std: 0.89
                       Mean reward: 10063.40
               Mean episode length: 406.48
                 Mean success rate: 79.00
                  Mean reward/step: 23.94
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 21602304
                    Iteration time: 2.54s
                        Total time: 6916.40s
                               ETA: 517652.3s

################################################################################
                    [1m Learning iteration 2637/200000 [0m

                       Computation: 3193 steps/s (collection: 0.458s, learning 2.107s)
               Value function loss: 107528.4682
                    Surrogate loss: 0.0124
             Mean action noise std: 0.89
                       Mean reward: 9970.70
               Mean episode length: 403.00
                 Mean success rate: 77.50
                  Mean reward/step: 24.11
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 21610496
                    Iteration time: 2.57s
                        Total time: 6918.97s
                               ETA: 517645.4s

################################################################################
                    [1m Learning iteration 2638/200000 [0m

                       Computation: 3238 steps/s (collection: 0.460s, learning 2.069s)
               Value function loss: 65571.5178
                    Surrogate loss: 0.0138
             Mean action noise std: 0.89
                       Mean reward: 9946.45
               Mean episode length: 400.41
                 Mean success rate: 77.50
                  Mean reward/step: 24.80
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 21618688
                    Iteration time: 2.53s
                        Total time: 6921.50s
                               ETA: 517635.8s

################################################################################
                    [1m Learning iteration 2639/200000 [0m

                       Computation: 3203 steps/s (collection: 0.512s, learning 2.045s)
               Value function loss: 69667.4243
                    Surrogate loss: 0.0136
             Mean action noise std: 0.89
                       Mean reward: 9416.62
               Mean episode length: 383.88
                 Mean success rate: 73.50
                  Mean reward/step: 25.55
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 2.56s
                        Total time: 6924.06s
                               ETA: 517628.2s

################################################################################
                    [1m Learning iteration 2640/200000 [0m

                       Computation: 3249 steps/s (collection: 0.488s, learning 2.033s)
               Value function loss: 82734.7362
                    Surrogate loss: 0.0113
             Mean action noise std: 0.89
                       Mean reward: 9090.32
               Mean episode length: 375.49
                 Mean success rate: 71.00
                  Mean reward/step: 26.17
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 21635072
                    Iteration time: 2.52s
                        Total time: 6926.58s
                               ETA: 517618.0s

################################################################################
                    [1m Learning iteration 2641/200000 [0m

                       Computation: 3316 steps/s (collection: 0.451s, learning 2.019s)
               Value function loss: 85790.4591
                    Surrogate loss: 0.0108
             Mean action noise std: 0.89
                       Mean reward: 8898.18
               Mean episode length: 368.19
                 Mean success rate: 70.00
                  Mean reward/step: 26.15
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 21643264
                    Iteration time: 2.47s
                        Total time: 6929.05s
                               ETA: 517604.0s

################################################################################
                    [1m Learning iteration 2642/200000 [0m

                       Computation: 3284 steps/s (collection: 0.459s, learning 2.035s)
               Value function loss: 111495.0532
                    Surrogate loss: 0.0136
             Mean action noise std: 0.89
                       Mean reward: 8742.67
               Mean episode length: 362.94
                 Mean success rate: 68.00
                  Mean reward/step: 26.04
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 21651456
                    Iteration time: 2.49s
                        Total time: 6931.54s
                               ETA: 517591.7s

################################################################################
                    [1m Learning iteration 2643/200000 [0m

                       Computation: 3226 steps/s (collection: 0.476s, learning 2.063s)
               Value function loss: 80576.9728
                    Surrogate loss: 0.0098
             Mean action noise std: 0.89
                       Mean reward: 8798.67
               Mean episode length: 362.39
                 Mean success rate: 68.50
                  Mean reward/step: 24.96
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 21659648
                    Iteration time: 2.54s
                        Total time: 6934.08s
                               ETA: 517582.9s

################################################################################
                    [1m Learning iteration 2644/200000 [0m

                       Computation: 3192 steps/s (collection: 0.496s, learning 2.070s)
               Value function loss: 95184.6903
                    Surrogate loss: 0.0115
             Mean action noise std: 0.89
                       Mean reward: 8920.17
               Mean episode length: 366.12
                 Mean success rate: 69.50
                  Mean reward/step: 25.33
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 21667840
                    Iteration time: 2.57s
                        Total time: 6936.65s
                               ETA: 517576.0s

################################################################################
                    [1m Learning iteration 2645/200000 [0m

                       Computation: 3303 steps/s (collection: 0.419s, learning 2.060s)
               Value function loss: 81325.3949
                    Surrogate loss: 0.0093
             Mean action noise std: 0.89
                       Mean reward: 9147.05
               Mean episode length: 372.60
                 Mean success rate: 71.00
                  Mean reward/step: 25.73
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 21676032
                    Iteration time: 2.48s
                        Total time: 6939.13s
                               ETA: 517562.8s

################################################################################
                    [1m Learning iteration 2646/200000 [0m

                       Computation: 3294 steps/s (collection: 0.444s, learning 2.043s)
               Value function loss: 108760.8979
                    Surrogate loss: 0.0098
             Mean action noise std: 0.89
                       Mean reward: 9469.95
               Mean episode length: 378.99
                 Mean success rate: 73.50
                  Mean reward/step: 25.36
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 21684224
                    Iteration time: 2.49s
                        Total time: 6941.61s
                               ETA: 517550.0s

################################################################################
                    [1m Learning iteration 2647/200000 [0m

                       Computation: 3242 steps/s (collection: 0.472s, learning 2.054s)
               Value function loss: 93171.6687
                    Surrogate loss: 0.0113
             Mean action noise std: 0.89
                       Mean reward: 9655.34
               Mean episode length: 384.86
                 Mean success rate: 75.00
                  Mean reward/step: 25.74
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 21692416
                    Iteration time: 2.53s
                        Total time: 6944.14s
                               ETA: 517540.2s

################################################################################
                    [1m Learning iteration 2648/200000 [0m

                       Computation: 3244 steps/s (collection: 0.461s, learning 2.064s)
               Value function loss: 95821.1684
                    Surrogate loss: 0.0102
             Mean action noise std: 0.89
                       Mean reward: 9845.75
               Mean episode length: 389.67
                 Mean success rate: 76.50
                  Mean reward/step: 25.85
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 21700608
                    Iteration time: 2.53s
                        Total time: 6946.66s
                               ETA: 517530.3s

################################################################################
                    [1m Learning iteration 2649/200000 [0m

                       Computation: 3336 steps/s (collection: 0.417s, learning 2.038s)
               Value function loss: 93679.8813
                    Surrogate loss: 0.0077
             Mean action noise std: 0.89
                       Mean reward: 10148.73
               Mean episode length: 398.70
                 Mean success rate: 79.00
                  Mean reward/step: 25.63
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 21708800
                    Iteration time: 2.46s
                        Total time: 6949.12s
                               ETA: 517515.3s

################################################################################
                    [1m Learning iteration 2650/200000 [0m

                       Computation: 3317 steps/s (collection: 0.427s, learning 2.042s)
               Value function loss: 102480.1849
                    Surrogate loss: 0.0156
             Mean action noise std: 0.89
                       Mean reward: 10339.31
               Mean episode length: 403.70
                 Mean success rate: 80.50
                  Mean reward/step: 25.40
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 21716992
                    Iteration time: 2.47s
                        Total time: 6951.59s
                               ETA: 517501.2s

################################################################################
                    [1m Learning iteration 2651/200000 [0m

                       Computation: 3289 steps/s (collection: 0.452s, learning 2.038s)
               Value function loss: 150992.7621
                    Surrogate loss: 0.0156
             Mean action noise std: 0.89
                       Mean reward: 10427.19
               Mean episode length: 404.57
                 Mean success rate: 81.00
                  Mean reward/step: 25.09
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 2.49s
                        Total time: 6954.08s
                               ETA: 517488.8s

################################################################################
                    [1m Learning iteration 2652/200000 [0m

                       Computation: 3278 steps/s (collection: 0.456s, learning 2.043s)
               Value function loss: 74684.0278
                    Surrogate loss: 0.0109
             Mean action noise std: 0.89
                       Mean reward: 10493.33
               Mean episode length: 407.27
                 Mean success rate: 81.50
                  Mean reward/step: 24.91
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 21733376
                    Iteration time: 2.50s
                        Total time: 6956.58s
                               ETA: 517477.0s

################################################################################
                    [1m Learning iteration 2653/200000 [0m

                       Computation: 3288 steps/s (collection: 0.457s, learning 2.034s)
               Value function loss: 103783.0136
                    Surrogate loss: 0.0113
             Mean action noise std: 0.89
                       Mean reward: 10327.70
               Mean episode length: 403.60
                 Mean success rate: 80.00
                  Mean reward/step: 25.20
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 21741568
                    Iteration time: 2.49s
                        Total time: 6959.07s
                               ETA: 517464.6s

################################################################################
                    [1m Learning iteration 2654/200000 [0m

                       Computation: 3282 steps/s (collection: 0.440s, learning 2.056s)
               Value function loss: 84055.4430
                    Surrogate loss: 0.0099
             Mean action noise std: 0.89
                       Mean reward: 10587.73
               Mean episode length: 412.25
                 Mean success rate: 82.00
                  Mean reward/step: 25.19
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 21749760
                    Iteration time: 2.50s
                        Total time: 6961.56s
                               ETA: 517452.6s

################################################################################
                    [1m Learning iteration 2655/200000 [0m

                       Computation: 3284 steps/s (collection: 0.416s, learning 2.078s)
               Value function loss: 97609.8871
                    Surrogate loss: 0.0133
             Mean action noise std: 0.89
                       Mean reward: 10500.39
               Mean episode length: 409.88
                 Mean success rate: 81.50
                  Mean reward/step: 25.19
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 21757952
                    Iteration time: 2.49s
                        Total time: 6964.06s
                               ETA: 517440.4s

################################################################################
                    [1m Learning iteration 2656/200000 [0m

                       Computation: 3332 steps/s (collection: 0.441s, learning 2.017s)
               Value function loss: 85860.8057
                    Surrogate loss: 0.0116
             Mean action noise std: 0.89
                       Mean reward: 10417.58
               Mean episode length: 407.74
                 Mean success rate: 81.00
                  Mean reward/step: 25.18
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 21766144
                    Iteration time: 2.46s
                        Total time: 6966.51s
                               ETA: 517425.6s

################################################################################
                    [1m Learning iteration 2657/200000 [0m

                       Computation: 3302 steps/s (collection: 0.439s, learning 2.041s)
               Value function loss: 100490.6375
                    Surrogate loss: 0.0118
             Mean action noise std: 0.89
                       Mean reward: 10302.93
               Mean episode length: 404.58
                 Mean success rate: 80.50
                  Mean reward/step: 25.16
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 21774336
                    Iteration time: 2.48s
                        Total time: 6969.00s
                               ETA: 517412.5s

################################################################################
                    [1m Learning iteration 2658/200000 [0m

                       Computation: 3317 steps/s (collection: 0.448s, learning 2.021s)
               Value function loss: 116601.3138
                    Surrogate loss: 0.0102
             Mean action noise std: 0.89
                       Mean reward: 9944.59
               Mean episode length: 394.30
                 Mean success rate: 78.50
                  Mean reward/step: 24.66
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 21782528
                    Iteration time: 2.47s
                        Total time: 6971.46s
                               ETA: 517398.6s

################################################################################
                    [1m Learning iteration 2659/200000 [0m

                       Computation: 3232 steps/s (collection: 0.458s, learning 2.077s)
               Value function loss: 90431.5143
                    Surrogate loss: 0.0103
             Mean action noise std: 0.89
                       Mean reward: 9855.53
               Mean episode length: 391.06
                 Mean success rate: 77.50
                  Mean reward/step: 24.25
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 21790720
                    Iteration time: 2.53s
                        Total time: 6974.00s
                               ETA: 517389.5s

################################################################################
                    [1m Learning iteration 2660/200000 [0m

                       Computation: 3288 steps/s (collection: 0.431s, learning 2.060s)
               Value function loss: 80443.2364
                    Surrogate loss: 0.0124
             Mean action noise std: 0.89
                       Mean reward: 9626.68
               Mean episode length: 382.89
                 Mean success rate: 75.50
                  Mean reward/step: 25.07
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 21798912
                    Iteration time: 2.49s
                        Total time: 6976.49s
                               ETA: 517377.1s

################################################################################
                    [1m Learning iteration 2661/200000 [0m

                       Computation: 3268 steps/s (collection: 0.432s, learning 2.074s)
               Value function loss: 97496.3119
                    Surrogate loss: 0.0105
             Mean action noise std: 0.89
                       Mean reward: 9327.27
               Mean episode length: 373.98
                 Mean success rate: 73.00
                  Mean reward/step: 25.77
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 21807104
                    Iteration time: 2.51s
                        Total time: 6979.00s
                               ETA: 517365.9s

################################################################################
                    [1m Learning iteration 2662/200000 [0m

                       Computation: 3310 steps/s (collection: 0.446s, learning 2.028s)
               Value function loss: 81034.6583
                    Surrogate loss: 0.0092
             Mean action noise std: 0.89
                       Mean reward: 9162.21
               Mean episode length: 368.39
                 Mean success rate: 72.00
                  Mean reward/step: 25.41
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 21815296
                    Iteration time: 2.47s
                        Total time: 6981.47s
                               ETA: 517352.4s

################################################################################
                    [1m Learning iteration 2663/200000 [0m

                       Computation: 3125 steps/s (collection: 0.480s, learning 2.141s)
               Value function loss: 92838.0100
                    Surrogate loss: 0.0109
             Mean action noise std: 0.89
                       Mean reward: 9286.90
               Mean episode length: 371.53
                 Mean success rate: 72.00
                  Mean reward/step: 26.19
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 2.62s
                        Total time: 6984.09s
                               ETA: 517349.7s

################################################################################
                    [1m Learning iteration 2664/200000 [0m

                       Computation: 3101 steps/s (collection: 0.510s, learning 2.131s)
               Value function loss: 128859.9473
                    Surrogate loss: 0.0145
             Mean action noise std: 0.89
                       Mean reward: 9171.81
               Mean episode length: 369.75
                 Mean success rate: 71.50
                  Mean reward/step: 26.21
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 21831680
                    Iteration time: 2.64s
                        Total time: 6986.73s
                               ETA: 517348.5s

################################################################################
                    [1m Learning iteration 2665/200000 [0m

                       Computation: 3073 steps/s (collection: 0.510s, learning 2.156s)
               Value function loss: 108804.7475
                    Surrogate loss: 0.0130
             Mean action noise std: 0.89
                       Mean reward: 8930.30
               Mean episode length: 360.94
                 Mean success rate: 69.50
                  Mean reward/step: 25.75
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 21839872
                    Iteration time: 2.67s
                        Total time: 6989.40s
                               ETA: 517349.1s

################################################################################
                    [1m Learning iteration 2666/200000 [0m

                       Computation: 3148 steps/s (collection: 0.450s, learning 2.151s)
               Value function loss: 76764.5277
                    Surrogate loss: 0.0109
             Mean action noise std: 0.89
                       Mean reward: 9130.48
               Mean episode length: 368.30
                 Mean success rate: 71.00
                  Mean reward/step: 25.53
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 21848064
                    Iteration time: 2.60s
                        Total time: 6992.00s
                               ETA: 517345.0s

################################################################################
                    [1m Learning iteration 2667/200000 [0m

                       Computation: 3182 steps/s (collection: 0.490s, learning 2.084s)
               Value function loss: 112893.8785
                    Surrogate loss: 0.0094
             Mean action noise std: 0.89
                       Mean reward: 9472.09
               Mean episode length: 376.82
                 Mean success rate: 73.00
                  Mean reward/step: 25.74
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 21856256
                    Iteration time: 2.57s
                        Total time: 6994.57s
                               ETA: 517338.9s

################################################################################
                    [1m Learning iteration 2668/200000 [0m

                       Computation: 3142 steps/s (collection: 0.447s, learning 2.160s)
               Value function loss: 88558.7367
                    Surrogate loss: 0.0083
             Mean action noise std: 0.89
                       Mean reward: 9669.66
               Mean episode length: 384.55
                 Mean success rate: 74.50
                  Mean reward/step: 25.71
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 21864448
                    Iteration time: 2.61s
                        Total time: 6997.18s
                               ETA: 517335.2s

################################################################################
                    [1m Learning iteration 2669/200000 [0m

                       Computation: 3198 steps/s (collection: 0.449s, learning 2.112s)
               Value function loss: 90506.1396
                    Surrogate loss: 0.0111
             Mean action noise std: 0.89
                       Mean reward: 9939.73
               Mean episode length: 393.18
                 Mean success rate: 76.00
                  Mean reward/step: 26.10
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 21872640
                    Iteration time: 2.56s
                        Total time: 6999.74s
                               ETA: 517328.1s

################################################################################
                    [1m Learning iteration 2670/200000 [0m

                       Computation: 3202 steps/s (collection: 0.448s, learning 2.110s)
               Value function loss: 87309.6472
                    Surrogate loss: 0.0109
             Mean action noise std: 0.89
                       Mean reward: 10161.65
               Mean episode length: 398.38
                 Mean success rate: 78.50
                  Mean reward/step: 25.91
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 21880832
                    Iteration time: 2.56s
                        Total time: 7002.30s
                               ETA: 517320.7s

################################################################################
                    [1m Learning iteration 2671/200000 [0m

                       Computation: 3122 steps/s (collection: 0.482s, learning 2.142s)
               Value function loss: 78160.0420
                    Surrogate loss: 0.0127
             Mean action noise std: 0.89
                       Mean reward: 10021.47
               Mean episode length: 393.83
                 Mean success rate: 77.50
                  Mean reward/step: 25.84
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 21889024
                    Iteration time: 2.62s
                        Total time: 7004.92s
                               ETA: 517318.3s

################################################################################
                    [1m Learning iteration 2672/200000 [0m

                       Computation: 3183 steps/s (collection: 0.477s, learning 2.097s)
               Value function loss: 80942.5121
                    Surrogate loss: 0.0092
             Mean action noise std: 0.89
                       Mean reward: 9756.72
               Mean episode length: 386.62
                 Mean success rate: 76.50
                  Mean reward/step: 25.97
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 21897216
                    Iteration time: 2.57s
                        Total time: 7007.50s
                               ETA: 517312.1s

################################################################################
                    [1m Learning iteration 2673/200000 [0m

                       Computation: 3127 steps/s (collection: 0.486s, learning 2.133s)
               Value function loss: 112327.2531
                    Surrogate loss: 0.0086
             Mean action noise std: 0.89
                       Mean reward: 9754.48
               Mean episode length: 387.88
                 Mean success rate: 77.00
                  Mean reward/step: 26.81
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 21905408
                    Iteration time: 2.62s
                        Total time: 7010.11s
                               ETA: 517309.3s

################################################################################
                    [1m Learning iteration 2674/200000 [0m

                       Computation: 3099 steps/s (collection: 0.492s, learning 2.151s)
               Value function loss: 131585.6986
                    Surrogate loss: 0.0112
             Mean action noise std: 0.89
                       Mean reward: 10187.33
               Mean episode length: 400.41
                 Mean success rate: 80.00
                  Mean reward/step: 24.96
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 21913600
                    Iteration time: 2.64s
                        Total time: 7012.76s
                               ETA: 517308.2s

################################################################################
                    [1m Learning iteration 2675/200000 [0m

                       Computation: 3086 steps/s (collection: 0.509s, learning 2.145s)
               Value function loss: 80893.8605
                    Surrogate loss: 0.0114
             Mean action noise std: 0.89
                       Mean reward: 10133.54
               Mean episode length: 399.45
                 Mean success rate: 79.50
                  Mean reward/step: 24.98
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 2.65s
                        Total time: 7015.41s
                               ETA: 517308.0s

################################################################################
                    [1m Learning iteration 2676/200000 [0m

                       Computation: 3040 steps/s (collection: 0.488s, learning 2.206s)
               Value function loss: 94871.1658
                    Surrogate loss: 0.0130
             Mean action noise std: 0.89
                       Mean reward: 10365.35
               Mean episode length: 402.99
                 Mean success rate: 80.00
                  Mean reward/step: 25.69
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 21929984
                    Iteration time: 2.69s
                        Total time: 7018.11s
                               ETA: 517310.7s

################################################################################
                    [1m Learning iteration 2677/200000 [0m

                       Computation: 3114 steps/s (collection: 0.507s, learning 2.123s)
               Value function loss: 109174.4726
                    Surrogate loss: 0.0092
             Mean action noise std: 0.89
                       Mean reward: 10429.55
               Mean episode length: 403.39
                 Mean success rate: 80.00
                  Mean reward/step: 25.53
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 21938176
                    Iteration time: 2.63s
                        Total time: 7020.74s
                               ETA: 517308.8s

################################################################################
                    [1m Learning iteration 2678/200000 [0m

                       Computation: 3083 steps/s (collection: 0.543s, learning 2.114s)
               Value function loss: 93987.8463
                    Surrogate loss: 0.0110
             Mean action noise std: 0.89
                       Mean reward: 10329.90
               Mean episode length: 399.33
                 Mean success rate: 79.50
                  Mean reward/step: 25.66
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 21946368
                    Iteration time: 2.66s
                        Total time: 7023.39s
                               ETA: 517308.7s

################################################################################
                    [1m Learning iteration 2679/200000 [0m

                       Computation: 3152 steps/s (collection: 0.436s, learning 2.163s)
               Value function loss: 50162.4396
                    Surrogate loss: 0.0098
             Mean action noise std: 0.89
                       Mean reward: 10235.15
               Mean episode length: 396.88
                 Mean success rate: 79.00
                  Mean reward/step: 25.62
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 21954560
                    Iteration time: 2.60s
                        Total time: 7025.99s
                               ETA: 517304.4s

################################################################################
                    [1m Learning iteration 2680/200000 [0m

                       Computation: 3019 steps/s (collection: 0.526s, learning 2.187s)
               Value function loss: 104374.5379
                    Surrogate loss: 0.0087
             Mean action noise std: 0.89
                       Mean reward: 10568.87
               Mean episode length: 408.39
                 Mean success rate: 80.50
                  Mean reward/step: 25.72
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 21962752
                    Iteration time: 2.71s
                        Total time: 7028.71s
                               ETA: 517308.5s

################################################################################
                    [1m Learning iteration 2681/200000 [0m

                       Computation: 3085 steps/s (collection: 0.546s, learning 2.109s)
               Value function loss: 95665.8428
                    Surrogate loss: 0.0103
             Mean action noise std: 0.89
                       Mean reward: 10794.78
               Mean episode length: 414.01
                 Mean success rate: 82.50
                  Mean reward/step: 25.78
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 21970944
                    Iteration time: 2.65s
                        Total time: 7031.36s
                               ETA: 517308.4s

################################################################################
                    [1m Learning iteration 2682/200000 [0m

                       Computation: 3131 steps/s (collection: 0.480s, learning 2.136s)
               Value function loss: 72342.8574
                    Surrogate loss: 0.0108
             Mean action noise std: 0.89
                       Mean reward: 10998.59
               Mean episode length: 418.73
                 Mean success rate: 84.00
                  Mean reward/step: 25.81
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 21979136
                    Iteration time: 2.62s
                        Total time: 7033.98s
                               ETA: 517305.3s

################################################################################
                    [1m Learning iteration 2683/200000 [0m

                       Computation: 3112 steps/s (collection: 0.501s, learning 2.131s)
               Value function loss: 105167.4232
                    Surrogate loss: 0.0114
             Mean action noise std: 0.89
                       Mean reward: 11181.11
               Mean episode length: 424.44
                 Mean success rate: 84.50
                  Mean reward/step: 25.32
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 21987328
                    Iteration time: 2.63s
                        Total time: 7036.61s
                               ETA: 517303.4s

################################################################################
                    [1m Learning iteration 2684/200000 [0m

                       Computation: 3151 steps/s (collection: 0.461s, learning 2.139s)
               Value function loss: 98833.3623
                    Surrogate loss: 0.0110
             Mean action noise std: 0.89
                       Mean reward: 11217.07
               Mean episode length: 424.15
                 Mean success rate: 84.50
                  Mean reward/step: 25.30
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 21995520
                    Iteration time: 2.60s
                        Total time: 7039.21s
                               ETA: 517299.2s

################################################################################
                    [1m Learning iteration 2685/200000 [0m

                       Computation: 3144 steps/s (collection: 0.494s, learning 2.111s)
               Value function loss: 104484.8410
                    Surrogate loss: 0.0105
             Mean action noise std: 0.89
                       Mean reward: 11250.11
               Mean episode length: 426.70
                 Mean success rate: 84.50
                  Mean reward/step: 25.60
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 22003712
                    Iteration time: 2.61s
                        Total time: 7041.81s
                               ETA: 517295.4s

################################################################################
                    [1m Learning iteration 2686/200000 [0m

                       Computation: 3290 steps/s (collection: 0.458s, learning 2.032s)
               Value function loss: 83291.8352
                    Surrogate loss: 0.0090
             Mean action noise std: 0.89
                       Mean reward: 11331.09
               Mean episode length: 428.30
                 Mean success rate: 85.00
                  Mean reward/step: 25.31
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 22011904
                    Iteration time: 2.49s
                        Total time: 7044.30s
                               ETA: 517283.1s

################################################################################
                    [1m Learning iteration 2687/200000 [0m

                       Computation: 3282 steps/s (collection: 0.457s, learning 2.038s)
               Value function loss: 97153.2646
                    Surrogate loss: 0.0099
             Mean action noise std: 0.89
                       Mean reward: 11205.75
               Mean episode length: 426.49
                 Mean success rate: 84.50
                  Mean reward/step: 25.65
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 2.50s
                        Total time: 7046.80s
                               ETA: 517271.2s

################################################################################
                    [1m Learning iteration 2688/200000 [0m

                       Computation: 3271 steps/s (collection: 0.468s, learning 2.036s)
               Value function loss: 88264.8917
                    Surrogate loss: 0.0136
             Mean action noise std: 0.89
                       Mean reward: 11268.48
               Mean episode length: 431.98
                 Mean success rate: 85.50
                  Mean reward/step: 25.54
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 22028288
                    Iteration time: 2.50s
                        Total time: 7049.30s
                               ETA: 517259.9s

################################################################################
                    [1m Learning iteration 2689/200000 [0m

                       Computation: 3303 steps/s (collection: 0.453s, learning 2.027s)
               Value function loss: 122720.6488
                    Surrogate loss: 0.0138
             Mean action noise std: 0.89
                       Mean reward: 11237.26
               Mean episode length: 433.42
                 Mean success rate: 86.00
                  Mean reward/step: 24.68
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 22036480
                    Iteration time: 2.48s
                        Total time: 7051.78s
                               ETA: 517246.9s

################################################################################
                    [1m Learning iteration 2690/200000 [0m

                       Computation: 3321 steps/s (collection: 0.443s, learning 2.024s)
               Value function loss: 77981.8478
                    Surrogate loss: 0.0078
             Mean action noise std: 0.89
                       Mean reward: 11162.09
               Mean episode length: 431.42
                 Mean success rate: 85.50
                  Mean reward/step: 23.74
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 22044672
                    Iteration time: 2.47s
                        Total time: 7054.25s
                               ETA: 517232.9s

################################################################################
                    [1m Learning iteration 2691/200000 [0m

                       Computation: 3321 steps/s (collection: 0.452s, learning 2.014s)
               Value function loss: 86225.2044
                    Surrogate loss: 0.0095
             Mean action noise std: 0.89
                       Mean reward: 11273.49
               Mean episode length: 436.80
                 Mean success rate: 86.00
                  Mean reward/step: 24.05
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 22052864
                    Iteration time: 2.47s
                        Total time: 7056.71s
                               ETA: 517218.9s

################################################################################
                    [1m Learning iteration 2692/200000 [0m

                       Computation: 3249 steps/s (collection: 0.456s, learning 2.064s)
               Value function loss: 105923.8188
                    Surrogate loss: 0.0116
             Mean action noise std: 0.89
                       Mean reward: 11182.91
               Mean episode length: 437.10
                 Mean success rate: 85.50
                  Mean reward/step: 24.50
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 22061056
                    Iteration time: 2.52s
                        Total time: 7059.24s
                               ETA: 517208.9s

################################################################################
                    [1m Learning iteration 2693/200000 [0m

                       Computation: 3282 steps/s (collection: 0.447s, learning 2.049s)
               Value function loss: 90295.4480
                    Surrogate loss: 0.0103
             Mean action noise std: 0.89
                       Mean reward: 11289.47
               Mean episode length: 440.19
                 Mean success rate: 86.50
                  Mean reward/step: 24.46
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 22069248
                    Iteration time: 2.50s
                        Total time: 7061.73s
                               ETA: 517197.1s

################################################################################
                    [1m Learning iteration 2694/200000 [0m

                       Computation: 3282 steps/s (collection: 0.450s, learning 2.045s)
               Value function loss: 100880.7923
                    Surrogate loss: 0.0106
             Mean action noise std: 0.89
                       Mean reward: 11064.38
               Mean episode length: 435.68
                 Mean success rate: 85.50
                  Mean reward/step: 24.78
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 22077440
                    Iteration time: 2.50s
                        Total time: 7064.23s
                               ETA: 517185.3s

################################################################################
                    [1m Learning iteration 2695/200000 [0m

                       Computation: 3267 steps/s (collection: 0.460s, learning 2.047s)
               Value function loss: 71808.4537
                    Surrogate loss: 0.0120
             Mean action noise std: 0.89
                       Mean reward: 11136.40
               Mean episode length: 438.61
                 Mean success rate: 86.50
                  Mean reward/step: 25.35
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 22085632
                    Iteration time: 2.51s
                        Total time: 7066.73s
                               ETA: 517174.3s

################################################################################
                    [1m Learning iteration 2696/200000 [0m

                       Computation: 3213 steps/s (collection: 0.475s, learning 2.074s)
               Value function loss: 80832.6359
                    Surrogate loss: 0.0094
             Mean action noise std: 0.89
                       Mean reward: 11014.31
               Mean episode length: 435.53
                 Mean success rate: 86.00
                  Mean reward/step: 25.50
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 22093824
                    Iteration time: 2.55s
                        Total time: 7069.28s
                               ETA: 517166.4s

################################################################################
                    [1m Learning iteration 2697/200000 [0m

                       Computation: 3249 steps/s (collection: 0.465s, learning 2.056s)
               Value function loss: 81872.9683
                    Surrogate loss: 0.0100
             Mean action noise std: 0.89
                       Mean reward: 10897.65
               Mean episode length: 434.59
                 Mean success rate: 86.00
                  Mean reward/step: 25.40
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 22102016
                    Iteration time: 2.52s
                        Total time: 7071.80s
                               ETA: 517156.5s

################################################################################
                    [1m Learning iteration 2698/200000 [0m

                       Computation: 3095 steps/s (collection: 0.542s, learning 2.105s)
               Value function loss: 129742.2291
                    Surrogate loss: 0.0133
             Mean action noise std: 0.89
                       Mean reward: 11004.27
               Mean episode length: 435.71
                 Mean success rate: 86.50
                  Mean reward/step: 25.44
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 22110208
                    Iteration time: 2.65s
                        Total time: 7074.45s
                               ETA: 517155.7s

################################################################################
                    [1m Learning iteration 2699/200000 [0m

                       Computation: 3140 steps/s (collection: 0.496s, learning 2.112s)
               Value function loss: 81795.9800
                    Surrogate loss: 0.0129
             Mean action noise std: 0.89
                       Mean reward: 11055.03
               Mean episode length: 439.09
                 Mean success rate: 86.00
                  Mean reward/step: 25.41
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 2.61s
                        Total time: 7077.06s
                               ETA: 517152.1s

################################################################################
                    [1m Learning iteration 2700/200000 [0m

                       Computation: 3193 steps/s (collection: 0.454s, learning 2.110s)
               Value function loss: 85824.4929
                    Surrogate loss: 0.0088
             Mean action noise std: 0.89
                       Mean reward: 11060.14
               Mean episode length: 439.56
                 Mean success rate: 86.50
                  Mean reward/step: 24.90
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 22126592
                    Iteration time: 2.56s
                        Total time: 7079.62s
                               ETA: 517145.4s

################################################################################
                    [1m Learning iteration 2701/200000 [0m

                       Computation: 3206 steps/s (collection: 0.452s, learning 2.103s)
               Value function loss: 95335.7318
                    Surrogate loss: 0.0091
             Mean action noise std: 0.89
                       Mean reward: 11188.23
               Mean episode length: 445.64
                 Mean success rate: 88.00
                  Mean reward/step: 25.24
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 22134784
                    Iteration time: 2.55s
                        Total time: 7082.18s
                               ETA: 517137.9s

################################################################################
                    [1m Learning iteration 2702/200000 [0m

                       Computation: 3175 steps/s (collection: 0.476s, learning 2.104s)
               Value function loss: 74974.1698
                    Surrogate loss: 0.0099
             Mean action noise std: 0.89
                       Mean reward: 11224.42
               Mean episode length: 447.05
                 Mean success rate: 88.50
                  Mean reward/step: 25.66
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 22142976
                    Iteration time: 2.58s
                        Total time: 7084.76s
                               ETA: 517132.3s

################################################################################
                    [1m Learning iteration 2703/200000 [0m

                       Computation: 3156 steps/s (collection: 0.497s, learning 2.098s)
               Value function loss: 99355.4582
                    Surrogate loss: 0.0106
             Mean action noise std: 0.89
                       Mean reward: 11082.13
               Mean episode length: 446.79
                 Mean success rate: 88.00
                  Mean reward/step: 25.56
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 22151168
                    Iteration time: 2.59s
                        Total time: 7087.35s
                               ETA: 517127.7s

################################################################################
                    [1m Learning iteration 2704/200000 [0m

                       Computation: 3191 steps/s (collection: 0.479s, learning 2.088s)
               Value function loss: 100034.1136
                    Surrogate loss: 0.0142
             Mean action noise std: 0.89
                       Mean reward: 11126.46
               Mean episode length: 447.11
                 Mean success rate: 88.50
                  Mean reward/step: 25.63
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 22159360
                    Iteration time: 2.57s
                        Total time: 7089.92s
                               ETA: 517121.2s

################################################################################
                    [1m Learning iteration 2705/200000 [0m

                       Computation: 3133 steps/s (collection: 0.523s, learning 2.091s)
               Value function loss: 113000.8391
                    Surrogate loss: 0.0118
             Mean action noise std: 0.89
                       Mean reward: 11121.82
               Mean episode length: 446.68
                 Mean success rate: 88.50
                  Mean reward/step: 25.10
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 22167552
                    Iteration time: 2.61s
                        Total time: 7092.53s
                               ETA: 517118.0s

################################################################################
                    [1m Learning iteration 2706/200000 [0m

                       Computation: 3125 steps/s (collection: 0.490s, learning 2.131s)
               Value function loss: 92214.0146
                    Surrogate loss: 0.0116
             Mean action noise std: 0.89
                       Mean reward: 11179.41
               Mean episode length: 450.48
                 Mean success rate: 89.00
                  Mean reward/step: 24.73
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 22175744
                    Iteration time: 2.62s
                        Total time: 7095.15s
                               ETA: 517115.4s

################################################################################
                    [1m Learning iteration 2707/200000 [0m

                       Computation: 3173 steps/s (collection: 0.440s, learning 2.141s)
               Value function loss: 77742.2471
                    Surrogate loss: 0.0102
             Mean action noise std: 0.89
                       Mean reward: 10940.84
               Mean episode length: 443.42
                 Mean success rate: 88.00
                  Mean reward/step: 24.69
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 22183936
                    Iteration time: 2.58s
                        Total time: 7097.74s
                               ETA: 517109.9s

################################################################################
                    [1m Learning iteration 2708/200000 [0m

                       Computation: 3152 steps/s (collection: 0.488s, learning 2.111s)
               Value function loss: 114785.9002
                    Surrogate loss: 0.0089
             Mean action noise std: 0.89
                       Mean reward: 11026.25
               Mean episode length: 444.74
                 Mean success rate: 88.50
                  Mean reward/step: 24.81
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 22192128
                    Iteration time: 2.60s
                        Total time: 7100.34s
                               ETA: 517105.7s

################################################################################
                    [1m Learning iteration 2709/200000 [0m

                       Computation: 3132 steps/s (collection: 0.484s, learning 2.131s)
               Value function loss: 83514.0093
                    Surrogate loss: 0.0097
             Mean action noise std: 0.89
                       Mean reward: 10772.61
               Mean episode length: 431.73
                 Mean success rate: 87.00
                  Mean reward/step: 24.61
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 22200320
                    Iteration time: 2.62s
                        Total time: 7102.95s
                               ETA: 517102.7s

################################################################################
                    [1m Learning iteration 2710/200000 [0m

                       Computation: 3168 steps/s (collection: 0.488s, learning 2.097s)
               Value function loss: 70264.3339
                    Surrogate loss: 0.0089
             Mean action noise std: 0.89
                       Mean reward: 10528.44
               Mean episode length: 424.38
                 Mean success rate: 84.50
                  Mean reward/step: 24.23
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 22208512
                    Iteration time: 2.59s
                        Total time: 7105.54s
                               ETA: 517097.5s

################################################################################
                    [1m Learning iteration 2711/200000 [0m

                       Computation: 3175 steps/s (collection: 0.429s, learning 2.151s)
               Value function loss: 106524.3274
                    Surrogate loss: 0.0105
             Mean action noise std: 0.89
                       Mean reward: 10346.08
               Mean episode length: 414.56
                 Mean success rate: 82.50
                  Mean reward/step: 24.80
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 2.58s
                        Total time: 7108.12s
                               ETA: 517091.9s

################################################################################
                    [1m Learning iteration 2712/200000 [0m

                       Computation: 3114 steps/s (collection: 0.508s, learning 2.122s)
               Value function loss: 69362.4572
                    Surrogate loss: 0.0088
             Mean action noise std: 0.89
                       Mean reward: 10245.93
               Mean episode length: 413.38
                 Mean success rate: 81.50
                  Mean reward/step: 24.97
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 22224896
                    Iteration time: 2.63s
                        Total time: 7110.75s
                               ETA: 517089.9s

################################################################################
                    [1m Learning iteration 2713/200000 [0m

                       Computation: 3090 steps/s (collection: 0.530s, learning 2.120s)
               Value function loss: 95373.5713
                    Surrogate loss: 0.0120
             Mean action noise std: 0.89
                       Mean reward: 10233.95
               Mean episode length: 411.32
                 Mean success rate: 80.50
                  Mean reward/step: 26.20
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 22233088
                    Iteration time: 2.65s
                        Total time: 7113.40s
                               ETA: 517089.4s

################################################################################
                    [1m Learning iteration 2714/200000 [0m

                       Computation: 3188 steps/s (collection: 0.472s, learning 2.097s)
               Value function loss: 103161.9363
                    Surrogate loss: 0.0093
             Mean action noise std: 0.89
                       Mean reward: 9909.37
               Mean episode length: 401.69
                 Mean success rate: 78.00
                  Mean reward/step: 25.60
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 22241280
                    Iteration time: 2.57s
                        Total time: 7115.97s
                               ETA: 517083.1s

################################################################################
                    [1m Learning iteration 2715/200000 [0m

                       Computation: 3230 steps/s (collection: 0.481s, learning 2.055s)
               Value function loss: 108080.3782
                    Surrogate loss: 0.0095
             Mean action noise std: 0.89
                       Mean reward: 9967.02
               Mean episode length: 401.21
                 Mean success rate: 78.50
                  Mean reward/step: 25.35
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 22249472
                    Iteration time: 2.54s
                        Total time: 7118.50s
                               ETA: 517074.2s

################################################################################
                    [1m Learning iteration 2716/200000 [0m

                       Computation: 3164 steps/s (collection: 0.503s, learning 2.086s)
               Value function loss: 62950.5699
                    Surrogate loss: 0.0139
             Mean action noise std: 0.89
                       Mean reward: 9879.95
               Mean episode length: 397.25
                 Mean success rate: 77.50
                  Mean reward/step: 24.58
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 22257664
                    Iteration time: 2.59s
                        Total time: 7121.09s
                               ETA: 517069.3s

################################################################################
                    [1m Learning iteration 2717/200000 [0m

                       Computation: 3143 steps/s (collection: 0.477s, learning 2.130s)
               Value function loss: 82586.1176
                    Surrogate loss: 0.0095
             Mean action noise std: 0.89
                       Mean reward: 9517.34
               Mean episode length: 388.48
                 Mean success rate: 74.50
                  Mean reward/step: 24.96
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 22265856
                    Iteration time: 2.61s
                        Total time: 7123.70s
                               ETA: 517065.6s

################################################################################
                    [1m Learning iteration 2718/200000 [0m

                       Computation: 3124 steps/s (collection: 0.502s, learning 2.120s)
               Value function loss: 98758.8287
                    Surrogate loss: 0.0125
             Mean action noise std: 0.89
                       Mean reward: 9315.06
               Mean episode length: 380.70
                 Mean success rate: 72.50
                  Mean reward/step: 25.40
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 22274048
                    Iteration time: 2.62s
                        Total time: 7126.32s
                               ETA: 517063.0s

################################################################################
                    [1m Learning iteration 2719/200000 [0m

                       Computation: 3177 steps/s (collection: 0.460s, learning 2.118s)
               Value function loss: 94476.5499
                    Surrogate loss: 0.0217
             Mean action noise std: 0.89
                       Mean reward: 9585.44
               Mean episode length: 391.07
                 Mean success rate: 74.00
                  Mean reward/step: 25.32
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 22282240
                    Iteration time: 2.58s
                        Total time: 7128.90s
                               ETA: 517057.3s

################################################################################
                    [1m Learning iteration 2720/200000 [0m

                       Computation: 3095 steps/s (collection: 0.549s, learning 2.098s)
               Value function loss: 114927.4657
                    Surrogate loss: 0.0095
             Mean action noise std: 0.89
                       Mean reward: 9541.30
               Mean episode length: 388.67
                 Mean success rate: 74.00
                  Mean reward/step: 25.35
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 22290432
                    Iteration time: 2.65s
                        Total time: 7131.54s
                               ETA: 517056.6s

################################################################################
                    [1m Learning iteration 2721/200000 [0m

                       Computation: 3222 steps/s (collection: 0.460s, learning 2.082s)
               Value function loss: 101215.9454
                    Surrogate loss: 0.0074
             Mean action noise std: 0.89
                       Mean reward: 9744.43
               Mean episode length: 394.93
                 Mean success rate: 75.50
                  Mean reward/step: 24.85
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 22298624
                    Iteration time: 2.54s
                        Total time: 7134.09s
                               ETA: 517048.2s

################################################################################
                    [1m Learning iteration 2722/200000 [0m

                       Computation: 3202 steps/s (collection: 0.465s, learning 2.093s)
               Value function loss: 99951.4972
                    Surrogate loss: 0.0099
             Mean action noise std: 0.89
                       Mean reward: 9869.90
               Mean episode length: 395.18
                 Mean success rate: 76.00
                  Mean reward/step: 25.25
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 22306816
                    Iteration time: 2.56s
                        Total time: 7136.64s
                               ETA: 517041.1s

################################################################################
                    [1m Learning iteration 2723/200000 [0m

                       Computation: 3286 steps/s (collection: 0.472s, learning 2.021s)
               Value function loss: 113734.1415
                    Surrogate loss: 0.0099
             Mean action noise std: 0.89
                       Mean reward: 10025.17
               Mean episode length: 398.65
                 Mean success rate: 76.50
                  Mean reward/step: 25.09
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 2.49s
                        Total time: 7139.14s
                               ETA: 517029.2s

################################################################################
                    [1m Learning iteration 2724/200000 [0m

                       Computation: 3208 steps/s (collection: 0.463s, learning 2.090s)
               Value function loss: 107814.2826
                    Surrogate loss: 0.0108
             Mean action noise std: 0.89
                       Mean reward: 10278.20
               Mean episode length: 406.01
                 Mean success rate: 78.50
                  Mean reward/step: 24.82
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 22323200
                    Iteration time: 2.55s
                        Total time: 7141.69s
                               ETA: 517021.7s

################################################################################
                    [1m Learning iteration 2725/200000 [0m

                       Computation: 3212 steps/s (collection: 0.476s, learning 2.073s)
               Value function loss: 100147.5738
                    Surrogate loss: 0.0119
             Mean action noise std: 0.89
                       Mean reward: 10422.97
               Mean episode length: 410.12
                 Mean success rate: 79.00
                  Mean reward/step: 25.36
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 22331392
                    Iteration time: 2.55s
                        Total time: 7144.24s
                               ETA: 517013.9s

################################################################################
                    [1m Learning iteration 2726/200000 [0m

                       Computation: 3170 steps/s (collection: 0.489s, learning 2.095s)
               Value function loss: 79242.7547
                    Surrogate loss: 0.0112
             Mean action noise std: 0.89
                       Mean reward: 10510.81
               Mean episode length: 411.97
                 Mean success rate: 80.00
                  Mean reward/step: 26.05
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 22339584
                    Iteration time: 2.58s
                        Total time: 7146.82s
                               ETA: 517008.6s

################################################################################
                    [1m Learning iteration 2727/200000 [0m

                       Computation: 3251 steps/s (collection: 0.453s, learning 2.066s)
               Value function loss: 118210.4124
                    Surrogate loss: 0.0135
             Mean action noise std: 0.89
                       Mean reward: 10857.22
               Mean episode length: 422.43
                 Mean success rate: 82.50
                  Mean reward/step: 26.18
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 22347776
                    Iteration time: 2.52s
                        Total time: 7149.34s
                               ETA: 516998.7s

################################################################################
                    [1m Learning iteration 2728/200000 [0m

                       Computation: 3204 steps/s (collection: 0.487s, learning 2.069s)
               Value function loss: 75691.0513
                    Surrogate loss: 0.0092
             Mean action noise std: 0.89
                       Mean reward: 10876.92
               Mean episode length: 426.62
                 Mean success rate: 84.00
                  Mean reward/step: 25.52
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 22355968
                    Iteration time: 2.56s
                        Total time: 7151.90s
                               ETA: 516991.4s

################################################################################
                    [1m Learning iteration 2729/200000 [0m

                       Computation: 3159 steps/s (collection: 0.519s, learning 2.073s)
               Value function loss: 110593.3271
                    Surrogate loss: 0.0102
             Mean action noise std: 0.89
                       Mean reward: 11058.57
               Mean episode length: 431.12
                 Mean success rate: 85.00
                  Mean reward/step: 26.08
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 22364160
                    Iteration time: 2.59s
                        Total time: 7154.49s
                               ETA: 516986.8s

################################################################################
                    [1m Learning iteration 2730/200000 [0m

                       Computation: 3156 steps/s (collection: 0.499s, learning 2.096s)
               Value function loss: 89094.1926
                    Surrogate loss: 0.0105
             Mean action noise std: 0.89
                       Mean reward: 11080.49
               Mean episode length: 431.52
                 Mean success rate: 84.50
                  Mean reward/step: 26.31
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 22372352
                    Iteration time: 2.60s
                        Total time: 7157.09s
                               ETA: 516982.3s

################################################################################
                    [1m Learning iteration 2731/200000 [0m

                       Computation: 3206 steps/s (collection: 0.493s, learning 2.062s)
               Value function loss: 115504.6338
                    Surrogate loss: 0.0113
             Mean action noise std: 0.89
                       Mean reward: 11218.12
               Mean episode length: 435.64
                 Mean success rate: 86.00
                  Mean reward/step: 26.62
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 22380544
                    Iteration time: 2.55s
                        Total time: 7159.64s
                               ETA: 516974.9s

################################################################################
                    [1m Learning iteration 2732/200000 [0m

                       Computation: 3303 steps/s (collection: 0.434s, learning 2.046s)
               Value function loss: 72878.2188
                    Surrogate loss: 0.0110
             Mean action noise std: 0.89
                       Mean reward: 11071.19
               Mean episode length: 432.48
                 Mean success rate: 85.50
                  Mean reward/step: 26.70
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 22388736
                    Iteration time: 2.48s
                        Total time: 7162.12s
                               ETA: 516962.1s

################################################################################
                    [1m Learning iteration 2733/200000 [0m

                       Computation: 3234 steps/s (collection: 0.476s, learning 2.057s)
               Value function loss: 89230.9245
                    Surrogate loss: 0.0118
             Mean action noise std: 0.89
                       Mean reward: 11106.17
               Mean episode length: 433.63
                 Mean success rate: 86.00
                  Mean reward/step: 25.86
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 22396928
                    Iteration time: 2.53s
                        Total time: 7164.65s
                               ETA: 516953.2s

################################################################################
                    [1m Learning iteration 2734/200000 [0m

                       Computation: 3268 steps/s (collection: 0.436s, learning 2.070s)
               Value function loss: 108171.3970
                    Surrogate loss: 0.0134
             Mean action noise std: 0.89
                       Mean reward: 11170.70
               Mean episode length: 436.12
                 Mean success rate: 86.50
                  Mean reward/step: 26.14
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 22405120
                    Iteration time: 2.51s
                        Total time: 7167.16s
                               ETA: 516942.3s

################################################################################
                    [1m Learning iteration 2735/200000 [0m

                       Computation: 3249 steps/s (collection: 0.472s, learning 2.050s)
               Value function loss: 106558.2743
                    Surrogate loss: 0.0122
             Mean action noise std: 0.89
                       Mean reward: 11198.87
               Mean episode length: 436.62
                 Mean success rate: 86.50
                  Mean reward/step: 26.59
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 2.52s
                        Total time: 7169.68s
                               ETA: 516932.5s

################################################################################
                    [1m Learning iteration 2736/200000 [0m

                       Computation: 3250 steps/s (collection: 0.488s, learning 2.031s)
               Value function loss: 122136.5791
                    Surrogate loss: 0.0098
             Mean action noise std: 0.89
                       Mean reward: 11330.47
               Mean episode length: 441.01
                 Mean success rate: 87.50
                  Mean reward/step: 26.07
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 22421504
                    Iteration time: 2.52s
                        Total time: 7172.20s
                               ETA: 516922.7s

################################################################################
                    [1m Learning iteration 2737/200000 [0m

                       Computation: 3261 steps/s (collection: 0.426s, learning 2.086s)
               Value function loss: 62252.2656
                    Surrogate loss: 0.0116
             Mean action noise std: 0.89
                       Mean reward: 11342.40
               Mean episode length: 441.27
                 Mean success rate: 87.00
                  Mean reward/step: 26.11
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 22429696
                    Iteration time: 2.51s
                        Total time: 7174.71s
                               ETA: 516912.2s

################################################################################
                    [1m Learning iteration 2738/200000 [0m

                       Computation: 3205 steps/s (collection: 0.474s, learning 2.081s)
               Value function loss: 121620.6160
                    Surrogate loss: 0.0100
             Mean action noise std: 0.89
                       Mean reward: 11485.08
               Mean episode length: 442.57
                 Mean success rate: 88.00
                  Mean reward/step: 26.30
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 22437888
                    Iteration time: 2.56s
                        Total time: 7177.27s
                               ETA: 516904.9s

################################################################################
                    [1m Learning iteration 2739/200000 [0m

                       Computation: 3210 steps/s (collection: 0.491s, learning 2.060s)
               Value function loss: 113592.6676
                    Surrogate loss: 0.0102
             Mean action noise std: 0.89
                       Mean reward: 11500.26
               Mean episode length: 441.51
                 Mean success rate: 87.00
                  Mean reward/step: 25.49
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 22446080
                    Iteration time: 2.55s
                        Total time: 7179.82s
                               ETA: 516897.3s

################################################################################
                    [1m Learning iteration 2740/200000 [0m

                       Computation: 3185 steps/s (collection: 0.516s, learning 2.056s)
               Value function loss: 96814.2271
                    Surrogate loss: 0.0133
             Mean action noise std: 0.89
                       Mean reward: 11728.09
               Mean episode length: 447.01
                 Mean success rate: 88.50
                  Mean reward/step: 24.31
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 22454272
                    Iteration time: 2.57s
                        Total time: 7182.39s
                               ETA: 516891.2s

################################################################################
                    [1m Learning iteration 2741/200000 [0m

                       Computation: 3259 steps/s (collection: 0.477s, learning 2.036s)
               Value function loss: 112651.9813
                    Surrogate loss: 0.0116
             Mean action noise std: 0.89
                       Mean reward: 11235.14
               Mean episode length: 434.13
                 Mean success rate: 85.00
                  Mean reward/step: 24.62
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 22462464
                    Iteration time: 2.51s
                        Total time: 7184.91s
                               ETA: 516880.9s

################################################################################
                    [1m Learning iteration 2742/200000 [0m

                       Computation: 3292 steps/s (collection: 0.470s, learning 2.019s)
               Value function loss: 109617.6809
                    Surrogate loss: 0.0081
             Mean action noise std: 0.89
                       Mean reward: 11225.29
               Mean episode length: 434.20
                 Mean success rate: 85.00
                  Mean reward/step: 25.09
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 22470656
                    Iteration time: 2.49s
                        Total time: 7187.39s
                               ETA: 516868.8s

################################################################################
                    [1m Learning iteration 2743/200000 [0m

                       Computation: 3234 steps/s (collection: 0.478s, learning 2.055s)
               Value function loss: 100521.4283
                    Surrogate loss: 0.0108
             Mean action noise std: 0.89
                       Mean reward: 11470.29
               Mean episode length: 439.93
                 Mean success rate: 86.50
                  Mean reward/step: 24.61
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 22478848
                    Iteration time: 2.53s
                        Total time: 7189.93s
                               ETA: 516859.8s

################################################################################
                    [1m Learning iteration 2744/200000 [0m

                       Computation: 3261 steps/s (collection: 0.468s, learning 2.044s)
               Value function loss: 87566.5036
                    Surrogate loss: 0.0119
             Mean action noise std: 0.89
                       Mean reward: 11258.03
               Mean episode length: 434.12
                 Mean success rate: 85.00
                  Mean reward/step: 25.29
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 22487040
                    Iteration time: 2.51s
                        Total time: 7192.44s
                               ETA: 516849.4s

################################################################################
                    [1m Learning iteration 2745/200000 [0m

                       Computation: 3244 steps/s (collection: 0.486s, learning 2.039s)
               Value function loss: 73359.0253
                    Surrogate loss: 0.0122
             Mean action noise std: 0.89
                       Mean reward: 11302.50
               Mean episode length: 434.27
                 Mean success rate: 85.00
                  Mean reward/step: 25.95
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 22495232
                    Iteration time: 2.53s
                        Total time: 7194.96s
                               ETA: 516840.0s

################################################################################
                    [1m Learning iteration 2746/200000 [0m

                       Computation: 3200 steps/s (collection: 0.485s, learning 2.074s)
               Value function loss: 95278.5352
                    Surrogate loss: 0.0103
             Mean action noise std: 0.89
                       Mean reward: 10986.62
               Mean episode length: 423.74
                 Mean success rate: 83.00
                  Mean reward/step: 26.01
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 22503424
                    Iteration time: 2.56s
                        Total time: 7197.52s
                               ETA: 516833.0s

################################################################################
                    [1m Learning iteration 2747/200000 [0m

                       Computation: 3232 steps/s (collection: 0.469s, learning 2.066s)
               Value function loss: 79844.3709
                    Surrogate loss: 0.0115
             Mean action noise std: 0.89
                       Mean reward: 10849.64
               Mean episode length: 419.50
                 Mean success rate: 82.00
                  Mean reward/step: 25.36
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 2.53s
                        Total time: 7200.06s
                               ETA: 516824.2s

################################################################################
                    [1m Learning iteration 2748/200000 [0m

                       Computation: 3275 steps/s (collection: 0.445s, learning 2.056s)
               Value function loss: 93603.5295
                    Surrogate loss: 0.0112
             Mean action noise std: 0.89
                       Mean reward: 10627.68
               Mean episode length: 415.37
                 Mean success rate: 80.50
                  Mean reward/step: 25.82
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 22519808
                    Iteration time: 2.50s
                        Total time: 7202.56s
                               ETA: 516813.1s

################################################################################
                    [1m Learning iteration 2749/200000 [0m

                       Computation: 3318 steps/s (collection: 0.441s, learning 2.027s)
               Value function loss: 79682.8200
                    Surrogate loss: 0.0133
             Mean action noise std: 0.89
                       Mean reward: 10896.34
               Mean episode length: 423.44
                 Mean success rate: 82.00
                  Mean reward/step: 26.74
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 22528000
                    Iteration time: 2.47s
                        Total time: 7205.03s
                               ETA: 516799.6s

################################################################################
                    [1m Learning iteration 2750/200000 [0m

                       Computation: 3212 steps/s (collection: 0.468s, learning 2.082s)
               Value function loss: 109686.3584
                    Surrogate loss: 0.0111
             Mean action noise std: 0.89
                       Mean reward: 10603.04
               Mean episode length: 413.97
                 Mean success rate: 80.00
                  Mean reward/step: 26.26
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 22536192
                    Iteration time: 2.55s
                        Total time: 7207.58s
                               ETA: 516792.0s

################################################################################
                    [1m Learning iteration 2751/200000 [0m

                       Computation: 3274 steps/s (collection: 0.459s, learning 2.042s)
               Value function loss: 126898.3014
                    Surrogate loss: 0.0135
             Mean action noise std: 0.89
                       Mean reward: 10909.29
               Mean episode length: 423.84
                 Mean success rate: 82.50
                  Mean reward/step: 26.29
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 22544384
                    Iteration time: 2.50s
                        Total time: 7210.08s
                               ETA: 516780.9s

################################################################################
                    [1m Learning iteration 2752/200000 [0m

                       Computation: 3226 steps/s (collection: 0.490s, learning 2.049s)
               Value function loss: 94511.0000
                    Surrogate loss: 0.0123
             Mean action noise std: 0.89
                       Mean reward: 11025.05
               Mean episode length: 426.11
                 Mean success rate: 83.00
                  Mean reward/step: 25.89
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 22552576
                    Iteration time: 2.54s
                        Total time: 7212.62s
                               ETA: 516772.5s

################################################################################
                    [1m Learning iteration 2753/200000 [0m

                       Computation: 3160 steps/s (collection: 0.503s, learning 2.089s)
               Value function loss: 87358.0963
                    Surrogate loss: 0.0110
             Mean action noise std: 0.89
                       Mean reward: 11086.40
               Mean episode length: 429.86
                 Mean success rate: 83.50
                  Mean reward/step: 25.86
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 22560768
                    Iteration time: 2.59s
                        Total time: 7215.21s
                               ETA: 516767.9s

################################################################################
                    [1m Learning iteration 2754/200000 [0m

                       Computation: 3227 steps/s (collection: 0.477s, learning 2.061s)
               Value function loss: 119886.8626
                    Surrogate loss: 0.0132
             Mean action noise std: 0.89
                       Mean reward: 10598.12
               Mean episode length: 417.17
                 Mean success rate: 80.50
                  Mean reward/step: 25.36
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 22568960
                    Iteration time: 2.54s
                        Total time: 7217.75s
                               ETA: 516759.4s

################################################################################
                    [1m Learning iteration 2755/200000 [0m

                       Computation: 3203 steps/s (collection: 0.511s, learning 2.046s)
               Value function loss: 96273.2572
                    Surrogate loss: 0.0175
             Mean action noise std: 0.89
                       Mean reward: 10737.85
               Mean episode length: 421.50
                 Mean success rate: 82.00
                  Mean reward/step: 25.60
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 22577152
                    Iteration time: 2.56s
                        Total time: 7220.31s
                               ETA: 516752.2s

################################################################################
                    [1m Learning iteration 2756/200000 [0m

                       Computation: 3233 steps/s (collection: 0.494s, learning 2.040s)
               Value function loss: 80165.7770
                    Surrogate loss: 0.0170
             Mean action noise std: 0.89
                       Mean reward: 10806.97
               Mean episode length: 425.65
                 Mean success rate: 83.00
                  Mean reward/step: 25.76
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 22585344
                    Iteration time: 2.53s
                        Total time: 7222.84s
                               ETA: 516743.4s

################################################################################
                    [1m Learning iteration 2757/200000 [0m

                       Computation: 3287 steps/s (collection: 0.418s, learning 2.074s)
               Value function loss: 121687.4690
                    Surrogate loss: 0.0129
             Mean action noise std: 0.89
                       Mean reward: 11188.01
               Mean episode length: 435.99
                 Mean success rate: 85.50
                  Mean reward/step: 25.59
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 22593536
                    Iteration time: 2.49s
                        Total time: 7225.33s
                               ETA: 516731.7s

################################################################################
                    [1m Learning iteration 2758/200000 [0m

                       Computation: 3342 steps/s (collection: 0.421s, learning 2.031s)
               Value function loss: 104874.2790
                    Surrogate loss: 0.0103
             Mean action noise std: 0.89
                       Mean reward: 11134.66
               Mean episode length: 432.47
                 Mean success rate: 85.50
                  Mean reward/step: 25.20
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 22601728
                    Iteration time: 2.45s
                        Total time: 7227.78s
                               ETA: 516717.0s

################################################################################
                    [1m Learning iteration 2759/200000 [0m

                       Computation: 3275 steps/s (collection: 0.429s, learning 2.072s)
               Value function loss: 61017.4932
                    Surrogate loss: 0.0171
             Mean action noise std: 0.89
                       Mean reward: 11055.30
               Mean episode length: 427.94
                 Mean success rate: 84.50
                  Mean reward/step: 25.31
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 2.50s
                        Total time: 7230.28s
                               ETA: 516705.9s

################################################################################
                    [1m Learning iteration 2760/200000 [0m

                       Computation: 3264 steps/s (collection: 0.495s, learning 2.015s)
               Value function loss: 75262.4374
                    Surrogate loss: 0.0128
             Mean action noise std: 0.89
                       Mean reward: 10824.23
               Mean episode length: 422.44
                 Mean success rate: 83.00
                  Mean reward/step: 25.71
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 22618112
                    Iteration time: 2.51s
                        Total time: 7232.79s
                               ETA: 516695.4s

################################################################################
                    [1m Learning iteration 2761/200000 [0m

                       Computation: 3263 steps/s (collection: 0.452s, learning 2.058s)
               Value function loss: 75821.2035
                    Surrogate loss: 0.0092
             Mean action noise std: 0.89
                       Mean reward: 10966.17
               Mean episode length: 429.17
                 Mean success rate: 84.50
                  Mean reward/step: 25.59
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 22626304
                    Iteration time: 2.51s
                        Total time: 7235.30s
                               ETA: 516685.0s

################################################################################
                    [1m Learning iteration 2762/200000 [0m

                       Computation: 3143 steps/s (collection: 0.499s, learning 2.107s)
               Value function loss: 121110.5430
                    Surrogate loss: 0.0106
             Mean action noise std: 0.89
                       Mean reward: 10725.72
               Mean episode length: 421.33
                 Mean success rate: 83.50
                  Mean reward/step: 25.55
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 22634496
                    Iteration time: 2.61s
                        Total time: 7237.91s
                               ETA: 516681.4s

################################################################################
                    [1m Learning iteration 2763/200000 [0m

                       Computation: 3239 steps/s (collection: 0.452s, learning 2.077s)
               Value function loss: 69643.3641
                    Surrogate loss: 0.0110
             Mean action noise std: 0.89
                       Mean reward: 11028.55
               Mean episode length: 428.44
                 Mean success rate: 85.50
                  Mean reward/step: 25.01
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 22642688
                    Iteration time: 2.53s
                        Total time: 7240.44s
                               ETA: 516672.3s

################################################################################
                    [1m Learning iteration 2764/200000 [0m

                       Computation: 3239 steps/s (collection: 0.459s, learning 2.070s)
               Value function loss: 107688.3375
                    Surrogate loss: 0.0105
             Mean action noise std: 0.89
                       Mean reward: 11198.31
               Mean episode length: 431.05
                 Mean success rate: 86.00
                  Mean reward/step: 25.95
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 22650880
                    Iteration time: 2.53s
                        Total time: 7242.97s
                               ETA: 516663.2s

################################################################################
                    [1m Learning iteration 2765/200000 [0m

                       Computation: 3230 steps/s (collection: 0.458s, learning 2.078s)
               Value function loss: 110788.5461
                    Surrogate loss: 0.0136
             Mean action noise std: 0.89
                       Mean reward: 11183.85
               Mean episode length: 430.85
                 Mean success rate: 86.00
                  Mean reward/step: 26.19
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 22659072
                    Iteration time: 2.54s
                        Total time: 7245.50s
                               ETA: 516654.6s

################################################################################
                    [1m Learning iteration 2766/200000 [0m

                       Computation: 3253 steps/s (collection: 0.443s, learning 2.075s)
               Value function loss: 59050.1537
                    Surrogate loss: 0.0092
             Mean action noise std: 0.89
                       Mean reward: 10866.30
               Mean episode length: 423.50
                 Mean success rate: 84.00
                  Mean reward/step: 25.41
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 22667264
                    Iteration time: 2.52s
                        Total time: 7248.02s
                               ETA: 516644.8s

################################################################################
                    [1m Learning iteration 2767/200000 [0m

                       Computation: 3215 steps/s (collection: 0.473s, learning 2.075s)
               Value function loss: 101637.6858
                    Surrogate loss: 0.0092
             Mean action noise std: 0.89
                       Mean reward: 10878.11
               Mean episode length: 423.54
                 Mean success rate: 83.50
                  Mean reward/step: 26.03
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 22675456
                    Iteration time: 2.55s
                        Total time: 7250.57s
                               ETA: 516637.0s

################################################################################
                    [1m Learning iteration 2768/200000 [0m

                       Computation: 3232 steps/s (collection: 0.467s, learning 2.067s)
               Value function loss: 83037.8512
                    Surrogate loss: 0.0099
             Mean action noise std: 0.89
                       Mean reward: 10773.84
               Mean episode length: 420.63
                 Mean success rate: 83.00
                  Mean reward/step: 25.83
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 22683648
                    Iteration time: 2.53s
                        Total time: 7253.10s
                               ETA: 516628.4s

################################################################################
                    [1m Learning iteration 2769/200000 [0m

                       Computation: 3210 steps/s (collection: 0.471s, learning 2.081s)
               Value function loss: 99983.3463
                    Surrogate loss: 0.0102
             Mean action noise std: 0.89
                       Mean reward: 10640.29
               Mean episode length: 416.53
                 Mean success rate: 81.50
                  Mean reward/step: 25.92
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 22691840
                    Iteration time: 2.55s
                        Total time: 7255.65s
                               ETA: 516621.0s

################################################################################
                    [1m Learning iteration 2770/200000 [0m

                       Computation: 3194 steps/s (collection: 0.490s, learning 2.074s)
               Value function loss: 112993.1668
                    Surrogate loss: 0.0109
             Mean action noise std: 0.89
                       Mean reward: 10876.61
               Mean episode length: 424.93
                 Mean success rate: 84.00
                  Mean reward/step: 25.19
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 22700032
                    Iteration time: 2.56s
                        Total time: 7258.22s
                               ETA: 516614.4s

################################################################################
                    [1m Learning iteration 2771/200000 [0m

                       Computation: 3244 steps/s (collection: 0.457s, learning 2.068s)
               Value function loss: 97676.2252
                    Surrogate loss: 0.0101
             Mean action noise std: 0.89
                       Mean reward: 10775.43
               Mean episode length: 420.15
                 Mean success rate: 82.50
                  Mean reward/step: 25.42
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 2.52s
                        Total time: 7260.74s
                               ETA: 516605.1s

################################################################################
                    [1m Learning iteration 2772/200000 [0m

                       Computation: 3183 steps/s (collection: 0.489s, learning 2.084s)
               Value function loss: 120572.0811
                    Surrogate loss: 0.0157
             Mean action noise std: 0.89
                       Mean reward: 11075.09
               Mean episode length: 428.26
                 Mean success rate: 84.00
                  Mean reward/step: 25.41
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 22716416
                    Iteration time: 2.57s
                        Total time: 7263.32s
                               ETA: 516599.2s

################################################################################
                    [1m Learning iteration 2773/200000 [0m

                       Computation: 3239 steps/s (collection: 0.431s, learning 2.098s)
               Value function loss: 98420.5286
                    Surrogate loss: 0.0109
             Mean action noise std: 0.89
                       Mean reward: 10721.39
               Mean episode length: 419.70
                 Mean success rate: 82.50
                  Mean reward/step: 25.26
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 22724608
                    Iteration time: 2.53s
                        Total time: 7265.85s
                               ETA: 516590.1s

################################################################################
                    [1m Learning iteration 2774/200000 [0m

                       Computation: 3324 steps/s (collection: 0.420s, learning 2.044s)
               Value function loss: 78288.7481
                    Surrogate loss: 0.0098
             Mean action noise std: 0.89
                       Mean reward: 10704.67
               Mean episode length: 420.08
                 Mean success rate: 83.00
                  Mean reward/step: 24.83
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 22732800
                    Iteration time: 2.46s
                        Total time: 7268.31s
                               ETA: 516576.5s

################################################################################
                    [1m Learning iteration 2775/200000 [0m

                       Computation: 3246 steps/s (collection: 0.446s, learning 2.077s)
               Value function loss: 73356.9167
                    Surrogate loss: 0.0125
             Mean action noise std: 0.89
                       Mean reward: 10623.09
               Mean episode length: 417.05
                 Mean success rate: 82.00
                  Mean reward/step: 26.06
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 22740992
                    Iteration time: 2.52s
                        Total time: 7270.83s
                               ETA: 516567.0s

################################################################################
                    [1m Learning iteration 2776/200000 [0m

                       Computation: 3255 steps/s (collection: 0.480s, learning 2.036s)
               Value function loss: 104982.0908
                    Surrogate loss: 0.0126
             Mean action noise std: 0.89
                       Mean reward: 11096.95
               Mean episode length: 430.86
                 Mean success rate: 85.00
                  Mean reward/step: 26.42
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 22749184
                    Iteration time: 2.52s
                        Total time: 7273.35s
                               ETA: 516557.1s

################################################################################
                    [1m Learning iteration 2777/200000 [0m

                       Computation: 3206 steps/s (collection: 0.468s, learning 2.086s)
               Value function loss: 106366.9664
                    Surrogate loss: 0.0086
             Mean action noise std: 0.89
                       Mean reward: 11159.67
               Mean episode length: 432.54
                 Mean success rate: 85.50
                  Mean reward/step: 26.02
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 22757376
                    Iteration time: 2.55s
                        Total time: 7275.90s
                               ETA: 516549.9s

################################################################################
                    [1m Learning iteration 2778/200000 [0m

                       Computation: 3281 steps/s (collection: 0.452s, learning 2.045s)
               Value function loss: 110176.5480
                    Surrogate loss: 0.0103
             Mean action noise std: 0.89
                       Mean reward: 11006.56
               Mean episode length: 429.79
                 Mean success rate: 84.50
                  Mean reward/step: 25.37
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 22765568
                    Iteration time: 2.50s
                        Total time: 7278.40s
                               ETA: 516538.6s

################################################################################
                    [1m Learning iteration 2779/200000 [0m

                       Computation: 3295 steps/s (collection: 0.457s, learning 2.028s)
               Value function loss: 75741.9412
                    Surrogate loss: 0.0105
             Mean action noise std: 0.89
                       Mean reward: 11003.99
               Mean episode length: 429.27
                 Mean success rate: 85.00
                  Mean reward/step: 24.98
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 22773760
                    Iteration time: 2.49s
                        Total time: 7280.89s
                               ETA: 516526.5s

################################################################################
                    [1m Learning iteration 2780/200000 [0m

                       Computation: 3333 steps/s (collection: 0.420s, learning 2.038s)
               Value function loss: 76644.0723
                    Surrogate loss: 0.0135
             Mean action noise std: 0.89
                       Mean reward: 10941.21
               Mean episode length: 426.19
                 Mean success rate: 84.00
                  Mean reward/step: 25.33
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 22781952
                    Iteration time: 2.46s
                        Total time: 7283.34s
                               ETA: 516512.4s

################################################################################
                    [1m Learning iteration 2781/200000 [0m

                       Computation: 3198 steps/s (collection: 0.505s, learning 2.056s)
               Value function loss: 123897.0641
                    Surrogate loss: 0.0103
             Mean action noise std: 0.89
                       Mean reward: 11018.33
               Mean episode length: 428.73
                 Mean success rate: 84.50
                  Mean reward/step: 25.62
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 22790144
                    Iteration time: 2.56s
                        Total time: 7285.91s
                               ETA: 516505.7s

################################################################################
                    [1m Learning iteration 2782/200000 [0m

                       Computation: 3250 steps/s (collection: 0.464s, learning 2.057s)
               Value function loss: 70908.8562
                    Surrogate loss: 0.0126
             Mean action noise std: 0.89
                       Mean reward: 10883.26
               Mean episode length: 424.49
                 Mean success rate: 84.00
                  Mean reward/step: 25.57
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 22798336
                    Iteration time: 2.52s
                        Total time: 7288.43s
                               ETA: 516496.1s

################################################################################
                    [1m Learning iteration 2783/200000 [0m

                       Computation: 3210 steps/s (collection: 0.506s, learning 2.046s)
               Value function loss: 107597.2846
                    Surrogate loss: 0.0106
             Mean action noise std: 0.89
                       Mean reward: 10654.91
               Mean episode length: 415.90
                 Mean success rate: 82.00
                  Mean reward/step: 25.18
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 2.55s
                        Total time: 7290.98s
                               ETA: 516488.7s

################################################################################
                    [1m Learning iteration 2784/200000 [0m

                       Computation: 3197 steps/s (collection: 0.457s, learning 2.105s)
               Value function loss: 97627.9688
                    Surrogate loss: 0.0130
             Mean action noise std: 0.89
                       Mean reward: 10824.42
               Mean episode length: 423.71
                 Mean success rate: 83.00
                  Mean reward/step: 24.71
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 22814720
                    Iteration time: 2.56s
                        Total time: 7293.54s
                               ETA: 516482.1s

################################################################################
                    [1m Learning iteration 2785/200000 [0m

                       Computation: 3239 steps/s (collection: 0.466s, learning 2.062s)
               Value function loss: 113470.5198
                    Surrogate loss: 0.0115
             Mean action noise std: 0.89
                       Mean reward: 11020.94
               Mean episode length: 431.08
                 Mean success rate: 84.00
                  Mean reward/step: 24.43
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 22822912
                    Iteration time: 2.53s
                        Total time: 7296.07s
                               ETA: 516473.1s

################################################################################
                    [1m Learning iteration 2786/200000 [0m

                       Computation: 3146 steps/s (collection: 0.491s, learning 2.113s)
               Value function loss: 85567.6724
                    Surrogate loss: 0.0098
             Mean action noise std: 0.89
                       Mean reward: 10514.68
               Mean episode length: 415.38
                 Mean success rate: 81.00
                  Mean reward/step: 24.38
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 22831104
                    Iteration time: 2.60s
                        Total time: 7298.67s
                               ETA: 516469.4s

################################################################################
                    [1m Learning iteration 2787/200000 [0m

                       Computation: 3165 steps/s (collection: 0.503s, learning 2.085s)
               Value function loss: 88365.1979
                    Surrogate loss: 0.0114
             Mean action noise std: 0.89
                       Mean reward: 10267.85
               Mean episode length: 406.96
                 Mean success rate: 79.00
                  Mean reward/step: 24.51
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 22839296
                    Iteration time: 2.59s
                        Total time: 7301.26s
                               ETA: 516464.6s

################################################################################
                    [1m Learning iteration 2788/200000 [0m

                       Computation: 3222 steps/s (collection: 0.475s, learning 2.066s)
               Value function loss: 120670.3656
                    Surrogate loss: 0.0112
             Mean action noise std: 0.89
                       Mean reward: 10123.08
               Mean episode length: 401.46
                 Mean success rate: 79.00
                  Mean reward/step: 24.20
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 22847488
                    Iteration time: 2.54s
                        Total time: 7303.80s
                               ETA: 516456.6s

################################################################################
                    [1m Learning iteration 2789/200000 [0m

                       Computation: 3272 steps/s (collection: 0.455s, learning 2.048s)
               Value function loss: 120234.3562
                    Surrogate loss: 0.0100
             Mean action noise std: 0.89
                       Mean reward: 10174.22
               Mean episode length: 405.65
                 Mean success rate: 80.00
                  Mean reward/step: 24.34
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 22855680
                    Iteration time: 2.50s
                        Total time: 7306.31s
                               ETA: 516445.8s

################################################################################
                    [1m Learning iteration 2790/200000 [0m

                       Computation: 3245 steps/s (collection: 0.483s, learning 2.041s)
               Value function loss: 82674.1132
                    Surrogate loss: 0.0117
             Mean action noise std: 0.89
                       Mean reward: 10008.04
               Mean episode length: 402.06
                 Mean success rate: 80.00
                  Mean reward/step: 24.39
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 22863872
                    Iteration time: 2.52s
                        Total time: 7308.83s
                               ETA: 516436.5s

################################################################################
                    [1m Learning iteration 2791/200000 [0m

                       Computation: 3282 steps/s (collection: 0.445s, learning 2.051s)
               Value function loss: 67697.2123
                    Surrogate loss: 0.0123
             Mean action noise std: 0.89
                       Mean reward: 9715.58
               Mean episode length: 393.18
                 Mean success rate: 78.00
                  Mean reward/step: 25.22
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 22872064
                    Iteration time: 2.50s
                        Total time: 7311.32s
                               ETA: 516425.1s

################################################################################
                    [1m Learning iteration 2792/200000 [0m

                       Computation: 3282 steps/s (collection: 0.412s, learning 2.084s)
               Value function loss: 78034.8669
                    Surrogate loss: 0.0094
             Mean action noise std: 0.89
                       Mean reward: 9657.39
               Mean episode length: 390.95
                 Mean success rate: 77.50
                  Mean reward/step: 26.06
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 22880256
                    Iteration time: 2.50s
                        Total time: 7313.82s
                               ETA: 516413.8s

################################################################################
                    [1m Learning iteration 2793/200000 [0m

                       Computation: 3334 steps/s (collection: 0.445s, learning 2.012s)
               Value function loss: 103101.5849
                    Surrogate loss: 0.0097
             Mean action noise std: 0.89
                       Mean reward: 9885.86
               Mean episode length: 397.55
                 Mean success rate: 79.00
                  Mean reward/step: 26.25
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 22888448
                    Iteration time: 2.46s
                        Total time: 7316.28s
                               ETA: 516399.8s

################################################################################
                    [1m Learning iteration 2794/200000 [0m

                       Computation: 3270 steps/s (collection: 0.459s, learning 2.046s)
               Value function loss: 82714.2210
                    Surrogate loss: 0.0116
             Mean action noise std: 0.89
                       Mean reward: 9836.87
               Mean episode length: 395.33
                 Mean success rate: 79.00
                  Mean reward/step: 25.56
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 22896640
                    Iteration time: 2.50s
                        Total time: 7318.78s
                               ETA: 516389.1s

################################################################################
                    [1m Learning iteration 2795/200000 [0m

                       Computation: 3308 steps/s (collection: 0.429s, learning 2.047s)
               Value function loss: 107468.7888
                    Surrogate loss: 0.0128
             Mean action noise std: 0.89
                       Mean reward: 9894.86
               Mean episode length: 395.81
                 Mean success rate: 79.00
                  Mean reward/step: 25.72
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 2.48s
                        Total time: 7321.26s
                               ETA: 516376.4s

################################################################################
                    [1m Learning iteration 2796/200000 [0m

                       Computation: 3316 steps/s (collection: 0.429s, learning 2.041s)
               Value function loss: 65432.2910
                    Surrogate loss: 0.0108
             Mean action noise std: 0.89
                       Mean reward: 9609.87
               Mean episode length: 386.64
                 Mean success rate: 78.00
                  Mean reward/step: 26.17
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 22913024
                    Iteration time: 2.47s
                        Total time: 7323.73s
                               ETA: 516363.4s

################################################################################
                    [1m Learning iteration 2797/200000 [0m

                       Computation: 3245 steps/s (collection: 0.458s, learning 2.066s)
               Value function loss: 83073.8517
                    Surrogate loss: 0.0110
             Mean action noise std: 0.89
                       Mean reward: 9690.65
               Mean episode length: 389.11
                 Mean success rate: 78.50
                  Mean reward/step: 26.28
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 22921216
                    Iteration time: 2.52s
                        Total time: 7326.25s
                               ETA: 516354.1s

################################################################################
                    [1m Learning iteration 2798/200000 [0m

                       Computation: 3170 steps/s (collection: 0.470s, learning 2.113s)
               Value function loss: 106552.6728
                    Surrogate loss: 0.0150
             Mean action noise std: 0.89
                       Mean reward: 9661.95
               Mean episode length: 387.02
                 Mean success rate: 77.00
                  Mean reward/step: 26.55
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 22929408
                    Iteration time: 2.58s
                        Total time: 7328.84s
                               ETA: 516349.0s

################################################################################
                    [1m Learning iteration 2799/200000 [0m

                       Computation: 3300 steps/s (collection: 0.433s, learning 2.049s)
               Value function loss: 107942.3127
                    Surrogate loss: 0.0091
             Mean action noise std: 0.89
                       Mean reward: 9620.34
               Mean episode length: 385.68
                 Mean success rate: 76.00
                  Mean reward/step: 26.18
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 22937600
                    Iteration time: 2.48s
                        Total time: 7331.32s
                               ETA: 516336.8s

################################################################################
                    [1m Learning iteration 2800/200000 [0m

                       Computation: 3319 steps/s (collection: 0.433s, learning 2.035s)
               Value function loss: 95007.6098
                    Surrogate loss: 0.0108
             Mean action noise std: 0.89
                       Mean reward: 9724.28
               Mean episode length: 386.57
                 Mean success rate: 76.00
                  Mean reward/step: 26.47
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 22945792
                    Iteration time: 2.47s
                        Total time: 7333.79s
                               ETA: 516323.6s

################################################################################
                    [1m Learning iteration 2801/200000 [0m

                       Computation: 3338 steps/s (collection: 0.425s, learning 2.028s)
               Value function loss: 101918.7698
                    Surrogate loss: 0.0103
             Mean action noise std: 0.89
                       Mean reward: 10128.30
               Mean episode length: 399.52
                 Mean success rate: 78.00
                  Mean reward/step: 25.68
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 22953984
                    Iteration time: 2.45s
                        Total time: 7336.24s
                               ETA: 516309.4s

################################################################################
                    [1m Learning iteration 2802/200000 [0m

                       Computation: 3281 steps/s (collection: 0.430s, learning 2.066s)
               Value function loss: 132442.0215
                    Surrogate loss: 0.0108
             Mean action noise std: 0.89
                       Mean reward: 10317.17
               Mean episode length: 404.54
                 Mean success rate: 79.50
                  Mean reward/step: 25.38
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 22962176
                    Iteration time: 2.50s
                        Total time: 7338.74s
                               ETA: 516298.2s

################################################################################
                    [1m Learning iteration 2803/200000 [0m

                       Computation: 3299 steps/s (collection: 0.432s, learning 2.051s)
               Value function loss: 81351.6691
                    Surrogate loss: 0.0115
             Mean action noise std: 0.89
                       Mean reward: 10325.23
               Mean episode length: 403.68
                 Mean success rate: 79.50
                  Mean reward/step: 25.02
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 22970368
                    Iteration time: 2.48s
                        Total time: 7341.22s
                               ETA: 516286.1s

################################################################################
                    [1m Learning iteration 2804/200000 [0m

                       Computation: 3226 steps/s (collection: 0.454s, learning 2.085s)
               Value function loss: 117601.5318
                    Surrogate loss: 0.0125
             Mean action noise std: 0.89
                       Mean reward: 10650.79
               Mean episode length: 414.73
                 Mean success rate: 81.50
                  Mean reward/step: 24.43
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 22978560
                    Iteration time: 2.54s
                        Total time: 7343.76s
                               ETA: 516277.9s

################################################################################
                    [1m Learning iteration 2805/200000 [0m

                       Computation: 3256 steps/s (collection: 0.422s, learning 2.094s)
               Value function loss: 90135.9130
                    Surrogate loss: 0.0113
             Mean action noise std: 0.89
                       Mean reward: 10531.46
               Mean episode length: 411.69
                 Mean success rate: 81.00
                  Mean reward/step: 24.22
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 22986752
                    Iteration time: 2.52s
                        Total time: 7346.27s
                               ETA: 516268.1s

################################################################################
                    [1m Learning iteration 2806/200000 [0m

                       Computation: 3282 steps/s (collection: 0.453s, learning 2.043s)
               Value function loss: 92504.5560
                    Surrogate loss: 0.0092
             Mean action noise std: 0.89
                       Mean reward: 11052.73
               Mean episode length: 427.29
                 Mean success rate: 84.00
                  Mean reward/step: 24.86
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 22994944
                    Iteration time: 2.50s
                        Total time: 7348.77s
                               ETA: 516256.9s

################################################################################
                    [1m Learning iteration 2807/200000 [0m

                       Computation: 3328 steps/s (collection: 0.430s, learning 2.031s)
               Value function loss: 56900.4982
                    Surrogate loss: 0.0115
             Mean action noise std: 0.89
                       Mean reward: 10914.17
               Mean episode length: 422.61
                 Mean success rate: 82.50
                  Mean reward/step: 26.07
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 2.46s
                        Total time: 7351.23s
                               ETA: 516243.3s

################################################################################
                    [1m Learning iteration 2808/200000 [0m

                       Computation: 3197 steps/s (collection: 0.471s, learning 2.091s)
               Value function loss: 100507.5473
                    Surrogate loss: 0.0116
             Mean action noise std: 0.89
                       Mean reward: 11031.48
               Mean episode length: 427.64
                 Mean success rate: 83.50
                  Mean reward/step: 26.26
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 23011328
                    Iteration time: 2.56s
                        Total time: 7353.79s
                               ETA: 516236.7s

################################################################################
                    [1m Learning iteration 2809/200000 [0m

                       Computation: 3186 steps/s (collection: 0.476s, learning 2.095s)
               Value function loss: 114112.2685
                    Surrogate loss: 0.0125
             Mean action noise std: 0.89
                       Mean reward: 11310.22
               Mean episode length: 436.83
                 Mean success rate: 85.50
                  Mean reward/step: 25.86
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 23019520
                    Iteration time: 2.57s
                        Total time: 7356.36s
                               ETA: 516230.8s

################################################################################
                    [1m Learning iteration 2810/200000 [0m

                       Computation: 3243 steps/s (collection: 0.451s, learning 2.075s)
               Value function loss: 76454.9489
                    Surrogate loss: 0.0116
             Mean action noise std: 0.89
                       Mean reward: 11400.07
               Mean episode length: 438.31
                 Mean success rate: 86.50
                  Mean reward/step: 25.16
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 23027712
                    Iteration time: 2.53s
                        Total time: 7358.89s
                               ETA: 516221.7s

################################################################################
                    [1m Learning iteration 2811/200000 [0m

                       Computation: 3155 steps/s (collection: 0.448s, learning 2.148s)
               Value function loss: 92434.0366
                    Surrogate loss: 0.0115
             Mean action noise std: 0.89
                       Mean reward: 11080.45
               Mean episode length: 429.51
                 Mean success rate: 84.50
                  Mean reward/step: 25.40
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 23035904
                    Iteration time: 2.60s
                        Total time: 7361.48s
                               ETA: 516217.6s

################################################################################
                    [1m Learning iteration 2812/200000 [0m

                       Computation: 3145 steps/s (collection: 0.448s, learning 2.157s)
               Value function loss: 114604.4055
                    Surrogate loss: 0.0120
             Mean action noise std: 0.89
                       Mean reward: 11114.17
               Mean episode length: 428.24
                 Mean success rate: 84.50
                  Mean reward/step: 25.88
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 23044096
                    Iteration time: 2.60s
                        Total time: 7364.09s
                               ETA: 516214.0s

################################################################################
                    [1m Learning iteration 2813/200000 [0m

                       Computation: 3204 steps/s (collection: 0.486s, learning 2.071s)
               Value function loss: 66240.6875
                    Surrogate loss: 0.0110
             Mean action noise std: 0.89
                       Mean reward: 10929.43
               Mean episode length: 423.35
                 Mean success rate: 83.00
                  Mean reward/step: 25.31
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 23052288
                    Iteration time: 2.56s
                        Total time: 7366.65s
                               ETA: 516207.1s

################################################################################
                    [1m Learning iteration 2814/200000 [0m

                       Computation: 3204 steps/s (collection: 0.449s, learning 2.107s)
               Value function loss: 131780.6980
                    Surrogate loss: 0.0123
             Mean action noise std: 0.89
                       Mean reward: 10726.79
               Mean episode length: 419.01
                 Mean success rate: 81.50
                  Mean reward/step: 25.08
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 23060480
                    Iteration time: 2.56s
                        Total time: 7369.20s
                               ETA: 516200.2s

################################################################################
                    [1m Learning iteration 2815/200000 [0m

                       Computation: 3111 steps/s (collection: 0.491s, learning 2.142s)
               Value function loss: 74090.1249
                    Surrogate loss: 0.0122
             Mean action noise std: 0.89
                       Mean reward: 10638.78
               Mean episode length: 414.91
                 Mean success rate: 81.00
                  Mean reward/step: 24.39
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 23068672
                    Iteration time: 2.63s
                        Total time: 7371.84s
                               ETA: 516198.7s

################################################################################
                    [1m Learning iteration 2816/200000 [0m

                       Computation: 3116 steps/s (collection: 0.490s, learning 2.138s)
               Value function loss: 100376.9447
                    Surrogate loss: 0.0128
             Mean action noise std: 0.89
                       Mean reward: 10583.44
               Mean episode length: 416.56
                 Mean success rate: 81.50
                  Mean reward/step: 24.76
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 23076864
                    Iteration time: 2.63s
                        Total time: 7374.46s
                               ETA: 516196.8s

################################################################################
                    [1m Learning iteration 2817/200000 [0m

                       Computation: 3164 steps/s (collection: 0.472s, learning 2.116s)
               Value function loss: 131721.6840
                    Surrogate loss: 0.0122
             Mean action noise std: 0.89
                       Mean reward: 10423.16
               Mean episode length: 413.33
                 Mean success rate: 80.50
                  Mean reward/step: 24.45
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 23085056
                    Iteration time: 2.59s
                        Total time: 7377.05s
                               ETA: 516192.1s

################################################################################
                    [1m Learning iteration 2818/200000 [0m

                       Computation: 3124 steps/s (collection: 0.484s, learning 2.138s)
               Value function loss: 94509.3886
                    Surrogate loss: 0.0103
             Mean action noise std: 0.89
                       Mean reward: 10071.58
               Mean episode length: 402.06
                 Mean success rate: 78.00
                  Mean reward/step: 23.63
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 23093248
                    Iteration time: 2.62s
                        Total time: 7379.68s
                               ETA: 516189.8s

################################################################################
                    [1m Learning iteration 2819/200000 [0m

                       Computation: 3111 steps/s (collection: 0.504s, learning 2.129s)
               Value function loss: 135207.4492
                    Surrogate loss: 0.0112
             Mean action noise std: 0.89
                       Mean reward: 10056.93
               Mean episode length: 402.27
                 Mean success rate: 77.50
                  Mean reward/step: 24.03
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 2.63s
                        Total time: 7382.31s
                               ETA: 516188.2s

################################################################################
                    [1m Learning iteration 2820/200000 [0m

                       Computation: 3137 steps/s (collection: 0.478s, learning 2.133s)
               Value function loss: 96597.4361
                    Surrogate loss: 0.0100
             Mean action noise std: 0.89
                       Mean reward: 10203.10
               Mean episode length: 407.39
                 Mean success rate: 78.50
                  Mean reward/step: 24.10
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 23109632
                    Iteration time: 2.61s
                        Total time: 7384.92s
                               ETA: 516185.1s

################################################################################
                    [1m Learning iteration 2821/200000 [0m

                       Computation: 3144 steps/s (collection: 0.501s, learning 2.104s)
               Value function loss: 64117.7443
                    Surrogate loss: 0.0116
             Mean action noise std: 0.89
                       Mean reward: 10149.63
               Mean episode length: 406.96
                 Mean success rate: 78.00
                  Mean reward/step: 24.59
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 23117824
                    Iteration time: 2.61s
                        Total time: 7387.52s
                               ETA: 516181.6s

################################################################################
                    [1m Learning iteration 2822/200000 [0m

                       Computation: 3187 steps/s (collection: 0.487s, learning 2.083s)
               Value function loss: 69294.5683
                    Surrogate loss: 0.0116
             Mean action noise std: 0.89
                       Mean reward: 10144.58
               Mean episode length: 407.23
                 Mean success rate: 78.50
                  Mean reward/step: 24.99
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 23126016
                    Iteration time: 2.57s
                        Total time: 7390.09s
                               ETA: 516175.7s

################################################################################
                    [1m Learning iteration 2823/200000 [0m

                       Computation: 3212 steps/s (collection: 0.452s, learning 2.098s)
               Value function loss: 65545.3398
                    Surrogate loss: 0.0113
             Mean action noise std: 0.89
                       Mean reward: 10043.73
               Mean episode length: 403.81
                 Mean success rate: 78.00
                  Mean reward/step: 25.91
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 23134208
                    Iteration time: 2.55s
                        Total time: 7392.64s
                               ETA: 516168.3s

################################################################################
                    [1m Learning iteration 2824/200000 [0m

                       Computation: 3170 steps/s (collection: 0.478s, learning 2.105s)
               Value function loss: 78998.7083
                    Surrogate loss: 0.0104
             Mean action noise std: 0.89
                       Mean reward: 9898.35
               Mean episode length: 399.81
                 Mean success rate: 77.50
                  Mean reward/step: 26.47
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 23142400
                    Iteration time: 2.58s
                        Total time: 7395.23s
                               ETA: 516163.3s

################################################################################
                    [1m Learning iteration 2825/200000 [0m

                       Computation: 3115 steps/s (collection: 0.492s, learning 2.137s)
               Value function loss: 94296.6948
                    Surrogate loss: 0.0110
             Mean action noise std: 0.89
                       Mean reward: 9773.36
               Mean episode length: 395.51
                 Mean success rate: 76.00
                  Mean reward/step: 26.00
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 23150592
                    Iteration time: 2.63s
                        Total time: 7397.86s
                               ETA: 516161.5s

################################################################################
                    [1m Learning iteration 2826/200000 [0m

                       Computation: 3186 steps/s (collection: 0.456s, learning 2.115s)
               Value function loss: 110124.0592
                    Surrogate loss: 0.0112
             Mean action noise std: 0.89
                       Mean reward: 9976.83
               Mean episode length: 398.47
                 Mean success rate: 76.00
                  Mean reward/step: 25.41
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 23158784
                    Iteration time: 2.57s
                        Total time: 7400.43s
                               ETA: 516155.6s

################################################################################
                    [1m Learning iteration 2827/200000 [0m

                       Computation: 3178 steps/s (collection: 0.488s, learning 2.090s)
               Value function loss: 69999.6784
                    Surrogate loss: 0.0096
             Mean action noise std: 0.89
                       Mean reward: 9843.31
               Mean episode length: 394.52
                 Mean success rate: 76.00
                  Mean reward/step: 25.31
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 23166976
                    Iteration time: 2.58s
                        Total time: 7403.01s
                               ETA: 516150.2s

################################################################################
                    [1m Learning iteration 2828/200000 [0m

                       Computation: 3257 steps/s (collection: 0.470s, learning 2.045s)
               Value function loss: 125485.7283
                    Surrogate loss: 0.0108
             Mean action noise std: 0.89
                       Mean reward: 10117.64
               Mean episode length: 403.52
                 Mean success rate: 78.50
                  Mean reward/step: 24.96
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 23175168
                    Iteration time: 2.52s
                        Total time: 7405.52s
                               ETA: 516140.4s

################################################################################
                    [1m Learning iteration 2829/200000 [0m

                       Computation: 3158 steps/s (collection: 0.535s, learning 2.058s)
               Value function loss: 74736.6491
                    Surrogate loss: 0.0104
             Mean action noise std: 0.89
                       Mean reward: 10135.24
               Mean episode length: 405.13
                 Mean success rate: 79.00
                  Mean reward/step: 24.97
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 23183360
                    Iteration time: 2.59s
                        Total time: 7408.11s
                               ETA: 516136.1s

################################################################################
                    [1m Learning iteration 2830/200000 [0m

                       Computation: 3278 steps/s (collection: 0.471s, learning 2.027s)
               Value function loss: 104546.3785
                    Surrogate loss: 0.0092
             Mean action noise std: 0.89
                       Mean reward: 10369.91
               Mean episode length: 412.27
                 Mean success rate: 80.50
                  Mean reward/step: 24.96
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 23191552
                    Iteration time: 2.50s
                        Total time: 7410.61s
                               ETA: 516125.2s

################################################################################
                    [1m Learning iteration 2831/200000 [0m

                       Computation: 3240 steps/s (collection: 0.468s, learning 2.060s)
               Value function loss: 74333.9617
                    Surrogate loss: 0.0120
             Mean action noise std: 0.89
                       Mean reward: 10085.84
               Mean episode length: 403.57
                 Mean success rate: 79.00
                  Mean reward/step: 25.33
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 2.53s
                        Total time: 7413.14s
                               ETA: 516116.4s

################################################################################
                    [1m Learning iteration 2832/200000 [0m

                       Computation: 3256 steps/s (collection: 0.467s, learning 2.048s)
               Value function loss: 120105.6560
                    Surrogate loss: 0.0133
             Mean action noise std: 0.89
                       Mean reward: 10086.84
               Mean episode length: 403.93
                 Mean success rate: 79.00
                  Mean reward/step: 24.81
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 23207936
                    Iteration time: 2.52s
                        Total time: 7415.66s
                               ETA: 516106.6s

################################################################################
                    [1m Learning iteration 2833/200000 [0m

                       Computation: 3272 steps/s (collection: 0.500s, learning 2.003s)
               Value function loss: 134809.1871
                    Surrogate loss: 0.0111
             Mean action noise std: 0.89
                       Mean reward: 10463.38
               Mean episode length: 415.87
                 Mean success rate: 81.00
                  Mean reward/step: 23.98
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 23216128
                    Iteration time: 2.50s
                        Total time: 7418.16s
                               ETA: 516096.1s

################################################################################
                    [1m Learning iteration 2834/200000 [0m

                       Computation: 3306 steps/s (collection: 0.442s, learning 2.036s)
               Value function loss: 65758.6891
                    Surrogate loss: 0.0112
             Mean action noise std: 0.89
                       Mean reward: 10344.78
               Mean episode length: 410.68
                 Mean success rate: 80.00
                  Mean reward/step: 23.66
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 23224320
                    Iteration time: 2.48s
                        Total time: 7420.64s
                               ETA: 516083.7s

################################################################################
                    [1m Learning iteration 2835/200000 [0m

                       Computation: 3248 steps/s (collection: 0.481s, learning 2.041s)
               Value function loss: 118287.9094
                    Surrogate loss: 0.0104
             Mean action noise std: 0.89
                       Mean reward: 10803.38
               Mean episode length: 425.66
                 Mean success rate: 83.50
                  Mean reward/step: 23.80
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 23232512
                    Iteration time: 2.52s
                        Total time: 7423.16s
                               ETA: 516074.5s

################################################################################
                    [1m Learning iteration 2836/200000 [0m

                       Computation: 3267 steps/s (collection: 0.476s, learning 2.031s)
               Value function loss: 98860.5228
                    Surrogate loss: 0.0099
             Mean action noise std: 0.89
                       Mean reward: 10757.15
               Mean episode length: 425.21
                 Mean success rate: 83.00
                  Mean reward/step: 24.23
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 23240704
                    Iteration time: 2.51s
                        Total time: 7425.67s
                               ETA: 516064.2s

################################################################################
                    [1m Learning iteration 2837/200000 [0m

                       Computation: 3212 steps/s (collection: 0.492s, learning 2.058s)
               Value function loss: 63724.5995
                    Surrogate loss: 0.0148
             Mean action noise std: 0.89
                       Mean reward: 10952.11
               Mean episode length: 431.60
                 Mean success rate: 83.50
                  Mean reward/step: 24.50
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 23248896
                    Iteration time: 2.55s
                        Total time: 7428.22s
                               ETA: 516056.9s

################################################################################
                    [1m Learning iteration 2838/200000 [0m

                       Computation: 3233 steps/s (collection: 0.481s, learning 2.053s)
               Value function loss: 70742.6974
                    Surrogate loss: 0.0123
             Mean action noise std: 0.89
                       Mean reward: 9948.69
               Mean episode length: 401.58
                 Mean success rate: 76.50
                  Mean reward/step: 25.06
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 23257088
                    Iteration time: 2.53s
                        Total time: 7430.75s
                               ETA: 516048.5s

################################################################################
                    [1m Learning iteration 2839/200000 [0m

                       Computation: 3164 steps/s (collection: 0.472s, learning 2.117s)
               Value function loss: 81034.4116
                    Surrogate loss: 0.0118
             Mean action noise std: 0.89
                       Mean reward: 9408.18
               Mean episode length: 384.11
                 Mean success rate: 72.50
                  Mean reward/step: 25.93
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 23265280
                    Iteration time: 2.59s
                        Total time: 7433.34s
                               ETA: 516043.8s

################################################################################
                    [1m Learning iteration 2840/200000 [0m

                       Computation: 3175 steps/s (collection: 0.462s, learning 2.117s)
               Value function loss: 122293.4708
                    Surrogate loss: 0.0114
             Mean action noise std: 0.89
                       Mean reward: 9758.87
               Mean episode length: 395.41
                 Mean success rate: 75.00
                  Mean reward/step: 25.80
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 23273472
                    Iteration time: 2.58s
                        Total time: 7435.92s
                               ETA: 516038.6s

################################################################################
                    [1m Learning iteration 2841/200000 [0m

                       Computation: 3154 steps/s (collection: 0.495s, learning 2.101s)
               Value function loss: 75858.4336
                    Surrogate loss: 0.0137
             Mean action noise std: 0.89
                       Mean reward: 9784.35
               Mean episode length: 396.56
                 Mean success rate: 75.50
                  Mean reward/step: 25.43
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 23281664
                    Iteration time: 2.60s
                        Total time: 7438.52s
                               ETA: 516034.6s

################################################################################
                    [1m Learning iteration 2842/200000 [0m

                       Computation: 3194 steps/s (collection: 0.469s, learning 2.096s)
               Value function loss: 117713.8396
                    Surrogate loss: 0.0143
             Mean action noise std: 0.89
                       Mean reward: 10063.41
               Mean episode length: 404.56
                 Mean success rate: 77.50
                  Mean reward/step: 25.33
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 23289856
                    Iteration time: 2.56s
                        Total time: 7441.08s
                               ETA: 516028.3s

################################################################################
                    [1m Learning iteration 2843/200000 [0m

                       Computation: 3226 steps/s (collection: 0.448s, learning 2.091s)
               Value function loss: 120998.6773
                    Surrogate loss: 0.0087
             Mean action noise std: 0.89
                       Mean reward: 9839.94
               Mean episode length: 399.77
                 Mean success rate: 76.50
                  Mean reward/step: 24.59
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 2.54s
                        Total time: 7443.62s
                               ETA: 516020.2s

################################################################################
                    [1m Learning iteration 2844/200000 [0m

                       Computation: 3150 steps/s (collection: 0.474s, learning 2.125s)
               Value function loss: 69233.4157
                    Surrogate loss: 0.0100
             Mean action noise std: 0.89
                       Mean reward: 10016.81
               Mean episode length: 405.43
                 Mean success rate: 77.50
                  Mean reward/step: 24.55
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 23306240
                    Iteration time: 2.60s
                        Total time: 7446.22s
                               ETA: 516016.4s

################################################################################
                    [1m Learning iteration 2845/200000 [0m

                       Computation: 3159 steps/s (collection: 0.498s, learning 2.095s)
               Value function loss: 93303.1716
                    Surrogate loss: 0.0100
             Mean action noise std: 0.89
                       Mean reward: 9780.51
               Mean episode length: 397.33
                 Mean success rate: 76.50
                  Mean reward/step: 25.14
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 23314432
                    Iteration time: 2.59s
                        Total time: 7448.81s
                               ETA: 516012.1s

################################################################################
                    [1m Learning iteration 2846/200000 [0m

                       Computation: 3203 steps/s (collection: 0.465s, learning 2.092s)
               Value function loss: 76288.8525
                    Surrogate loss: 0.0091
             Mean action noise std: 0.89
                       Mean reward: 9792.11
               Mean episode length: 397.16
                 Mean success rate: 77.00
                  Mean reward/step: 25.57
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 23322624
                    Iteration time: 2.56s
                        Total time: 7451.37s
                               ETA: 516005.4s

################################################################################
                    [1m Learning iteration 2847/200000 [0m

                       Computation: 3160 steps/s (collection: 0.489s, learning 2.103s)
               Value function loss: 84007.2664
                    Surrogate loss: 0.0113
             Mean action noise std: 0.89
                       Mean reward: 9595.91
               Mean episode length: 391.13
                 Mean success rate: 75.50
                  Mean reward/step: 25.45
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 23330816
                    Iteration time: 2.59s
                        Total time: 7453.96s
                               ETA: 516001.0s

################################################################################
                    [1m Learning iteration 2848/200000 [0m

                       Computation: 3068 steps/s (collection: 0.531s, learning 2.139s)
               Value function loss: 163572.2455
                    Surrogate loss: 0.0155
             Mean action noise std: 0.89
                       Mean reward: 10560.88
               Mean episode length: 422.13
                 Mean success rate: 82.50
                  Mean reward/step: 24.95
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 23339008
                    Iteration time: 2.67s
                        Total time: 7456.63s
                               ETA: 516002.0s

################################################################################
                    [1m Learning iteration 2849/200000 [0m

                       Computation: 3139 steps/s (collection: 0.484s, learning 2.126s)
               Value function loss: 117363.5150
                    Surrogate loss: 0.0128
             Mean action noise std: 0.89
                       Mean reward: 10984.47
               Mean episode length: 432.38
                 Mean success rate: 85.00
                  Mean reward/step: 23.45
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 23347200
                    Iteration time: 2.61s
                        Total time: 7459.24s
                               ETA: 515998.9s

################################################################################
                    [1m Learning iteration 2850/200000 [0m

                       Computation: 3186 steps/s (collection: 0.463s, learning 2.108s)
               Value function loss: 86833.7802
                    Surrogate loss: 0.0129
             Mean action noise std: 0.89
                       Mean reward: 10985.54
               Mean episode length: 432.92
                 Mean success rate: 85.00
                  Mean reward/step: 23.13
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 23355392
                    Iteration time: 2.57s
                        Total time: 7461.81s
                               ETA: 515993.0s

################################################################################
                    [1m Learning iteration 2851/200000 [0m

                       Computation: 3189 steps/s (collection: 0.464s, learning 2.104s)
               Value function loss: 107083.0080
                    Surrogate loss: 0.0115
             Mean action noise std: 0.89
                       Mean reward: 10876.78
               Mean episode length: 428.69
                 Mean success rate: 84.00
                  Mean reward/step: 22.84
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 23363584
                    Iteration time: 2.57s
                        Total time: 7464.38s
                               ETA: 515987.0s

################################################################################
                    [1m Learning iteration 2852/200000 [0m

                       Computation: 3161 steps/s (collection: 0.478s, learning 2.113s)
               Value function loss: 73455.7553
                    Surrogate loss: 0.0143
             Mean action noise std: 0.89
                       Mean reward: 10599.19
               Mean episode length: 421.21
                 Mean success rate: 83.00
                  Mean reward/step: 22.92
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 23371776
                    Iteration time: 2.59s
                        Total time: 7466.97s
                               ETA: 515982.6s

################################################################################
                    [1m Learning iteration 2853/200000 [0m

                       Computation: 3225 steps/s (collection: 0.483s, learning 2.056s)
               Value function loss: 124760.3930
                    Surrogate loss: 0.0119
             Mean action noise std: 0.89
                       Mean reward: 10617.08
               Mean episode length: 421.72
                 Mean success rate: 82.50
                  Mean reward/step: 23.93
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 23379968
                    Iteration time: 2.54s
                        Total time: 7469.51s
                               ETA: 515974.6s

################################################################################
                    [1m Learning iteration 2854/200000 [0m

                       Computation: 3193 steps/s (collection: 0.472s, learning 2.094s)
               Value function loss: 67761.8730
                    Surrogate loss: 0.0121
             Mean action noise std: 0.89
                       Mean reward: 10504.70
               Mean episode length: 418.08
                 Mean success rate: 82.00
                  Mean reward/step: 23.39
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 23388160
                    Iteration time: 2.57s
                        Total time: 7472.08s
                               ETA: 515968.4s

################################################################################
                    [1m Learning iteration 2855/200000 [0m

                       Computation: 3164 steps/s (collection: 0.526s, learning 2.063s)
               Value function loss: 112527.5698
                    Surrogate loss: 0.0124
             Mean action noise std: 0.89
                       Mean reward: 10172.11
               Mean episode length: 410.57
                 Mean success rate: 79.00
                  Mean reward/step: 24.42
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 2.59s
                        Total time: 7474.66s
                               ETA: 515963.8s

################################################################################
                    [1m Learning iteration 2856/200000 [0m

                       Computation: 3166 steps/s (collection: 0.489s, learning 2.098s)
               Value function loss: 90421.5972
                    Surrogate loss: 0.0115
             Mean action noise std: 0.89
                       Mean reward: 9633.85
               Mean episode length: 393.50
                 Mean success rate: 75.50
                  Mean reward/step: 24.28
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 23404544
                    Iteration time: 2.59s
                        Total time: 7477.25s
                               ETA: 515959.1s

################################################################################
                    [1m Learning iteration 2857/200000 [0m

                       Computation: 3252 steps/s (collection: 0.474s, learning 2.045s)
               Value function loss: 83562.9287
                    Surrogate loss: 0.0131
             Mean action noise std: 0.89
                       Mean reward: 9779.35
               Mean episode length: 397.90
                 Mean success rate: 76.50
                  Mean reward/step: 24.27
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 23412736
                    Iteration time: 2.52s
                        Total time: 7479.77s
                               ETA: 515949.7s

################################################################################
                    [1m Learning iteration 2858/200000 [0m

                       Computation: 3159 steps/s (collection: 0.491s, learning 2.102s)
               Value function loss: 115521.7788
                    Surrogate loss: 0.0103
             Mean action noise std: 0.89
                       Mean reward: 9493.00
               Mean episode length: 391.20
                 Mean success rate: 75.50
                  Mean reward/step: 24.86
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 23420928
                    Iteration time: 2.59s
                        Total time: 7482.36s
                               ETA: 515945.4s

################################################################################
                    [1m Learning iteration 2859/200000 [0m

                       Computation: 3166 steps/s (collection: 0.502s, learning 2.085s)
               Value function loss: 76372.6354
                    Surrogate loss: 0.0126
             Mean action noise std: 0.89
                       Mean reward: 9510.77
               Mean episode length: 393.63
                 Mean success rate: 76.00
                  Mean reward/step: 25.15
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 23429120
                    Iteration time: 2.59s
                        Total time: 7484.95s
                               ETA: 515940.7s

################################################################################
                    [1m Learning iteration 2860/200000 [0m

                       Computation: 3189 steps/s (collection: 0.503s, learning 2.065s)
               Value function loss: 47765.9718
                    Surrogate loss: 0.0124
             Mean action noise std: 0.89
                       Mean reward: 8839.56
               Mean episode length: 375.05
                 Mean success rate: 71.50
                  Mean reward/step: 25.43
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 23437312
                    Iteration time: 2.57s
                        Total time: 7487.52s
                               ETA: 515934.8s

################################################################################
                    [1m Learning iteration 2861/200000 [0m

                       Computation: 3192 steps/s (collection: 0.481s, learning 2.085s)
               Value function loss: 79693.2508
                    Surrogate loss: 0.0095
             Mean action noise std: 0.89
                       Mean reward: 8845.29
               Mean episode length: 374.63
                 Mean success rate: 72.00
                  Mean reward/step: 26.00
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 23445504
                    Iteration time: 2.57s
                        Total time: 7490.08s
                               ETA: 515928.6s

################################################################################
                    [1m Learning iteration 2862/200000 [0m

                       Computation: 3194 steps/s (collection: 0.467s, learning 2.098s)
               Value function loss: 61122.4288
                    Surrogate loss: 0.0123
             Mean action noise std: 0.89
                       Mean reward: 9041.33
               Mean episode length: 380.42
                 Mean success rate: 72.00
                  Mean reward/step: 26.92
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 23453696
                    Iteration time: 2.56s
                        Total time: 7492.65s
                               ETA: 515922.4s

################################################################################
                    [1m Learning iteration 2863/200000 [0m

                       Computation: 3214 steps/s (collection: 0.488s, learning 2.060s)
               Value function loss: 109705.8359
                    Surrogate loss: 0.0088
             Mean action noise std: 0.89
                       Mean reward: 9273.22
               Mean episode length: 389.30
                 Mean success rate: 74.00
                  Mean reward/step: 27.17
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 23461888
                    Iteration time: 2.55s
                        Total time: 7495.20s
                               ETA: 515915.1s

################################################################################
                    [1m Learning iteration 2864/200000 [0m

                       Computation: 3170 steps/s (collection: 0.478s, learning 2.106s)
               Value function loss: 121088.4062
                    Surrogate loss: 0.0128
             Mean action noise std: 0.89
                       Mean reward: 9631.52
               Mean episode length: 401.73
                 Mean success rate: 77.00
                  Mean reward/step: 26.75
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 23470080
                    Iteration time: 2.58s
                        Total time: 7497.78s
                               ETA: 515910.2s

################################################################################
                    [1m Learning iteration 2865/200000 [0m

                       Computation: 3199 steps/s (collection: 0.448s, learning 2.112s)
               Value function loss: 101519.0522
                    Surrogate loss: 0.0122
             Mean action noise std: 0.89
                       Mean reward: 9601.61
               Mean episode length: 396.45
                 Mean success rate: 77.00
                  Mean reward/step: 25.86
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 23478272
                    Iteration time: 2.56s
                        Total time: 7500.34s
                               ETA: 515903.7s

################################################################################
                    [1m Learning iteration 2866/200000 [0m

                       Computation: 3138 steps/s (collection: 0.464s, learning 2.146s)
               Value function loss: 96297.3623
                    Surrogate loss: 0.0132
             Mean action noise std: 0.89
                       Mean reward: 9904.34
               Mean episode length: 405.55
                 Mean success rate: 80.00
                  Mean reward/step: 25.47
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 23486464
                    Iteration time: 2.61s
                        Total time: 7502.95s
                               ETA: 515900.6s

################################################################################
                    [1m Learning iteration 2867/200000 [0m

                       Computation: 3181 steps/s (collection: 0.456s, learning 2.119s)
               Value function loss: 106889.7909
                    Surrogate loss: 0.0139
             Mean action noise std: 0.89
                       Mean reward: 9758.55
               Mean episode length: 400.97
                 Mean success rate: 79.50
                  Mean reward/step: 25.10
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 2.57s
                        Total time: 7505.53s
                               ETA: 515895.0s

################################################################################
                    [1m Learning iteration 2868/200000 [0m

                       Computation: 3189 steps/s (collection: 0.469s, learning 2.099s)
               Value function loss: 58852.1685
                    Surrogate loss: 0.0121
             Mean action noise std: 0.89
                       Mean reward: 9704.48
               Mean episode length: 395.51
                 Mean success rate: 78.00
                  Mean reward/step: 25.64
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 23502848
                    Iteration time: 2.57s
                        Total time: 7508.09s
                               ETA: 515889.1s

################################################################################
                    [1m Learning iteration 2869/200000 [0m

                       Computation: 3156 steps/s (collection: 0.503s, learning 2.093s)
               Value function loss: 145383.5382
                    Surrogate loss: 0.0105
             Mean action noise std: 0.89
                       Mean reward: 9717.74
               Mean episode length: 394.70
                 Mean success rate: 77.50
                  Mean reward/step: 26.55
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 23511040
                    Iteration time: 2.60s
                        Total time: 7510.69s
                               ETA: 515885.0s

################################################################################
                    [1m Learning iteration 2870/200000 [0m

                       Computation: 3243 steps/s (collection: 0.445s, learning 2.080s)
               Value function loss: 61660.9946
                    Surrogate loss: 0.0115
             Mean action noise std: 0.89
                       Mean reward: 10111.83
               Mean episode length: 403.96
                 Mean success rate: 79.50
                  Mean reward/step: 26.39
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 23519232
                    Iteration time: 2.53s
                        Total time: 7513.22s
                               ETA: 515876.1s

################################################################################
                    [1m Learning iteration 2871/200000 [0m

                       Computation: 3176 steps/s (collection: 0.464s, learning 2.115s)
               Value function loss: 154263.2893
                    Surrogate loss: 0.0114
             Mean action noise std: 0.89
                       Mean reward: 10663.24
               Mean episode length: 416.32
                 Mean success rate: 82.50
                  Mean reward/step: 25.88
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 23527424
                    Iteration time: 2.58s
                        Total time: 7515.79s
                               ETA: 515870.9s

################################################################################
                    [1m Learning iteration 2872/200000 [0m

                       Computation: 3112 steps/s (collection: 0.502s, learning 2.130s)
               Value function loss: 99395.7218
                    Surrogate loss: 0.0140
             Mean action noise std: 0.89
                       Mean reward: 10305.48
               Mean episode length: 403.90
                 Mean success rate: 80.00
                  Mean reward/step: 24.26
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 23535616
                    Iteration time: 2.63s
                        Total time: 7518.43s
                               ETA: 515869.3s

################################################################################
                    [1m Learning iteration 2873/200000 [0m

                       Computation: 3211 steps/s (collection: 0.464s, learning 2.087s)
               Value function loss: 92418.0244
                    Surrogate loss: 0.0114
             Mean action noise std: 0.89
                       Mean reward: 10156.59
               Mean episode length: 396.93
                 Mean success rate: 79.00
                  Mean reward/step: 24.41
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 23543808
                    Iteration time: 2.55s
                        Total time: 7520.98s
                               ETA: 515862.1s

################################################################################
                    [1m Learning iteration 2874/200000 [0m

                       Computation: 3207 steps/s (collection: 0.474s, learning 2.080s)
               Value function loss: 107512.6359
                    Surrogate loss: 0.0126
             Mean action noise std: 0.89
                       Mean reward: 10276.19
               Mean episode length: 400.31
                 Mean success rate: 79.00
                  Mean reward/step: 24.56
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 23552000
                    Iteration time: 2.55s
                        Total time: 7523.53s
                               ETA: 515855.2s

################################################################################
                    [1m Learning iteration 2875/200000 [0m

                       Computation: 3137 steps/s (collection: 0.487s, learning 2.124s)
               Value function loss: 99734.8187
                    Surrogate loss: 0.0096
             Mean action noise std: 0.89
                       Mean reward: 10473.79
               Mean episode length: 406.29
                 Mean success rate: 80.50
                  Mean reward/step: 25.13
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 23560192
                    Iteration time: 2.61s
                        Total time: 7526.14s
                               ETA: 515852.2s

################################################################################
                    [1m Learning iteration 2876/200000 [0m

                       Computation: 3173 steps/s (collection: 0.503s, learning 2.078s)
               Value function loss: 84896.8063
                    Surrogate loss: 0.0135
             Mean action noise std: 0.89
                       Mean reward: 10434.99
               Mean episode length: 404.98
                 Mean success rate: 80.50
                  Mean reward/step: 25.93
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 23568384
                    Iteration time: 2.58s
                        Total time: 7528.72s
                               ETA: 515847.1s

################################################################################
                    [1m Learning iteration 2877/200000 [0m

                       Computation: 3163 steps/s (collection: 0.503s, learning 2.087s)
               Value function loss: 55098.2144
                    Surrogate loss: 0.0124
             Mean action noise std: 0.89
                       Mean reward: 10445.46
               Mean episode length: 402.25
                 Mean success rate: 79.50
                  Mean reward/step: 26.68
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 23576576
                    Iteration time: 2.59s
                        Total time: 7531.31s
                               ETA: 515842.6s

################################################################################
                    [1m Learning iteration 2878/200000 [0m

                       Computation: 3183 steps/s (collection: 0.480s, learning 2.093s)
               Value function loss: 84064.8070
                    Surrogate loss: 0.0167
             Mean action noise std: 0.89
                       Mean reward: 10554.70
               Mean episode length: 406.77
                 Mean success rate: 80.50
                  Mean reward/step: 27.22
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 23584768
                    Iteration time: 2.57s
                        Total time: 7533.89s
                               ETA: 515837.0s

################################################################################
                    [1m Learning iteration 2879/200000 [0m

                       Computation: 3254 steps/s (collection: 0.446s, learning 2.072s)
               Value function loss: 106729.9789
                    Surrogate loss: 0.0123
             Mean action noise std: 0.89
                       Mean reward: 10552.31
               Mean episode length: 405.88
                 Mean success rate: 80.50
                  Mean reward/step: 26.56
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 2.52s
                        Total time: 7536.40s
                               ETA: 515827.6s

################################################################################
                    [1m Learning iteration 2880/200000 [0m

                       Computation: 3203 steps/s (collection: 0.489s, learning 2.068s)
               Value function loss: 89403.8424
                    Surrogate loss: 0.0102
             Mean action noise std: 0.89
                       Mean reward: 10552.71
               Mean episode length: 405.53
                 Mean success rate: 81.50
                  Mean reward/step: 25.22
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 23601152
                    Iteration time: 2.56s
                        Total time: 7538.96s
                               ETA: 515820.9s

################################################################################
                    [1m Learning iteration 2881/200000 [0m

                       Computation: 3226 steps/s (collection: 0.469s, learning 2.070s)
               Value function loss: 104134.1926
                    Surrogate loss: 0.0083
             Mean action noise std: 0.89
                       Mean reward: 10430.79
               Mean episode length: 404.76
                 Mean success rate: 81.50
                  Mean reward/step: 24.73
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 23609344
                    Iteration time: 2.54s
                        Total time: 7541.50s
                               ETA: 515813.0s

################################################################################
                    [1m Learning iteration 2882/200000 [0m

                       Computation: 3129 steps/s (collection: 0.470s, learning 2.147s)
               Value function loss: 95264.2051
                    Surrogate loss: 0.0112
             Mean action noise std: 0.89
                       Mean reward: 10434.99
               Mean episode length: 403.95
                 Mean success rate: 81.50
                  Mean reward/step: 24.76
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 23617536
                    Iteration time: 2.62s
                        Total time: 7544.12s
                               ETA: 515810.4s

################################################################################
                    [1m Learning iteration 2883/200000 [0m

                       Computation: 3202 steps/s (collection: 0.453s, learning 2.105s)
               Value function loss: 108445.7940
                    Surrogate loss: 0.0083
             Mean action noise std: 0.89
                       Mean reward: 10818.52
               Mean episode length: 413.77
                 Mean success rate: 84.00
                  Mean reward/step: 24.64
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 23625728
                    Iteration time: 2.56s
                        Total time: 7546.68s
                               ETA: 515803.8s

################################################################################
                    [1m Learning iteration 2884/200000 [0m

                       Computation: 3131 steps/s (collection: 0.493s, learning 2.123s)
               Value function loss: 84148.4338
                    Surrogate loss: 0.0130
             Mean action noise std: 0.89
                       Mean reward: 10775.19
               Mean episode length: 412.19
                 Mean success rate: 84.00
                  Mean reward/step: 25.69
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 23633920
                    Iteration time: 2.62s
                        Total time: 7549.29s
                               ETA: 515801.1s

################################################################################
                    [1m Learning iteration 2885/200000 [0m

                       Computation: 3257 steps/s (collection: 0.446s, learning 2.069s)
               Value function loss: 92712.7635
                    Surrogate loss: 0.0109
             Mean action noise std: 0.89
                       Mean reward: 10613.98
               Mean episode length: 408.94
                 Mean success rate: 82.50
                  Mean reward/step: 25.49
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 23642112
                    Iteration time: 2.51s
                        Total time: 7551.81s
                               ETA: 515791.5s

################################################################################
                    [1m Learning iteration 2886/200000 [0m

                       Computation: 3269 steps/s (collection: 0.444s, learning 2.062s)
               Value function loss: 123042.9291
                    Surrogate loss: 0.0139
             Mean action noise std: 0.89
                       Mean reward: 10825.12
               Mean episode length: 420.05
                 Mean success rate: 84.50
                  Mean reward/step: 25.45
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 23650304
                    Iteration time: 2.51s
                        Total time: 7554.31s
                               ETA: 515781.3s

################################################################################
                    [1m Learning iteration 2887/200000 [0m

                       Computation: 3254 steps/s (collection: 0.501s, learning 2.016s)
               Value function loss: 153101.3750
                    Surrogate loss: 0.0110
             Mean action noise std: 0.89
                       Mean reward: 10948.00
               Mean episode length: 424.15
                 Mean success rate: 85.00
                  Mean reward/step: 24.00
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 23658496
                    Iteration time: 2.52s
                        Total time: 7556.83s
                               ETA: 515771.9s

################################################################################
                    [1m Learning iteration 2888/200000 [0m

                       Computation: 3213 steps/s (collection: 0.476s, learning 2.073s)
               Value function loss: 71213.0982
                    Surrogate loss: 0.0107
             Mean action noise std: 0.89
                       Mean reward: 10984.43
               Mean episode length: 426.95
                 Mean success rate: 85.50
                  Mean reward/step: 23.40
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 23666688
                    Iteration time: 2.55s
                        Total time: 7559.38s
                               ETA: 515764.7s

################################################################################
                    [1m Learning iteration 2889/200000 [0m

                       Computation: 3243 steps/s (collection: 0.478s, learning 2.048s)
               Value function loss: 118621.2947
                    Surrogate loss: 0.0146
             Mean action noise std: 0.89
                       Mean reward: 10788.78
               Mean episode length: 419.75
                 Mean success rate: 84.00
                  Mean reward/step: 24.29
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 23674880
                    Iteration time: 2.53s
                        Total time: 7561.90s
                               ETA: 515755.9s

################################################################################
                    [1m Learning iteration 2890/200000 [0m

                       Computation: 3170 steps/s (collection: 0.514s, learning 2.070s)
               Value function loss: 94785.2616
                    Surrogate loss: 0.0128
             Mean action noise std: 0.89
                       Mean reward: 10877.75
               Mean episode length: 425.27
                 Mean success rate: 84.00
                  Mean reward/step: 25.31
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 23683072
                    Iteration time: 2.58s
                        Total time: 7564.49s
                               ETA: 515751.0s

################################################################################
                    [1m Learning iteration 2891/200000 [0m

                       Computation: 3185 steps/s (collection: 0.508s, learning 2.063s)
               Value function loss: 88871.7266
                    Surrogate loss: 0.0102
             Mean action noise std: 0.89
                       Mean reward: 10763.36
               Mean episode length: 418.94
                 Mean success rate: 83.00
                  Mean reward/step: 25.56
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 2.57s
                        Total time: 7567.06s
                               ETA: 515745.3s

################################################################################
                    [1m Learning iteration 2892/200000 [0m

                       Computation: 3153 steps/s (collection: 0.541s, learning 2.056s)
               Value function loss: 74813.8534
                    Surrogate loss: 0.0119
             Mean action noise std: 0.89
                       Mean reward: 10529.23
               Mean episode length: 414.76
                 Mean success rate: 82.00
                  Mean reward/step: 26.65
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 23699456
                    Iteration time: 2.60s
                        Total time: 7569.66s
                               ETA: 515741.4s

################################################################################
                    [1m Learning iteration 2893/200000 [0m

                       Computation: 3182 steps/s (collection: 0.500s, learning 2.074s)
               Value function loss: 56509.2301
                    Surrogate loss: 0.0111
             Mean action noise std: 0.89
                       Mean reward: 10394.73
               Mean episode length: 412.56
                 Mean success rate: 81.50
                  Mean reward/step: 27.23
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 23707648
                    Iteration time: 2.57s
                        Total time: 7572.23s
                               ETA: 515735.9s

################################################################################
                    [1m Learning iteration 2894/200000 [0m

                       Computation: 3185 steps/s (collection: 0.510s, learning 2.062s)
               Value function loss: 91776.5447
                    Surrogate loss: 0.0129
             Mean action noise std: 0.89
                       Mean reward: 10433.29
               Mean episode length: 415.15
                 Mean success rate: 81.00
                  Mean reward/step: 27.49
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 23715840
                    Iteration time: 2.57s
                        Total time: 7574.80s
                               ETA: 515730.3s

################################################################################
                    [1m Learning iteration 2895/200000 [0m

                       Computation: 3187 steps/s (collection: 0.476s, learning 2.094s)
               Value function loss: 97528.5571
                    Surrogate loss: 0.0129
             Mean action noise std: 0.89
                       Mean reward: 10661.90
               Mean episode length: 422.19
                 Mean success rate: 83.00
                  Mean reward/step: 27.21
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 23724032
                    Iteration time: 2.57s
                        Total time: 7577.37s
                               ETA: 515724.5s

################################################################################
                    [1m Learning iteration 2896/200000 [0m

                       Computation: 3189 steps/s (collection: 0.480s, learning 2.088s)
               Value function loss: 108950.2986
                    Surrogate loss: 0.0095
             Mean action noise std: 0.89
                       Mean reward: 10571.02
               Mean episode length: 417.85
                 Mean success rate: 82.50
                  Mean reward/step: 26.83
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 23732224
                    Iteration time: 2.57s
                        Total time: 7579.94s
                               ETA: 515718.6s

################################################################################
                    [1m Learning iteration 2897/200000 [0m

                       Computation: 3242 steps/s (collection: 0.464s, learning 2.063s)
               Value function loss: 90942.1401
                    Surrogate loss: 0.0087
             Mean action noise std: 0.89
                       Mean reward: 10700.73
               Mean episode length: 422.85
                 Mean success rate: 83.50
                  Mean reward/step: 26.51
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 23740416
                    Iteration time: 2.53s
                        Total time: 7582.47s
                               ETA: 515709.8s

################################################################################
                    [1m Learning iteration 2898/200000 [0m

                       Computation: 3314 steps/s (collection: 0.436s, learning 2.036s)
               Value function loss: 93055.5906
                    Surrogate loss: 0.0137
             Mean action noise std: 0.89
                       Mean reward: 10537.70
               Mean episode length: 417.92
                 Mean success rate: 82.50
                  Mean reward/step: 27.14
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 23748608
                    Iteration time: 2.47s
                        Total time: 7584.94s
                               ETA: 515697.4s

################################################################################
                    [1m Learning iteration 2899/200000 [0m

                       Computation: 3298 steps/s (collection: 0.443s, learning 2.040s)
               Value function loss: 85438.0067
                    Surrogate loss: 0.0132
             Mean action noise std: 0.89
                       Mean reward: 10581.25
               Mean episode length: 418.51
                 Mean success rate: 82.50
                  Mean reward/step: 26.88
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 23756800
                    Iteration time: 2.48s
                        Total time: 7587.42s
                               ETA: 515685.7s

################################################################################
                    [1m Learning iteration 2900/200000 [0m

                       Computation: 3264 steps/s (collection: 0.469s, learning 2.040s)
               Value function loss: 138717.1965
                    Surrogate loss: 0.0100
             Mean action noise std: 0.89
                       Mean reward: 10854.85
               Mean episode length: 428.87
                 Mean success rate: 85.00
                  Mean reward/step: 26.62
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 23764992
                    Iteration time: 2.51s
                        Total time: 7589.93s
                               ETA: 515675.8s

################################################################################
                    [1m Learning iteration 2901/200000 [0m

                       Computation: 3267 steps/s (collection: 0.433s, learning 2.074s)
               Value function loss: 62265.2225
                    Surrogate loss: 0.0110
             Mean action noise std: 0.89
                       Mean reward: 11072.25
               Mean episode length: 433.74
                 Mean success rate: 86.00
                  Mean reward/step: 25.65
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 23773184
                    Iteration time: 2.51s
                        Total time: 7592.44s
                               ETA: 515665.8s

################################################################################
                    [1m Learning iteration 2902/200000 [0m

                       Computation: 3204 steps/s (collection: 0.488s, learning 2.068s)
               Value function loss: 141223.5553
                    Surrogate loss: 0.0123
             Mean action noise std: 0.89
                       Mean reward: 11093.32
               Mean episode length: 433.61
                 Mean success rate: 86.50
                  Mean reward/step: 25.75
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 23781376
                    Iteration time: 2.56s
                        Total time: 7594.99s
                               ETA: 515659.1s

################################################################################
                    [1m Learning iteration 2903/200000 [0m

                       Computation: 3222 steps/s (collection: 0.467s, learning 2.075s)
               Value function loss: 116610.1270
                    Surrogate loss: 0.0109
             Mean action noise std: 0.89
                       Mean reward: 11385.22
               Mean episode length: 436.80
                 Mean success rate: 87.00
                  Mean reward/step: 25.37
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 2.54s
                        Total time: 7597.54s
                               ETA: 515651.4s

################################################################################
                    [1m Learning iteration 2904/200000 [0m

                       Computation: 3262 steps/s (collection: 0.459s, learning 2.052s)
               Value function loss: 75617.7617
                    Surrogate loss: 0.0150
             Mean action noise std: 0.89
                       Mean reward: 11512.06
               Mean episode length: 441.06
                 Mean success rate: 88.50
                  Mean reward/step: 25.57
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 23797760
                    Iteration time: 2.51s
                        Total time: 7600.05s
                               ETA: 515641.7s

################################################################################
                    [1m Learning iteration 2905/200000 [0m

                       Computation: 3250 steps/s (collection: 0.463s, learning 2.057s)
               Value function loss: 119806.6076
                    Surrogate loss: 0.0147
             Mean action noise std: 0.89
                       Mean reward: 11420.56
               Mean episode length: 434.42
                 Mean success rate: 87.00
                  Mean reward/step: 26.18
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 23805952
                    Iteration time: 2.52s
                        Total time: 7602.57s
                               ETA: 515632.6s

################################################################################
                    [1m Learning iteration 2906/200000 [0m

                       Computation: 3225 steps/s (collection: 0.455s, learning 2.085s)
               Value function loss: 115489.9232
                    Surrogate loss: 0.0181
             Mean action noise std: 0.89
                       Mean reward: 11390.33
               Mean episode length: 432.54
                 Mean success rate: 86.50
                  Mean reward/step: 25.65
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 23814144
                    Iteration time: 2.54s
                        Total time: 7605.11s
                               ETA: 515624.8s

################################################################################
                    [1m Learning iteration 2907/200000 [0m

                       Computation: 3230 steps/s (collection: 0.439s, learning 2.096s)
               Value function loss: 87751.9016
                    Surrogate loss: 0.0157
             Mean action noise std: 0.89
                       Mean reward: 11025.19
               Mean episode length: 418.94
                 Mean success rate: 84.00
                  Mean reward/step: 25.21
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 23822336
                    Iteration time: 2.54s
                        Total time: 7607.64s
                               ETA: 515616.7s

################################################################################
                    [1m Learning iteration 2908/200000 [0m

                       Computation: 3264 steps/s (collection: 0.456s, learning 2.053s)
               Value function loss: 79542.5431
                    Surrogate loss: 0.0128
             Mean action noise std: 0.89
                       Mean reward: 11138.73
               Mean episode length: 423.24
                 Mean success rate: 85.00
                  Mean reward/step: 25.76
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 23830528
                    Iteration time: 2.51s
                        Total time: 7610.15s
                               ETA: 515606.8s

################################################################################
                    [1m Learning iteration 2909/200000 [0m

                       Computation: 3184 steps/s (collection: 0.462s, learning 2.110s)
               Value function loss: 96689.3641
                    Surrogate loss: 0.0107
             Mean action noise std: 0.89
                       Mean reward: 11007.40
               Mean episode length: 418.14
                 Mean success rate: 84.50
                  Mean reward/step: 26.76
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 23838720
                    Iteration time: 2.57s
                        Total time: 7612.73s
                               ETA: 515601.3s

################################################################################
                    [1m Learning iteration 2910/200000 [0m

                       Computation: 3221 steps/s (collection: 0.445s, learning 2.098s)
               Value function loss: 112885.8329
                    Surrogate loss: 0.0104
             Mean action noise std: 0.89
                       Mean reward: 10178.19
               Mean episode length: 390.60
                 Mean success rate: 79.00
                  Mean reward/step: 25.90
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 23846912
                    Iteration time: 2.54s
                        Total time: 7615.27s
                               ETA: 515593.7s

################################################################################
                    [1m Learning iteration 2911/200000 [0m

                       Computation: 3220 steps/s (collection: 0.451s, learning 2.093s)
               Value function loss: 98196.3671
                    Surrogate loss: 0.0115
             Mean action noise std: 0.89
                       Mean reward: 10310.47
               Mean episode length: 394.96
                 Mean success rate: 79.50
                  Mean reward/step: 25.63
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 23855104
                    Iteration time: 2.54s
                        Total time: 7617.81s
                               ETA: 515586.2s

################################################################################
                    [1m Learning iteration 2912/200000 [0m

                       Computation: 3256 steps/s (collection: 0.447s, learning 2.068s)
               Value function loss: 113369.7065
                    Surrogate loss: 0.0111
             Mean action noise std: 0.89
                       Mean reward: 10039.68
               Mean episode length: 387.72
                 Mean success rate: 77.50
                  Mean reward/step: 25.47
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 23863296
                    Iteration time: 2.52s
                        Total time: 7620.33s
                               ETA: 515576.8s

################################################################################
                    [1m Learning iteration 2913/200000 [0m

                       Computation: 3269 steps/s (collection: 0.431s, learning 2.075s)
               Value function loss: 83340.7281
                    Surrogate loss: 0.0104
             Mean action noise std: 0.89
                       Mean reward: 9874.25
               Mean episode length: 383.94
                 Mean success rate: 77.00
                  Mean reward/step: 25.95
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 23871488
                    Iteration time: 2.51s
                        Total time: 7622.83s
                               ETA: 515566.7s

################################################################################
                    [1m Learning iteration 2914/200000 [0m

                       Computation: 3211 steps/s (collection: 0.475s, learning 2.076s)
               Value function loss: 102433.9367
                    Surrogate loss: 0.0112
             Mean action noise std: 0.88
                       Mean reward: 10013.43
               Mean episode length: 388.38
                 Mean success rate: 78.50
                  Mean reward/step: 26.14
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 23879680
                    Iteration time: 2.55s
                        Total time: 7625.38s
                               ETA: 515559.6s

################################################################################
                    [1m Learning iteration 2915/200000 [0m

                       Computation: 3226 steps/s (collection: 0.444s, learning 2.095s)
               Value function loss: 73630.8423
                    Surrogate loss: 0.0123
             Mean action noise std: 0.88
                       Mean reward: 9790.58
               Mean episode length: 383.26
                 Mean success rate: 77.00
                  Mean reward/step: 26.78
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 2.54s
                        Total time: 7627.92s
                               ETA: 515551.8s

################################################################################
                    [1m Learning iteration 2916/200000 [0m

                       Computation: 3125 steps/s (collection: 0.486s, learning 2.135s)
               Value function loss: 143631.8469
                    Surrogate loss: 0.0109
             Mean action noise std: 0.88
                       Mean reward: 10113.19
               Mean episode length: 391.43
                 Mean success rate: 78.50
                  Mean reward/step: 26.31
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 23896064
                    Iteration time: 2.62s
                        Total time: 7630.54s
                               ETA: 515549.5s

################################################################################
                    [1m Learning iteration 2917/200000 [0m

                       Computation: 3209 steps/s (collection: 0.462s, learning 2.091s)
               Value function loss: 62278.0416
                    Surrogate loss: 0.0114
             Mean action noise std: 0.88
                       Mean reward: 9955.74
               Mean episode length: 389.43
                 Mean success rate: 78.00
                  Mean reward/step: 26.26
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 23904256
                    Iteration time: 2.55s
                        Total time: 7633.10s
                               ETA: 515542.7s

################################################################################
                    [1m Learning iteration 2918/200000 [0m

                       Computation: 3203 steps/s (collection: 0.448s, learning 2.109s)
               Value function loss: 166160.0564
                    Surrogate loss: 0.0127
             Mean action noise std: 0.88
                       Mean reward: 10220.32
               Mean episode length: 397.75
                 Mean success rate: 78.50
                  Mean reward/step: 25.45
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 23912448
                    Iteration time: 2.56s
                        Total time: 7635.65s
                               ETA: 515536.1s

################################################################################
                    [1m Learning iteration 2919/200000 [0m

                       Computation: 3201 steps/s (collection: 0.479s, learning 2.080s)
               Value function loss: 80339.6771
                    Surrogate loss: 0.0123
             Mean action noise std: 0.88
                       Mean reward: 10177.16
               Mean episode length: 399.46
                 Mean success rate: 78.50
                  Mean reward/step: 24.33
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 23920640
                    Iteration time: 2.56s
                        Total time: 7638.21s
                               ETA: 515529.6s

################################################################################
                    [1m Learning iteration 2920/200000 [0m

                       Computation: 3190 steps/s (collection: 0.478s, learning 2.090s)
               Value function loss: 110848.5120
                    Surrogate loss: 0.0129
             Mean action noise std: 0.88
                       Mean reward: 9980.66
               Mean episode length: 391.54
                 Mean success rate: 76.50
                  Mean reward/step: 25.07
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 23928832
                    Iteration time: 2.57s
                        Total time: 7640.78s
                               ETA: 515523.8s

################################################################################
                    [1m Learning iteration 2921/200000 [0m

                       Computation: 3311 steps/s (collection: 0.431s, learning 2.043s)
               Value function loss: 106217.3166
                    Surrogate loss: 0.0136
             Mean action noise std: 0.88
                       Mean reward: 10087.99
               Mean episode length: 394.60
                 Mean success rate: 77.00
                  Mean reward/step: 25.38
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 23937024
                    Iteration time: 2.47s
                        Total time: 7643.25s
                               ETA: 515511.6s

################################################################################
                    [1m Learning iteration 2922/200000 [0m

                       Computation: 3276 steps/s (collection: 0.457s, learning 2.043s)
               Value function loss: 122767.4431
                    Surrogate loss: 0.0159
             Mean action noise std: 0.88
                       Mean reward: 10321.48
               Mean episode length: 399.81
                 Mean success rate: 78.00
                  Mean reward/step: 25.62
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 23945216
                    Iteration time: 2.50s
                        Total time: 7645.75s
                               ETA: 515501.2s

################################################################################
                    [1m Learning iteration 2923/200000 [0m

                       Computation: 3303 steps/s (collection: 0.445s, learning 2.035s)
               Value function loss: 91167.2108
                    Surrogate loss: 0.0133
             Mean action noise std: 0.88
                       Mean reward: 10422.16
               Mean episode length: 403.73
                 Mean success rate: 78.50
                  Mean reward/step: 24.96
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 23953408
                    Iteration time: 2.48s
                        Total time: 7648.23s
                               ETA: 515489.4s

################################################################################
                    [1m Learning iteration 2924/200000 [0m

                       Computation: 3294 steps/s (collection: 0.437s, learning 2.050s)
               Value function loss: 66793.3969
                    Surrogate loss: 0.0128
             Mean action noise std: 0.88
                       Mean reward: 10117.19
               Mean episode length: 393.06
                 Mean success rate: 76.00
                  Mean reward/step: 25.20
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 23961600
                    Iteration time: 2.49s
                        Total time: 7650.72s
                               ETA: 515478.1s

################################################################################
                    [1m Learning iteration 2925/200000 [0m

                       Computation: 3296 steps/s (collection: 0.462s, learning 2.023s)
               Value function loss: 129760.0752
                    Surrogate loss: 0.0137
             Mean action noise std: 0.88
                       Mean reward: 10094.57
               Mean episode length: 393.41
                 Mean success rate: 76.00
                  Mean reward/step: 25.41
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 23969792
                    Iteration time: 2.49s
                        Total time: 7653.21s
                               ETA: 515466.7s

################################################################################
                    [1m Learning iteration 2926/200000 [0m

                       Computation: 3346 steps/s (collection: 0.432s, learning 2.016s)
               Value function loss: 112229.0893
                    Surrogate loss: 0.0109
             Mean action noise std: 0.88
                       Mean reward: 10250.74
               Mean episode length: 395.80
                 Mean success rate: 77.00
                  Mean reward/step: 24.41
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 23977984
                    Iteration time: 2.45s
                        Total time: 7655.65s
                               ETA: 515452.8s

################################################################################
                    [1m Learning iteration 2927/200000 [0m

                       Computation: 3301 steps/s (collection: 0.428s, learning 2.053s)
               Value function loss: 98420.7174
                    Surrogate loss: 0.0104
             Mean action noise std: 0.88
                       Mean reward: 10183.96
               Mean episode length: 391.98
                 Mean success rate: 77.00
                  Mean reward/step: 24.13
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 2.48s
                        Total time: 7658.14s
                               ETA: 515441.1s

################################################################################
                    [1m Learning iteration 2928/200000 [0m

                       Computation: 3296 steps/s (collection: 0.441s, learning 2.044s)
               Value function loss: 95594.7130
                    Surrogate loss: 0.0093
             Mean action noise std: 0.88
                       Mean reward: 9964.86
               Mean episode length: 387.15
                 Mean success rate: 76.00
                  Mean reward/step: 24.17
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 23994368
                    Iteration time: 2.49s
                        Total time: 7660.62s
                               ETA: 515429.8s

################################################################################
                    [1m Learning iteration 2929/200000 [0m

                       Computation: 3212 steps/s (collection: 0.483s, learning 2.066s)
               Value function loss: 80645.8869
                    Surrogate loss: 0.0120
             Mean action noise std: 0.88
                       Mean reward: 10189.89
               Mean episode length: 396.89
                 Mean success rate: 77.50
                  Mean reward/step: 24.46
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 24002560
                    Iteration time: 2.55s
                        Total time: 7663.17s
                               ETA: 515422.7s

################################################################################
                    [1m Learning iteration 2930/200000 [0m

                       Computation: 3270 steps/s (collection: 0.443s, learning 2.061s)
               Value function loss: 74071.9190
                    Surrogate loss: 0.0111
             Mean action noise std: 0.88
                       Mean reward: 9915.33
               Mean episode length: 386.74
                 Mean success rate: 75.50
                  Mean reward/step: 25.42
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 24010752
                    Iteration time: 2.50s
                        Total time: 7665.67s
                               ETA: 515412.7s

################################################################################
                    [1m Learning iteration 2931/200000 [0m

                       Computation: 3221 steps/s (collection: 0.475s, learning 2.068s)
               Value function loss: 123479.7756
                    Surrogate loss: 0.0125
             Mean action noise std: 0.88
                       Mean reward: 9860.29
               Mean episode length: 387.58
                 Mean success rate: 75.50
                  Mean reward/step: 26.17
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 24018944
                    Iteration time: 2.54s
                        Total time: 7668.22s
                               ETA: 515405.2s

################################################################################
                    [1m Learning iteration 2932/200000 [0m

                       Computation: 3213 steps/s (collection: 0.479s, learning 2.070s)
               Value function loss: 78274.4498
                    Surrogate loss: 0.0101
             Mean action noise std: 0.88
                       Mean reward: 9501.01
               Mean episode length: 377.62
                 Mean success rate: 73.50
                  Mean reward/step: 25.28
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 24027136
                    Iteration time: 2.55s
                        Total time: 7670.77s
                               ETA: 515398.1s

################################################################################
                    [1m Learning iteration 2933/200000 [0m

                       Computation: 3368 steps/s (collection: 0.410s, learning 2.022s)
               Value function loss: 116003.8873
                    Surrogate loss: 0.0109
             Mean action noise std: 0.88
                       Mean reward: 9854.17
               Mean episode length: 388.75
                 Mean success rate: 75.50
                  Mean reward/step: 25.42
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 24035328
                    Iteration time: 2.43s
                        Total time: 7673.20s
                               ETA: 515383.2s

################################################################################
                    [1m Learning iteration 2934/200000 [0m

                       Computation: 3263 steps/s (collection: 0.460s, learning 2.051s)
               Value function loss: 113514.6470
                    Surrogate loss: 0.0096
             Mean action noise std: 0.88
                       Mean reward: 9655.00
               Mean episode length: 382.53
                 Mean success rate: 74.00
                  Mean reward/step: 25.50
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 24043520
                    Iteration time: 2.51s
                        Total time: 7675.71s
                               ETA: 515373.5s

################################################################################
                    [1m Learning iteration 2935/200000 [0m

                       Computation: 3366 steps/s (collection: 0.413s, learning 2.020s)
               Value function loss: 80198.1901
                    Surrogate loss: 0.0105
             Mean action noise std: 0.88
                       Mean reward: 9432.83
               Mean episode length: 376.76
                 Mean success rate: 73.50
                  Mean reward/step: 24.71
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 24051712
                    Iteration time: 2.43s
                        Total time: 7678.14s
                               ETA: 515358.7s

################################################################################
                    [1m Learning iteration 2936/200000 [0m

                       Computation: 3270 steps/s (collection: 0.444s, learning 2.061s)
               Value function loss: 95357.5077
                    Surrogate loss: 0.0128
             Mean action noise std: 0.88
                       Mean reward: 9487.39
               Mean episode length: 378.92
                 Mean success rate: 74.00
                  Mean reward/step: 24.29
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 24059904
                    Iteration time: 2.51s
                        Total time: 7680.65s
                               ETA: 515348.7s

################################################################################
                    [1m Learning iteration 2937/200000 [0m

                       Computation: 3300 steps/s (collection: 0.419s, learning 2.063s)
               Value function loss: 95910.5117
                    Surrogate loss: 0.0117
             Mean action noise std: 0.88
                       Mean reward: 9665.93
               Mean episode length: 382.89
                 Mean success rate: 75.00
                  Mean reward/step: 25.00
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 24068096
                    Iteration time: 2.48s
                        Total time: 7683.13s
                               ETA: 515337.2s

################################################################################
                    [1m Learning iteration 2938/200000 [0m

                       Computation: 3300 steps/s (collection: 0.429s, learning 2.053s)
               Value function loss: 102896.5132
                    Surrogate loss: 0.0113
             Mean action noise std: 0.88
                       Mean reward: 9851.89
               Mean episode length: 386.68
                 Mean success rate: 76.50
                  Mean reward/step: 25.79
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 24076288
                    Iteration time: 2.48s
                        Total time: 7685.61s
                               ETA: 515325.6s

################################################################################
                    [1m Learning iteration 2939/200000 [0m

                       Computation: 3283 steps/s (collection: 0.450s, learning 2.044s)
               Value function loss: 99681.4648
                    Surrogate loss: 0.0108
             Mean action noise std: 0.88
                       Mean reward: 10085.23
               Mean episode length: 393.11
                 Mean success rate: 79.00
                  Mean reward/step: 25.39
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 2.49s
                        Total time: 7688.11s
                               ETA: 515315.0s

################################################################################
                    [1m Learning iteration 2940/200000 [0m

                       Computation: 3277 steps/s (collection: 0.444s, learning 2.056s)
               Value function loss: 70118.3561
                    Surrogate loss: 0.0124
             Mean action noise std: 0.88
                       Mean reward: 10186.26
               Mean episode length: 397.35
                 Mean success rate: 79.50
                  Mean reward/step: 25.46
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 24092672
                    Iteration time: 2.50s
                        Total time: 7690.61s
                               ETA: 515304.6s

################################################################################
                    [1m Learning iteration 2941/200000 [0m

                       Computation: 3281 steps/s (collection: 0.419s, learning 2.078s)
               Value function loss: 124028.5170
                    Surrogate loss: 0.0126
             Mean action noise std: 0.88
                       Mean reward: 10244.47
               Mean episode length: 400.18
                 Mean success rate: 80.50
                  Mean reward/step: 25.64
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 24100864
                    Iteration time: 2.50s
                        Total time: 7693.10s
                               ETA: 515294.1s

################################################################################
                    [1m Learning iteration 2942/200000 [0m

                       Computation: 3313 steps/s (collection: 0.432s, learning 2.040s)
               Value function loss: 76436.3672
                    Surrogate loss: 0.0143
             Mean action noise std: 0.88
                       Mean reward: 10474.63
               Mean episode length: 406.64
                 Mean success rate: 82.00
                  Mean reward/step: 24.73
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 24109056
                    Iteration time: 2.47s
                        Total time: 7695.58s
                               ETA: 515281.9s

################################################################################
                    [1m Learning iteration 2943/200000 [0m

                       Computation: 3248 steps/s (collection: 0.468s, learning 2.053s)
               Value function loss: 125736.0623
                    Surrogate loss: 0.0130
             Mean action noise std: 0.88
                       Mean reward: 10372.05
               Mean episode length: 405.14
                 Mean success rate: 81.00
                  Mean reward/step: 24.15
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 24117248
                    Iteration time: 2.52s
                        Total time: 7698.10s
                               ETA: 515273.0s

################################################################################
                    [1m Learning iteration 2944/200000 [0m

                       Computation: 3285 steps/s (collection: 0.465s, learning 2.029s)
               Value function loss: 73879.0822
                    Surrogate loss: 0.0100
             Mean action noise std: 0.88
                       Mean reward: 10165.49
               Mean episode length: 399.67
                 Mean success rate: 80.50
                  Mean reward/step: 23.79
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 24125440
                    Iteration time: 2.49s
                        Total time: 7700.59s
                               ETA: 515262.3s

################################################################################
                    [1m Learning iteration 2945/200000 [0m

                       Computation: 3192 steps/s (collection: 0.440s, learning 2.126s)
               Value function loss: 78617.3666
                    Surrogate loss: 0.0134
             Mean action noise std: 0.88
                       Mean reward: 10121.34
               Mean episode length: 400.48
                 Mean success rate: 80.00
                  Mean reward/step: 24.22
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 24133632
                    Iteration time: 2.57s
                        Total time: 7703.16s
                               ETA: 515256.4s

################################################################################
                    [1m Learning iteration 2946/200000 [0m

                       Computation: 3222 steps/s (collection: 0.479s, learning 2.063s)
               Value function loss: 73416.7642
                    Surrogate loss: 0.0115
             Mean action noise std: 0.88
                       Mean reward: 9989.86
               Mean episode length: 397.15
                 Mean success rate: 78.00
                  Mean reward/step: 25.34
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 24141824
                    Iteration time: 2.54s
                        Total time: 7705.70s
                               ETA: 515248.9s

################################################################################
                    [1m Learning iteration 2947/200000 [0m

                       Computation: 3193 steps/s (collection: 0.476s, learning 2.090s)
               Value function loss: 138546.8064
                    Surrogate loss: 0.0090
             Mean action noise std: 0.88
                       Mean reward: 9988.15
               Mean episode length: 397.25
                 Mean success rate: 78.00
                  Mean reward/step: 25.52
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 24150016
                    Iteration time: 2.57s
                        Total time: 7708.26s
                               ETA: 515243.0s

################################################################################
                    [1m Learning iteration 2948/200000 [0m

                       Computation: 3209 steps/s (collection: 0.463s, learning 2.089s)
               Value function loss: 81689.9531
                    Surrogate loss: 0.0097
             Mean action noise std: 0.88
                       Mean reward: 10030.56
               Mean episode length: 400.02
                 Mean success rate: 78.50
                  Mean reward/step: 24.95
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 24158208
                    Iteration time: 2.55s
                        Total time: 7710.82s
                               ETA: 515236.2s

################################################################################
                    [1m Learning iteration 2949/200000 [0m

                       Computation: 3200 steps/s (collection: 0.449s, learning 2.110s)
               Value function loss: 137178.7045
                    Surrogate loss: 0.0110
             Mean action noise std: 0.88
                       Mean reward: 10195.06
               Mean episode length: 410.06
                 Mean success rate: 80.00
                  Mean reward/step: 24.82
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 24166400
                    Iteration time: 2.56s
                        Total time: 7713.38s
                               ETA: 515229.9s

################################################################################
                    [1m Learning iteration 2950/200000 [0m

                       Computation: 3226 steps/s (collection: 0.449s, learning 2.090s)
               Value function loss: 112416.7043
                    Surrogate loss: 0.0120
             Mean action noise std: 0.88
                       Mean reward: 10084.52
               Mean episode length: 405.38
                 Mean success rate: 79.00
                  Mean reward/step: 24.20
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 24174592
                    Iteration time: 2.54s
                        Total time: 7715.91s
                               ETA: 515222.2s

################################################################################
                    [1m Learning iteration 2951/200000 [0m

                       Computation: 3160 steps/s (collection: 0.498s, learning 2.093s)
               Value function loss: 68481.8539
                    Surrogate loss: 0.0121
             Mean action noise std: 0.88
                       Mean reward: 9714.56
               Mean episode length: 394.01
                 Mean success rate: 77.00
                  Mean reward/step: 24.49
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 2.59s
                        Total time: 7718.51s
                               ETA: 515218.1s

################################################################################
                    [1m Learning iteration 2952/200000 [0m

                       Computation: 3198 steps/s (collection: 0.474s, learning 2.087s)
               Value function loss: 78235.9542
                    Surrogate loss: 0.0087
             Mean action noise std: 0.88
                       Mean reward: 9876.31
               Mean episode length: 399.06
                 Mean success rate: 78.50
                  Mean reward/step: 25.89
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 24190976
                    Iteration time: 2.56s
                        Total time: 7721.07s
                               ETA: 515211.9s

################################################################################
                    [1m Learning iteration 2953/200000 [0m

                       Computation: 3254 steps/s (collection: 0.435s, learning 2.082s)
               Value function loss: 101279.6400
                    Surrogate loss: 0.0105
             Mean action noise std: 0.88
                       Mean reward: 10120.22
               Mean episode length: 404.79
                 Mean success rate: 80.50
                  Mean reward/step: 26.76
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 24199168
                    Iteration time: 2.52s
                        Total time: 7723.58s
                               ETA: 515202.8s

################################################################################
                    [1m Learning iteration 2954/200000 [0m

                       Computation: 3153 steps/s (collection: 0.482s, learning 2.115s)
               Value function loss: 82201.5516
                    Surrogate loss: 0.0118
             Mean action noise std: 0.88
                       Mean reward: 10140.59
               Mean episode length: 405.10
                 Mean success rate: 80.50
                  Mean reward/step: 26.67
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 24207360
                    Iteration time: 2.60s
                        Total time: 7726.18s
                               ETA: 515199.0s

################################################################################
                    [1m Learning iteration 2955/200000 [0m

                       Computation: 3205 steps/s (collection: 0.461s, learning 2.094s)
               Value function loss: 84204.5554
                    Surrogate loss: 0.0115
             Mean action noise std: 0.88
                       Mean reward: 10222.00
               Mean episode length: 406.00
                 Mean success rate: 80.50
                  Mean reward/step: 26.48
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 24215552
                    Iteration time: 2.56s
                        Total time: 7728.74s
                               ETA: 515192.5s

################################################################################
                    [1m Learning iteration 2956/200000 [0m

                       Computation: 3197 steps/s (collection: 0.488s, learning 2.073s)
               Value function loss: 85678.1418
                    Surrogate loss: 0.0125
             Mean action noise std: 0.88
                       Mean reward: 10541.31
               Mean episode length: 415.77
                 Mean success rate: 83.00
                  Mean reward/step: 26.04
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 24223744
                    Iteration time: 2.56s
                        Total time: 7731.30s
                               ETA: 515186.3s

################################################################################
                    [1m Learning iteration 2957/200000 [0m

                       Computation: 3207 steps/s (collection: 0.472s, learning 2.083s)
               Value function loss: 128284.6716
                    Surrogate loss: 0.0125
             Mean action noise std: 0.88
                       Mean reward: 10151.43
               Mean episode length: 409.44
                 Mean success rate: 81.00
                  Mean reward/step: 25.71
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 24231936
                    Iteration time: 2.55s
                        Total time: 7733.85s
                               ETA: 515179.7s

################################################################################
                    [1m Learning iteration 2958/200000 [0m

                       Computation: 3115 steps/s (collection: 0.511s, learning 2.118s)
               Value function loss: 84766.5830
                    Surrogate loss: 0.0106
             Mean action noise std: 0.88
                       Mean reward: 10172.85
               Mean episode length: 410.54
                 Mean success rate: 81.50
                  Mean reward/step: 25.22
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 24240128
                    Iteration time: 2.63s
                        Total time: 7736.48s
                               ETA: 515178.1s

################################################################################
                    [1m Learning iteration 2959/200000 [0m

                       Computation: 3086 steps/s (collection: 0.518s, learning 2.137s)
               Value function loss: 129246.3968
                    Surrogate loss: 0.0132
             Mean action noise std: 0.88
                       Mean reward: 9980.81
               Mean episode length: 401.40
                 Mean success rate: 79.50
                  Mean reward/step: 24.91
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 24248320
                    Iteration time: 2.65s
                        Total time: 7739.14s
                               ETA: 515178.1s

################################################################################
                    [1m Learning iteration 2960/200000 [0m

                       Computation: 3181 steps/s (collection: 0.468s, learning 2.107s)
               Value function loss: 68861.1915
                    Surrogate loss: 0.0127
             Mean action noise std: 0.88
                       Mean reward: 10129.19
               Mean episode length: 405.29
                 Mean success rate: 80.50
                  Mean reward/step: 25.19
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 24256512
                    Iteration time: 2.57s
                        Total time: 7741.71s
                               ETA: 515172.9s

################################################################################
                    [1m Learning iteration 2961/200000 [0m

                       Computation: 3203 steps/s (collection: 0.453s, learning 2.105s)
               Value function loss: 72953.5762
                    Surrogate loss: 0.0128
             Mean action noise std: 0.88
                       Mean reward: 10385.42
               Mean episode length: 414.64
                 Mean success rate: 82.00
                  Mean reward/step: 25.72
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 24264704
                    Iteration time: 2.56s
                        Total time: 7744.27s
                               ETA: 515166.4s

################################################################################
                    [1m Learning iteration 2962/200000 [0m

                       Computation: 3145 steps/s (collection: 0.483s, learning 2.121s)
               Value function loss: 49253.2688
                    Surrogate loss: 0.0105
             Mean action noise std: 0.88
                       Mean reward: 10544.91
               Mean episode length: 418.65
                 Mean success rate: 83.00
                  Mean reward/step: 25.92
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 24272896
                    Iteration time: 2.60s
                        Total time: 7746.87s
                               ETA: 515163.1s

################################################################################
                    [1m Learning iteration 2963/200000 [0m

                       Computation: 3210 steps/s (collection: 0.472s, learning 2.079s)
               Value function loss: 125709.4580
                    Surrogate loss: 0.0121
             Mean action noise std: 0.88
                       Mean reward: 10812.35
               Mean episode length: 425.51
                 Mean success rate: 84.50
                  Mean reward/step: 26.09
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 2.55s
                        Total time: 7749.42s
                               ETA: 515156.4s

################################################################################
                    [1m Learning iteration 2964/200000 [0m

                       Computation: 3177 steps/s (collection: 0.472s, learning 2.106s)
               Value function loss: 78044.0005
                    Surrogate loss: 0.0122
             Mean action noise std: 0.88
                       Mean reward: 10597.55
               Mean episode length: 418.57
                 Mean success rate: 82.50
                  Mean reward/step: 25.80
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 24289280
                    Iteration time: 2.58s
                        Total time: 7752.00s
                               ETA: 515151.3s

################################################################################
                    [1m Learning iteration 2965/200000 [0m

                       Computation: 3148 steps/s (collection: 0.508s, learning 2.094s)
               Value function loss: 135428.2632
                    Surrogate loss: 0.0113
             Mean action noise std: 0.88
                       Mean reward: 10645.07
               Mean episode length: 418.06
                 Mean success rate: 82.50
                  Mean reward/step: 25.11
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 24297472
                    Iteration time: 2.60s
                        Total time: 7754.60s
                               ETA: 515147.9s

################################################################################
                    [1m Learning iteration 2966/200000 [0m

                       Computation: 3162 steps/s (collection: 0.460s, learning 2.130s)
               Value function loss: 83030.5130
                    Surrogate loss: 0.0123
             Mean action noise std: 0.88
                       Mean reward: 10689.44
               Mean episode length: 420.18
                 Mean success rate: 83.00
                  Mean reward/step: 24.54
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 24305664
                    Iteration time: 2.59s
                        Total time: 7757.20s
                               ETA: 515143.6s

################################################################################
                    [1m Learning iteration 2967/200000 [0m

                       Computation: 3162 steps/s (collection: 0.477s, learning 2.113s)
               Value function loss: 99239.5553
                    Surrogate loss: 0.0108
             Mean action noise std: 0.88
                       Mean reward: 10923.19
               Mean episode length: 424.16
                 Mean success rate: 84.00
                  Mean reward/step: 25.55
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 24313856
                    Iteration time: 2.59s
                        Total time: 7759.79s
                               ETA: 515139.5s

################################################################################
                    [1m Learning iteration 2968/200000 [0m

                       Computation: 3268 steps/s (collection: 0.466s, learning 2.040s)
               Value function loss: 71299.2346
                    Surrogate loss: 0.0161
             Mean action noise std: 0.88
                       Mean reward: 10727.25
               Mean episode length: 416.76
                 Mean success rate: 83.00
                  Mean reward/step: 25.97
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 24322048
                    Iteration time: 2.51s
                        Total time: 7762.29s
                               ETA: 515129.7s

################################################################################
                    [1m Learning iteration 2969/200000 [0m

                       Computation: 3258 steps/s (collection: 0.463s, learning 2.050s)
               Value function loss: 105226.5614
                    Surrogate loss: 0.0124
             Mean action noise std: 0.88
                       Mean reward: 11125.86
               Mean episode length: 430.92
                 Mean success rate: 85.50
                  Mean reward/step: 26.31
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 24330240
                    Iteration time: 2.51s
                        Total time: 7764.81s
                               ETA: 515120.4s

################################################################################
                    [1m Learning iteration 2970/200000 [0m

                       Computation: 3291 steps/s (collection: 0.444s, learning 2.045s)
               Value function loss: 105760.0708
                    Surrogate loss: 0.0139
             Mean action noise std: 0.88
                       Mean reward: 10942.79
               Mean episode length: 422.75
                 Mean success rate: 83.50
                  Mean reward/step: 26.12
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 24338432
                    Iteration time: 2.49s
                        Total time: 7767.29s
                               ETA: 515109.4s

################################################################################
                    [1m Learning iteration 2971/200000 [0m

                       Computation: 3253 steps/s (collection: 0.460s, learning 2.058s)
               Value function loss: 58035.6595
                    Surrogate loss: 0.0155
             Mean action noise std: 0.88
                       Mean reward: 11008.33
               Mean episode length: 424.68
                 Mean success rate: 84.00
                  Mean reward/step: 25.64
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 24346624
                    Iteration time: 2.52s
                        Total time: 7769.81s
                               ETA: 515100.4s

################################################################################
                    [1m Learning iteration 2972/200000 [0m

                       Computation: 3243 steps/s (collection: 0.465s, learning 2.060s)
               Value function loss: 118903.5217
                    Surrogate loss: 0.0176
             Mean action noise std: 0.88
                       Mean reward: 11029.70
               Mean episode length: 426.65
                 Mean success rate: 84.50
                  Mean reward/step: 26.10
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 24354816
                    Iteration time: 2.53s
                        Total time: 7772.34s
                               ETA: 515091.9s

################################################################################
                    [1m Learning iteration 2973/200000 [0m

                       Computation: 3210 steps/s (collection: 0.442s, learning 2.110s)
               Value function loss: 88888.9178
                    Surrogate loss: 0.0139
             Mean action noise std: 0.88
                       Mean reward: 10738.67
               Mean episode length: 419.83
                 Mean success rate: 84.00
                  Mean reward/step: 25.91
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 24363008
                    Iteration time: 2.55s
                        Total time: 7774.89s
                               ETA: 515085.1s

################################################################################
                    [1m Learning iteration 2974/200000 [0m

                       Computation: 3300 steps/s (collection: 0.456s, learning 2.027s)
               Value function loss: 122035.9029
                    Surrogate loss: 0.0117
             Mean action noise std: 0.88
                       Mean reward: 10977.40
               Mean episode length: 429.40
                 Mean success rate: 86.50
                  Mean reward/step: 25.64
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 24371200
                    Iteration time: 2.48s
                        Total time: 7777.37s
                               ETA: 515073.8s

################################################################################
                    [1m Learning iteration 2975/200000 [0m

                       Computation: 3191 steps/s (collection: 0.491s, learning 2.075s)
               Value function loss: 80092.0992
                    Surrogate loss: 0.0131
             Mean action noise std: 0.88
                       Mean reward: 10984.98
               Mean episode length: 435.29
                 Mean success rate: 88.00
                  Mean reward/step: 25.54
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 2.57s
                        Total time: 7779.94s
                               ETA: 515068.0s

################################################################################
                    [1m Learning iteration 2976/200000 [0m

                       Computation: 3262 steps/s (collection: 0.451s, learning 2.060s)
               Value function loss: 67788.6948
                    Surrogate loss: 0.0154
             Mean action noise std: 0.88
                       Mean reward: 11263.03
               Mean episode length: 443.24
                 Mean success rate: 90.00
                  Mean reward/step: 25.69
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 24387584
                    Iteration time: 2.51s
                        Total time: 7782.45s
                               ETA: 515058.6s

################################################################################
                    [1m Learning iteration 2977/200000 [0m

                       Computation: 3243 steps/s (collection: 0.484s, learning 2.041s)
               Value function loss: 82384.5463
                    Surrogate loss: 0.0110
             Mean action noise std: 0.88
                       Mean reward: 11175.81
               Mean episode length: 440.24
                 Mean success rate: 90.00
                  Mean reward/step: 26.11
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 24395776
                    Iteration time: 2.53s
                        Total time: 7784.98s
                               ETA: 515050.1s

################################################################################
                    [1m Learning iteration 2978/200000 [0m

                       Computation: 3195 steps/s (collection: 0.494s, learning 2.070s)
               Value function loss: 74619.1823
                    Surrogate loss: 0.0115
             Mean action noise std: 0.88
                       Mean reward: 11165.67
               Mean episode length: 438.38
                 Mean success rate: 90.00
                  Mean reward/step: 26.59
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 24403968
                    Iteration time: 2.56s
                        Total time: 7787.54s
                               ETA: 515044.1s

################################################################################
                    [1m Learning iteration 2979/200000 [0m

                       Computation: 3223 steps/s (collection: 0.475s, learning 2.066s)
               Value function loss: 95267.3403
                    Surrogate loss: 0.0113
             Mean action noise std: 0.88
                       Mean reward: 11108.11
               Mean episode length: 437.31
                 Mean success rate: 89.00
                  Mean reward/step: 27.03
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 24412160
                    Iteration time: 2.54s
                        Total time: 7790.08s
                               ETA: 515036.7s

################################################################################
                    [1m Learning iteration 2980/200000 [0m

                       Computation: 3140 steps/s (collection: 0.479s, learning 2.130s)
               Value function loss: 142386.1041
                    Surrogate loss: 0.0119
             Mean action noise std: 0.88
                       Mean reward: 11196.40
               Mean episode length: 441.57
                 Mean success rate: 90.00
                  Mean reward/step: 26.50
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 24420352
                    Iteration time: 2.61s
                        Total time: 7792.69s
                               ETA: 515033.8s

################################################################################
                    [1m Learning iteration 2981/200000 [0m

                       Computation: 3212 steps/s (collection: 0.458s, learning 2.092s)
               Value function loss: 98661.4326
                    Surrogate loss: 0.0149
             Mean action noise std: 0.88
                       Mean reward: 11013.36
               Mean episode length: 437.38
                 Mean success rate: 89.00
                  Mean reward/step: 25.86
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 24428544
                    Iteration time: 2.55s
                        Total time: 7795.24s
                               ETA: 515026.9s

################################################################################
                    [1m Learning iteration 2982/200000 [0m

                       Computation: 3177 steps/s (collection: 0.465s, learning 2.113s)
               Value function loss: 107152.0451
                    Surrogate loss: 0.0143
             Mean action noise std: 0.88
                       Mean reward: 11064.94
               Mean episode length: 432.70
                 Mean success rate: 88.00
                  Mean reward/step: 25.41
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 24436736
                    Iteration time: 2.58s
                        Total time: 7797.82s
                               ETA: 515021.9s

################################################################################
                    [1m Learning iteration 2983/200000 [0m

                       Computation: 3126 steps/s (collection: 0.518s, learning 2.102s)
               Value function loss: 103343.1342
                    Surrogate loss: 0.0101
             Mean action noise std: 0.88
                       Mean reward: 10885.13
               Mean episode length: 425.69
                 Mean success rate: 86.50
                  Mean reward/step: 25.67
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 24444928
                    Iteration time: 2.62s
                        Total time: 7800.44s
                               ETA: 515019.7s

################################################################################
                    [1m Learning iteration 2984/200000 [0m

                       Computation: 3172 steps/s (collection: 0.462s, learning 2.120s)
               Value function loss: 90887.6491
                    Surrogate loss: 0.0116
             Mean action noise std: 0.88
                       Mean reward: 10846.33
               Mean episode length: 422.85
                 Mean success rate: 85.50
                  Mean reward/step: 26.42
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 24453120
                    Iteration time: 2.58s
                        Total time: 7803.02s
                               ETA: 515015.0s

################################################################################
                    [1m Learning iteration 2985/200000 [0m

                       Computation: 3194 steps/s (collection: 0.463s, learning 2.101s)
               Value function loss: 115383.6025
                    Surrogate loss: 0.0136
             Mean action noise std: 0.88
                       Mean reward: 10970.89
               Mean episode length: 425.94
                 Mean success rate: 86.00
                  Mean reward/step: 26.79
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 24461312
                    Iteration time: 2.56s
                        Total time: 7805.58s
                               ETA: 515009.1s

################################################################################
                    [1m Learning iteration 2986/200000 [0m

                       Computation: 3183 steps/s (collection: 0.462s, learning 2.111s)
               Value function loss: 91796.8553
                    Surrogate loss: 0.0142
             Mean action noise std: 0.88
                       Mean reward: 11092.45
               Mean episode length: 425.87
                 Mean success rate: 85.50
                  Mean reward/step: 26.38
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 24469504
                    Iteration time: 2.57s
                        Total time: 7808.16s
                               ETA: 515003.8s

################################################################################
                    [1m Learning iteration 2987/200000 [0m

                       Computation: 3220 steps/s (collection: 0.450s, learning 2.094s)
               Value function loss: 72243.0981
                    Surrogate loss: 0.0132
             Mean action noise std: 0.88
                       Mean reward: 11256.25
               Mean episode length: 429.02
                 Mean success rate: 86.00
                  Mean reward/step: 26.25
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 2.54s
                        Total time: 7810.70s
                               ETA: 514996.5s

################################################################################
                    [1m Learning iteration 2988/200000 [0m

                       Computation: 3153 steps/s (collection: 0.469s, learning 2.129s)
               Value function loss: 144537.6494
                    Surrogate loss: 0.0107
             Mean action noise std: 0.88
                       Mean reward: 11014.11
               Mean episode length: 423.22
                 Mean success rate: 85.00
                  Mean reward/step: 26.48
       Mean episode length/episode: 28.54
--------------------------------------------------------------------------------
                   Total timesteps: 24485888
                    Iteration time: 2.60s
                        Total time: 7813.30s
                               ETA: 514992.8s

################################################################################
                    [1m Learning iteration 2989/200000 [0m

                       Computation: 3211 steps/s (collection: 0.456s, learning 2.095s)
               Value function loss: 101481.8822
                    Surrogate loss: 0.0109
             Mean action noise std: 0.88
                       Mean reward: 11271.63
               Mean episode length: 431.32
                 Mean success rate: 87.00
                  Mean reward/step: 26.25
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 24494080
                    Iteration time: 2.55s
                        Total time: 7815.85s
                               ETA: 514986.1s

################################################################################
                    [1m Learning iteration 2990/200000 [0m

                       Computation: 3155 steps/s (collection: 0.466s, learning 2.130s)
               Value function loss: 112566.2979
                    Surrogate loss: 0.0142
             Mean action noise std: 0.88
                       Mean reward: 11209.92
               Mean episode length: 427.86
                 Mean success rate: 86.50
                  Mean reward/step: 25.48
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 24502272
                    Iteration time: 2.60s
                        Total time: 7818.45s
                               ETA: 514982.2s

################################################################################
                    [1m Learning iteration 2991/200000 [0m

                       Computation: 3140 steps/s (collection: 0.499s, learning 2.109s)
               Value function loss: 90795.2173
                    Surrogate loss: 0.0137
             Mean action noise std: 0.88
                       Mean reward: 11316.94
               Mean episode length: 432.35
                 Mean success rate: 87.50
                  Mean reward/step: 25.03
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 24510464
                    Iteration time: 2.61s
                        Total time: 7821.05s
                               ETA: 514979.2s

################################################################################
                    [1m Learning iteration 2992/200000 [0m

                       Computation: 3244 steps/s (collection: 0.453s, learning 2.072s)
               Value function loss: 63926.9710
                    Surrogate loss: 0.0134
             Mean action noise std: 0.88
                       Mean reward: 11289.88
               Mean episode length: 430.67
                 Mean success rate: 87.50
                  Mean reward/step: 25.27
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 24518656
                    Iteration time: 2.52s
                        Total time: 7823.58s
                               ETA: 514970.8s

################################################################################
                    [1m Learning iteration 2993/200000 [0m

                       Computation: 3237 steps/s (collection: 0.445s, learning 2.086s)
               Value function loss: 88650.0237
                    Surrogate loss: 0.0102
             Mean action noise std: 0.88
                       Mean reward: 11260.19
               Mean episode length: 431.61
                 Mean success rate: 87.50
                  Mean reward/step: 25.80
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 24526848
                    Iteration time: 2.53s
                        Total time: 7826.11s
                               ETA: 514962.7s

################################################################################
                    [1m Learning iteration 2994/200000 [0m

                       Computation: 3136 steps/s (collection: 0.500s, learning 2.111s)
               Value function loss: 113916.0941
                    Surrogate loss: 0.0142
             Mean action noise std: 0.88
                       Mean reward: 11429.47
               Mean episode length: 438.69
                 Mean success rate: 88.50
                  Mean reward/step: 25.99
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 24535040
                    Iteration time: 2.61s
                        Total time: 7828.72s
                               ETA: 514959.9s

################################################################################
                    [1m Learning iteration 2995/200000 [0m

                       Computation: 3253 steps/s (collection: 0.440s, learning 2.078s)
               Value function loss: 91970.6150
                    Surrogate loss: 0.0123
             Mean action noise std: 0.88
                       Mean reward: 11475.48
               Mean episode length: 439.39
                 Mean success rate: 88.50
                  Mean reward/step: 25.76
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 24543232
                    Iteration time: 2.52s
                        Total time: 7831.24s
                               ETA: 514951.0s

################################################################################
                    [1m Learning iteration 2996/200000 [0m

                       Computation: 3174 steps/s (collection: 0.485s, learning 2.095s)
               Value function loss: 137855.1299
                    Surrogate loss: 0.0106
             Mean action noise std: 0.88
                       Mean reward: 11259.21
               Mean episode length: 431.55
                 Mean success rate: 87.00
                  Mean reward/step: 25.64
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 24551424
                    Iteration time: 2.58s
                        Total time: 7833.82s
                               ETA: 514946.2s

################################################################################
                    [1m Learning iteration 2997/200000 [0m

                       Computation: 3180 steps/s (collection: 0.500s, learning 2.076s)
               Value function loss: 92406.5802
                    Surrogate loss: 0.0109
             Mean action noise std: 0.88
                       Mean reward: 11093.48
               Mean episode length: 427.51
                 Mean success rate: 86.50
                  Mean reward/step: 25.33
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 24559616
                    Iteration time: 2.58s
                        Total time: 7836.40s
                               ETA: 514941.1s

################################################################################
                    [1m Learning iteration 2998/200000 [0m

                       Computation: 3156 steps/s (collection: 0.473s, learning 2.123s)
               Value function loss: 108763.0913
                    Surrogate loss: 0.0107
             Mean action noise std: 0.88
                       Mean reward: 11261.82
               Mean episode length: 431.23
                 Mean success rate: 87.00
                  Mean reward/step: 25.84
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 24567808
                    Iteration time: 2.60s
                        Total time: 7838.99s
                               ETA: 514937.2s

################################################################################
                    [1m Learning iteration 2999/200000 [0m

                       Computation: 3104 steps/s (collection: 0.505s, learning 2.134s)
               Value function loss: 88444.1413
                    Surrogate loss: 0.0119
             Mean action noise std: 0.88
                       Mean reward: 11561.54
               Mean episode length: 439.65
                 Mean success rate: 88.50
                  Mean reward/step: 26.69
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 2.64s
                        Total time: 7841.63s
                               ETA: 514936.2s

################################################################################
                    [1m Learning iteration 3000/200000 [0m

                       Computation: 3193 steps/s (collection: 0.468s, learning 2.098s)
               Value function loss: 83961.9177
                    Surrogate loss: 0.0084
             Mean action noise std: 0.88
                       Mean reward: 11729.05
               Mean episode length: 443.92
                 Mean success rate: 89.50
                  Mean reward/step: 27.14
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 24584192
                    Iteration time: 2.57s
                        Total time: 7844.19s
                               ETA: 514930.4s

################################################################################
                    [1m Learning iteration 3001/200000 [0m

                       Computation: 3174 steps/s (collection: 0.489s, learning 2.092s)
               Value function loss: 89766.1328
                    Surrogate loss: 0.0084
             Mean action noise std: 0.88
                       Mean reward: 11845.61
               Mean episode length: 447.38
                 Mean success rate: 90.00
                  Mean reward/step: 27.15
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 24592384
                    Iteration time: 2.58s
                        Total time: 7846.77s
                               ETA: 514925.6s

################################################################################
                    [1m Learning iteration 3002/200000 [0m

                       Computation: 3188 steps/s (collection: 0.453s, learning 2.116s)
               Value function loss: 68950.3736
                    Surrogate loss: 0.0091
             Mean action noise std: 0.88
                       Mean reward: 11736.40
               Mean episode length: 443.26
                 Mean success rate: 89.00
                  Mean reward/step: 27.36
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 24600576
                    Iteration time: 2.57s
                        Total time: 7849.34s
                               ETA: 514920.1s

################################################################################
                    [1m Learning iteration 3003/200000 [0m

                       Computation: 3198 steps/s (collection: 0.454s, learning 2.107s)
               Value function loss: 87181.9708
                    Surrogate loss: 0.0125
             Mean action noise std: 0.88
                       Mean reward: 11801.12
               Mean episode length: 446.11
                 Mean success rate: 89.00
                  Mean reward/step: 27.93
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 24608768
                    Iteration time: 2.56s
                        Total time: 7851.90s
                               ETA: 514914.0s

################################################################################
                    [1m Learning iteration 3004/200000 [0m

                       Computation: 3152 steps/s (collection: 0.488s, learning 2.111s)
               Value function loss: 121972.1735
                    Surrogate loss: 0.0139
             Mean action noise std: 0.88
                       Mean reward: 11805.44
               Mean episode length: 447.94
                 Mean success rate: 89.50
                  Mean reward/step: 26.89
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 24616960
                    Iteration time: 2.60s
                        Total time: 7854.50s
                               ETA: 514910.4s

################################################################################
                    [1m Learning iteration 3005/200000 [0m

                       Computation: 3180 steps/s (collection: 0.488s, learning 2.088s)
               Value function loss: 126384.9676
                    Surrogate loss: 0.0115
             Mean action noise std: 0.88
                       Mean reward: 11895.02
               Mean episode length: 452.43
                 Mean success rate: 90.50
                  Mean reward/step: 25.99
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 24625152
                    Iteration time: 2.58s
                        Total time: 7857.08s
                               ETA: 514905.3s

################################################################################
                    [1m Learning iteration 3006/200000 [0m

                       Computation: 3148 steps/s (collection: 0.472s, learning 2.130s)
               Value function loss: 82264.0477
                    Surrogate loss: 0.0120
             Mean action noise std: 0.88
                       Mean reward: 11729.76
               Mean episode length: 447.75
                 Mean success rate: 89.50
                  Mean reward/step: 26.06
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 24633344
                    Iteration time: 2.60s
                        Total time: 7859.68s
                               ETA: 514901.9s

################################################################################
                    [1m Learning iteration 3007/200000 [0m

                       Computation: 3184 steps/s (collection: 0.458s, learning 2.114s)
               Value function loss: 79320.6549
                    Surrogate loss: 0.0090
             Mean action noise std: 0.88
                       Mean reward: 11746.63
               Mean episode length: 448.65
                 Mean success rate: 89.50
                  Mean reward/step: 27.20
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 24641536
                    Iteration time: 2.57s
                        Total time: 7862.25s
                               ETA: 514896.6s

################################################################################
                    [1m Learning iteration 3008/200000 [0m

                       Computation: 3210 steps/s (collection: 0.474s, learning 2.078s)
               Value function loss: 84593.9963
                    Surrogate loss: 0.0093
             Mean action noise std: 0.88
                       Mean reward: 11915.40
               Mean episode length: 453.62
                 Mean success rate: 90.50
                  Mean reward/step: 27.19
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 24649728
                    Iteration time: 2.55s
                        Total time: 7864.81s
                               ETA: 514889.9s

################################################################################
                    [1m Learning iteration 3009/200000 [0m

                       Computation: 3189 steps/s (collection: 0.478s, learning 2.090s)
               Value function loss: 79257.0669
                    Surrogate loss: 0.0108
             Mean action noise std: 0.88
                       Mean reward: 11561.91
               Mean episode length: 443.18
                 Mean success rate: 88.50
                  Mean reward/step: 27.23
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 24657920
                    Iteration time: 2.57s
                        Total time: 7867.37s
                               ETA: 514884.3s

################################################################################
                    [1m Learning iteration 3010/200000 [0m

                       Computation: 3211 steps/s (collection: 0.462s, learning 2.089s)
               Value function loss: 116929.6451
                    Surrogate loss: 0.0098
             Mean action noise std: 0.88
                       Mean reward: 11548.18
               Mean episode length: 443.77
                 Mean success rate: 88.50
                  Mean reward/step: 26.66
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 24666112
                    Iteration time: 2.55s
                        Total time: 7869.92s
                               ETA: 514877.6s

################################################################################
                    [1m Learning iteration 3011/200000 [0m

                       Computation: 3203 steps/s (collection: 0.452s, learning 2.105s)
               Value function loss: 126545.3334
                    Surrogate loss: 0.0143
             Mean action noise std: 0.88
                       Mean reward: 11171.94
               Mean episode length: 431.92
                 Mean success rate: 86.00
                  Mean reward/step: 26.26
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 2.56s
                        Total time: 7872.48s
                               ETA: 514871.3s

################################################################################
                    [1m Learning iteration 3012/200000 [0m

                       Computation: 3143 steps/s (collection: 0.481s, learning 2.125s)
               Value function loss: 109790.2251
                    Surrogate loss: 0.0108
             Mean action noise std: 0.88
                       Mean reward: 11193.91
               Mean episode length: 433.10
                 Mean success rate: 86.00
                  Mean reward/step: 25.41
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 24682496
                    Iteration time: 2.61s
                        Total time: 7875.09s
                               ETA: 514868.1s

################################################################################
                    [1m Learning iteration 3013/200000 [0m

                       Computation: 3207 steps/s (collection: 0.449s, learning 2.105s)
               Value function loss: 82559.9441
                    Surrogate loss: 0.0121
             Mean action noise std: 0.88
                       Mean reward: 11373.42
               Mean episode length: 437.44
                 Mean success rate: 87.00
                  Mean reward/step: 25.31
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 24690688
                    Iteration time: 2.55s
                        Total time: 7877.64s
                               ETA: 514861.6s

################################################################################
                    [1m Learning iteration 3014/200000 [0m

                       Computation: 3256 steps/s (collection: 0.449s, learning 2.067s)
               Value function loss: 112065.4375
                    Surrogate loss: 0.0092
             Mean action noise std: 0.88
                       Mean reward: 11254.63
               Mean episode length: 431.12
                 Mean success rate: 86.00
                  Mean reward/step: 25.72
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 24698880
                    Iteration time: 2.52s
                        Total time: 7880.16s
                               ETA: 514852.6s

################################################################################
                    [1m Learning iteration 3015/200000 [0m

                       Computation: 3169 steps/s (collection: 0.469s, learning 2.116s)
               Value function loss: 88274.8157
                    Surrogate loss: 0.0124
             Mean action noise std: 0.88
                       Mean reward: 11484.83
               Mean episode length: 434.35
                 Mean success rate: 86.50
                  Mean reward/step: 25.84
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 24707072
                    Iteration time: 2.58s
                        Total time: 7882.74s
                               ETA: 514848.1s

################################################################################
                    [1m Learning iteration 3016/200000 [0m

                       Computation: 3203 steps/s (collection: 0.462s, learning 2.095s)
               Value function loss: 86194.8384
                    Surrogate loss: 0.0118
             Mean action noise std: 0.88
                       Mean reward: 11299.70
               Mean episode length: 425.75
                 Mean success rate: 85.50
                  Mean reward/step: 25.82
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 24715264
                    Iteration time: 2.56s
                        Total time: 7885.30s
                               ETA: 514841.8s

################################################################################
                    [1m Learning iteration 3017/200000 [0m

                       Computation: 3188 steps/s (collection: 0.457s, learning 2.113s)
               Value function loss: 95215.8109
                    Surrogate loss: 0.0110
             Mean action noise std: 0.88
                       Mean reward: 11664.15
               Mean episode length: 437.39
                 Mean success rate: 88.00
                  Mean reward/step: 26.42
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 24723456
                    Iteration time: 2.57s
                        Total time: 7887.87s
                               ETA: 514836.3s

################################################################################
                    [1m Learning iteration 3018/200000 [0m

                       Computation: 3178 steps/s (collection: 0.474s, learning 2.104s)
               Value function loss: 50371.8871
                    Surrogate loss: 0.0132
             Mean action noise std: 0.88
                       Mean reward: 11813.39
               Mean episode length: 440.38
                 Mean success rate: 88.50
                  Mean reward/step: 26.99
       Mean episode length/episode: 31.03
--------------------------------------------------------------------------------
                   Total timesteps: 24731648
                    Iteration time: 2.58s
                        Total time: 7890.45s
                               ETA: 514831.3s

################################################################################
                    [1m Learning iteration 3019/200000 [0m

                       Computation: 3170 steps/s (collection: 0.462s, learning 2.122s)
               Value function loss: 176536.3539
                    Surrogate loss: 0.0146
             Mean action noise std: 0.88
                       Mean reward: 11778.65
               Mean episode length: 440.05
                 Mean success rate: 88.50
                  Mean reward/step: 27.33
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 24739840
                    Iteration time: 2.58s
                        Total time: 7893.03s
                               ETA: 514826.8s

################################################################################
                    [1m Learning iteration 3020/200000 [0m

                       Computation: 3128 steps/s (collection: 0.500s, learning 2.119s)
               Value function loss: 74950.5795
                    Surrogate loss: 0.0125
             Mean action noise std: 0.88
                       Mean reward: 11885.07
               Mean episode length: 442.55
                 Mean success rate: 89.00
                  Mean reward/step: 25.90
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 24748032
                    Iteration time: 2.62s
                        Total time: 7895.65s
                               ETA: 514824.5s

################################################################################
                    [1m Learning iteration 3021/200000 [0m

                       Computation: 3163 steps/s (collection: 0.445s, learning 2.145s)
               Value function loss: 136531.1428
                    Surrogate loss: 0.0134
             Mean action noise std: 0.88
                       Mean reward: 12112.76
               Mean episode length: 449.74
                 Mean success rate: 90.00
                  Mean reward/step: 24.97
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 24756224
                    Iteration time: 2.59s
                        Total time: 7898.24s
                               ETA: 514820.3s

################################################################################
                    [1m Learning iteration 3022/200000 [0m

                       Computation: 3284 steps/s (collection: 0.424s, learning 2.070s)
               Value function loss: 75113.4876
                    Surrogate loss: 0.0152
             Mean action noise std: 0.88
                       Mean reward: 11871.60
               Mean episode length: 445.46
                 Mean success rate: 89.50
                  Mean reward/step: 25.39
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 24764416
                    Iteration time: 2.49s
                        Total time: 7900.73s
                               ETA: 514809.9s

################################################################################
                    [1m Learning iteration 3023/200000 [0m

                       Computation: 3250 steps/s (collection: 0.467s, learning 2.053s)
               Value function loss: 86974.0607
                    Surrogate loss: 0.0164
             Mean action noise std: 0.88
                       Mean reward: 11734.27
               Mean episode length: 439.10
                 Mean success rate: 89.00
                  Mean reward/step: 26.34
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 2.52s
                        Total time: 7903.25s
                               ETA: 514801.2s

################################################################################
                    [1m Learning iteration 3024/200000 [0m

                       Computation: 3198 steps/s (collection: 0.453s, learning 2.108s)
               Value function loss: 77885.8624
                    Surrogate loss: 0.0132
             Mean action noise std: 0.88
                       Mean reward: 11317.76
               Mean episode length: 429.00
                 Mean success rate: 87.00
                  Mean reward/step: 26.66
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 24780800
                    Iteration time: 2.56s
                        Total time: 7905.81s
                               ETA: 514795.2s

################################################################################
                    [1m Learning iteration 3025/200000 [0m

                       Computation: 3145 steps/s (collection: 0.515s, learning 2.089s)
               Value function loss: 98863.4257
                    Surrogate loss: 0.0138
             Mean action noise std: 0.88
                       Mean reward: 11306.29
               Mean episode length: 429.87
                 Mean success rate: 86.50
                  Mean reward/step: 27.29
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 24788992
                    Iteration time: 2.60s
                        Total time: 7908.42s
                               ETA: 514792.0s

################################################################################
                    [1m Learning iteration 3026/200000 [0m

                       Computation: 3266 steps/s (collection: 0.438s, learning 2.070s)
               Value function loss: 111464.3609
                    Surrogate loss: 0.0111
             Mean action noise std: 0.88
                       Mean reward: 11042.14
               Mean episode length: 426.28
                 Mean success rate: 86.00
                  Mean reward/step: 26.64
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 24797184
                    Iteration time: 2.51s
                        Total time: 7910.93s
                               ETA: 514782.5s

################################################################################
                    [1m Learning iteration 3027/200000 [0m

                       Computation: 3242 steps/s (collection: 0.472s, learning 2.055s)
               Value function loss: 124739.8070
                    Surrogate loss: 0.0116
             Mean action noise std: 0.88
                       Mean reward: 10719.53
               Mean episode length: 416.95
                 Mean success rate: 84.50
                  Mean reward/step: 25.95
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 24805376
                    Iteration time: 2.53s
                        Total time: 7913.45s
                               ETA: 514774.2s

################################################################################
                    [1m Learning iteration 3028/200000 [0m

                       Computation: 3203 steps/s (collection: 0.469s, learning 2.088s)
               Value function loss: 96381.6386
                    Surrogate loss: 0.0125
             Mean action noise std: 0.88
                       Mean reward: 10507.98
               Mean episode length: 409.09
                 Mean success rate: 83.00
                  Mean reward/step: 25.53
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 24813568
                    Iteration time: 2.56s
                        Total time: 7916.01s
                               ETA: 514768.0s

################################################################################
                    [1m Learning iteration 3029/200000 [0m

                       Computation: 3220 steps/s (collection: 0.495s, learning 2.049s)
               Value function loss: 73205.2515
                    Surrogate loss: 0.0146
             Mean action noise std: 0.88
                       Mean reward: 10686.42
               Mean episode length: 416.09
                 Mean success rate: 84.50
                  Mean reward/step: 26.06
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 24821760
                    Iteration time: 2.54s
                        Total time: 7918.55s
                               ETA: 514760.8s

################################################################################
                    [1m Learning iteration 3030/200000 [0m

                       Computation: 3222 steps/s (collection: 0.466s, learning 2.076s)
               Value function loss: 88723.2609
                    Surrogate loss: 0.0075
             Mean action noise std: 0.88
                       Mean reward: 10573.42
               Mean episode length: 411.76
                 Mean success rate: 83.50
                  Mean reward/step: 26.65
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 24829952
                    Iteration time: 2.54s
                        Total time: 7921.10s
                               ETA: 514753.6s

################################################################################
                    [1m Learning iteration 3031/200000 [0m

                       Computation: 3228 steps/s (collection: 0.447s, learning 2.091s)
               Value function loss: 90807.0572
                    Surrogate loss: 0.0095
             Mean action noise std: 0.88
                       Mean reward: 10616.69
               Mean episode length: 411.47
                 Mean success rate: 84.50
                  Mean reward/step: 27.25
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 24838144
                    Iteration time: 2.54s
                        Total time: 7923.63s
                               ETA: 514746.1s

################################################################################
                    [1m Learning iteration 3032/200000 [0m

                       Computation: 3174 steps/s (collection: 0.481s, learning 2.099s)
               Value function loss: 100006.4118
                    Surrogate loss: 0.0102
             Mean action noise std: 0.88
                       Mean reward: 10559.54
               Mean episode length: 407.56
                 Mean success rate: 84.00
                  Mean reward/step: 27.45
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 24846336
                    Iteration time: 2.58s
                        Total time: 7926.21s
                               ETA: 514741.3s

################################################################################
                    [1m Learning iteration 3033/200000 [0m

                       Computation: 3254 steps/s (collection: 0.465s, learning 2.052s)
               Value function loss: 87365.9348
                    Surrogate loss: 0.0126
             Mean action noise std: 0.88
                       Mean reward: 10774.27
               Mean episode length: 414.54
                 Mean success rate: 85.00
                  Mean reward/step: 27.14
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 24854528
                    Iteration time: 2.52s
                        Total time: 7928.73s
                               ETA: 514732.5s

################################################################################
                    [1m Learning iteration 3034/200000 [0m

                       Computation: 3290 steps/s (collection: 0.451s, learning 2.039s)
               Value function loss: 63520.7164
                    Surrogate loss: 0.0137
             Mean action noise std: 0.88
                       Mean reward: 10831.05
               Mean episode length: 415.33
                 Mean success rate: 85.00
                  Mean reward/step: 28.57
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 24862720
                    Iteration time: 2.49s
                        Total time: 7931.22s
                               ETA: 514721.8s

################################################################################
                    [1m Learning iteration 3035/200000 [0m

                       Computation: 3207 steps/s (collection: 0.510s, learning 2.045s)
               Value function loss: 156761.9834
                    Surrogate loss: 0.0122
             Mean action noise std: 0.88
                       Mean reward: 11066.72
               Mean episode length: 418.64
                 Mean success rate: 86.00
                  Mean reward/step: 27.39
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 2.55s
                        Total time: 7933.77s
                               ETA: 514715.4s

################################################################################
                    [1m Learning iteration 3036/200000 [0m

                       Computation: 3224 steps/s (collection: 0.491s, learning 2.050s)
               Value function loss: 120794.7573
                    Surrogate loss: 0.0118
             Mean action noise std: 0.88
                       Mean reward: 11327.96
               Mean episode length: 423.60
                 Mean success rate: 86.50
                  Mean reward/step: 26.24
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 24879104
                    Iteration time: 2.54s
                        Total time: 7936.32s
                               ETA: 514708.1s

################################################################################
                    [1m Learning iteration 3037/200000 [0m

                       Computation: 3250 steps/s (collection: 0.493s, learning 2.027s)
               Value function loss: 102157.5751
                    Surrogate loss: 0.0136
             Mean action noise std: 0.88
                       Mean reward: 11442.82
               Mean episode length: 424.93
                 Mean success rate: 87.00
                  Mean reward/step: 25.46
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 24887296
                    Iteration time: 2.52s
                        Total time: 7938.84s
                               ETA: 514699.4s

################################################################################
                    [1m Learning iteration 3038/200000 [0m

                       Computation: 3242 steps/s (collection: 0.469s, learning 2.057s)
               Value function loss: 96884.9176
                    Surrogate loss: 0.0173
             Mean action noise std: 0.88
                       Mean reward: 11582.38
               Mean episode length: 427.53
                 Mean success rate: 86.50
                  Mean reward/step: 26.31
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 24895488
                    Iteration time: 2.53s
                        Total time: 7941.36s
                               ETA: 514691.2s

################################################################################
                    [1m Learning iteration 3039/200000 [0m

                       Computation: 3169 steps/s (collection: 0.464s, learning 2.121s)
               Value function loss: 87252.6406
                    Surrogate loss: 0.0137
             Mean action noise std: 0.88
                       Mean reward: 11306.83
               Mean episode length: 420.23
                 Mean success rate: 85.50
                  Mean reward/step: 26.40
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 24903680
                    Iteration time: 2.58s
                        Total time: 7943.95s
                               ETA: 514686.8s

################################################################################
                    [1m Learning iteration 3040/200000 [0m

                       Computation: 3291 steps/s (collection: 0.460s, learning 2.029s)
               Value function loss: 94860.8871
                    Surrogate loss: 0.0136
             Mean action noise std: 0.88
                       Mean reward: 11238.35
               Mean episode length: 417.65
                 Mean success rate: 85.00
                  Mean reward/step: 26.75
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 24911872
                    Iteration time: 2.49s
                        Total time: 7946.44s
                               ETA: 514676.1s

################################################################################
                    [1m Learning iteration 3041/200000 [0m

                       Computation: 3181 steps/s (collection: 0.517s, learning 2.059s)
               Value function loss: 120301.1148
                    Surrogate loss: 0.0110
             Mean action noise std: 0.88
                       Mean reward: 11482.67
               Mean episode length: 427.03
                 Mean success rate: 86.50
                  Mean reward/step: 26.60
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 24920064
                    Iteration time: 2.58s
                        Total time: 7949.01s
                               ETA: 514671.0s

################################################################################
                    [1m Learning iteration 3042/200000 [0m

                       Computation: 3235 steps/s (collection: 0.503s, learning 2.029s)
               Value function loss: 107940.3453
                    Surrogate loss: 0.0119
             Mean action noise std: 0.88
                       Mean reward: 11315.08
               Mean episode length: 423.04
                 Mean success rate: 85.50
                  Mean reward/step: 26.16
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 24928256
                    Iteration time: 2.53s
                        Total time: 7951.54s
                               ETA: 514663.1s

################################################################################
                    [1m Learning iteration 3043/200000 [0m

                       Computation: 3197 steps/s (collection: 0.493s, learning 2.069s)
               Value function loss: 116148.6987
                    Surrogate loss: 0.0141
             Mean action noise std: 0.88
                       Mean reward: 11331.25
               Mean episode length: 422.33
                 Mean success rate: 85.00
                  Mean reward/step: 25.61
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 24936448
                    Iteration time: 2.56s
                        Total time: 7954.10s
                               ETA: 514657.2s

################################################################################
                    [1m Learning iteration 3044/200000 [0m

                       Computation: 3238 steps/s (collection: 0.516s, learning 2.014s)
               Value function loss: 100347.4820
                    Surrogate loss: 0.0141
             Mean action noise std: 0.88
                       Mean reward: 11109.01
               Mean episode length: 416.02
                 Mean success rate: 84.00
                  Mean reward/step: 26.41
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 24944640
                    Iteration time: 2.53s
                        Total time: 7956.63s
                               ETA: 514649.2s

################################################################################
                    [1m Learning iteration 3045/200000 [0m

                       Computation: 3434 steps/s (collection: 0.406s, learning 1.980s)
               Value function loss: 107521.4336
                    Surrogate loss: 0.0151
             Mean action noise std: 0.88
                       Mean reward: 11350.74
               Mean episode length: 424.14
                 Mean success rate: 86.00
                  Mean reward/step: 26.87
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 24952832
                    Iteration time: 2.39s
                        Total time: 7959.02s
                               ETA: 514631.8s

################################################################################
                    [1m Learning iteration 3046/200000 [0m

                       Computation: 3379 steps/s (collection: 0.420s, learning 2.003s)
               Value function loss: 88644.5187
                    Surrogate loss: 0.0157
             Mean action noise std: 0.88
                       Mean reward: 11056.28
               Mean episode length: 416.56
                 Mean success rate: 84.50
                  Mean reward/step: 26.47
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 24961024
                    Iteration time: 2.42s
                        Total time: 7961.44s
                               ETA: 514617.0s

################################################################################
                    [1m Learning iteration 3047/200000 [0m

                       Computation: 3427 steps/s (collection: 0.413s, learning 1.977s)
               Value function loss: 112106.3142
                    Surrogate loss: 0.0112
             Mean action noise std: 0.88
                       Mean reward: 11015.53
               Mean episode length: 415.31
                 Mean success rate: 85.00
                  Mean reward/step: 26.73
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 2.39s
                        Total time: 7963.83s
                               ETA: 514600.0s

################################################################################
                    [1m Learning iteration 3048/200000 [0m

                       Computation: 3396 steps/s (collection: 0.417s, learning 1.995s)
               Value function loss: 89916.9166
                    Surrogate loss: 0.0107
             Mean action noise std: 0.88
                       Mean reward: 10914.36
               Mean episode length: 412.94
                 Mean success rate: 84.50
                  Mean reward/step: 26.47
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 24977408
                    Iteration time: 2.41s
                        Total time: 7966.24s
                               ETA: 514584.4s

################################################################################
                    [1m Learning iteration 3049/200000 [0m

                       Computation: 3394 steps/s (collection: 0.422s, learning 1.991s)
               Value function loss: 58479.9180
                    Surrogate loss: 0.0145
             Mean action noise std: 0.88
                       Mean reward: 10729.69
               Mean episode length: 406.83
                 Mean success rate: 83.00
                  Mean reward/step: 26.97
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 24985600
                    Iteration time: 2.41s
                        Total time: 7968.66s
                               ETA: 514568.9s

################################################################################
                    [1m Learning iteration 3050/200000 [0m

                       Computation: 3362 steps/s (collection: 0.409s, learning 2.027s)
               Value function loss: 102940.3668
                    Surrogate loss: 0.0113
             Mean action noise std: 0.88
                       Mean reward: 10944.82
               Mean episode length: 413.33
                 Mean success rate: 84.00
                  Mean reward/step: 27.84
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 24993792
                    Iteration time: 2.44s
                        Total time: 7971.09s
                               ETA: 514554.9s

################################################################################
                    [1m Learning iteration 3051/200000 [0m

                       Computation: 3302 steps/s (collection: 0.420s, learning 2.060s)
               Value function loss: 107520.6126
                    Surrogate loss: 0.0114
             Mean action noise std: 0.88
                       Mean reward: 10680.78
               Mean episode length: 404.25
                 Mean success rate: 82.00
                  Mean reward/step: 26.53
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 25001984
                    Iteration time: 2.48s
                        Total time: 7973.57s
                               ETA: 514543.7s

################################################################################
                    [1m Learning iteration 3052/200000 [0m

                       Computation: 3260 steps/s (collection: 0.440s, learning 2.073s)
               Value function loss: 128360.3096
                    Surrogate loss: 0.0120
             Mean action noise std: 0.88
                       Mean reward: 10767.29
               Mean episode length: 407.49
                 Mean success rate: 82.00
                  Mean reward/step: 25.97
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 25010176
                    Iteration time: 2.51s
                        Total time: 7976.09s
                               ETA: 514534.7s

################################################################################
                    [1m Learning iteration 3053/200000 [0m

                       Computation: 3230 steps/s (collection: 0.470s, learning 2.066s)
               Value function loss: 87869.3433
                    Surrogate loss: 0.0121
             Mean action noise std: 0.88
                       Mean reward: 10655.31
               Mean episode length: 404.13
                 Mean success rate: 82.00
                  Mean reward/step: 25.50
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 25018368
                    Iteration time: 2.54s
                        Total time: 7978.62s
                               ETA: 514527.1s

################################################################################
                    [1m Learning iteration 3054/200000 [0m

                       Computation: 3258 steps/s (collection: 0.486s, learning 2.028s)
               Value function loss: 84661.0865
                    Surrogate loss: 0.0150
             Mean action noise std: 0.88
                       Mean reward: 10708.46
               Mean episode length: 405.37
                 Mean success rate: 82.00
                  Mean reward/step: 26.21
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 25026560
                    Iteration time: 2.51s
                        Total time: 7981.14s
                               ETA: 514518.2s

################################################################################
                    [1m Learning iteration 3055/200000 [0m

                       Computation: 3249 steps/s (collection: 0.449s, learning 2.072s)
               Value function loss: 89018.5621
                    Surrogate loss: 0.0156
             Mean action noise std: 0.88
                       Mean reward: 10747.01
               Mean episode length: 408.14
                 Mean success rate: 82.50
                  Mean reward/step: 26.23
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 25034752
                    Iteration time: 2.52s
                        Total time: 7983.66s
                               ETA: 514509.6s

################################################################################
                    [1m Learning iteration 3056/200000 [0m

                       Computation: 3245 steps/s (collection: 0.455s, learning 2.069s)
               Value function loss: 90310.4369
                    Surrogate loss: 0.0115
             Mean action noise std: 0.88
                       Mean reward: 10781.69
               Mean episode length: 409.14
                 Mean success rate: 82.50
                  Mean reward/step: 26.80
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 25042944
                    Iteration time: 2.52s
                        Total time: 7986.18s
                               ETA: 514501.3s

################################################################################
                    [1m Learning iteration 3057/200000 [0m

                       Computation: 3230 steps/s (collection: 0.482s, learning 2.054s)
               Value function loss: 146854.7069
                    Surrogate loss: 0.0089
             Mean action noise std: 0.88
                       Mean reward: 10816.58
               Mean episode length: 408.55
                 Mean success rate: 81.50
                  Mean reward/step: 26.20
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 25051136
                    Iteration time: 2.54s
                        Total time: 7988.72s
                               ETA: 514493.8s

################################################################################
                    [1m Learning iteration 3058/200000 [0m

                       Computation: 3194 steps/s (collection: 0.479s, learning 2.085s)
               Value function loss: 97585.3706
                    Surrogate loss: 0.0135
             Mean action noise std: 0.88
                       Mean reward: 11070.33
               Mean episode length: 417.35
                 Mean success rate: 83.00
                  Mean reward/step: 25.18
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 25059328
                    Iteration time: 2.56s
                        Total time: 7991.28s
                               ETA: 514488.1s

################################################################################
                    [1m Learning iteration 3059/200000 [0m

                       Computation: 3174 steps/s (collection: 0.494s, learning 2.087s)
               Value function loss: 86285.4375
                    Surrogate loss: 0.0120
             Mean action noise std: 0.88
                       Mean reward: 11327.32
               Mean episode length: 425.36
                 Mean success rate: 85.00
                  Mean reward/step: 25.76
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 2.58s
                        Total time: 7993.86s
                               ETA: 514483.4s

################################################################################
                    [1m Learning iteration 3060/200000 [0m

                       Computation: 3192 steps/s (collection: 0.477s, learning 2.089s)
               Value function loss: 98773.3539
                    Surrogate loss: 0.0136
             Mean action noise std: 0.88
                       Mean reward: 11263.33
               Mean episode length: 425.36
                 Mean success rate: 85.00
                  Mean reward/step: 26.62
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 25075712
                    Iteration time: 2.57s
                        Total time: 7996.43s
                               ETA: 514477.8s

################################################################################
                    [1m Learning iteration 3061/200000 [0m

                       Computation: 3282 steps/s (collection: 0.450s, learning 2.046s)
               Value function loss: 98049.3353
                    Surrogate loss: 0.0121
             Mean action noise std: 0.88
                       Mean reward: 11541.11
               Mean episode length: 434.25
                 Mean success rate: 87.00
                  Mean reward/step: 26.27
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 25083904
                    Iteration time: 2.50s
                        Total time: 7998.92s
                               ETA: 514467.6s

################################################################################
                    [1m Learning iteration 3062/200000 [0m

                       Computation: 3295 steps/s (collection: 0.422s, learning 2.064s)
               Value function loss: 78345.0677
                    Surrogate loss: 0.0105
             Mean action noise std: 0.88
                       Mean reward: 11396.19
               Mean episode length: 431.62
                 Mean success rate: 86.00
                  Mean reward/step: 26.16
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 25092096
                    Iteration time: 2.49s
                        Total time: 8001.41s
                               ETA: 514456.9s

################################################################################
                    [1m Learning iteration 3063/200000 [0m

                       Computation: 3286 steps/s (collection: 0.437s, learning 2.056s)
               Value function loss: 88610.5905
                    Surrogate loss: 0.0117
             Mean action noise std: 0.88
                       Mean reward: 11452.30
               Mean episode length: 433.40
                 Mean success rate: 86.50
                  Mean reward/step: 26.92
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 25100288
                    Iteration time: 2.49s
                        Total time: 8003.90s
                               ETA: 514446.6s

################################################################################
                    [1m Learning iteration 3064/200000 [0m

                       Computation: 3246 steps/s (collection: 0.467s, learning 2.057s)
               Value function loss: 92851.0965
                    Surrogate loss: 0.0134
             Mean action noise std: 0.88
                       Mean reward: 11467.30
               Mean episode length: 432.39
                 Mean success rate: 86.00
                  Mean reward/step: 26.97
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 25108480
                    Iteration time: 2.52s
                        Total time: 8006.43s
                               ETA: 514438.3s

################################################################################
                    [1m Learning iteration 3065/200000 [0m

                       Computation: 3265 steps/s (collection: 0.472s, learning 2.037s)
               Value function loss: 46216.3146
                    Surrogate loss: 0.0114
             Mean action noise std: 0.88
                       Mean reward: 11505.14
               Mean episode length: 432.39
                 Mean success rate: 86.00
                  Mean reward/step: 27.29
       Mean episode length/episode: 31.27
--------------------------------------------------------------------------------
                   Total timesteps: 25116672
                    Iteration time: 2.51s
                        Total time: 8008.93s
                               ETA: 514429.0s

################################################################################
                    [1m Learning iteration 3066/200000 [0m

                       Computation: 3211 steps/s (collection: 0.479s, learning 2.072s)
               Value function loss: 167236.0234
                    Surrogate loss: 0.0115
             Mean action noise std: 0.88
                       Mean reward: 11173.46
               Mean episode length: 423.74
                 Mean success rate: 84.00
                  Mean reward/step: 27.70
       Mean episode length/episode: 28.44
--------------------------------------------------------------------------------
                   Total timesteps: 25124864
                    Iteration time: 2.55s
                        Total time: 8011.48s
                               ETA: 514422.5s

################################################################################
                    [1m Learning iteration 3067/200000 [0m

                       Computation: 3250 steps/s (collection: 0.493s, learning 2.027s)
               Value function loss: 61882.7334
                    Surrogate loss: 0.0119
             Mean action noise std: 0.88
                       Mean reward: 11341.59
               Mean episode length: 430.38
                 Mean success rate: 85.00
                  Mean reward/step: 26.33
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 25133056
                    Iteration time: 2.52s
                        Total time: 8014.01s
                               ETA: 514414.0s

################################################################################
                    [1m Learning iteration 3068/200000 [0m

                       Computation: 3159 steps/s (collection: 0.532s, learning 2.061s)
               Value function loss: 131339.7725
                    Surrogate loss: 0.0120
             Mean action noise std: 0.88
                       Mean reward: 11515.00
               Mean episode length: 437.50
                 Mean success rate: 86.50
                  Mean reward/step: 26.30
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 25141248
                    Iteration time: 2.59s
                        Total time: 8016.60s
                               ETA: 514410.1s

################################################################################
                    [1m Learning iteration 3069/200000 [0m

                       Computation: 3227 steps/s (collection: 0.479s, learning 2.059s)
               Value function loss: 91753.6641
                    Surrogate loss: 0.0131
             Mean action noise std: 0.88
                       Mean reward: 11497.47
               Mean episode length: 437.82
                 Mean success rate: 87.00
                  Mean reward/step: 26.72
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 25149440
                    Iteration time: 2.54s
                        Total time: 8019.14s
                               ETA: 514402.8s

################################################################################
                    [1m Learning iteration 3070/200000 [0m

                       Computation: 3302 steps/s (collection: 0.425s, learning 2.055s)
               Value function loss: 94848.9324
                    Surrogate loss: 0.0119
             Mean action noise std: 0.88
                       Mean reward: 11622.79
               Mean episode length: 441.34
                 Mean success rate: 87.50
                  Mean reward/step: 26.89
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 25157632
                    Iteration time: 2.48s
                        Total time: 8021.62s
                               ETA: 514391.7s

################################################################################
                    [1m Learning iteration 3071/200000 [0m

                       Computation: 3246 steps/s (collection: 0.454s, learning 2.070s)
               Value function loss: 75045.4413
                    Surrogate loss: 0.0104
             Mean action noise std: 0.88
                       Mean reward: 11218.88
               Mean episode length: 428.06
                 Mean success rate: 84.50
                  Mean reward/step: 27.13
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 2.52s
                        Total time: 8024.14s
                               ETA: 514383.4s

################################################################################
                    [1m Learning iteration 3072/200000 [0m

                       Computation: 3272 steps/s (collection: 0.474s, learning 2.029s)
               Value function loss: 115078.0727
                    Surrogate loss: 0.0134
             Mean action noise std: 0.88
                       Mean reward: 10814.67
               Mean episode length: 415.24
                 Mean success rate: 81.50
                  Mean reward/step: 27.37
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 25174016
                    Iteration time: 2.50s
                        Total time: 8026.64s
                               ETA: 514373.9s

################################################################################
                    [1m Learning iteration 3073/200000 [0m

                       Computation: 3260 steps/s (collection: 0.486s, learning 2.027s)
               Value function loss: 158406.6814
                    Surrogate loss: 0.0131
             Mean action noise std: 0.88
                       Mean reward: 11128.60
               Mean episode length: 423.68
                 Mean success rate: 83.50
                  Mean reward/step: 26.59
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 25182208
                    Iteration time: 2.51s
                        Total time: 8029.16s
                               ETA: 514364.9s

################################################################################
                    [1m Learning iteration 3074/200000 [0m

                       Computation: 3339 steps/s (collection: 0.434s, learning 2.019s)
               Value function loss: 111540.9570
                    Surrogate loss: 0.0155
             Mean action noise std: 0.88
                       Mean reward: 11130.37
               Mean episode length: 423.45
                 Mean success rate: 84.00
                  Mean reward/step: 25.44
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 25190400
                    Iteration time: 2.45s
                        Total time: 8031.61s
                               ETA: 514352.1s

################################################################################
                    [1m Learning iteration 3075/200000 [0m

                       Computation: 3274 steps/s (collection: 0.467s, learning 2.034s)
               Value function loss: 101805.4670
                    Surrogate loss: 0.0174
             Mean action noise std: 0.88
                       Mean reward: 11422.96
               Mean episode length: 431.99
                 Mean success rate: 86.00
                  Mean reward/step: 25.43
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 25198592
                    Iteration time: 2.50s
                        Total time: 8034.11s
                               ETA: 514342.4s

################################################################################
                    [1m Learning iteration 3076/200000 [0m

                       Computation: 3303 steps/s (collection: 0.477s, learning 2.003s)
               Value function loss: 90366.8094
                    Surrogate loss: 0.0134
             Mean action noise std: 0.88
                       Mean reward: 11216.27
               Mean episode length: 422.79
                 Mean success rate: 84.00
                  Mean reward/step: 25.81
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 25206784
                    Iteration time: 2.48s
                        Total time: 8036.59s
                               ETA: 514331.3s

################################################################################
                    [1m Learning iteration 3077/200000 [0m

                       Computation: 3308 steps/s (collection: 0.440s, learning 2.036s)
               Value function loss: 77217.4085
                    Surrogate loss: 0.0125
             Mean action noise std: 0.88
                       Mean reward: 11148.47
               Mean episode length: 417.57
                 Mean success rate: 83.00
                  Mean reward/step: 25.55
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 25214976
                    Iteration time: 2.48s
                        Total time: 8039.07s
                               ETA: 514320.1s

################################################################################
                    [1m Learning iteration 3078/200000 [0m

                       Computation: 3269 steps/s (collection: 0.444s, learning 2.062s)
               Value function loss: 89924.7066
                    Surrogate loss: 0.0145
             Mean action noise std: 0.88
                       Mean reward: 10769.29
               Mean episode length: 404.81
                 Mean success rate: 80.00
                  Mean reward/step: 26.01
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 25223168
                    Iteration time: 2.51s
                        Total time: 8041.57s
                               ETA: 514310.7s

################################################################################
                    [1m Learning iteration 3079/200000 [0m

                       Computation: 3294 steps/s (collection: 0.453s, learning 2.034s)
               Value function loss: 109832.3730
                    Surrogate loss: 0.0081
             Mean action noise std: 0.88
                       Mean reward: 10886.55
               Mean episode length: 406.51
                 Mean success rate: 80.00
                  Mean reward/step: 25.75
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 25231360
                    Iteration time: 2.49s
                        Total time: 8044.06s
                               ETA: 514300.1s

################################################################################
                    [1m Learning iteration 3080/200000 [0m

                       Computation: 3318 steps/s (collection: 0.475s, learning 1.993s)
               Value function loss: 75960.6182
                    Surrogate loss: 0.0119
             Mean action noise std: 0.88
                       Mean reward: 10665.88
               Mean episode length: 399.85
                 Mean success rate: 78.50
                  Mean reward/step: 26.29
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 25239552
                    Iteration time: 2.47s
                        Total time: 8046.53s
                               ETA: 514288.3s

################################################################################
                    [1m Learning iteration 3081/200000 [0m

                       Computation: 3304 steps/s (collection: 0.456s, learning 2.023s)
               Value function loss: 71349.5834
                    Surrogate loss: 0.0131
             Mean action noise std: 0.88
                       Mean reward: 10874.17
               Mean episode length: 406.85
                 Mean success rate: 80.00
                  Mean reward/step: 27.20
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 25247744
                    Iteration time: 2.48s
                        Total time: 8049.01s
                               ETA: 514277.2s

################################################################################
                    [1m Learning iteration 3082/200000 [0m

                       Computation: 3331 steps/s (collection: 0.426s, learning 2.033s)
               Value function loss: 150446.0777
                    Surrogate loss: 0.0096
             Mean action noise std: 0.88
                       Mean reward: 11349.49
               Mean episode length: 420.40
                 Mean success rate: 83.00
                  Mean reward/step: 26.78
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 25255936
                    Iteration time: 2.46s
                        Total time: 8051.47s
                               ETA: 514264.8s

################################################################################
                    [1m Learning iteration 3083/200000 [0m

                       Computation: 3383 steps/s (collection: 0.409s, learning 2.012s)
               Value function loss: 80052.7182
                    Surrogate loss: 0.0116
             Mean action noise std: 0.88
                       Mean reward: 11422.46
               Mean episode length: 422.90
                 Mean success rate: 83.50
                  Mean reward/step: 26.27
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 2.42s
                        Total time: 8053.89s
                               ETA: 514250.1s

################################################################################
                    [1m Learning iteration 3084/200000 [0m

                       Computation: 3325 steps/s (collection: 0.455s, learning 2.009s)
               Value function loss: 113238.7465
                    Surrogate loss: 0.0131
             Mean action noise std: 0.88
                       Mean reward: 11615.63
               Mean episode length: 430.97
                 Mean success rate: 85.00
                  Mean reward/step: 26.17
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 25272320
                    Iteration time: 2.46s
                        Total time: 8056.35s
                               ETA: 514238.0s

################################################################################
                    [1m Learning iteration 3085/200000 [0m

                       Computation: 3301 steps/s (collection: 0.476s, learning 2.005s)
               Value function loss: 95959.9413
                    Surrogate loss: 0.0127
             Mean action noise std: 0.88
                       Mean reward: 11743.55
               Mean episode length: 435.08
                 Mean success rate: 85.50
                  Mean reward/step: 26.08
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 25280512
                    Iteration time: 2.48s
                        Total time: 8058.83s
                               ETA: 514227.1s

################################################################################
                    [1m Learning iteration 3086/200000 [0m

                       Computation: 3303 steps/s (collection: 0.455s, learning 2.025s)
               Value function loss: 109777.3064
                    Surrogate loss: 0.0109
             Mean action noise std: 0.88
                       Mean reward: 11718.52
               Mean episode length: 435.05
                 Mean success rate: 85.50
                  Mean reward/step: 25.94
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 25288704
                    Iteration time: 2.48s
                        Total time: 8061.31s
                               ETA: 514216.1s

################################################################################
                    [1m Learning iteration 3087/200000 [0m

                       Computation: 3349 steps/s (collection: 0.422s, learning 2.024s)
               Value function loss: 78077.8586
                    Surrogate loss: 0.0164
             Mean action noise std: 0.88
                       Mean reward: 11492.67
               Mean episode length: 430.89
                 Mean success rate: 85.00
                  Mean reward/step: 26.25
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 25296896
                    Iteration time: 2.45s
                        Total time: 8063.76s
                               ETA: 514202.9s

################################################################################
                    [1m Learning iteration 3088/200000 [0m

                       Computation: 3327 steps/s (collection: 0.444s, learning 2.018s)
               Value function loss: 126866.5379
                    Surrogate loss: 0.0126
             Mean action noise std: 0.88
                       Mean reward: 11330.03
               Mean episode length: 428.55
                 Mean success rate: 84.00
                  Mean reward/step: 25.42
       Mean episode length/episode: 28.35
--------------------------------------------------------------------------------
                   Total timesteps: 25305088
                    Iteration time: 2.46s
                        Total time: 8066.22s
                               ETA: 514190.8s

################################################################################
                    [1m Learning iteration 3089/200000 [0m

                       Computation: 3388 steps/s (collection: 0.414s, learning 2.004s)
               Value function loss: 100189.9185
                    Surrogate loss: 0.0131
             Mean action noise std: 0.88
                       Mean reward: 11423.90
               Mean episode length: 433.03
                 Mean success rate: 85.00
                  Mean reward/step: 24.37
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 25313280
                    Iteration time: 2.42s
                        Total time: 8068.64s
                               ETA: 514175.8s

################################################################################
                    [1m Learning iteration 3090/200000 [0m

                       Computation: 3377 steps/s (collection: 0.408s, learning 2.017s)
               Value function loss: 96963.9590
                    Surrogate loss: 0.0099
             Mean action noise std: 0.88
                       Mean reward: 11526.76
               Mean episode length: 437.52
                 Mean success rate: 86.00
                  Mean reward/step: 25.20
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 25321472
                    Iteration time: 2.43s
                        Total time: 8071.06s
                               ETA: 514161.4s

################################################################################
                    [1m Learning iteration 3091/200000 [0m

                       Computation: 3377 steps/s (collection: 0.416s, learning 2.010s)
               Value function loss: 93172.4009
                    Surrogate loss: 0.0109
             Mean action noise std: 0.88
                       Mean reward: 11142.21
               Mean episode length: 427.28
                 Mean success rate: 84.00
                  Mean reward/step: 25.62
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 25329664
                    Iteration time: 2.43s
                        Total time: 8073.49s
                               ETA: 514146.9s

################################################################################
                    [1m Learning iteration 3092/200000 [0m

                       Computation: 3299 steps/s (collection: 0.459s, learning 2.023s)
               Value function loss: 124152.1788
                    Surrogate loss: 0.0071
             Mean action noise std: 0.88
                       Mean reward: 11051.00
               Mean episode length: 425.29
                 Mean success rate: 84.00
                  Mean reward/step: 25.86
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 25337856
                    Iteration time: 2.48s
                        Total time: 8075.97s
                               ETA: 514136.1s

################################################################################
                    [1m Learning iteration 3093/200000 [0m

                       Computation: 3345 steps/s (collection: 0.427s, learning 2.022s)
               Value function loss: 211804.8234
                    Surrogate loss: 0.0061
             Mean action noise std: 0.88
                       Mean reward: 10599.36
               Mean episode length: 413.58
                 Mean success rate: 81.00
                  Mean reward/step: 25.91
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 25346048
                    Iteration time: 2.45s
                        Total time: 8078.42s
                               ETA: 514123.2s

################################################################################
                    [1m Learning iteration 3094/200000 [0m

                       Computation: 3343 steps/s (collection: 0.438s, learning 2.012s)
               Value function loss: 120351.4560
                    Surrogate loss: 0.0090
             Mean action noise std: 0.88
                       Mean reward: 10095.76
               Mean episode length: 398.07
                 Mean success rate: 77.00
                  Mean reward/step: 26.54
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 25354240
                    Iteration time: 2.45s
                        Total time: 8080.87s
                               ETA: 514110.4s

################################################################################
                    [1m Learning iteration 3095/200000 [0m

                       Computation: 3249 steps/s (collection: 0.466s, learning 2.056s)
               Value function loss: 109842.7808
                    Surrogate loss: 0.0113
             Mean action noise std: 0.88
                       Mean reward: 10034.87
               Mean episode length: 397.82
                 Mean success rate: 76.50
                  Mean reward/step: 25.85
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 2.52s
                        Total time: 8083.39s
                               ETA: 514102.0s

################################################################################
                    [1m Learning iteration 3096/200000 [0m

                       Computation: 3364 steps/s (collection: 0.435s, learning 2.000s)
               Value function loss: 48081.9517
                    Surrogate loss: 0.0134
             Mean action noise std: 0.88
                       Mean reward: 9945.72
               Mean episode length: 393.98
                 Mean success rate: 75.00
                  Mean reward/step: 25.90
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 25370624
                    Iteration time: 2.43s
                        Total time: 8085.83s
                               ETA: 514088.2s

################################################################################
                    [1m Learning iteration 3097/200000 [0m

                       Computation: 3385 steps/s (collection: 0.406s, learning 2.014s)
               Value function loss: 111316.7133
                    Surrogate loss: 0.0199
             Mean action noise std: 0.88
                       Mean reward: 10388.12
               Mean episode length: 406.41
                 Mean success rate: 78.50
                  Mean reward/step: 27.00
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 25378816
                    Iteration time: 2.42s
                        Total time: 8088.25s
                               ETA: 514073.5s

################################################################################
                    [1m Learning iteration 3098/200000 [0m

                       Computation: 3384 steps/s (collection: 0.407s, learning 2.014s)
               Value function loss: 66358.0043
                    Surrogate loss: 0.0125
             Mean action noise std: 0.88
                       Mean reward: 10687.29
               Mean episode length: 416.09
                 Mean success rate: 81.50
                  Mean reward/step: 25.73
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 25387008
                    Iteration time: 2.42s
                        Total time: 8090.67s
                               ETA: 514058.8s

################################################################################
                    [1m Learning iteration 3099/200000 [0m

                       Computation: 3140 steps/s (collection: 0.486s, learning 2.122s)
               Value function loss: 93107.6773
                    Surrogate loss: 0.0126
             Mean action noise std: 0.88
                       Mean reward: 10539.27
               Mean episode length: 411.92
                 Mean success rate: 81.00
                  Mean reward/step: 26.00
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 25395200
                    Iteration time: 2.61s
                        Total time: 8093.27s
                               ETA: 514056.1s

################################################################################
                    [1m Learning iteration 3100/200000 [0m

                       Computation: 3181 steps/s (collection: 0.450s, learning 2.125s)
               Value function loss: 86714.2975
                    Surrogate loss: 0.0114
             Mean action noise std: 0.88
                       Mean reward: 10566.23
               Mean episode length: 411.92
                 Mean success rate: 81.00
                  Mean reward/step: 26.13
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 25403392
                    Iteration time: 2.57s
                        Total time: 8095.85s
                               ETA: 514051.2s

################################################################################
                    [1m Learning iteration 3101/200000 [0m

                       Computation: 3182 steps/s (collection: 0.451s, learning 2.123s)
               Value function loss: 89683.5983
                    Surrogate loss: 0.0108
             Mean action noise std: 0.88
                       Mean reward: 10592.98
               Mean episode length: 408.45
                 Mean success rate: 80.50
                  Mean reward/step: 26.28
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 25411584
                    Iteration time: 2.57s
                        Total time: 8098.42s
                               ETA: 514046.3s

################################################################################
                    [1m Learning iteration 3102/200000 [0m

                       Computation: 3168 steps/s (collection: 0.457s, learning 2.128s)
               Value function loss: 102509.9725
                    Surrogate loss: 0.0116
             Mean action noise std: 0.88
                       Mean reward: 10779.34
               Mean episode length: 414.99
                 Mean success rate: 81.50
                  Mean reward/step: 25.68
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 25419776
                    Iteration time: 2.59s
                        Total time: 8101.01s
                               ETA: 514042.0s

################################################################################
                    [1m Learning iteration 3103/200000 [0m

                       Computation: 3105 steps/s (collection: 0.496s, learning 2.142s)
               Value function loss: 132848.5577
                    Surrogate loss: 0.0127
             Mean action noise std: 0.88
                       Mean reward: 10692.32
               Mean episode length: 409.79
                 Mean success rate: 81.00
                  Mean reward/step: 24.90
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 25427968
                    Iteration time: 2.64s
                        Total time: 8103.65s
                               ETA: 514041.1s

################################################################################
                    [1m Learning iteration 3104/200000 [0m

                       Computation: 3137 steps/s (collection: 0.446s, learning 2.165s)
               Value function loss: 113710.8906
                    Surrogate loss: 0.0129
             Mean action noise std: 0.88
                       Mean reward: 10751.23
               Mean episode length: 411.78
                 Mean success rate: 82.00
                  Mean reward/step: 24.15
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 25436160
                    Iteration time: 2.61s
                        Total time: 8106.26s
                               ETA: 514038.6s

################################################################################
                    [1m Learning iteration 3105/200000 [0m

                       Computation: 3139 steps/s (collection: 0.490s, learning 2.119s)
               Value function loss: 98056.8211
                    Surrogate loss: 0.0133
             Mean action noise std: 0.88
                       Mean reward: 10952.53
               Mean episode length: 417.74
                 Mean success rate: 83.50
                  Mean reward/step: 23.83
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 25444352
                    Iteration time: 2.61s
                        Total time: 8108.87s
                               ETA: 514035.9s

################################################################################
                    [1m Learning iteration 3106/200000 [0m

                       Computation: 3196 steps/s (collection: 0.470s, learning 2.093s)
               Value function loss: 94433.5645
                    Surrogate loss: 0.0167
             Mean action noise std: 0.88
                       Mean reward: 10866.99
               Mean episode length: 417.31
                 Mean success rate: 83.50
                  Mean reward/step: 24.78
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 25452544
                    Iteration time: 2.56s
                        Total time: 8111.43s
                               ETA: 514030.2s

################################################################################
                    [1m Learning iteration 3107/200000 [0m

                       Computation: 3103 steps/s (collection: 0.510s, learning 2.130s)
               Value function loss: 91233.3526
                    Surrogate loss: 0.0114
             Mean action noise std: 0.88
                       Mean reward: 10359.63
               Mean episode length: 402.90
                 Mean success rate: 80.00
                  Mean reward/step: 24.69
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 2.64s
                        Total time: 8114.07s
                               ETA: 514029.5s

################################################################################
                    [1m Learning iteration 3108/200000 [0m

                       Computation: 3122 steps/s (collection: 0.484s, learning 2.140s)
               Value function loss: 85367.2669
                    Surrogate loss: 0.0129
             Mean action noise std: 0.88
                       Mean reward: 10324.62
               Mean episode length: 402.84
                 Mean success rate: 79.00
                  Mean reward/step: 25.10
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 25468928
                    Iteration time: 2.62s
                        Total time: 8116.69s
                               ETA: 514027.7s

################################################################################
                    [1m Learning iteration 3109/200000 [0m

                       Computation: 3158 steps/s (collection: 0.472s, learning 2.122s)
               Value function loss: 97151.8572
                    Surrogate loss: 0.0122
             Mean action noise std: 0.88
                       Mean reward: 9940.45
               Mean episode length: 392.05
                 Mean success rate: 76.00
                  Mean reward/step: 25.59
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 25477120
                    Iteration time: 2.59s
                        Total time: 8119.29s
                               ETA: 514024.0s

################################################################################
                    [1m Learning iteration 3110/200000 [0m

                       Computation: 3253 steps/s (collection: 0.443s, learning 2.074s)
               Value function loss: 87990.9744
                    Surrogate loss: 0.0111
             Mean action noise std: 0.88
                       Mean reward: 10155.39
               Mean episode length: 398.76
                 Mean success rate: 77.50
                  Mean reward/step: 25.69
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 25485312
                    Iteration time: 2.52s
                        Total time: 8121.80s
                               ETA: 514015.5s

################################################################################
                    [1m Learning iteration 3111/200000 [0m

                       Computation: 3191 steps/s (collection: 0.503s, learning 2.064s)
               Value function loss: 77424.8705
                    Surrogate loss: 0.0135
             Mean action noise std: 0.88
                       Mean reward: 10077.44
               Mean episode length: 395.01
                 Mean success rate: 77.00
                  Mean reward/step: 26.39
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 25493504
                    Iteration time: 2.57s
                        Total time: 8124.37s
                               ETA: 514010.1s

################################################################################
                    [1m Learning iteration 3112/200000 [0m

                       Computation: 3177 steps/s (collection: 0.494s, learning 2.084s)
               Value function loss: 79350.9184
                    Surrogate loss: 0.0114
             Mean action noise std: 0.88
                       Mean reward: 10122.75
               Mean episode length: 397.69
                 Mean success rate: 77.00
                  Mean reward/step: 27.20
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 25501696
                    Iteration time: 2.58s
                        Total time: 8126.95s
                               ETA: 514005.4s

################################################################################
                    [1m Learning iteration 3113/200000 [0m

                       Computation: 3226 steps/s (collection: 0.467s, learning 2.072s)
               Value function loss: 126404.3867
                    Surrogate loss: 0.0096
             Mean action noise std: 0.88
                       Mean reward: 10349.42
               Mean episode length: 407.19
                 Mean success rate: 79.00
                  Mean reward/step: 27.19
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 25509888
                    Iteration time: 2.54s
                        Total time: 8129.49s
                               ETA: 513998.3s

################################################################################
                    [1m Learning iteration 3114/200000 [0m

                       Computation: 3209 steps/s (collection: 0.514s, learning 2.038s)
               Value function loss: 53504.9967
                    Surrogate loss: 0.0130
             Mean action noise std: 0.88
                       Mean reward: 9920.67
               Mean episode length: 393.80
                 Mean success rate: 76.50
                  Mean reward/step: 26.73
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 25518080
                    Iteration time: 2.55s
                        Total time: 8132.04s
                               ETA: 513992.0s

################################################################################
                    [1m Learning iteration 3115/200000 [0m

                       Computation: 3224 steps/s (collection: 0.468s, learning 2.073s)
               Value function loss: 97148.0707
                    Surrogate loss: 0.0127
             Mean action noise std: 0.88
                       Mean reward: 9665.62
               Mean episode length: 389.33
                 Mean success rate: 75.50
                  Mean reward/step: 26.68
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 25526272
                    Iteration time: 2.54s
                        Total time: 8134.58s
                               ETA: 513985.0s

################################################################################
                    [1m Learning iteration 3116/200000 [0m

                       Computation: 3230 steps/s (collection: 0.459s, learning 2.076s)
               Value function loss: 101785.1233
                    Surrogate loss: 0.0125
             Mean action noise std: 0.88
                       Mean reward: 9945.27
               Mean episode length: 393.67
                 Mean success rate: 77.00
                  Mean reward/step: 26.77
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 25534464
                    Iteration time: 2.54s
                        Total time: 8137.12s
                               ETA: 513977.6s

################################################################################
                    [1m Learning iteration 3117/200000 [0m

                       Computation: 3198 steps/s (collection: 0.484s, learning 2.077s)
               Value function loss: 72463.9306
                    Surrogate loss: 0.0101
             Mean action noise std: 0.88
                       Mean reward: 10157.86
               Mean episode length: 400.91
                 Mean success rate: 78.50
                  Mean reward/step: 26.75
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 25542656
                    Iteration time: 2.56s
                        Total time: 8139.68s
                               ETA: 513971.9s

################################################################################
                    [1m Learning iteration 3118/200000 [0m

                       Computation: 3201 steps/s (collection: 0.511s, learning 2.048s)
               Value function loss: 99075.5525
                    Surrogate loss: 0.0120
             Mean action noise std: 0.88
                       Mean reward: 10593.01
               Mean episode length: 411.96
                 Mean success rate: 81.50
                  Mean reward/step: 27.27
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 25550848
                    Iteration time: 2.56s
                        Total time: 8142.24s
                               ETA: 513966.0s

################################################################################
                    [1m Learning iteration 3119/200000 [0m

                       Computation: 3188 steps/s (collection: 0.519s, learning 2.050s)
               Value function loss: 147408.9680
                    Surrogate loss: 0.0147
             Mean action noise std: 0.88
                       Mean reward: 10626.77
               Mean episode length: 411.74
                 Mean success rate: 82.50
                  Mean reward/step: 26.74
       Mean episode length/episode: 28.64
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 2.57s
                        Total time: 8144.81s
                               ETA: 513960.8s

################################################################################
                    [1m Learning iteration 3120/200000 [0m

                       Computation: 3284 steps/s (collection: 0.434s, learning 2.060s)
               Value function loss: 119455.1729
                    Surrogate loss: 0.0120
             Mean action noise std: 0.88
                       Mean reward: 10715.97
               Mean episode length: 414.00
                 Mean success rate: 83.00
                  Mean reward/step: 25.71
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 25567232
                    Iteration time: 2.49s
                        Total time: 8147.30s
                               ETA: 513950.9s

################################################################################
                    [1m Learning iteration 3121/200000 [0m

                       Computation: 3215 steps/s (collection: 0.496s, learning 2.052s)
               Value function loss: 70712.2521
                    Surrogate loss: 0.0131
             Mean action noise std: 0.88
                       Mean reward: 10820.05
               Mean episode length: 419.04
                 Mean success rate: 83.50
                  Mean reward/step: 26.23
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 25575424
                    Iteration time: 2.55s
                        Total time: 8149.85s
                               ETA: 513944.3s

################################################################################
                    [1m Learning iteration 3122/200000 [0m

                       Computation: 3115 steps/s (collection: 0.509s, learning 2.120s)
               Value function loss: 152987.8713
                    Surrogate loss: 0.0116
             Mean action noise std: 0.88
                       Mean reward: 10990.99
               Mean episode length: 422.45
                 Mean success rate: 84.50
                  Mean reward/step: 26.86
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 25583616
                    Iteration time: 2.63s
                        Total time: 8152.48s
                               ETA: 513942.9s

################################################################################
                    [1m Learning iteration 3123/200000 [0m

                       Computation: 3086 steps/s (collection: 0.550s, learning 2.105s)
               Value function loss: 103588.6844
                    Surrogate loss: 0.0125
             Mean action noise std: 0.88
                       Mean reward: 10829.58
               Mean episode length: 416.94
                 Mean success rate: 83.50
                  Mean reward/step: 25.95
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 25591808
                    Iteration time: 2.65s
                        Total time: 8155.13s
                               ETA: 513943.0s

################################################################################
                    [1m Learning iteration 3124/200000 [0m

                       Computation: 3068 steps/s (collection: 0.523s, learning 2.147s)
               Value function loss: 91034.2442
                    Surrogate loss: 0.0128
             Mean action noise std: 0.88
                       Mean reward: 11111.25
               Mean episode length: 424.81
                 Mean success rate: 85.00
                  Mean reward/step: 26.16
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 25600000
                    Iteration time: 2.67s
                        Total time: 8157.80s
                               ETA: 513944.2s

################################################################################
                    [1m Learning iteration 3125/200000 [0m

                       Computation: 3163 steps/s (collection: 0.489s, learning 2.101s)
               Value function loss: 115628.9605
                    Surrogate loss: 0.0118
             Mean action noise std: 0.88
                       Mean reward: 11575.04
               Mean episode length: 435.46
                 Mean success rate: 87.50
                  Mean reward/step: 27.06
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 25608192
                    Iteration time: 2.59s
                        Total time: 8160.39s
                               ETA: 513940.2s

################################################################################
                    [1m Learning iteration 3126/200000 [0m

                       Computation: 3142 steps/s (collection: 0.491s, learning 2.115s)
               Value function loss: 98120.2915
                    Surrogate loss: 0.0109
             Mean action noise std: 0.88
                       Mean reward: 11389.27
               Mean episode length: 429.18
                 Mean success rate: 85.50
                  Mean reward/step: 26.72
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 25616384
                    Iteration time: 2.61s
                        Total time: 8163.00s
                               ETA: 513937.4s

################################################################################
                    [1m Learning iteration 3127/200000 [0m

                       Computation: 3183 steps/s (collection: 0.495s, learning 2.079s)
               Value function loss: 57617.5826
                    Surrogate loss: 0.0122
             Mean action noise std: 0.88
                       Mean reward: 11379.93
               Mean episode length: 429.18
                 Mean success rate: 85.50
                  Mean reward/step: 26.63
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 25624576
                    Iteration time: 2.57s
                        Total time: 8165.57s
                               ETA: 513932.4s

################################################################################
                    [1m Learning iteration 3128/200000 [0m

                       Computation: 3044 steps/s (collection: 0.544s, learning 2.146s)
               Value function loss: 89254.9465
                    Surrogate loss: 0.0117
             Mean action noise std: 0.88
                       Mean reward: 11332.70
               Mean episode length: 428.58
                 Mean success rate: 85.00
                  Mean reward/step: 26.69
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 25632768
                    Iteration time: 2.69s
                        Total time: 8168.26s
                               ETA: 513934.9s

################################################################################
                    [1m Learning iteration 3129/200000 [0m

                       Computation: 3091 steps/s (collection: 0.523s, learning 2.126s)
               Value function loss: 97844.8038
                    Surrogate loss: 0.0110
             Mean action noise std: 0.88
                       Mean reward: 11483.25
               Mean episode length: 430.98
                 Mean success rate: 85.50
                  Mean reward/step: 26.77
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 25640960
                    Iteration time: 2.65s
                        Total time: 8170.91s
                               ETA: 513934.7s

################################################################################
                    [1m Learning iteration 3130/200000 [0m

                       Computation: 3096 steps/s (collection: 0.517s, learning 2.128s)
               Value function loss: 112633.1416
                    Surrogate loss: 0.0110
             Mean action noise std: 0.88
                       Mean reward: 11473.40
               Mean episode length: 430.92
                 Mean success rate: 85.00
                  Mean reward/step: 27.45
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 25649152
                    Iteration time: 2.65s
                        Total time: 8173.56s
                               ETA: 513934.3s

################################################################################
                    [1m Learning iteration 3131/200000 [0m

                       Computation: 3200 steps/s (collection: 0.482s, learning 2.078s)
               Value function loss: 86739.2322
                    Surrogate loss: 0.0123
             Mean action noise std: 0.88
                       Mean reward: 11467.98
               Mean episode length: 431.90
                 Mean success rate: 85.00
                  Mean reward/step: 27.44
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 2.56s
                        Total time: 8176.12s
                               ETA: 513928.5s

################################################################################
                    [1m Learning iteration 3132/200000 [0m

                       Computation: 3073 steps/s (collection: 0.547s, learning 2.118s)
               Value function loss: 85727.7646
                    Surrogate loss: 0.0135
             Mean action noise std: 0.88
                       Mean reward: 11702.41
               Mean episode length: 436.61
                 Mean success rate: 86.50
                  Mean reward/step: 27.12
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 25665536
                    Iteration time: 2.67s
                        Total time: 8178.78s
                               ETA: 513929.4s

################################################################################
                    [1m Learning iteration 3133/200000 [0m

                       Computation: 3098 steps/s (collection: 0.551s, learning 2.093s)
               Value function loss: 74117.1326
                    Surrogate loss: 0.0132
             Mean action noise std: 0.88
                       Mean reward: 11534.47
               Mean episode length: 433.63
                 Mean success rate: 85.50
                  Mean reward/step: 26.84
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 25673728
                    Iteration time: 2.64s
                        Total time: 8181.43s
                               ETA: 513928.9s

################################################################################
                    [1m Learning iteration 3134/200000 [0m

                       Computation: 3171 steps/s (collection: 0.450s, learning 2.133s)
               Value function loss: 143361.5820
                    Surrogate loss: 0.0165
             Mean action noise std: 0.88
                       Mean reward: 11653.20
               Mean episode length: 436.87
                 Mean success rate: 85.50
                  Mean reward/step: 26.85
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 25681920
                    Iteration time: 2.58s
                        Total time: 8184.01s
                               ETA: 513924.5s

################################################################################
                    [1m Learning iteration 3135/200000 [0m

                       Computation: 3084 steps/s (collection: 0.530s, learning 2.126s)
               Value function loss: 134551.0680
                    Surrogate loss: 0.0124
             Mean action noise std: 0.88
                       Mean reward: 11836.86
               Mean episode length: 441.59
                 Mean success rate: 86.50
                  Mean reward/step: 25.78
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 25690112
                    Iteration time: 2.66s
                        Total time: 8186.67s
                               ETA: 513924.8s

################################################################################
                    [1m Learning iteration 3136/200000 [0m

                       Computation: 3116 steps/s (collection: 0.500s, learning 2.128s)
               Value function loss: 63343.2305
                    Surrogate loss: 0.0120
             Mean action noise std: 0.88
                       Mean reward: 11712.04
               Mean episode length: 438.38
                 Mean success rate: 85.50
                  Mean reward/step: 25.40
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 25698304
                    Iteration time: 2.63s
                        Total time: 8189.30s
                               ETA: 513923.3s

################################################################################
                    [1m Learning iteration 3137/200000 [0m

                       Computation: 3140 steps/s (collection: 0.474s, learning 2.134s)
               Value function loss: 99235.9306
                    Surrogate loss: 0.0126
             Mean action noise std: 0.88
                       Mean reward: 11752.75
               Mean episode length: 440.13
                 Mean success rate: 86.00
                  Mean reward/step: 25.98
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 25706496
                    Iteration time: 2.61s
                        Total time: 8191.90s
                               ETA: 513920.5s

################################################################################
                    [1m Learning iteration 3138/200000 [0m

                       Computation: 3156 steps/s (collection: 0.491s, learning 2.105s)
               Value function loss: 149719.7477
                    Surrogate loss: 0.0132
             Mean action noise std: 0.88
                       Mean reward: 11279.25
               Mean episode length: 423.07
                 Mean success rate: 82.50
                  Mean reward/step: 25.35
       Mean episode length/episode: 27.77
--------------------------------------------------------------------------------
                   Total timesteps: 25714688
                    Iteration time: 2.60s
                        Total time: 8194.50s
                               ETA: 513917.0s

################################################################################
                    [1m Learning iteration 3139/200000 [0m

                       Computation: 3113 steps/s (collection: 0.524s, learning 2.107s)
               Value function loss: 103931.8677
                    Surrogate loss: 0.0115
             Mean action noise std: 0.88
                       Mean reward: 11140.02
               Mean episode length: 420.04
                 Mean success rate: 81.50
                  Mean reward/step: 24.92
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 25722880
                    Iteration time: 2.63s
                        Total time: 8197.13s
                               ETA: 513915.7s

################################################################################
                    [1m Learning iteration 3140/200000 [0m

                       Computation: 3197 steps/s (collection: 0.501s, learning 2.061s)
               Value function loss: 86277.6567
                    Surrogate loss: 0.0117
             Mean action noise std: 0.88
                       Mean reward: 10896.30
               Mean episode length: 412.64
                 Mean success rate: 80.00
                  Mean reward/step: 25.29
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 25731072
                    Iteration time: 2.56s
                        Total time: 8199.69s
                               ETA: 513910.0s

################################################################################
                    [1m Learning iteration 3141/200000 [0m

                       Computation: 3097 steps/s (collection: 0.495s, learning 2.150s)
               Value function loss: 111624.1938
                    Surrogate loss: 0.0112
             Mean action noise std: 0.88
                       Mean reward: 11029.89
               Mean episode length: 415.92
                 Mean success rate: 81.00
                  Mean reward/step: 26.53
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 25739264
                    Iteration time: 2.65s
                        Total time: 8202.34s
                               ETA: 513909.6s

################################################################################
                    [1m Learning iteration 3142/200000 [0m

                       Computation: 3160 steps/s (collection: 0.502s, learning 2.090s)
               Value function loss: 109253.8255
                    Surrogate loss: 0.0122
             Mean action noise std: 0.88
                       Mean reward: 11138.44
               Mean episode length: 416.73
                 Mean success rate: 81.50
                  Mean reward/step: 26.65
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 25747456
                    Iteration time: 2.59s
                        Total time: 8204.93s
                               ETA: 513905.8s

################################################################################
                    [1m Learning iteration 3143/200000 [0m

                       Computation: 3182 steps/s (collection: 0.471s, learning 2.103s)
               Value function loss: 80713.3814
                    Surrogate loss: 0.0098
             Mean action noise std: 0.88
                       Mean reward: 11270.89
               Mean episode length: 420.62
                 Mean success rate: 82.00
                  Mean reward/step: 26.57
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 2.57s
                        Total time: 8207.50s
                               ETA: 513900.9s

################################################################################
                    [1m Learning iteration 3144/200000 [0m

                       Computation: 3229 steps/s (collection: 0.462s, learning 2.074s)
               Value function loss: 124797.5846
                    Surrogate loss: 0.0117
             Mean action noise std: 0.88
                       Mean reward: 10955.56
               Mean episode length: 412.13
                 Mean success rate: 80.00
                  Mean reward/step: 27.01
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 25763840
                    Iteration time: 2.54s
                        Total time: 8210.04s
                               ETA: 513893.6s

################################################################################
                    [1m Learning iteration 3145/200000 [0m

                       Computation: 3155 steps/s (collection: 0.495s, learning 2.101s)
               Value function loss: 47242.9553
                    Surrogate loss: 0.0093
             Mean action noise std: 0.88
                       Mean reward: 10744.55
               Mean episode length: 406.31
                 Mean success rate: 79.50
                  Mean reward/step: 26.65
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 25772032
                    Iteration time: 2.60s
                        Total time: 8212.63s
                               ETA: 513890.1s

################################################################################
                    [1m Learning iteration 3146/200000 [0m

                       Computation: 3168 steps/s (collection: 0.463s, learning 2.122s)
               Value function loss: 113840.4974
                    Surrogate loss: 0.0113
             Mean action noise std: 0.88
                       Mean reward: 10691.60
               Mean episode length: 406.26
                 Mean success rate: 79.50
                  Mean reward/step: 26.84
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 25780224
                    Iteration time: 2.59s
                        Total time: 8215.22s
                               ETA: 513885.9s

################################################################################
                    [1m Learning iteration 3147/200000 [0m

                       Computation: 3162 steps/s (collection: 0.508s, learning 2.083s)
               Value function loss: 68677.9741
                    Surrogate loss: 0.0115
             Mean action noise std: 0.88
                       Mean reward: 10816.11
               Mean episode length: 410.60
                 Mean success rate: 81.00
                  Mean reward/step: 27.12
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 25788416
                    Iteration time: 2.59s
                        Total time: 8217.81s
                               ETA: 513882.0s

################################################################################
                    [1m Learning iteration 3148/200000 [0m

                       Computation: 3230 steps/s (collection: 0.454s, learning 2.082s)
               Value function loss: 69285.0833
                    Surrogate loss: 0.0096
             Mean action noise std: 0.88
                       Mean reward: 10458.13
               Mean episode length: 401.54
                 Mean success rate: 79.00
                  Mean reward/step: 27.52
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 25796608
                    Iteration time: 2.54s
                        Total time: 8220.35s
                               ETA: 513874.7s

################################################################################
                    [1m Learning iteration 3149/200000 [0m

                       Computation: 3153 steps/s (collection: 0.500s, learning 2.098s)
               Value function loss: 87395.0891
                    Surrogate loss: 0.0092
             Mean action noise std: 0.88
                       Mean reward: 11005.89
               Mean episode length: 418.91
                 Mean success rate: 82.50
                  Mean reward/step: 28.27
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 25804800
                    Iteration time: 2.60s
                        Total time: 8222.94s
                               ETA: 513871.4s

################################################################################
                    [1m Learning iteration 3150/200000 [0m

                       Computation: 3221 steps/s (collection: 0.467s, learning 2.076s)
               Value function loss: 91340.7848
                    Surrogate loss: 0.0124
             Mean action noise std: 0.88
                       Mean reward: 11237.02
               Mean episode length: 426.19
                 Mean success rate: 85.00
                  Mean reward/step: 27.47
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 25812992
                    Iteration time: 2.54s
                        Total time: 8225.49s
                               ETA: 513864.5s

################################################################################
                    [1m Learning iteration 3151/200000 [0m

                       Computation: 3316 steps/s (collection: 0.438s, learning 2.032s)
               Value function loss: 139233.4416
                    Surrogate loss: 0.0110
             Mean action noise std: 0.88
                       Mean reward: 11495.61
               Mean episode length: 433.24
                 Mean success rate: 86.50
                  Mean reward/step: 26.74
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 25821184
                    Iteration time: 2.47s
                        Total time: 8227.96s
                               ETA: 513853.1s

################################################################################
                    [1m Learning iteration 3152/200000 [0m

                       Computation: 3180 steps/s (collection: 0.481s, learning 2.094s)
               Value function loss: 93664.5020
                    Surrogate loss: 0.0100
             Mean action noise std: 0.88
                       Mean reward: 11249.37
               Mean episode length: 424.42
                 Mean success rate: 85.00
                  Mean reward/step: 27.32
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 25829376
                    Iteration time: 2.58s
                        Total time: 8230.53s
                               ETA: 513848.3s

################################################################################
                    [1m Learning iteration 3153/200000 [0m

                       Computation: 3178 steps/s (collection: 0.489s, learning 2.089s)
               Value function loss: 130372.4865
                    Surrogate loss: 0.0114
             Mean action noise std: 0.88
                       Mean reward: 11130.30
               Mean episode length: 421.99
                 Mean success rate: 84.50
                  Mean reward/step: 27.07
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 25837568
                    Iteration time: 2.58s
                        Total time: 8233.11s
                               ETA: 513843.7s

################################################################################
                    [1m Learning iteration 3154/200000 [0m

                       Computation: 3179 steps/s (collection: 0.485s, learning 2.091s)
               Value function loss: 135081.1582
                    Surrogate loss: 0.0123
             Mean action noise std: 0.88
                       Mean reward: 11224.50
               Mean episode length: 426.15
                 Mean success rate: 86.00
                  Mean reward/step: 25.73
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 25845760
                    Iteration time: 2.58s
                        Total time: 8235.69s
                               ETA: 513838.9s

################################################################################
                    [1m Learning iteration 3155/200000 [0m

                       Computation: 3231 steps/s (collection: 0.458s, learning 2.077s)
               Value function loss: 103357.1566
                    Surrogate loss: 0.0126
             Mean action noise std: 0.88
                       Mean reward: 11528.59
               Mean episode length: 432.80
                 Mean success rate: 87.00
                  Mean reward/step: 24.78
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 2.54s
                        Total time: 8238.22s
                               ETA: 513831.7s

################################################################################
                    [1m Learning iteration 3156/200000 [0m

                       Computation: 3185 steps/s (collection: 0.484s, learning 2.088s)
               Value function loss: 84618.3324
                    Surrogate loss: 0.0150
             Mean action noise std: 0.88
                       Mean reward: 11467.00
               Mean episode length: 429.42
                 Mean success rate: 86.50
                  Mean reward/step: 25.98
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 25862144
                    Iteration time: 2.57s
                        Total time: 8240.79s
                               ETA: 513826.6s

################################################################################
                    [1m Learning iteration 3157/200000 [0m

                       Computation: 3271 steps/s (collection: 0.437s, learning 2.067s)
               Value function loss: 130833.2479
                    Surrogate loss: 0.0134
             Mean action noise std: 0.88
                       Mean reward: 11625.17
               Mean episode length: 431.80
                 Mean success rate: 86.50
                  Mean reward/step: 26.32
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 25870336
                    Iteration time: 2.50s
                        Total time: 8243.30s
                               ETA: 513817.4s

################################################################################
                    [1m Learning iteration 3158/200000 [0m

                       Computation: 3196 steps/s (collection: 0.439s, learning 2.123s)
               Value function loss: 79053.5553
                    Surrogate loss: 0.0150
             Mean action noise std: 0.88
                       Mean reward: 11832.97
               Mean episode length: 438.98
                 Mean success rate: 88.00
                  Mean reward/step: 26.09
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 25878528
                    Iteration time: 2.56s
                        Total time: 8245.86s
                               ETA: 513811.8s

################################################################################
                    [1m Learning iteration 3159/200000 [0m

                       Computation: 3212 steps/s (collection: 0.474s, learning 2.075s)
               Value function loss: 93199.0073
                    Surrogate loss: 0.0106
             Mean action noise std: 0.88
                       Mean reward: 11581.05
               Mean episode length: 429.18
                 Mean success rate: 86.00
                  Mean reward/step: 26.93
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 25886720
                    Iteration time: 2.55s
                        Total time: 8248.41s
                               ETA: 513805.4s

################################################################################
                    [1m Learning iteration 3160/200000 [0m

                       Computation: 3189 steps/s (collection: 0.481s, learning 2.087s)
               Value function loss: 104508.7085
                    Surrogate loss: 0.0126
             Mean action noise std: 0.88
                       Mean reward: 11516.62
               Mean episode length: 428.12
                 Mean success rate: 85.50
                  Mean reward/step: 26.46
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 25894912
                    Iteration time: 2.57s
                        Total time: 8250.98s
                               ETA: 513800.2s

################################################################################
                    [1m Learning iteration 3161/200000 [0m

                       Computation: 3173 steps/s (collection: 0.512s, learning 2.069s)
               Value function loss: 85967.8811
                    Surrogate loss: 0.0080
             Mean action noise std: 0.88
                       Mean reward: 11264.59
               Mean episode length: 421.54
                 Mean success rate: 84.00
                  Mean reward/step: 26.51
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 25903104
                    Iteration time: 2.58s
                        Total time: 8253.56s
                               ETA: 513795.8s

################################################################################
                    [1m Learning iteration 3162/200000 [0m

                       Computation: 3144 steps/s (collection: 0.491s, learning 2.115s)
               Value function loss: 95304.3726
                    Surrogate loss: 0.0080
             Mean action noise std: 0.88
                       Mean reward: 11385.59
               Mean episode length: 426.73
                 Mean success rate: 84.50
                  Mean reward/step: 27.06
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 25911296
                    Iteration time: 2.61s
                        Total time: 8256.16s
                               ETA: 513792.9s

################################################################################
                    [1m Learning iteration 3163/200000 [0m

                       Computation: 3196 steps/s (collection: 0.441s, learning 2.122s)
               Value function loss: 82661.5083
                    Surrogate loss: 0.0074
             Mean action noise std: 0.88
                       Mean reward: 11192.20
               Mean episode length: 421.36
                 Mean success rate: 83.00
                  Mean reward/step: 27.84
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 25919488
                    Iteration time: 2.56s
                        Total time: 8258.73s
                               ETA: 513787.3s

################################################################################
                    [1m Learning iteration 3164/200000 [0m

                       Computation: 3212 steps/s (collection: 0.459s, learning 2.092s)
               Value function loss: 77410.2106
                    Surrogate loss: 0.0123
             Mean action noise std: 0.88
                       Mean reward: 11324.61
               Mean episode length: 425.13
                 Mean success rate: 84.00
                  Mean reward/step: 28.51
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 25927680
                    Iteration time: 2.55s
                        Total time: 8261.28s
                               ETA: 513781.0s

################################################################################
                    [1m Learning iteration 3165/200000 [0m

                       Computation: 3188 steps/s (collection: 0.479s, learning 2.090s)
               Value function loss: 98574.0698
                    Surrogate loss: 0.0103
             Mean action noise std: 0.88
                       Mean reward: 11456.07
               Mean episode length: 429.53
                 Mean success rate: 84.50
                  Mean reward/step: 27.98
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 25935872
                    Iteration time: 2.57s
                        Total time: 8263.85s
                               ETA: 513775.8s

################################################################################
                    [1m Learning iteration 3166/200000 [0m

                       Computation: 3276 steps/s (collection: 0.454s, learning 2.046s)
               Value function loss: 120905.1857
                    Surrogate loss: 0.0113
             Mean action noise std: 0.88
                       Mean reward: 11560.59
               Mean episode length: 433.36
                 Mean success rate: 85.50
                  Mean reward/step: 27.69
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 25944064
                    Iteration time: 2.50s
                        Total time: 8266.35s
                               ETA: 513766.4s

################################################################################
                    [1m Learning iteration 3167/200000 [0m

                       Computation: 3252 steps/s (collection: 0.439s, learning 2.080s)
               Value function loss: 84424.5548
                    Surrogate loss: 0.0148
             Mean action noise std: 0.88
                       Mean reward: 11427.32
               Mean episode length: 430.17
                 Mean success rate: 85.00
                  Mean reward/step: 27.26
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 2.52s
                        Total time: 8268.87s
                               ETA: 513758.1s

################################################################################
                    [1m Learning iteration 3168/200000 [0m

                       Computation: 3227 steps/s (collection: 0.484s, learning 2.054s)
               Value function loss: 93834.1062
                    Surrogate loss: 0.0133
             Mean action noise std: 0.88
                       Mean reward: 11638.83
               Mean episode length: 436.46
                 Mean success rate: 86.50
                  Mean reward/step: 27.42
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 25960448
                    Iteration time: 2.54s
                        Total time: 8271.40s
                               ETA: 513751.0s

################################################################################
                    [1m Learning iteration 3169/200000 [0m

                       Computation: 3168 steps/s (collection: 0.502s, learning 2.083s)
               Value function loss: 155251.9926
                    Surrogate loss: 0.0119
             Mean action noise std: 0.88
                       Mean reward: 11596.42
               Mean episode length: 436.85
                 Mean success rate: 86.50
                  Mean reward/step: 26.74
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 25968640
                    Iteration time: 2.59s
                        Total time: 8273.99s
                               ETA: 513746.8s

################################################################################
                    [1m Learning iteration 3170/200000 [0m

                       Computation: 3261 steps/s (collection: 0.465s, learning 2.046s)
               Value function loss: 102214.2241
                    Surrogate loss: 0.0095
             Mean action noise std: 0.88
                       Mean reward: 12101.73
               Mean episode length: 452.77
                 Mean success rate: 89.50
                  Mean reward/step: 26.16
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 25976832
                    Iteration time: 2.51s
                        Total time: 8276.50s
                               ETA: 513738.1s

################################################################################
                    [1m Learning iteration 3171/200000 [0m

                       Computation: 3228 steps/s (collection: 0.473s, learning 2.065s)
               Value function loss: 95930.1258
                    Surrogate loss: 0.0109
             Mean action noise std: 0.88
                       Mean reward: 12255.08
               Mean episode length: 453.83
                 Mean success rate: 90.00
                  Mean reward/step: 26.16
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 25985024
                    Iteration time: 2.54s
                        Total time: 8279.04s
                               ETA: 513731.0s

################################################################################
                    [1m Learning iteration 3172/200000 [0m

                       Computation: 3211 steps/s (collection: 0.463s, learning 2.088s)
               Value function loss: 116051.0721
                    Surrogate loss: 0.0120
             Mean action noise std: 0.88
                       Mean reward: 12227.01
               Mean episode length: 452.50
                 Mean success rate: 90.00
                  Mean reward/step: 26.49
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 25993216
                    Iteration time: 2.55s
                        Total time: 8281.59s
                               ETA: 513724.7s

################################################################################
                    [1m Learning iteration 3173/200000 [0m

                       Computation: 3183 steps/s (collection: 0.480s, learning 2.093s)
               Value function loss: 115448.3288
                    Surrogate loss: 0.0138
             Mean action noise std: 0.88
                       Mean reward: 12169.06
               Mean episode length: 452.65
                 Mean success rate: 90.00
                  Mean reward/step: 26.29
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 26001408
                    Iteration time: 2.57s
                        Total time: 8284.16s
                               ETA: 513719.8s

################################################################################
                    [1m Learning iteration 3174/200000 [0m

                       Computation: 3174 steps/s (collection: 0.507s, learning 2.073s)
               Value function loss: 82971.3458
                    Surrogate loss: 0.0121
             Mean action noise std: 0.88
                       Mean reward: 12096.23
               Mean episode length: 448.57
                 Mean success rate: 89.50
                  Mean reward/step: 26.15
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 26009600
                    Iteration time: 2.58s
                        Total time: 8286.74s
                               ETA: 513715.4s

################################################################################
                    [1m Learning iteration 3175/200000 [0m

                       Computation: 3294 steps/s (collection: 0.458s, learning 2.029s)
               Value function loss: 101323.5575
                    Surrogate loss: 0.0096
             Mean action noise std: 0.88
                       Mean reward: 11818.40
               Mean episode length: 438.32
                 Mean success rate: 87.00
                  Mean reward/step: 26.63
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 26017792
                    Iteration time: 2.49s
                        Total time: 8289.23s
                               ETA: 513705.1s

################################################################################
                    [1m Learning iteration 3176/200000 [0m

                       Computation: 3200 steps/s (collection: 0.502s, learning 2.058s)
               Value function loss: 103093.2140
                    Surrogate loss: 0.0140
             Mean action noise std: 0.88
                       Mean reward: 11844.62
               Mean episode length: 438.32
                 Mean success rate: 87.00
                  Mean reward/step: 26.12
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 26025984
                    Iteration time: 2.56s
                        Total time: 8291.79s
                               ETA: 513699.4s

################################################################################
                    [1m Learning iteration 3177/200000 [0m

                       Computation: 3185 steps/s (collection: 0.473s, learning 2.098s)
               Value function loss: 124950.1225
                    Surrogate loss: 0.0149
             Mean action noise std: 0.88
                       Mean reward: 11918.58
               Mean episode length: 440.69
                 Mean success rate: 87.00
                  Mean reward/step: 25.60
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 26034176
                    Iteration time: 2.57s
                        Total time: 8294.36s
                               ETA: 513694.4s

################################################################################
                    [1m Learning iteration 3178/200000 [0m

                       Computation: 3244 steps/s (collection: 0.472s, learning 2.053s)
               Value function loss: 63372.4520
                    Surrogate loss: 0.0141
             Mean action noise std: 0.88
                       Mean reward: 11693.67
               Mean episode length: 434.44
                 Mean success rate: 85.50
                  Mean reward/step: 25.48
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 26042368
                    Iteration time: 2.53s
                        Total time: 8296.88s
                               ETA: 513686.5s

################################################################################
                    [1m Learning iteration 3179/200000 [0m

                       Computation: 3180 steps/s (collection: 0.492s, learning 2.083s)
               Value function loss: 67594.0178
                    Surrogate loss: 0.0133
             Mean action noise std: 0.88
                       Mean reward: 11316.23
               Mean episode length: 421.85
                 Mean success rate: 82.50
                  Mean reward/step: 26.03
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 2.58s
                        Total time: 8299.46s
                               ETA: 513681.8s

################################################################################
                    [1m Learning iteration 3180/200000 [0m

                       Computation: 3266 steps/s (collection: 0.473s, learning 2.035s)
               Value function loss: 95996.2569
                    Surrogate loss: 0.0159
             Mean action noise std: 0.88
                       Mean reward: 11069.11
               Mean episode length: 413.43
                 Mean success rate: 81.00
                  Mean reward/step: 26.49
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 26058752
                    Iteration time: 2.51s
                        Total time: 8301.97s
                               ETA: 513672.8s

################################################################################
                    [1m Learning iteration 3181/200000 [0m

                       Computation: 3257 steps/s (collection: 0.452s, learning 2.063s)
               Value function loss: 130795.2916
                    Surrogate loss: 0.0114
             Mean action noise std: 0.88
                       Mean reward: 10975.35
               Mean episode length: 411.63
                 Mean success rate: 80.50
                  Mean reward/step: 25.99
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 26066944
                    Iteration time: 2.51s
                        Total time: 8304.48s
                               ETA: 513664.3s

################################################################################
                    [1m Learning iteration 3182/200000 [0m

                       Computation: 3214 steps/s (collection: 0.486s, learning 2.063s)
               Value function loss: 95431.5157
                    Surrogate loss: 0.0147
             Mean action noise std: 0.88
                       Mean reward: 10648.59
               Mean episode length: 403.52
                 Mean success rate: 78.50
                  Mean reward/step: 25.88
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 26075136
                    Iteration time: 2.55s
                        Total time: 8307.03s
                               ETA: 513657.9s

################################################################################
                    [1m Learning iteration 3183/200000 [0m

                       Computation: 3227 steps/s (collection: 0.456s, learning 2.082s)
               Value function loss: 85125.7553
                    Surrogate loss: 0.0118
             Mean action noise std: 0.88
                       Mean reward: 10631.67
               Mean episode length: 403.04
                 Mean success rate: 78.50
                  Mean reward/step: 25.79
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 26083328
                    Iteration time: 2.54s
                        Total time: 8309.57s
                               ETA: 513650.9s

################################################################################
                    [1m Learning iteration 3184/200000 [0m

                       Computation: 3224 steps/s (collection: 0.457s, learning 2.083s)
               Value function loss: 92031.1199
                    Surrogate loss: 0.0156
             Mean action noise std: 0.88
                       Mean reward: 10734.46
               Mean episode length: 408.41
                 Mean success rate: 79.00
                  Mean reward/step: 26.22
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 26091520
                    Iteration time: 2.54s
                        Total time: 8312.11s
                               ETA: 513644.0s

################################################################################
                    [1m Learning iteration 3185/200000 [0m

                       Computation: 3276 steps/s (collection: 0.434s, learning 2.066s)
               Value function loss: 117743.9194
                    Surrogate loss: 0.0190
             Mean action noise std: 0.88
                       Mean reward: 10886.00
               Mean episode length: 416.16
                 Mean success rate: 80.50
                  Mean reward/step: 26.18
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 26099712
                    Iteration time: 2.50s
                        Total time: 8314.61s
                               ETA: 513634.6s

################################################################################
                    [1m Learning iteration 3186/200000 [0m

                       Computation: 3202 steps/s (collection: 0.475s, learning 2.084s)
               Value function loss: 98388.8067
                    Surrogate loss: 0.0130
             Mean action noise std: 0.88
                       Mean reward: 10732.07
               Mean episode length: 412.03
                 Mean success rate: 79.50
                  Mean reward/step: 26.05
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 26107904
                    Iteration time: 2.56s
                        Total time: 8317.17s
                               ETA: 513628.8s

################################################################################
                    [1m Learning iteration 3187/200000 [0m

                       Computation: 3249 steps/s (collection: 0.450s, learning 2.071s)
               Value function loss: 67625.0886
                    Surrogate loss: 0.0072
             Mean action noise std: 0.88
                       Mean reward: 10740.65
               Mean episode length: 411.26
                 Mean success rate: 79.50
                  Mean reward/step: 26.13
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 26116096
                    Iteration time: 2.52s
                        Total time: 8319.69s
                               ETA: 513620.7s

################################################################################
                    [1m Learning iteration 3188/200000 [0m

                       Computation: 3236 steps/s (collection: 0.471s, learning 2.061s)
               Value function loss: 134700.9319
                    Surrogate loss: 0.0103
             Mean action noise std: 0.88
                       Mean reward: 10765.68
               Mean episode length: 412.40
                 Mean success rate: 79.50
                  Mean reward/step: 26.53
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 26124288
                    Iteration time: 2.53s
                        Total time: 8322.22s
                               ETA: 513613.3s

################################################################################
                    [1m Learning iteration 3189/200000 [0m

                       Computation: 3265 steps/s (collection: 0.465s, learning 2.044s)
               Value function loss: 100292.6989
                    Surrogate loss: 0.0130
             Mean action noise std: 0.88
                       Mean reward: 11256.97
               Mean episode length: 426.49
                 Mean success rate: 83.00
                  Mean reward/step: 25.97
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 26132480
                    Iteration time: 2.51s
                        Total time: 8324.73s
                               ETA: 513604.4s

################################################################################
                    [1m Learning iteration 3190/200000 [0m

                       Computation: 3209 steps/s (collection: 0.473s, learning 2.080s)
               Value function loss: 90825.8769
                    Surrogate loss: 0.0097
             Mean action noise std: 0.88
                       Mean reward: 11115.77
               Mean episode length: 425.11
                 Mean success rate: 82.00
                  Mean reward/step: 26.07
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 26140672
                    Iteration time: 2.55s
                        Total time: 8327.28s
                               ETA: 513598.3s

################################################################################
                    [1m Learning iteration 3191/200000 [0m

                       Computation: 3305 steps/s (collection: 0.445s, learning 2.033s)
               Value function loss: 104281.4851
                    Surrogate loss: 0.0111
             Mean action noise std: 0.88
                       Mean reward: 11062.56
               Mean episode length: 423.10
                 Mean success rate: 81.50
                  Mean reward/step: 26.67
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 2.48s
                        Total time: 8329.76s
                               ETA: 513587.6s

################################################################################
                    [1m Learning iteration 3192/200000 [0m

                       Computation: 3209 steps/s (collection: 0.478s, learning 2.075s)
               Value function loss: 90101.7416
                    Surrogate loss: 0.0121
             Mean action noise std: 0.88
                       Mean reward: 11267.82
               Mean episode length: 427.25
                 Mean success rate: 82.00
                  Mean reward/step: 26.91
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 26157056
                    Iteration time: 2.55s
                        Total time: 8332.31s
                               ETA: 513581.4s

################################################################################
                    [1m Learning iteration 3193/200000 [0m

                       Computation: 3151 steps/s (collection: 0.478s, learning 2.121s)
               Value function loss: 122643.1938
                    Surrogate loss: 0.0144
             Mean action noise std: 0.88
                       Mean reward: 11614.21
               Mean episode length: 438.97
                 Mean success rate: 84.50
                  Mean reward/step: 26.51
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 26165248
                    Iteration time: 2.60s
                        Total time: 8334.91s
                               ETA: 513578.2s

################################################################################
                    [1m Learning iteration 3194/200000 [0m

                       Computation: 3240 steps/s (collection: 0.470s, learning 2.058s)
               Value function loss: 93399.4410
                    Surrogate loss: 0.0130
             Mean action noise std: 0.88
                       Mean reward: 11614.21
               Mean episode length: 438.17
                 Mean success rate: 84.50
                  Mean reward/step: 26.81
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 26173440
                    Iteration time: 2.53s
                        Total time: 8337.44s
                               ETA: 513570.5s

################################################################################
                    [1m Learning iteration 3195/200000 [0m

                       Computation: 3174 steps/s (collection: 0.478s, learning 2.103s)
               Value function loss: 104411.4318
                    Surrogate loss: 0.0119
             Mean action noise std: 0.88
                       Mean reward: 11770.68
               Mean episode length: 443.51
                 Mean success rate: 85.50
                  Mean reward/step: 26.88
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 26181632
                    Iteration time: 2.58s
                        Total time: 8340.02s
                               ETA: 513566.1s

################################################################################
                    [1m Learning iteration 3196/200000 [0m

                       Computation: 3202 steps/s (collection: 0.505s, learning 2.053s)
               Value function loss: 96825.2428
                    Surrogate loss: 0.0120
             Mean action noise std: 0.88
                       Mean reward: 11431.25
               Mean episode length: 430.84
                 Mean success rate: 83.50
                  Mean reward/step: 27.11
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 26189824
                    Iteration time: 2.56s
                        Total time: 8342.58s
                               ETA: 513560.4s

################################################################################
                    [1m Learning iteration 3197/200000 [0m

                       Computation: 3174 steps/s (collection: 0.493s, learning 2.088s)
               Value function loss: 120785.8813
                    Surrogate loss: 0.0110
             Mean action noise std: 0.88
                       Mean reward: 11401.94
               Mean episode length: 430.43
                 Mean success rate: 83.00
                  Mean reward/step: 26.87
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 26198016
                    Iteration time: 2.58s
                        Total time: 8345.16s
                               ETA: 513556.0s

################################################################################
                    [1m Learning iteration 3198/200000 [0m

                       Computation: 3250 steps/s (collection: 0.482s, learning 2.038s)
               Value function loss: 125896.4654
                    Surrogate loss: 0.0118
             Mean action noise std: 0.88
                       Mean reward: 11511.70
               Mean episode length: 433.99
                 Mean success rate: 84.50
                  Mean reward/step: 26.60
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 26206208
                    Iteration time: 2.52s
                        Total time: 8347.68s
                               ETA: 513547.8s

################################################################################
                    [1m Learning iteration 3199/200000 [0m

                       Computation: 3244 steps/s (collection: 0.462s, learning 2.062s)
               Value function loss: 103945.8474
                    Surrogate loss: 0.0169
             Mean action noise std: 0.88
                       Mean reward: 11679.94
               Mean episode length: 438.67
                 Mean success rate: 85.50
                  Mean reward/step: 26.66
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 26214400
                    Iteration time: 2.52s
                        Total time: 8350.20s
                               ETA: 513540.0s

################################################################################
                    [1m Learning iteration 3200/200000 [0m

                       Computation: 3291 steps/s (collection: 0.448s, learning 2.040s)
               Value function loss: 152006.9266
                    Surrogate loss: 0.0172
             Mean action noise std: 0.88
                       Mean reward: 11710.61
               Mean episode length: 438.87
                 Mean success rate: 85.50
                  Mean reward/step: 26.77
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 26222592
                    Iteration time: 2.49s
                        Total time: 8352.69s
                               ETA: 513530.0s

################################################################################
                    [1m Learning iteration 3201/200000 [0m

                       Computation: 3209 steps/s (collection: 0.469s, learning 2.083s)
               Value function loss: 105025.7881
                    Surrogate loss: 0.0123
             Mean action noise std: 0.88
                       Mean reward: 11848.04
               Mean episode length: 441.56
                 Mean success rate: 87.00
                  Mean reward/step: 26.19
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 26230784
                    Iteration time: 2.55s
                        Total time: 8355.24s
                               ETA: 513523.9s

################################################################################
                    [1m Learning iteration 3202/200000 [0m

                       Computation: 3154 steps/s (collection: 0.496s, learning 2.100s)
               Value function loss: 103093.3902
                    Surrogate loss: 0.0160
             Mean action noise std: 0.88
                       Mean reward: 11706.99
               Mean episode length: 437.12
                 Mean success rate: 86.50
                  Mean reward/step: 25.57
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 26238976
                    Iteration time: 2.60s
                        Total time: 8357.84s
                               ETA: 513520.5s

################################################################################
                    [1m Learning iteration 3203/200000 [0m

                       Computation: 3265 steps/s (collection: 0.452s, learning 2.057s)
               Value function loss: 96241.7093
                    Surrogate loss: 0.0142
             Mean action noise std: 0.88
                       Mean reward: 11743.07
               Mean episode length: 438.31
                 Mean success rate: 87.00
                  Mean reward/step: 25.93
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 2.51s
                        Total time: 8360.35s
                               ETA: 513511.7s

################################################################################
                    [1m Learning iteration 3204/200000 [0m

                       Computation: 3232 steps/s (collection: 0.468s, learning 2.067s)
               Value function loss: 133990.7271
                    Surrogate loss: 0.0137
             Mean action noise std: 0.88
                       Mean reward: 11476.77
               Mean episode length: 431.15
                 Mean success rate: 85.50
                  Mean reward/step: 25.99
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 26255360
                    Iteration time: 2.53s
                        Total time: 8362.88s
                               ETA: 513504.5s

################################################################################
                    [1m Learning iteration 3205/200000 [0m

                       Computation: 3258 steps/s (collection: 0.457s, learning 2.057s)
               Value function loss: 63569.9616
                    Surrogate loss: 0.0139
             Mean action noise std: 0.88
                       Mean reward: 11417.76
               Mean episode length: 428.64
                 Mean success rate: 85.00
                  Mean reward/step: 25.88
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 26263552
                    Iteration time: 2.51s
                        Total time: 8365.40s
                               ETA: 513496.0s

################################################################################
                    [1m Learning iteration 3206/200000 [0m

                       Computation: 3267 steps/s (collection: 0.448s, learning 2.059s)
               Value function loss: 85174.3616
                    Surrogate loss: 0.0140
             Mean action noise std: 0.88
                       Mean reward: 11611.20
               Mean episode length: 436.95
                 Mean success rate: 87.00
                  Mean reward/step: 26.75
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 26271744
                    Iteration time: 2.51s
                        Total time: 8367.90s
                               ETA: 513487.2s

################################################################################
                    [1m Learning iteration 3207/200000 [0m

                       Computation: 3278 steps/s (collection: 0.435s, learning 2.064s)
               Value function loss: 112899.7732
                    Surrogate loss: 0.0104
             Mean action noise std: 0.88
                       Mean reward: 11538.22
               Mean episode length: 435.01
                 Mean success rate: 86.50
                  Mean reward/step: 27.15
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 26279936
                    Iteration time: 2.50s
                        Total time: 8370.40s
                               ETA: 513477.8s

################################################################################
                    [1m Learning iteration 3208/200000 [0m

                       Computation: 3240 steps/s (collection: 0.451s, learning 2.077s)
               Value function loss: 87160.3430
                    Surrogate loss: 0.0111
             Mean action noise std: 0.88
                       Mean reward: 11579.91
               Mean episode length: 437.51
                 Mean success rate: 87.00
                  Mean reward/step: 27.24
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 26288128
                    Iteration time: 2.53s
                        Total time: 8372.93s
                               ETA: 513470.2s

################################################################################
                    [1m Learning iteration 3209/200000 [0m

                       Computation: 3236 steps/s (collection: 0.460s, learning 2.072s)
               Value function loss: 88083.8325
                    Surrogate loss: 0.0139
             Mean action noise std: 0.88
                       Mean reward: 11321.39
               Mean episode length: 430.48
                 Mean success rate: 85.50
                  Mean reward/step: 27.03
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 26296320
                    Iteration time: 2.53s
                        Total time: 8375.46s
                               ETA: 513462.8s

################################################################################
                    [1m Learning iteration 3210/200000 [0m

                       Computation: 3275 steps/s (collection: 0.455s, learning 2.046s)
               Value function loss: 84764.4728
                    Surrogate loss: 0.0137
             Mean action noise std: 0.88
                       Mean reward: 11182.80
               Mean episode length: 426.33
                 Mean success rate: 84.50
                  Mean reward/step: 26.71
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 26304512
                    Iteration time: 2.50s
                        Total time: 8377.96s
                               ETA: 513453.6s

################################################################################
                    [1m Learning iteration 3211/200000 [0m

                       Computation: 3231 steps/s (collection: 0.452s, learning 2.083s)
               Value function loss: 108791.5443
                    Surrogate loss: 0.0124
             Mean action noise std: 0.88
                       Mean reward: 10988.86
               Mean episode length: 420.39
                 Mean success rate: 83.00
                  Mean reward/step: 26.50
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 26312704
                    Iteration time: 2.54s
                        Total time: 8380.50s
                               ETA: 513446.4s

################################################################################
                    [1m Learning iteration 3212/200000 [0m

                       Computation: 3233 steps/s (collection: 0.469s, learning 2.064s)
               Value function loss: 134993.5677
                    Surrogate loss: 0.0129
             Mean action noise std: 0.88
                       Mean reward: 11225.62
               Mean episode length: 427.05
                 Mean success rate: 84.00
                  Mean reward/step: 26.21
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 26320896
                    Iteration time: 2.53s
                        Total time: 8383.03s
                               ETA: 513439.2s

################################################################################
                    [1m Learning iteration 3213/200000 [0m

                       Computation: 3230 steps/s (collection: 0.461s, learning 2.075s)
               Value function loss: 116725.2160
                    Surrogate loss: 0.0104
             Mean action noise std: 0.88
                       Mean reward: 10944.06
               Mean episode length: 418.51
                 Mean success rate: 82.00
                  Mean reward/step: 25.83
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 26329088
                    Iteration time: 2.54s
                        Total time: 8385.57s
                               ETA: 513432.1s

################################################################################
                    [1m Learning iteration 3214/200000 [0m

                       Computation: 3348 steps/s (collection: 0.433s, learning 2.014s)
               Value function loss: 105946.0713
                    Surrogate loss: 0.0120
             Mean action noise std: 0.88
                       Mean reward: 10748.89
               Mean episode length: 412.53
                 Mean success rate: 81.50
                  Mean reward/step: 25.44
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 26337280
                    Iteration time: 2.45s
                        Total time: 8388.01s
                               ETA: 513419.5s

################################################################################
                    [1m Learning iteration 3215/200000 [0m

                       Computation: 3229 steps/s (collection: 0.467s, learning 2.069s)
               Value function loss: 82981.7553
                    Surrogate loss: 0.0103
             Mean action noise std: 0.88
                       Mean reward: 10721.27
               Mean episode length: 410.92
                 Mean success rate: 81.50
                  Mean reward/step: 26.16
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 2.54s
                        Total time: 8390.55s
                               ETA: 513412.4s

################################################################################
                    [1m Learning iteration 3216/200000 [0m

                       Computation: 3037 steps/s (collection: 0.644s, learning 2.053s)
               Value function loss: 143666.7102
                    Surrogate loss: 0.0132
             Mean action noise std: 0.88
                       Mean reward: 10927.68
               Mean episode length: 414.57
                 Mean success rate: 83.00
                  Mean reward/step: 26.29
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 26353664
                    Iteration time: 2.70s
                        Total time: 8393.25s
                               ETA: 513415.2s

################################################################################
                    [1m Learning iteration 3217/200000 [0m

                       Computation: 3260 steps/s (collection: 0.452s, learning 2.061s)
               Value function loss: 80787.5032
                    Surrogate loss: 0.0154
             Mean action noise std: 0.88
                       Mean reward: 10607.63
               Mean episode length: 403.56
                 Mean success rate: 80.50
                  Mean reward/step: 25.72
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 26361856
                    Iteration time: 2.51s
                        Total time: 8395.76s
                               ETA: 513406.7s

################################################################################
                    [1m Learning iteration 3218/200000 [0m

                       Computation: 3285 steps/s (collection: 0.425s, learning 2.068s)
               Value function loss: 94589.5122
                    Surrogate loss: 0.0121
             Mean action noise std: 0.88
                       Mean reward: 10371.53
               Mean episode length: 395.56
                 Mean success rate: 79.50
                  Mean reward/step: 25.43
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 26370048
                    Iteration time: 2.49s
                        Total time: 8398.25s
                               ETA: 513397.0s

################################################################################
                    [1m Learning iteration 3219/200000 [0m

                       Computation: 3279 steps/s (collection: 0.449s, learning 2.049s)
               Value function loss: 148427.0574
                    Surrogate loss: 0.0121
             Mean action noise std: 0.88
                       Mean reward: 10047.27
               Mean episode length: 384.31
                 Mean success rate: 77.50
                  Mean reward/step: 25.54
       Mean episode length/episode: 28.25
--------------------------------------------------------------------------------
                   Total timesteps: 26378240
                    Iteration time: 2.50s
                        Total time: 8400.75s
                               ETA: 513387.6s

################################################################################
                    [1m Learning iteration 3220/200000 [0m

                       Computation: 3262 steps/s (collection: 0.422s, learning 2.089s)
               Value function loss: 126948.5535
                    Surrogate loss: 0.0111
             Mean action noise std: 0.88
                       Mean reward: 10027.89
               Mean episode length: 384.85
                 Mean success rate: 77.50
                  Mean reward/step: 24.76
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 26386432
                    Iteration time: 2.51s
                        Total time: 8403.26s
                               ETA: 513379.0s

################################################################################
                    [1m Learning iteration 3221/200000 [0m

                       Computation: 3310 steps/s (collection: 0.430s, learning 2.044s)
               Value function loss: 83866.1839
                    Surrogate loss: 0.0127
             Mean action noise std: 0.88
                       Mean reward: 9592.00
               Mean episode length: 373.08
                 Mean success rate: 75.00
                  Mean reward/step: 24.73
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 26394624
                    Iteration time: 2.47s
                        Total time: 8405.74s
                               ETA: 513368.2s

################################################################################
                    [1m Learning iteration 3222/200000 [0m

                       Computation: 3244 steps/s (collection: 0.440s, learning 2.085s)
               Value function loss: 90028.7734
                    Surrogate loss: 0.0126
             Mean action noise std: 0.88
                       Mean reward: 9703.99
               Mean episode length: 376.94
                 Mean success rate: 75.50
                  Mean reward/step: 25.58
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 26402816
                    Iteration time: 2.53s
                        Total time: 8408.26s
                               ETA: 513360.4s

################################################################################
                    [1m Learning iteration 3223/200000 [0m

                       Computation: 3315 steps/s (collection: 0.432s, learning 2.039s)
               Value function loss: 77895.9889
                    Surrogate loss: 0.0120
             Mean action noise std: 0.88
                       Mean reward: 9372.94
               Mean episode length: 366.06
                 Mean success rate: 72.50
                  Mean reward/step: 26.45
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 26411008
                    Iteration time: 2.47s
                        Total time: 8410.73s
                               ETA: 513349.4s

################################################################################
                    [1m Learning iteration 3224/200000 [0m

                       Computation: 3245 steps/s (collection: 0.467s, learning 2.057s)
               Value function loss: 93352.6326
                    Surrogate loss: 0.0189
             Mean action noise std: 0.88
                       Mean reward: 9589.52
               Mean episode length: 373.08
                 Mean success rate: 73.50
                  Mean reward/step: 26.24
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 26419200
                    Iteration time: 2.52s
                        Total time: 8413.26s
                               ETA: 513341.6s

################################################################################
                    [1m Learning iteration 3225/200000 [0m

                       Computation: 3290 steps/s (collection: 0.448s, learning 2.041s)
               Value function loss: 64324.3953
                    Surrogate loss: 0.0184
             Mean action noise std: 0.88
                       Mean reward: 9537.29
               Mean episode length: 368.81
                 Mean success rate: 73.50
                  Mean reward/step: 26.16
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 26427392
                    Iteration time: 2.49s
                        Total time: 8415.74s
                               ETA: 513331.7s

################################################################################
                    [1m Learning iteration 3226/200000 [0m

                       Computation: 3299 steps/s (collection: 0.427s, learning 2.055s)
               Value function loss: 114008.5008
                    Surrogate loss: 0.0126
             Mean action noise std: 0.88
                       Mean reward: 9756.14
               Mean episode length: 377.61
                 Mean success rate: 75.00
                  Mean reward/step: 26.37
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 26435584
                    Iteration time: 2.48s
                        Total time: 8418.23s
                               ETA: 513321.4s

################################################################################
                    [1m Learning iteration 3227/200000 [0m

                       Computation: 3254 steps/s (collection: 0.438s, learning 2.079s)
               Value function loss: 109265.9840
                    Surrogate loss: 0.0149
             Mean action noise std: 0.88
                       Mean reward: 9921.95
               Mean episode length: 383.50
                 Mean success rate: 76.00
                  Mean reward/step: 26.01
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 2.52s
                        Total time: 8420.74s
                               ETA: 513313.3s

################################################################################
                    [1m Learning iteration 3228/200000 [0m

                       Computation: 3133 steps/s (collection: 0.509s, learning 2.106s)
               Value function loss: 108121.8978
                    Surrogate loss: 0.0131
             Mean action noise std: 0.88
                       Mean reward: 10201.13
               Mean episode length: 392.73
                 Mean success rate: 77.00
                  Mean reward/step: 25.12
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 26451968
                    Iteration time: 2.61s
                        Total time: 8423.36s
                               ETA: 513311.0s

################################################################################
                    [1m Learning iteration 3229/200000 [0m

                       Computation: 3087 steps/s (collection: 0.550s, learning 2.103s)
               Value function loss: 115679.9980
                    Surrogate loss: 0.0112
             Mean action noise std: 0.88
                       Mean reward: 10190.20
               Mean episode length: 393.95
                 Mean success rate: 78.00
                  Mean reward/step: 25.12
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 26460160
                    Iteration time: 2.65s
                        Total time: 8426.01s
                               ETA: 513311.1s

################################################################################
                    [1m Learning iteration 3230/200000 [0m

                       Computation: 3168 steps/s (collection: 0.489s, learning 2.097s)
               Value function loss: 72077.5178
                    Surrogate loss: 0.0100
             Mean action noise std: 0.88
                       Mean reward: 10425.63
               Mean episode length: 401.27
                 Mean success rate: 79.50
                  Mean reward/step: 25.28
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 26468352
                    Iteration time: 2.59s
                        Total time: 8428.60s
                               ETA: 513307.1s

################################################################################
                    [1m Learning iteration 3231/200000 [0m

                       Computation: 3212 steps/s (collection: 0.463s, learning 2.088s)
               Value function loss: 79031.4004
                    Surrogate loss: 0.0141
             Mean action noise std: 0.88
                       Mean reward: 10283.52
               Mean episode length: 397.23
                 Mean success rate: 78.00
                  Mean reward/step: 25.82
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 26476544
                    Iteration time: 2.55s
                        Total time: 8431.15s
                               ETA: 513301.0s

################################################################################
                    [1m Learning iteration 3232/200000 [0m

                       Computation: 3195 steps/s (collection: 0.484s, learning 2.080s)
               Value function loss: 105226.9299
                    Surrogate loss: 0.0112
             Mean action noise std: 0.88
                       Mean reward: 10868.88
               Mean episode length: 414.62
                 Mean success rate: 82.00
                  Mean reward/step: 25.73
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 26484736
                    Iteration time: 2.56s
                        Total time: 8433.71s
                               ETA: 513295.6s

################################################################################
                    [1m Learning iteration 3233/200000 [0m

                       Computation: 3179 steps/s (collection: 0.501s, learning 2.076s)
               Value function loss: 92618.5143
                    Surrogate loss: 0.0097
             Mean action noise std: 0.88
                       Mean reward: 10873.18
               Mean episode length: 414.43
                 Mean success rate: 82.00
                  Mean reward/step: 25.47
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 26492928
                    Iteration time: 2.58s
                        Total time: 8436.29s
                               ETA: 513291.1s

################################################################################
                    [1m Learning iteration 3234/200000 [0m

                       Computation: 3132 steps/s (collection: 0.516s, learning 2.099s)
               Value function loss: 98292.7061
                    Surrogate loss: 0.0112
             Mean action noise std: 0.88
                       Mean reward: 11440.02
               Mean episode length: 431.84
                 Mean success rate: 86.00
                  Mean reward/step: 26.55
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 26501120
                    Iteration time: 2.62s
                        Total time: 8438.90s
                               ETA: 513288.9s

################################################################################
                    [1m Learning iteration 3235/200000 [0m

                       Computation: 3170 steps/s (collection: 0.475s, learning 2.109s)
               Value function loss: 128713.4803
                    Surrogate loss: 0.0103
             Mean action noise std: 0.88
                       Mean reward: 11724.33
               Mean episode length: 442.59
                 Mean success rate: 88.00
                  Mean reward/step: 26.33
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 26509312
                    Iteration time: 2.58s
                        Total time: 8441.49s
                               ETA: 513284.8s

################################################################################
                    [1m Learning iteration 3236/200000 [0m

                       Computation: 3222 steps/s (collection: 0.444s, learning 2.098s)
               Value function loss: 110415.2074
                    Surrogate loss: 0.0117
             Mean action noise std: 0.88
                       Mean reward: 11623.19
               Mean episode length: 441.98
                 Mean success rate: 87.50
                  Mean reward/step: 24.27
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 26517504
                    Iteration time: 2.54s
                        Total time: 8444.03s
                               ETA: 513278.1s

################################################################################
                    [1m Learning iteration 3237/200000 [0m

                       Computation: 3085 steps/s (collection: 0.506s, learning 2.149s)
               Value function loss: 66612.2671
                    Surrogate loss: 0.0146
             Mean action noise std: 0.88
                       Mean reward: 11440.13
               Mean episode length: 435.85
                 Mean success rate: 86.00
                  Mean reward/step: 23.56
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 26525696
                    Iteration time: 2.65s
                        Total time: 8446.69s
                               ETA: 513278.3s

################################################################################
                    [1m Learning iteration 3238/200000 [0m

                       Computation: 3224 steps/s (collection: 0.478s, learning 2.062s)
               Value function loss: 113192.8107
                    Surrogate loss: 0.0130
             Mean action noise std: 0.88
                       Mean reward: 11056.54
               Mean episode length: 430.05
                 Mean success rate: 84.50
                  Mean reward/step: 24.10
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 26533888
                    Iteration time: 2.54s
                        Total time: 8449.23s
                               ETA: 513271.6s

################################################################################
                    [1m Learning iteration 3239/200000 [0m

                       Computation: 3129 steps/s (collection: 0.502s, learning 2.115s)
               Value function loss: 81455.5806
                    Surrogate loss: 0.0098
             Mean action noise std: 0.88
                       Mean reward: 11141.13
               Mean episode length: 431.64
                 Mean success rate: 84.00
                  Mean reward/step: 24.43
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 2.62s
                        Total time: 8451.84s
                               ETA: 513269.5s

################################################################################
                    [1m Learning iteration 3240/200000 [0m

                       Computation: 3191 steps/s (collection: 0.480s, learning 2.087s)
               Value function loss: 93342.1167
                    Surrogate loss: 0.0128
             Mean action noise std: 0.88
                       Mean reward: 10990.25
               Mean episode length: 429.24
                 Mean success rate: 83.50
                  Mean reward/step: 25.24
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 26550272
                    Iteration time: 2.57s
                        Total time: 8454.41s
                               ETA: 513264.4s

################################################################################
                    [1m Learning iteration 3241/200000 [0m

                       Computation: 3225 steps/s (collection: 0.443s, learning 2.096s)
               Value function loss: 72937.0229
                    Surrogate loss: 0.0133
             Mean action noise std: 0.88
                       Mean reward: 10952.90
               Mean episode length: 429.84
                 Mean success rate: 84.00
                  Mean reward/step: 25.82
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 26558464
                    Iteration time: 2.54s
                        Total time: 8456.95s
                               ETA: 513257.6s

################################################################################
                    [1m Learning iteration 3242/200000 [0m

                       Computation: 3152 steps/s (collection: 0.472s, learning 2.127s)
               Value function loss: 98738.6845
                    Surrogate loss: 0.0110
             Mean action noise std: 0.88
                       Mean reward: 10785.15
               Mean episode length: 425.58
                 Mean success rate: 83.00
                  Mean reward/step: 26.48
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 26566656
                    Iteration time: 2.60s
                        Total time: 8459.55s
                               ETA: 513254.4s

################################################################################
                    [1m Learning iteration 3243/200000 [0m

                       Computation: 3155 steps/s (collection: 0.476s, learning 2.121s)
               Value function loss: 130808.2855
                    Surrogate loss: 0.0113
             Mean action noise std: 0.88
                       Mean reward: 10511.91
               Mean episode length: 417.84
                 Mean success rate: 81.00
                  Mean reward/step: 26.29
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 26574848
                    Iteration time: 2.60s
                        Total time: 8462.15s
                               ETA: 513251.0s

################################################################################
                    [1m Learning iteration 3244/200000 [0m

                       Computation: 3153 steps/s (collection: 0.486s, learning 2.112s)
               Value function loss: 101478.3909
                    Surrogate loss: 0.0141
             Mean action noise std: 0.88
                       Mean reward: 10351.70
               Mean episode length: 414.69
                 Mean success rate: 80.00
                  Mean reward/step: 25.32
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 26583040
                    Iteration time: 2.60s
                        Total time: 8464.74s
                               ETA: 513247.7s

################################################################################
                    [1m Learning iteration 3245/200000 [0m

                       Computation: 3184 steps/s (collection: 0.454s, learning 2.119s)
               Value function loss: 80874.6237
                    Surrogate loss: 0.0088
             Mean action noise std: 0.88
                       Mean reward: 10236.21
               Mean episode length: 410.20
                 Mean success rate: 79.00
                  Mean reward/step: 25.77
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 26591232
                    Iteration time: 2.57s
                        Total time: 8467.31s
                               ETA: 513242.9s

################################################################################
                    [1m Learning iteration 3246/200000 [0m

                       Computation: 3176 steps/s (collection: 0.455s, learning 2.124s)
               Value function loss: 76520.1921
                    Surrogate loss: 0.0137
             Mean action noise std: 0.88
                       Mean reward: 10101.96
               Mean episode length: 405.51
                 Mean success rate: 77.50
                  Mean reward/step: 26.74
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 26599424
                    Iteration time: 2.58s
                        Total time: 8469.89s
                               ETA: 513238.5s

################################################################################
                    [1m Learning iteration 3247/200000 [0m

                       Computation: 3139 steps/s (collection: 0.480s, learning 2.129s)
               Value function loss: 109616.4691
                    Surrogate loss: 0.0105
             Mean action noise std: 0.88
                       Mean reward: 10121.74
               Mean episode length: 404.12
                 Mean success rate: 78.00
                  Mean reward/step: 26.75
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 26607616
                    Iteration time: 2.61s
                        Total time: 8472.50s
                               ETA: 513235.9s

################################################################################
                    [1m Learning iteration 3248/200000 [0m

                       Computation: 3094 steps/s (collection: 0.522s, learning 2.125s)
               Value function loss: 106577.3891
                    Surrogate loss: 0.0108
             Mean action noise std: 0.88
                       Mean reward: 10478.01
               Mean episode length: 410.57
                 Mean success rate: 79.00
                  Mean reward/step: 26.32
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 26615808
                    Iteration time: 2.65s
                        Total time: 8475.15s
                               ETA: 513235.7s

################################################################################
                    [1m Learning iteration 3249/200000 [0m

                       Computation: 3243 steps/s (collection: 0.431s, learning 2.094s)
               Value function loss: 85630.5979
                    Surrogate loss: 0.0100
             Mean action noise std: 0.88
                       Mean reward: 10731.63
               Mean episode length: 417.89
                 Mean success rate: 80.50
                  Mean reward/step: 25.70
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 26624000
                    Iteration time: 2.53s
                        Total time: 8477.67s
                               ETA: 513228.0s

################################################################################
                    [1m Learning iteration 3250/200000 [0m

                       Computation: 3245 steps/s (collection: 0.436s, learning 2.088s)
               Value function loss: 105736.3316
                    Surrogate loss: 0.0118
             Mean action noise std: 0.88
                       Mean reward: 10472.20
               Mean episode length: 409.54
                 Mean success rate: 79.00
                  Mean reward/step: 26.04
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 26632192
                    Iteration time: 2.52s
                        Total time: 8480.20s
                               ETA: 513220.3s

################################################################################
                    [1m Learning iteration 3251/200000 [0m

                       Computation: 3260 steps/s (collection: 0.468s, learning 2.045s)
               Value function loss: 147925.0894
                    Surrogate loss: 0.0129
             Mean action noise std: 0.88
                       Mean reward: 10828.68
               Mean episode length: 419.47
                 Mean success rate: 81.50
                  Mean reward/step: 25.31
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 2.51s
                        Total time: 8482.71s
                               ETA: 513211.9s

################################################################################
                    [1m Learning iteration 3252/200000 [0m

                       Computation: 3272 steps/s (collection: 0.470s, learning 2.034s)
               Value function loss: 87939.0932
                    Surrogate loss: 0.0138
             Mean action noise std: 0.88
                       Mean reward: 10506.65
               Mean episode length: 413.37
                 Mean success rate: 80.50
                  Mean reward/step: 24.30
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 26648576
                    Iteration time: 2.50s
                        Total time: 8485.21s
                               ETA: 513202.9s

################################################################################
                    [1m Learning iteration 3253/200000 [0m

                       Computation: 3269 steps/s (collection: 0.445s, learning 2.060s)
               Value function loss: 86989.0040
                    Surrogate loss: 0.0120
             Mean action noise std: 0.88
                       Mean reward: 10235.18
               Mean episode length: 402.69
                 Mean success rate: 78.00
                  Mean reward/step: 24.86
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 26656768
                    Iteration time: 2.51s
                        Total time: 8487.72s
                               ETA: 513194.1s

################################################################################
                    [1m Learning iteration 3254/200000 [0m

                       Computation: 3253 steps/s (collection: 0.435s, learning 2.083s)
               Value function loss: 119426.3625
                    Surrogate loss: 0.0133
             Mean action noise std: 0.88
                       Mean reward: 10453.98
               Mean episode length: 409.62
                 Mean success rate: 79.50
                  Mean reward/step: 25.37
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 26664960
                    Iteration time: 2.52s
                        Total time: 8490.24s
                               ETA: 513186.0s

################################################################################
                    [1m Learning iteration 3255/200000 [0m

                       Computation: 3331 steps/s (collection: 0.425s, learning 2.035s)
               Value function loss: 107687.9461
                    Surrogate loss: 0.0121
             Mean action noise std: 0.88
                       Mean reward: 10696.64
               Mean episode length: 417.53
                 Mean success rate: 81.50
                  Mean reward/step: 25.21
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 26673152
                    Iteration time: 2.46s
                        Total time: 8492.70s
                               ETA: 513174.4s

################################################################################
                    [1m Learning iteration 3256/200000 [0m

                       Computation: 3244 steps/s (collection: 0.452s, learning 2.073s)
               Value function loss: 64945.7562
                    Surrogate loss: 0.0104
             Mean action noise std: 0.88
                       Mean reward: 10539.48
               Mean episode length: 414.68
                 Mean success rate: 80.50
                  Mean reward/step: 25.22
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 26681344
                    Iteration time: 2.53s
                        Total time: 8495.22s
                               ETA: 513166.7s

################################################################################
                    [1m Learning iteration 3257/200000 [0m

                       Computation: 3331 steps/s (collection: 0.422s, learning 2.037s)
               Value function loss: 85429.1648
                    Surrogate loss: 0.0116
             Mean action noise std: 0.88
                       Mean reward: 10720.04
               Mean episode length: 418.51
                 Mean success rate: 81.00
                  Mean reward/step: 26.26
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 26689536
                    Iteration time: 2.46s
                        Total time: 8497.68s
                               ETA: 513155.1s

################################################################################
                    [1m Learning iteration 3258/200000 [0m

                       Computation: 3278 steps/s (collection: 0.457s, learning 2.042s)
               Value function loss: 101662.6464
                    Surrogate loss: 0.0119
             Mean action noise std: 0.89
                       Mean reward: 10604.29
               Mean episode length: 416.56
                 Mean success rate: 80.50
                  Mean reward/step: 26.26
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 26697728
                    Iteration time: 2.50s
                        Total time: 8500.18s
                               ETA: 513145.9s

################################################################################
                    [1m Learning iteration 3259/200000 [0m

                       Computation: 3327 steps/s (collection: 0.413s, learning 2.049s)
               Value function loss: 107173.6425
                    Surrogate loss: 0.0093
             Mean action noise std: 0.88
                       Mean reward: 10692.33
               Mean episode length: 417.41
                 Mean success rate: 80.50
                  Mean reward/step: 25.43
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 26705920
                    Iteration time: 2.46s
                        Total time: 8502.64s
                               ETA: 513134.4s

################################################################################
                    [1m Learning iteration 3260/200000 [0m

                       Computation: 3245 steps/s (collection: 0.455s, learning 2.069s)
               Value function loss: 75216.9498
                    Surrogate loss: 0.0142
             Mean action noise std: 0.88
                       Mean reward: 10440.53
               Mean episode length: 410.56
                 Mean success rate: 78.00
                  Mean reward/step: 25.15
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 26714112
                    Iteration time: 2.52s
                        Total time: 8505.17s
                               ETA: 513126.8s

################################################################################
                    [1m Learning iteration 3261/200000 [0m

                       Computation: 3332 steps/s (collection: 0.433s, learning 2.025s)
               Value function loss: 76098.4000
                    Surrogate loss: 0.0104
             Mean action noise std: 0.88
                       Mean reward: 10502.31
               Mean episode length: 410.89
                 Mean success rate: 78.00
                  Mean reward/step: 26.08
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 26722304
                    Iteration time: 2.46s
                        Total time: 8507.62s
                               ETA: 513115.1s

################################################################################
                    [1m Learning iteration 3262/200000 [0m

                       Computation: 3332 steps/s (collection: 0.408s, learning 2.050s)
               Value function loss: 77785.2531
                    Surrogate loss: 0.0075
             Mean action noise std: 0.88
                       Mean reward: 10249.95
               Mean episode length: 404.60
                 Mean success rate: 76.00
                  Mean reward/step: 27.01
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 26730496
                    Iteration time: 2.46s
                        Total time: 8510.08s
                               ETA: 513103.5s

################################################################################
                    [1m Learning iteration 3263/200000 [0m

                       Computation: 3200 steps/s (collection: 0.459s, learning 2.101s)
               Value function loss: 132926.4864
                    Surrogate loss: 0.0098
             Mean action noise std: 0.88
                       Mean reward: 10731.46
               Mean episode length: 417.12
                 Mean success rate: 79.50
                  Mean reward/step: 26.06
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 2.56s
                        Total time: 8512.64s
                               ETA: 513098.0s

################################################################################
                    [1m Learning iteration 3264/200000 [0m

                       Computation: 3158 steps/s (collection: 0.479s, learning 2.114s)
               Value function loss: 92507.5213
                    Surrogate loss: 0.0103
             Mean action noise std: 0.88
                       Mean reward: 10417.06
               Mean episode length: 407.52
                 Mean success rate: 79.00
                  Mean reward/step: 25.51
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 26746880
                    Iteration time: 2.59s
                        Total time: 8515.24s
                               ETA: 513094.5s

################################################################################
                    [1m Learning iteration 3265/200000 [0m

                       Computation: 3191 steps/s (collection: 0.470s, learning 2.097s)
               Value function loss: 65058.3378
                    Surrogate loss: 0.0100
             Mean action noise std: 0.88
                       Mean reward: 10455.36
               Mean episode length: 405.96
                 Mean success rate: 78.50
                  Mean reward/step: 25.96
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 26755072
                    Iteration time: 2.57s
                        Total time: 8517.80s
                               ETA: 513089.4s

################################################################################
                    [1m Learning iteration 3266/200000 [0m

                       Computation: 3228 steps/s (collection: 0.459s, learning 2.079s)
               Value function loss: 133994.0072
                    Surrogate loss: 0.0097
             Mean action noise std: 0.88
                       Mean reward: 10342.95
               Mean episode length: 403.51
                 Mean success rate: 78.50
                  Mean reward/step: 26.06
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 26763264
                    Iteration time: 2.54s
                        Total time: 8520.34s
                               ETA: 513082.5s

################################################################################
                    [1m Learning iteration 3267/200000 [0m

                       Computation: 3133 steps/s (collection: 0.472s, learning 2.142s)
               Value function loss: 129680.6304
                    Surrogate loss: 0.0124
             Mean action noise std: 0.88
                       Mean reward: 10237.39
               Mean episode length: 399.82
                 Mean success rate: 77.50
                  Mean reward/step: 25.18
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 26771456
                    Iteration time: 2.61s
                        Total time: 8522.95s
                               ETA: 513080.3s

################################################################################
                    [1m Learning iteration 3268/200000 [0m

                       Computation: 3188 steps/s (collection: 0.473s, learning 2.097s)
               Value function loss: 118494.8407
                    Surrogate loss: 0.0137
             Mean action noise std: 0.88
                       Mean reward: 10374.14
               Mean episode length: 407.89
                 Mean success rate: 79.50
                  Mean reward/step: 24.71
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 26779648
                    Iteration time: 2.57s
                        Total time: 8525.52s
                               ETA: 513075.3s

################################################################################
                    [1m Learning iteration 3269/200000 [0m

                       Computation: 3174 steps/s (collection: 0.508s, learning 2.072s)
               Value function loss: 105226.6608
                    Surrogate loss: 0.0118
             Mean action noise std: 0.88
                       Mean reward: 10462.14
               Mean episode length: 411.56
                 Mean success rate: 80.50
                  Mean reward/step: 25.58
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 26787840
                    Iteration time: 2.58s
                        Total time: 8528.10s
                               ETA: 513071.1s

################################################################################
                    [1m Learning iteration 3270/200000 [0m

                       Computation: 3184 steps/s (collection: 0.493s, learning 2.079s)
               Value function loss: 100329.0934
                    Surrogate loss: 0.0115
             Mean action noise std: 0.88
                       Mean reward: 10811.19
               Mean episode length: 419.81
                 Mean success rate: 84.00
                  Mean reward/step: 25.79
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 26796032
                    Iteration time: 2.57s
                        Total time: 8530.68s
                               ETA: 513066.3s

################################################################################
                    [1m Learning iteration 3271/200000 [0m

                       Computation: 3206 steps/s (collection: 0.464s, learning 2.091s)
               Value function loss: 84863.7029
                    Surrogate loss: 0.0131
             Mean action noise std: 0.88
                       Mean reward: 10903.32
               Mean episode length: 421.11
                 Mean success rate: 84.00
                  Mean reward/step: 26.85
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 26804224
                    Iteration time: 2.55s
                        Total time: 8533.23s
                               ETA: 513060.5s

################################################################################
                    [1m Learning iteration 3272/200000 [0m

                       Computation: 3194 steps/s (collection: 0.471s, learning 2.093s)
               Value function loss: 81980.8260
                    Surrogate loss: 0.0102
             Mean action noise std: 0.88
                       Mean reward: 11080.42
               Mean episode length: 425.44
                 Mean success rate: 85.50
                  Mean reward/step: 26.86
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 26812416
                    Iteration time: 2.56s
                        Total time: 8535.80s
                               ETA: 513055.3s

################################################################################
                    [1m Learning iteration 3273/200000 [0m

                       Computation: 3232 steps/s (collection: 0.454s, learning 2.080s)
               Value function loss: 102891.9322
                    Surrogate loss: 0.0106
             Mean action noise std: 0.88
                       Mean reward: 11150.09
               Mean episode length: 426.44
                 Mean success rate: 85.00
                  Mean reward/step: 26.66
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 26820608
                    Iteration time: 2.53s
                        Total time: 8538.33s
                               ETA: 513048.2s

################################################################################
                    [1m Learning iteration 3274/200000 [0m

                       Computation: 3215 steps/s (collection: 0.471s, learning 2.076s)
               Value function loss: 90460.3080
                    Surrogate loss: 0.0096
             Mean action noise std: 0.88
                       Mean reward: 11129.81
               Mean episode length: 430.73
                 Mean success rate: 85.50
                  Mean reward/step: 25.90
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 26828800
                    Iteration time: 2.55s
                        Total time: 8540.88s
                               ETA: 513042.0s

################################################################################
                    [1m Learning iteration 3275/200000 [0m

                       Computation: 3169 steps/s (collection: 0.461s, learning 2.124s)
               Value function loss: 102597.7130
                    Surrogate loss: 0.0083
             Mean action noise std: 0.89
                       Mean reward: 11269.74
               Mean episode length: 434.01
                 Mean success rate: 85.50
                  Mean reward/step: 26.00
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 2.58s
                        Total time: 8543.46s
                               ETA: 513038.0s

################################################################################
                    [1m Learning iteration 3276/200000 [0m

                       Computation: 3129 steps/s (collection: 0.513s, learning 2.105s)
               Value function loss: 90902.7484
                    Surrogate loss: 0.0120
             Mean action noise std: 0.88
                       Mean reward: 11154.91
               Mean episode length: 430.68
                 Mean success rate: 85.00
                  Mean reward/step: 26.10
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 26845184
                    Iteration time: 2.62s
                        Total time: 8546.08s
                               ETA: 513036.0s

################################################################################
                    [1m Learning iteration 3277/200000 [0m

                       Computation: 3147 steps/s (collection: 0.454s, learning 2.148s)
               Value function loss: 79437.1474
                    Surrogate loss: 0.0107
             Mean action noise std: 0.89
                       Mean reward: 11074.85
               Mean episode length: 428.76
                 Mean success rate: 84.50
                  Mean reward/step: 27.07
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 26853376
                    Iteration time: 2.60s
                        Total time: 8548.68s
                               ETA: 513033.1s

################################################################################
                    [1m Learning iteration 3278/200000 [0m

                       Computation: 3224 steps/s (collection: 0.481s, learning 2.060s)
               Value function loss: 138563.6434
                    Surrogate loss: 0.0155
             Mean action noise std: 0.89
                       Mean reward: 11050.68
               Mean episode length: 428.17
                 Mean success rate: 85.00
                  Mean reward/step: 27.36
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 26861568
                    Iteration time: 2.54s
                        Total time: 8551.22s
                               ETA: 513026.5s

################################################################################
                    [1m Learning iteration 3279/200000 [0m

                       Computation: 3129 steps/s (collection: 0.500s, learning 2.118s)
               Value function loss: 84597.8201
                    Surrogate loss: 0.0109
             Mean action noise std: 0.88
                       Mean reward: 11306.52
               Mean episode length: 435.32
                 Mean success rate: 86.50
                  Mean reward/step: 26.51
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 26869760
                    Iteration time: 2.62s
                        Total time: 8553.84s
                               ETA: 513024.5s

################################################################################
                    [1m Learning iteration 3280/200000 [0m

                       Computation: 3164 steps/s (collection: 0.445s, learning 2.144s)
               Value function loss: 115187.8715
                    Surrogate loss: 0.0142
             Mean action noise std: 0.88
                       Mean reward: 10885.38
               Mean episode length: 419.81
                 Mean success rate: 83.50
                  Mean reward/step: 26.24
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 26877952
                    Iteration time: 2.59s
                        Total time: 8556.43s
                               ETA: 513020.7s

################################################################################
                    [1m Learning iteration 3281/200000 [0m

                       Computation: 3108 steps/s (collection: 0.519s, learning 2.117s)
               Value function loss: 113996.2570
                    Surrogate loss: 0.0110
             Mean action noise std: 0.88
                       Mean reward: 10745.65
               Mean episode length: 415.88
                 Mean success rate: 82.50
                  Mean reward/step: 26.20
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 26886144
                    Iteration time: 2.64s
                        Total time: 8559.07s
                               ETA: 513019.8s

################################################################################
                    [1m Learning iteration 3282/200000 [0m

                       Computation: 3204 steps/s (collection: 0.464s, learning 2.092s)
               Value function loss: 139915.8682
                    Surrogate loss: 0.0130
             Mean action noise std: 0.88
                       Mean reward: 10805.73
               Mean episode length: 415.85
                 Mean success rate: 83.00
                  Mean reward/step: 26.06
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 26894336
                    Iteration time: 2.56s
                        Total time: 8561.62s
                               ETA: 513014.1s

################################################################################
                    [1m Learning iteration 3283/200000 [0m

                       Computation: 3165 steps/s (collection: 0.505s, learning 2.083s)
               Value function loss: 117748.6260
                    Surrogate loss: 0.0136
             Mean action noise std: 0.89
                       Mean reward: 10722.00
               Mean episode length: 410.06
                 Mean success rate: 83.50
                  Mean reward/step: 25.30
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 26902528
                    Iteration time: 2.59s
                        Total time: 8564.21s
                               ETA: 513010.3s

################################################################################
                    [1m Learning iteration 3284/200000 [0m

                       Computation: 3098 steps/s (collection: 0.492s, learning 2.152s)
               Value function loss: 101908.2270
                    Surrogate loss: 0.0127
             Mean action noise std: 0.89
                       Mean reward: 10893.44
               Mean episode length: 413.50
                 Mean success rate: 84.00
                  Mean reward/step: 24.76
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 26910720
                    Iteration time: 2.64s
                        Total time: 8566.85s
                               ETA: 513009.8s

################################################################################
                    [1m Learning iteration 3285/200000 [0m

                       Computation: 3099 steps/s (collection: 0.505s, learning 2.139s)
               Value function loss: 114502.8164
                    Surrogate loss: 0.0137
             Mean action noise std: 0.88
                       Mean reward: 11147.57
               Mean episode length: 422.74
                 Mean success rate: 85.00
                  Mean reward/step: 25.79
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 26918912
                    Iteration time: 2.64s
                        Total time: 8569.50s
                               ETA: 513009.3s

################################################################################
                    [1m Learning iteration 3286/200000 [0m

                       Computation: 3212 steps/s (collection: 0.437s, learning 2.113s)
               Value function loss: 75171.1863
                    Surrogate loss: 0.0173
             Mean action noise std: 0.88
                       Mean reward: 11026.65
               Mean episode length: 418.32
                 Mean success rate: 84.00
                  Mean reward/step: 26.59
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 26927104
                    Iteration time: 2.55s
                        Total time: 8572.05s
                               ETA: 513003.3s

################################################################################
                    [1m Learning iteration 3287/200000 [0m

                       Computation: 3237 steps/s (collection: 0.461s, learning 2.070s)
               Value function loss: 90240.0703
                    Surrogate loss: 0.0128
             Mean action noise std: 0.88
                       Mean reward: 11226.65
               Mean episode length: 426.06
                 Mean success rate: 85.00
                  Mean reward/step: 26.45
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 2.53s
                        Total time: 8574.58s
                               ETA: 512996.0s

################################################################################
                    [1m Learning iteration 3288/200000 [0m

                       Computation: 3206 steps/s (collection: 0.482s, learning 2.073s)
               Value function loss: 80039.1632
                    Surrogate loss: 0.0132
             Mean action noise std: 0.88
                       Mean reward: 11248.42
               Mean episode length: 426.06
                 Mean success rate: 85.00
                  Mean reward/step: 26.75
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 26943488
                    Iteration time: 2.55s
                        Total time: 8577.13s
                               ETA: 512990.3s

################################################################################
                    [1m Learning iteration 3289/200000 [0m

                       Computation: 3175 steps/s (collection: 0.479s, learning 2.101s)
               Value function loss: 112034.7769
                    Surrogate loss: 0.0086
             Mean action noise std: 0.88
                       Mean reward: 11039.39
               Mean episode length: 417.69
                 Mean success rate: 83.00
                  Mean reward/step: 26.73
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 26951680
                    Iteration time: 2.58s
                        Total time: 8579.71s
                               ETA: 512986.0s

################################################################################
                    [1m Learning iteration 3290/200000 [0m

                       Computation: 3207 steps/s (collection: 0.466s, learning 2.088s)
               Value function loss: 95557.4725
                    Surrogate loss: 0.0101
             Mean action noise std: 0.88
                       Mean reward: 11355.78
               Mean episode length: 427.88
                 Mean success rate: 85.50
                  Mean reward/step: 26.19
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 26959872
                    Iteration time: 2.55s
                        Total time: 8582.27s
                               ETA: 512980.2s

################################################################################
                    [1m Learning iteration 3291/200000 [0m

                       Computation: 3216 steps/s (collection: 0.460s, learning 2.087s)
               Value function loss: 122202.2371
                    Surrogate loss: 0.0120
             Mean action noise std: 0.88
                       Mean reward: 11578.27
               Mean episode length: 434.94
                 Mean success rate: 86.50
                  Mean reward/step: 25.72
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 26968064
                    Iteration time: 2.55s
                        Total time: 8584.81s
                               ETA: 512973.9s

################################################################################
                    [1m Learning iteration 3292/200000 [0m

                       Computation: 3270 steps/s (collection: 0.434s, learning 2.071s)
               Value function loss: 67600.7862
                    Surrogate loss: 0.0144
             Mean action noise std: 0.88
                       Mean reward: 11284.01
               Mean episode length: 428.89
                 Mean success rate: 85.00
                  Mean reward/step: 25.62
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 26976256
                    Iteration time: 2.50s
                        Total time: 8587.32s
                               ETA: 512965.2s

################################################################################
                    [1m Learning iteration 3293/200000 [0m

                       Computation: 3144 steps/s (collection: 0.485s, learning 2.120s)
               Value function loss: 66656.5122
                    Surrogate loss: 0.0078
             Mean action noise std: 0.88
                       Mean reward: 11388.23
               Mean episode length: 432.50
                 Mean success rate: 85.50
                  Mean reward/step: 26.46
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 26984448
                    Iteration time: 2.61s
                        Total time: 8589.92s
                               ETA: 512962.4s

################################################################################
                    [1m Learning iteration 3294/200000 [0m

                       Computation: 3190 steps/s (collection: 0.487s, learning 2.081s)
               Value function loss: 129097.1397
                    Surrogate loss: 0.0110
             Mean action noise std: 0.88
                       Mean reward: 11609.59
               Mean episode length: 441.33
                 Mean success rate: 86.50
                  Mean reward/step: 26.60
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 26992640
                    Iteration time: 2.57s
                        Total time: 8592.49s
                               ETA: 512957.4s

################################################################################
                    [1m Learning iteration 3295/200000 [0m

                       Computation: 3196 steps/s (collection: 0.464s, learning 2.099s)
               Value function loss: 77100.6414
                    Surrogate loss: 0.0133
             Mean action noise std: 0.88
                       Mean reward: 11748.57
               Mean episode length: 444.94
                 Mean success rate: 87.00
                  Mean reward/step: 26.20
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 27000832
                    Iteration time: 2.56s
                        Total time: 8595.05s
                               ETA: 512952.1s

################################################################################
                    [1m Learning iteration 3296/200000 [0m

                       Computation: 3166 steps/s (collection: 0.464s, learning 2.124s)
               Value function loss: 101186.0961
                    Surrogate loss: 0.0107
             Mean action noise std: 0.88
                       Mean reward: 11703.14
               Mean episode length: 446.49
                 Mean success rate: 88.00
                  Mean reward/step: 26.33
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 27009024
                    Iteration time: 2.59s
                        Total time: 8597.64s
                               ETA: 512948.3s

################################################################################
                    [1m Learning iteration 3297/200000 [0m

                       Computation: 3106 steps/s (collection: 0.484s, learning 2.153s)
               Value function loss: 117328.1214
                    Surrogate loss: 0.0127
             Mean action noise std: 0.88
                       Mean reward: 11645.84
               Mean episode length: 445.40
                 Mean success rate: 87.50
                  Mean reward/step: 26.32
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 27017216
                    Iteration time: 2.64s
                        Total time: 8600.28s
                               ETA: 512947.4s

################################################################################
                    [1m Learning iteration 3298/200000 [0m

                       Computation: 3157 steps/s (collection: 0.500s, learning 2.095s)
               Value function loss: 136124.9570
                    Surrogate loss: 0.0105
             Mean action noise std: 0.88
                       Mean reward: 11897.75
               Mean episode length: 450.70
                 Mean success rate: 89.00
                  Mean reward/step: 25.18
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 27025408
                    Iteration time: 2.59s
                        Total time: 8602.87s
                               ETA: 512944.1s

################################################################################
                    [1m Learning iteration 3299/200000 [0m

                       Computation: 3216 steps/s (collection: 0.457s, learning 2.090s)
               Value function loss: 124795.0832
                    Surrogate loss: 0.0154
             Mean action noise std: 0.88
                       Mean reward: 11739.29
               Mean episode length: 446.39
                 Mean success rate: 88.00
                  Mean reward/step: 25.14
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 2.55s
                        Total time: 8605.42s
                               ETA: 512937.8s

################################################################################
                    [1m Learning iteration 3300/200000 [0m

                       Computation: 3189 steps/s (collection: 0.467s, learning 2.101s)
               Value function loss: 109889.6271
                    Surrogate loss: 0.0143
             Mean action noise std: 0.88
                       Mean reward: 11957.60
               Mean episode length: 450.77
                 Mean success rate: 89.00
                  Mean reward/step: 24.27
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 27041792
                    Iteration time: 2.57s
                        Total time: 8607.99s
                               ETA: 512932.9s

################################################################################
                    [1m Learning iteration 3301/200000 [0m

                       Computation: 3177 steps/s (collection: 0.491s, learning 2.087s)
               Value function loss: 80661.0467
                    Surrogate loss: 0.0087
             Mean action noise std: 0.88
                       Mean reward: 11797.08
               Mean episode length: 450.69
                 Mean success rate: 89.00
                  Mean reward/step: 24.22
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 27049984
                    Iteration time: 2.58s
                        Total time: 8610.57s
                               ETA: 512928.5s

################################################################################
                    [1m Learning iteration 3302/200000 [0m

                       Computation: 3174 steps/s (collection: 0.482s, learning 2.098s)
               Value function loss: 95167.6820
                    Surrogate loss: 0.0127
             Mean action noise std: 0.88
                       Mean reward: 11759.48
               Mean episode length: 449.11
                 Mean success rate: 88.00
                  Mean reward/step: 25.53
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 27058176
                    Iteration time: 2.58s
                        Total time: 8613.15s
                               ETA: 512924.2s

################################################################################
                    [1m Learning iteration 3303/200000 [0m

                       Computation: 3261 steps/s (collection: 0.442s, learning 2.069s)
               Value function loss: 70042.6122
                    Surrogate loss: 0.0174
             Mean action noise std: 0.88
                       Mean reward: 11904.81
               Mean episode length: 453.39
                 Mean success rate: 89.00
                  Mean reward/step: 25.83
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 27066368
                    Iteration time: 2.51s
                        Total time: 8615.66s
                               ETA: 512915.9s

################################################################################
                    [1m Learning iteration 3304/200000 [0m

                       Computation: 3234 steps/s (collection: 0.461s, learning 2.071s)
               Value function loss: 89158.3854
                    Surrogate loss: 0.0123
             Mean action noise std: 0.88
                       Mean reward: 11782.67
               Mean episode length: 451.62
                 Mean success rate: 88.50
                  Mean reward/step: 26.03
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 27074560
                    Iteration time: 2.53s
                        Total time: 8618.19s
                               ETA: 512908.8s

################################################################################
                    [1m Learning iteration 3305/200000 [0m

                       Computation: 3174 steps/s (collection: 0.482s, learning 2.099s)
               Value function loss: 99880.3258
                    Surrogate loss: 0.0121
             Mean action noise std: 0.88
                       Mean reward: 11564.32
               Mean episode length: 444.39
                 Mean success rate: 87.00
                  Mean reward/step: 25.67
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 27082752
                    Iteration time: 2.58s
                        Total time: 8620.77s
                               ETA: 512904.6s

################################################################################
                    [1m Learning iteration 3306/200000 [0m

                       Computation: 3189 steps/s (collection: 0.462s, learning 2.107s)
               Value function loss: 104683.0988
                    Surrogate loss: 0.0126
             Mean action noise std: 0.89
                       Mean reward: 11184.29
               Mean episode length: 432.62
                 Mean success rate: 84.50
                  Mean reward/step: 25.33
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 27090944
                    Iteration time: 2.57s
                        Total time: 8623.34s
                               ETA: 512899.6s

################################################################################
                    [1m Learning iteration 3307/200000 [0m

                       Computation: 3220 steps/s (collection: 0.438s, learning 2.106s)
               Value function loss: 115057.8295
                    Surrogate loss: 0.0131
             Mean action noise std: 0.89
                       Mean reward: 11309.84
               Mean episode length: 439.23
                 Mean success rate: 86.00
                  Mean reward/step: 25.08
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 27099136
                    Iteration time: 2.54s
                        Total time: 8625.88s
                               ETA: 512893.2s

################################################################################
                    [1m Learning iteration 3308/200000 [0m

                       Computation: 3258 steps/s (collection: 0.430s, learning 2.084s)
               Value function loss: 61781.0527
                    Surrogate loss: 0.0129
             Mean action noise std: 0.89
                       Mean reward: 11175.75
               Mean episode length: 433.93
                 Mean success rate: 86.00
                  Mean reward/step: 24.72
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 27107328
                    Iteration time: 2.51s
                        Total time: 8628.40s
                               ETA: 512885.1s

################################################################################
                    [1m Learning iteration 3309/200000 [0m

                       Computation: 3226 steps/s (collection: 0.473s, learning 2.065s)
               Value function loss: 91613.0838
                    Surrogate loss: 0.0142
             Mean action noise std: 0.89
                       Mean reward: 10914.91
               Mean episode length: 428.01
                 Mean success rate: 85.00
                  Mean reward/step: 25.41
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 27115520
                    Iteration time: 2.54s
                        Total time: 8630.94s
                               ETA: 512878.4s

################################################################################
                    [1m Learning iteration 3310/200000 [0m

                       Computation: 3239 steps/s (collection: 0.482s, learning 2.047s)
               Value function loss: 86843.1601
                    Surrogate loss: 0.0113
             Mean action noise std: 0.88
                       Mean reward: 10773.70
               Mean episode length: 424.13
                 Mean success rate: 84.00
                  Mean reward/step: 25.20
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 27123712
                    Iteration time: 2.53s
                        Total time: 8633.46s
                               ETA: 512871.1s

################################################################################
                    [1m Learning iteration 3311/200000 [0m

                       Computation: 3173 steps/s (collection: 0.477s, learning 2.104s)
               Value function loss: 73655.3066
                    Surrogate loss: 0.0111
             Mean action noise std: 0.88
                       Mean reward: 10309.50
               Mean episode length: 413.28
                 Mean success rate: 81.00
                  Mean reward/step: 26.19
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 2.58s
                        Total time: 8636.05s
                               ETA: 512866.9s

################################################################################
                    [1m Learning iteration 3312/200000 [0m

                       Computation: 3179 steps/s (collection: 0.456s, learning 2.120s)
               Value function loss: 116147.7873
                    Surrogate loss: 0.0125
             Mean action noise std: 0.88
                       Mean reward: 10245.40
               Mean episode length: 409.64
                 Mean success rate: 80.50
                  Mean reward/step: 26.43
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 27140096
                    Iteration time: 2.58s
                        Total time: 8638.62s
                               ETA: 512862.4s

################################################################################
                    [1m Learning iteration 3313/200000 [0m

                       Computation: 3194 steps/s (collection: 0.480s, learning 2.085s)
               Value function loss: 109696.2643
                    Surrogate loss: 0.0115
             Mean action noise std: 0.88
                       Mean reward: 10212.49
               Mean episode length: 409.43
                 Mean success rate: 80.50
                  Mean reward/step: 25.43
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 27148288
                    Iteration time: 2.56s
                        Total time: 8641.19s
                               ETA: 512857.3s

################################################################################
                    [1m Learning iteration 3314/200000 [0m

                       Computation: 3204 steps/s (collection: 0.488s, learning 2.069s)
               Value function loss: 122522.4986
                    Surrogate loss: 0.0111
             Mean action noise std: 0.89
                       Mean reward: 9974.46
               Mean episode length: 399.58
                 Mean success rate: 79.00
                  Mean reward/step: 24.43
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 27156480
                    Iteration time: 2.56s
                        Total time: 8643.74s
                               ETA: 512851.6s

################################################################################
                    [1m Learning iteration 3315/200000 [0m

                       Computation: 3171 steps/s (collection: 0.481s, learning 2.102s)
               Value function loss: 89664.9208
                    Surrogate loss: 0.0089
             Mean action noise std: 0.89
                       Mean reward: 10058.90
               Mean episode length: 402.63
                 Mean success rate: 79.50
                  Mean reward/step: 24.13
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 27164672
                    Iteration time: 2.58s
                        Total time: 8646.33s
                               ETA: 512847.6s

################################################################################
                    [1m Learning iteration 3316/200000 [0m

                       Computation: 3221 steps/s (collection: 0.475s, learning 2.068s)
               Value function loss: 106711.3894
                    Surrogate loss: 0.0089
             Mean action noise std: 0.89
                       Mean reward: 10135.83
               Mean episode length: 401.95
                 Mean success rate: 80.00
                  Mean reward/step: 25.00
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 27172864
                    Iteration time: 2.54s
                        Total time: 8648.87s
                               ETA: 512841.2s

################################################################################
                    [1m Learning iteration 3317/200000 [0m

                       Computation: 3183 steps/s (collection: 0.482s, learning 2.091s)
               Value function loss: 81507.2105
                    Surrogate loss: 0.0114
             Mean action noise std: 0.89
                       Mean reward: 10085.12
               Mean episode length: 400.15
                 Mean success rate: 79.00
                  Mean reward/step: 25.41
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 27181056
                    Iteration time: 2.57s
                        Total time: 8651.44s
                               ETA: 512836.5s

################################################################################
                    [1m Learning iteration 3318/200000 [0m

                       Computation: 3231 steps/s (collection: 0.492s, learning 2.043s)
               Value function loss: 70310.3947
                    Surrogate loss: 0.0103
             Mean action noise std: 0.89
                       Mean reward: 9801.56
               Mean episode length: 392.03
                 Mean success rate: 76.50
                  Mean reward/step: 25.63
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 27189248
                    Iteration time: 2.53s
                        Total time: 8653.98s
                               ETA: 512829.6s

################################################################################
                    [1m Learning iteration 3319/200000 [0m

                       Computation: 3220 steps/s (collection: 0.459s, learning 2.085s)
               Value function loss: 97340.0396
                    Surrogate loss: 0.0114
             Mean action noise std: 0.89
                       Mean reward: 9807.20
               Mean episode length: 392.38
                 Mean success rate: 77.50
                  Mean reward/step: 25.94
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 27197440
                    Iteration time: 2.54s
                        Total time: 8656.52s
                               ETA: 512823.2s

################################################################################
                    [1m Learning iteration 3320/200000 [0m

                       Computation: 3206 steps/s (collection: 0.479s, learning 2.075s)
               Value function loss: 74369.5680
                    Surrogate loss: 0.0089
             Mean action noise std: 0.89
                       Mean reward: 9965.29
               Mean episode length: 395.90
                 Mean success rate: 78.00
                  Mean reward/step: 26.51
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 27205632
                    Iteration time: 2.55s
                        Total time: 8659.08s
                               ETA: 512817.5s

################################################################################
                    [1m Learning iteration 3321/200000 [0m

                       Computation: 3223 steps/s (collection: 0.470s, learning 2.071s)
               Value function loss: 103633.3173
                    Surrogate loss: 0.0113
             Mean action noise std: 0.89
                       Mean reward: 10141.79
               Mean episode length: 400.22
                 Mean success rate: 79.00
                  Mean reward/step: 25.83
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 27213824
                    Iteration time: 2.54s
                        Total time: 8661.62s
                               ETA: 512811.0s

################################################################################
                    [1m Learning iteration 3322/200000 [0m

                       Computation: 3247 steps/s (collection: 0.461s, learning 2.062s)
               Value function loss: 112674.7505
                    Surrogate loss: 0.0106
             Mean action noise std: 0.89
                       Mean reward: 10243.98
               Mean episode length: 403.08
                 Mean success rate: 80.00
                  Mean reward/step: 24.41
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 27222016
                    Iteration time: 2.52s
                        Total time: 8664.14s
                               ETA: 512803.4s

################################################################################
                    [1m Learning iteration 3323/200000 [0m

                       Computation: 3275 steps/s (collection: 0.459s, learning 2.042s)
               Value function loss: 73389.2302
                    Surrogate loss: 0.0090
             Mean action noise std: 0.89
                       Mean reward: 10032.53
               Mean episode length: 394.41
                 Mean success rate: 78.50
                  Mean reward/step: 23.97
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 2.50s
                        Total time: 8666.64s
                               ETA: 512794.5s

################################################################################
                    [1m Learning iteration 3324/200000 [0m

                       Computation: 3207 steps/s (collection: 0.486s, learning 2.067s)
               Value function loss: 63282.6496
                    Surrogate loss: 0.0108
             Mean action noise std: 0.89
                       Mean reward: 10045.46
               Mean episode length: 393.15
                 Mean success rate: 78.00
                  Mean reward/step: 25.30
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 27238400
                    Iteration time: 2.55s
                        Total time: 8669.19s
                               ETA: 512788.7s

################################################################################
                    [1m Learning iteration 3325/200000 [0m

                       Computation: 3217 steps/s (collection: 0.448s, learning 2.098s)
               Value function loss: 140617.0926
                    Surrogate loss: 0.0125
             Mean action noise std: 0.89
                       Mean reward: 9966.48
               Mean episode length: 391.25
                 Mean success rate: 77.00
                  Mean reward/step: 26.53
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 27246592
                    Iteration time: 2.55s
                        Total time: 8671.74s
                               ETA: 512782.5s

################################################################################
                    [1m Learning iteration 3326/200000 [0m

                       Computation: 3240 steps/s (collection: 0.467s, learning 2.061s)
               Value function loss: 60586.6627
                    Surrogate loss: 0.0097
             Mean action noise std: 0.89
                       Mean reward: 10265.23
               Mean episode length: 399.04
                 Mean success rate: 78.50
                  Mean reward/step: 26.00
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 27254784
                    Iteration time: 2.53s
                        Total time: 8674.27s
                               ETA: 512775.2s

################################################################################
                    [1m Learning iteration 3327/200000 [0m

                       Computation: 3169 steps/s (collection: 0.470s, learning 2.114s)
               Value function loss: 84011.4635
                    Surrogate loss: 0.0106
             Mean action noise std: 0.89
                       Mean reward: 10246.02
               Mean episode length: 399.76
                 Mean success rate: 78.50
                  Mean reward/step: 26.65
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 27262976
                    Iteration time: 2.58s
                        Total time: 8676.85s
                               ETA: 512771.2s

################################################################################
                    [1m Learning iteration 3328/200000 [0m

                       Computation: 3237 steps/s (collection: 0.467s, learning 2.064s)
               Value function loss: 125239.0738
                    Surrogate loss: 0.0146
             Mean action noise std: 0.89
                       Mean reward: 10521.61
               Mean episode length: 408.59
                 Mean success rate: 81.00
                  Mean reward/step: 26.66
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 27271168
                    Iteration time: 2.53s
                        Total time: 8679.38s
                               ETA: 512764.1s

################################################################################
                    [1m Learning iteration 3329/200000 [0m

                       Computation: 3260 steps/s (collection: 0.466s, learning 2.047s)
               Value function loss: 140824.5346
                    Surrogate loss: 0.0131
             Mean action noise std: 0.89
                       Mean reward: 10616.60
               Mean episode length: 412.65
                 Mean success rate: 81.00
                  Mean reward/step: 25.85
       Mean episode length/episode: 28.85
--------------------------------------------------------------------------------
                   Total timesteps: 27279360
                    Iteration time: 2.51s
                        Total time: 8681.90s
                               ETA: 512755.9s

################################################################################
                    [1m Learning iteration 3330/200000 [0m

                       Computation: 3158 steps/s (collection: 0.510s, learning 2.083s)
               Value function loss: 106487.6560
                    Surrogate loss: 0.0114
             Mean action noise std: 0.89
                       Mean reward: 10342.70
               Mean episode length: 403.94
                 Mean success rate: 79.00
                  Mean reward/step: 25.17
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 27287552
                    Iteration time: 2.59s
                        Total time: 8684.49s
                               ETA: 512752.5s

################################################################################
                    [1m Learning iteration 3331/200000 [0m

                       Computation: 3216 steps/s (collection: 0.478s, learning 2.069s)
               Value function loss: 72749.0202
                    Surrogate loss: 0.0117
             Mean action noise std: 0.89
                       Mean reward: 10099.53
               Mean episode length: 394.96
                 Mean success rate: 77.00
                  Mean reward/step: 25.99
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 27295744
                    Iteration time: 2.55s
                        Total time: 8687.04s
                               ETA: 512746.3s

################################################################################
                    [1m Learning iteration 3332/200000 [0m

                       Computation: 3225 steps/s (collection: 0.430s, learning 2.110s)
               Value function loss: 92786.5813
                    Surrogate loss: 0.0150
             Mean action noise std: 0.89
                       Mean reward: 10146.38
               Mean episode length: 396.83
                 Mean success rate: 77.00
                  Mean reward/step: 25.88
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 27303936
                    Iteration time: 2.54s
                        Total time: 8689.58s
                               ETA: 512739.7s

################################################################################
                    [1m Learning iteration 3333/200000 [0m

                       Computation: 3231 steps/s (collection: 0.449s, learning 2.086s)
               Value function loss: 100994.7898
                    Surrogate loss: 0.0103
             Mean action noise std: 0.89
                       Mean reward: 10476.56
               Mean episode length: 408.03
                 Mean success rate: 79.50
                  Mean reward/step: 26.25
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 27312128
                    Iteration time: 2.53s
                        Total time: 8692.11s
                               ETA: 512732.8s

################################################################################
                    [1m Learning iteration 3334/200000 [0m

                       Computation: 3209 steps/s (collection: 0.444s, learning 2.109s)
               Value function loss: 81659.2848
                    Surrogate loss: 0.0088
             Mean action noise std: 0.89
                       Mean reward: 10368.66
               Mean episode length: 406.62
                 Mean success rate: 79.50
                  Mean reward/step: 26.47
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 27320320
                    Iteration time: 2.55s
                        Total time: 8694.66s
                               ETA: 512727.0s

################################################################################
                    [1m Learning iteration 3335/200000 [0m

                       Computation: 3225 steps/s (collection: 0.435s, learning 2.105s)
               Value function loss: 58318.5615
                    Surrogate loss: 0.0153
             Mean action noise std: 0.88
                       Mean reward: 10480.89
               Mean episode length: 411.44
                 Mean success rate: 80.50
                  Mean reward/step: 27.70
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 2.54s
                        Total time: 8697.20s
                               ETA: 512720.5s

################################################################################
                    [1m Learning iteration 3336/200000 [0m

                       Computation: 3234 steps/s (collection: 0.445s, learning 2.088s)
               Value function loss: 86772.8692
                    Surrogate loss: 0.0081
             Mean action noise std: 0.88
                       Mean reward: 10415.78
               Mean episode length: 410.05
                 Mean success rate: 80.00
                  Mean reward/step: 27.74
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 27336704
                    Iteration time: 2.53s
                        Total time: 8699.74s
                               ETA: 512713.5s

################################################################################
                    [1m Learning iteration 3337/200000 [0m

                       Computation: 3298 steps/s (collection: 0.436s, learning 2.047s)
               Value function loss: 90565.5772
                    Surrogate loss: 0.0092
             Mean action noise std: 0.89
                       Mean reward: 10453.33
               Mean episode length: 410.70
                 Mean success rate: 79.50
                  Mean reward/step: 27.48
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 27344896
                    Iteration time: 2.48s
                        Total time: 8702.22s
                               ETA: 512703.6s

################################################################################
                    [1m Learning iteration 3338/200000 [0m

                       Computation: 3208 steps/s (collection: 0.472s, learning 2.082s)
               Value function loss: 100606.9017
                    Surrogate loss: 0.0114
             Mean action noise std: 0.89
                       Mean reward: 10587.67
               Mean episode length: 411.15
                 Mean success rate: 79.50
                  Mean reward/step: 27.02
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 27353088
                    Iteration time: 2.55s
                        Total time: 8704.77s
                               ETA: 512697.8s

################################################################################
                    [1m Learning iteration 3339/200000 [0m

                       Computation: 3189 steps/s (collection: 0.439s, learning 2.130s)
               Value function loss: 93888.5893
                    Surrogate loss: 0.0126
             Mean action noise std: 0.89
                       Mean reward: 10661.44
               Mean episode length: 412.31
                 Mean success rate: 80.50
                  Mean reward/step: 26.85
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 27361280
                    Iteration time: 2.57s
                        Total time: 8707.34s
                               ETA: 512692.9s

################################################################################
                    [1m Learning iteration 3340/200000 [0m

                       Computation: 3223 steps/s (collection: 0.462s, learning 2.079s)
               Value function loss: 96188.5670
                    Surrogate loss: 0.0130
             Mean action noise std: 0.89
                       Mean reward: 10954.93
               Mean episode length: 419.98
                 Mean success rate: 82.00
                  Mean reward/step: 27.21
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 27369472
                    Iteration time: 2.54s
                        Total time: 8709.88s
                               ETA: 512686.4s

################################################################################
                    [1m Learning iteration 3341/200000 [0m

                       Computation: 3261 steps/s (collection: 0.464s, learning 2.047s)
               Value function loss: 130376.2477
                    Surrogate loss: 0.0100
             Mean action noise std: 0.89
                       Mean reward: 11303.29
               Mean episode length: 429.19
                 Mean success rate: 84.00
                  Mean reward/step: 27.40
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 27377664
                    Iteration time: 2.51s
                        Total time: 8712.39s
                               ETA: 512678.2s

################################################################################
                    [1m Learning iteration 3342/200000 [0m

                       Computation: 3301 steps/s (collection: 0.438s, learning 2.043s)
               Value function loss: 59081.5839
                    Surrogate loss: 0.0131
             Mean action noise std: 0.89
                       Mean reward: 11520.70
               Mean episode length: 435.45
                 Mean success rate: 85.50
                  Mean reward/step: 27.10
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 27385856
                    Iteration time: 2.48s
                        Total time: 8714.87s
                               ETA: 512668.2s

################################################################################
                    [1m Learning iteration 3343/200000 [0m

                       Computation: 3250 steps/s (collection: 0.430s, learning 2.091s)
               Value function loss: 114656.9453
                    Surrogate loss: 0.0128
             Mean action noise std: 0.89
                       Mean reward: 11451.52
               Mean episode length: 435.34
                 Mean success rate: 85.50
                  Mean reward/step: 27.70
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 27394048
                    Iteration time: 2.52s
                        Total time: 8717.39s
                               ETA: 512660.5s

################################################################################
                    [1m Learning iteration 3344/200000 [0m

                       Computation: 3210 steps/s (collection: 0.481s, learning 2.071s)
               Value function loss: 143986.4090
                    Surrogate loss: 0.0103
             Mean action noise std: 0.89
                       Mean reward: 11777.95
               Mean episode length: 441.56
                 Mean success rate: 87.50
                  Mean reward/step: 27.28
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 27402240
                    Iteration time: 2.55s
                        Total time: 8719.95s
                               ETA: 512654.6s

################################################################################
                    [1m Learning iteration 3345/200000 [0m

                       Computation: 3214 steps/s (collection: 0.486s, learning 2.062s)
               Value function loss: 119140.1351
                    Surrogate loss: 0.0102
             Mean action noise std: 0.89
                       Mean reward: 12102.62
               Mean episode length: 449.20
                 Mean success rate: 89.00
                  Mean reward/step: 26.04
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 27410432
                    Iteration time: 2.55s
                        Total time: 8722.49s
                               ETA: 512648.6s

################################################################################
                    [1m Learning iteration 3346/200000 [0m

                       Computation: 3258 steps/s (collection: 0.462s, learning 2.051s)
               Value function loss: 107749.8065
                    Surrogate loss: 0.0116
             Mean action noise std: 0.89
                       Mean reward: 12271.91
               Mean episode length: 453.05
                 Mean success rate: 90.00
                  Mean reward/step: 25.84
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 27418624
                    Iteration time: 2.51s
                        Total time: 8725.01s
                               ETA: 512640.5s

################################################################################
                    [1m Learning iteration 3347/200000 [0m

                       Computation: 3266 steps/s (collection: 0.467s, learning 2.041s)
               Value function loss: 96276.8093
                    Surrogate loss: 0.0137
             Mean action noise std: 0.89
                       Mean reward: 12312.03
               Mean episode length: 453.24
                 Mean success rate: 90.00
                  Mean reward/step: 26.58
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 2.51s
                        Total time: 8727.52s
                               ETA: 512632.0s

################################################################################
                    [1m Learning iteration 3348/200000 [0m

                       Computation: 3219 steps/s (collection: 0.496s, learning 2.049s)
               Value function loss: 97036.6178
                    Surrogate loss: 0.0132
             Mean action noise std: 0.89
                       Mean reward: 12312.92
               Mean episode length: 452.76
                 Mean success rate: 90.00
                  Mean reward/step: 26.87
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 27435008
                    Iteration time: 2.54s
                        Total time: 8730.06s
                               ETA: 512625.8s

################################################################################
                    [1m Learning iteration 3349/200000 [0m

                       Computation: 3264 steps/s (collection: 0.458s, learning 2.052s)
               Value function loss: 98253.3167
                    Surrogate loss: 0.0123
             Mean action noise std: 0.89
                       Mean reward: 12356.64
               Mean episode length: 458.04
                 Mean success rate: 90.50
                  Mean reward/step: 26.93
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 27443200
                    Iteration time: 2.51s
                        Total time: 8732.57s
                               ETA: 512617.5s

################################################################################
                    [1m Learning iteration 3350/200000 [0m

                       Computation: 3289 steps/s (collection: 0.448s, learning 2.043s)
               Value function loss: 68090.2766
                    Surrogate loss: 0.0103
             Mean action noise std: 0.89
                       Mean reward: 12013.18
               Mean episode length: 448.70
                 Mean success rate: 88.00
                  Mean reward/step: 26.53
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 27451392
                    Iteration time: 2.49s
                        Total time: 8735.06s
                               ETA: 512608.0s

################################################################################
                    [1m Learning iteration 3351/200000 [0m

                       Computation: 3326 steps/s (collection: 0.430s, learning 2.032s)
               Value function loss: 89617.4751
                    Surrogate loss: 0.0094
             Mean action noise std: 0.89
                       Mean reward: 12158.15
               Mean episode length: 451.99
                 Mean success rate: 89.00
                  Mean reward/step: 27.00
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 27459584
                    Iteration time: 2.46s
                        Total time: 8737.52s
                               ETA: 512597.0s

################################################################################
                    [1m Learning iteration 3352/200000 [0m

                       Computation: 3251 steps/s (collection: 0.476s, learning 2.044s)
               Value function loss: 105241.6307
                    Surrogate loss: 0.0089
             Mean action noise std: 0.89
                       Mean reward: 12044.58
               Mean episode length: 447.75
                 Mean success rate: 88.00
                  Mean reward/step: 26.99
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 27467776
                    Iteration time: 2.52s
                        Total time: 8740.04s
                               ETA: 512589.3s

################################################################################
                    [1m Learning iteration 3353/200000 [0m

                       Computation: 3222 steps/s (collection: 0.463s, learning 2.079s)
               Value function loss: 102241.4791
                    Surrogate loss: 0.0100
             Mean action noise std: 0.89
                       Mean reward: 11849.01
               Mean episode length: 443.98
                 Mean success rate: 87.00
                  Mean reward/step: 27.12
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 27475968
                    Iteration time: 2.54s
                        Total time: 8742.58s
                               ETA: 512582.9s

################################################################################
                    [1m Learning iteration 3354/200000 [0m

                       Computation: 3325 steps/s (collection: 0.446s, learning 2.017s)
               Value function loss: 113866.2873
                    Surrogate loss: 0.0131
             Mean action noise std: 0.89
                       Mean reward: 11987.69
               Mean episode length: 445.64
                 Mean success rate: 87.00
                  Mean reward/step: 27.37
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 27484160
                    Iteration time: 2.46s
                        Total time: 8745.05s
                               ETA: 512571.9s

################################################################################
                    [1m Learning iteration 3355/200000 [0m

                       Computation: 3251 steps/s (collection: 0.439s, learning 2.081s)
               Value function loss: 97472.1768
                    Surrogate loss: 0.0128
             Mean action noise std: 0.89
                       Mean reward: 11729.30
               Mean episode length: 439.32
                 Mean success rate: 85.50
                  Mean reward/step: 27.37
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 27492352
                    Iteration time: 2.52s
                        Total time: 8747.57s
                               ETA: 512564.2s

################################################################################
                    [1m Learning iteration 3356/200000 [0m

                       Computation: 3238 steps/s (collection: 0.464s, learning 2.066s)
               Value function loss: 120623.0764
                    Surrogate loss: 0.0102
             Mean action noise std: 0.89
                       Mean reward: 11490.53
               Mean episode length: 431.90
                 Mean success rate: 83.50
                  Mean reward/step: 27.01
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 27500544
                    Iteration time: 2.53s
                        Total time: 8750.10s
                               ETA: 512557.1s

################################################################################
                    [1m Learning iteration 3357/200000 [0m

                       Computation: 3212 steps/s (collection: 0.464s, learning 2.086s)
               Value function loss: 95148.7531
                    Surrogate loss: 0.0119
             Mean action noise std: 0.89
                       Mean reward: 11182.72
               Mean episode length: 420.81
                 Mean success rate: 82.00
                  Mean reward/step: 25.80
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 27508736
                    Iteration time: 2.55s
                        Total time: 8752.65s
                               ETA: 512551.2s

################################################################################
                    [1m Learning iteration 3358/200000 [0m

                       Computation: 3317 steps/s (collection: 0.422s, learning 2.047s)
               Value function loss: 53545.9974
                    Surrogate loss: 0.0099
             Mean action noise std: 0.89
                       Mean reward: 10799.89
               Mean episode length: 408.48
                 Mean success rate: 80.00
                  Mean reward/step: 26.15
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 27516928
                    Iteration time: 2.47s
                        Total time: 8755.12s
                               ETA: 512540.6s

################################################################################
                    [1m Learning iteration 3359/200000 [0m

                       Computation: 3268 steps/s (collection: 0.452s, learning 2.054s)
               Value function loss: 154269.5998
                    Surrogate loss: 0.0142
             Mean action noise std: 0.89
                       Mean reward: 10907.69
               Mean episode length: 412.10
                 Mean success rate: 80.50
                  Mean reward/step: 26.34
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 2.51s
                        Total time: 8757.62s
                               ETA: 512532.1s

################################################################################
                    [1m Learning iteration 3360/200000 [0m

                       Computation: 3282 steps/s (collection: 0.450s, learning 2.046s)
               Value function loss: 137851.6313
                    Surrogate loss: 0.0106
             Mean action noise std: 0.89
                       Mean reward: 11145.78
               Mean episode length: 415.93
                 Mean success rate: 82.50
                  Mean reward/step: 25.49
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 27533312
                    Iteration time: 2.50s
                        Total time: 8760.12s
                               ETA: 512523.0s

################################################################################
                    [1m Learning iteration 3361/200000 [0m

                       Computation: 3275 steps/s (collection: 0.452s, learning 2.049s)
               Value function loss: 105607.1088
                    Surrogate loss: 0.0112
             Mean action noise std: 0.89
                       Mean reward: 11044.46
               Mean episode length: 414.24
                 Mean success rate: 82.50
                  Mean reward/step: 24.70
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 27541504
                    Iteration time: 2.50s
                        Total time: 8762.62s
                               ETA: 512514.2s

################################################################################
                    [1m Learning iteration 3362/200000 [0m

                       Computation: 3290 steps/s (collection: 0.447s, learning 2.043s)
               Value function loss: 77513.1229
                    Surrogate loss: 0.0117
             Mean action noise std: 0.89
                       Mean reward: 11109.84
               Mean episode length: 415.60
                 Mean success rate: 83.00
                  Mean reward/step: 24.80
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 27549696
                    Iteration time: 2.49s
                        Total time: 8765.11s
                               ETA: 512504.8s

################################################################################
                    [1m Learning iteration 3363/200000 [0m

                       Computation: 3248 steps/s (collection: 0.451s, learning 2.071s)
               Value function loss: 97700.7774
                    Surrogate loss: 0.0117
             Mean action noise std: 0.89
                       Mean reward: 10957.84
               Mean episode length: 411.53
                 Mean success rate: 82.00
                  Mean reward/step: 25.57
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 27557888
                    Iteration time: 2.52s
                        Total time: 8767.63s
                               ETA: 512497.2s

################################################################################
                    [1m Learning iteration 3364/200000 [0m

                       Computation: 3239 steps/s (collection: 0.466s, learning 2.063s)
               Value function loss: 81950.1068
                    Surrogate loss: 0.0124
             Mean action noise std: 0.89
                       Mean reward: 10878.66
               Mean episode length: 410.92
                 Mean success rate: 81.00
                  Mean reward/step: 25.74
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 27566080
                    Iteration time: 2.53s
                        Total time: 8770.16s
                               ETA: 512490.1s

################################################################################
                    [1m Learning iteration 3365/200000 [0m

                       Computation: 3291 steps/s (collection: 0.433s, learning 2.056s)
               Value function loss: 111374.7073
                    Surrogate loss: 0.0110
             Mean action noise std: 0.89
                       Mean reward: 10936.11
               Mean episode length: 411.30
                 Mean success rate: 81.50
                  Mean reward/step: 26.09
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 27574272
                    Iteration time: 2.49s
                        Total time: 8772.65s
                               ETA: 512480.6s

################################################################################
                    [1m Learning iteration 3366/200000 [0m

                       Computation: 3267 steps/s (collection: 0.466s, learning 2.041s)
               Value function loss: 84771.9992
                    Surrogate loss: 0.0094
             Mean action noise std: 0.89
                       Mean reward: 10995.10
               Mean episode length: 413.56
                 Mean success rate: 82.50
                  Mean reward/step: 26.22
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 27582464
                    Iteration time: 2.51s
                        Total time: 8775.16s
                               ETA: 512472.3s

################################################################################
                    [1m Learning iteration 3367/200000 [0m

                       Computation: 3287 steps/s (collection: 0.448s, learning 2.044s)
               Value function loss: 64303.3473
                    Surrogate loss: 0.0137
             Mean action noise std: 0.89
                       Mean reward: 11076.70
               Mean episode length: 418.19
                 Mean success rate: 82.50
                  Mean reward/step: 26.86
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 27590656
                    Iteration time: 2.49s
                        Total time: 8777.65s
                               ETA: 512463.0s

################################################################################
                    [1m Learning iteration 3368/200000 [0m

                       Computation: 3207 steps/s (collection: 0.456s, learning 2.098s)
               Value function loss: 108904.2646
                    Surrogate loss: 0.0109
             Mean action noise std: 0.89
                       Mean reward: 11190.48
               Mean episode length: 423.74
                 Mean success rate: 82.50
                  Mean reward/step: 27.06
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 27598848
                    Iteration time: 2.55s
                        Total time: 8780.20s
                               ETA: 512457.3s

################################################################################
                    [1m Learning iteration 3369/200000 [0m

                       Computation: 3350 steps/s (collection: 0.411s, learning 2.034s)
               Value function loss: 71461.8284
                    Surrogate loss: 0.0106
             Mean action noise std: 0.89
                       Mean reward: 10953.66
               Mean episode length: 416.69
                 Mean success rate: 81.00
                  Mean reward/step: 26.86
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 27607040
                    Iteration time: 2.44s
                        Total time: 8782.65s
                               ETA: 512445.3s

################################################################################
                    [1m Learning iteration 3370/200000 [0m

                       Computation: 3142 steps/s (collection: 0.505s, learning 2.102s)
               Value function loss: 107416.6652
                    Surrogate loss: 0.0108
             Mean action noise std: 0.89
                       Mean reward: 10897.82
               Mean episode length: 415.68
                 Mean success rate: 81.00
                  Mean reward/step: 27.29
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 27615232
                    Iteration time: 2.61s
                        Total time: 8785.25s
                               ETA: 512442.7s

################################################################################
                    [1m Learning iteration 3371/200000 [0m

                       Computation: 3193 steps/s (collection: 0.491s, learning 2.074s)
               Value function loss: 84484.0110
                    Surrogate loss: 0.0124
             Mean action noise std: 0.89
                       Mean reward: 10753.87
               Mean episode length: 413.23
                 Mean success rate: 80.50
                  Mean reward/step: 26.95
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 2.57s
                        Total time: 8787.82s
                               ETA: 512437.7s

################################################################################
                    [1m Learning iteration 3372/200000 [0m

                       Computation: 3173 steps/s (collection: 0.468s, learning 2.114s)
               Value function loss: 137805.3680
                    Surrogate loss: 0.0123
             Mean action noise std: 0.89
                       Mean reward: 10886.94
               Mean episode length: 417.48
                 Mean success rate: 81.00
                  Mean reward/step: 26.00
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 27631616
                    Iteration time: 2.58s
                        Total time: 8790.40s
                               ETA: 512433.7s

################################################################################
                    [1m Learning iteration 3373/200000 [0m

                       Computation: 3220 steps/s (collection: 0.440s, learning 2.104s)
               Value function loss: 88836.8477
                    Surrogate loss: 0.0085
             Mean action noise std: 0.89
                       Mean reward: 10888.48
               Mean episode length: 417.82
                 Mean success rate: 81.50
                  Mean reward/step: 25.28
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 27639808
                    Iteration time: 2.54s
                        Total time: 8792.94s
                               ETA: 512427.5s

################################################################################
                    [1m Learning iteration 3374/200000 [0m

                       Computation: 3134 steps/s (collection: 0.524s, learning 2.090s)
               Value function loss: 80324.5147
                    Surrogate loss: 0.0103
             Mean action noise std: 0.89
                       Mean reward: 10682.28
               Mean episode length: 410.72
                 Mean success rate: 80.00
                  Mean reward/step: 26.62
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 27648000
                    Iteration time: 2.61s
                        Total time: 8795.56s
                               ETA: 512425.3s

################################################################################
                    [1m Learning iteration 3375/200000 [0m

                       Computation: 3177 steps/s (collection: 0.472s, learning 2.107s)
               Value function loss: 118137.8545
                    Surrogate loss: 0.0108
             Mean action noise std: 0.89
                       Mean reward: 10730.97
               Mean episode length: 410.71
                 Mean success rate: 80.50
                  Mean reward/step: 26.60
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 27656192
                    Iteration time: 2.58s
                        Total time: 8798.14s
                               ETA: 512421.1s

################################################################################
                    [1m Learning iteration 3376/200000 [0m

                       Computation: 3217 steps/s (collection: 0.464s, learning 2.082s)
               Value function loss: 127197.8564
                    Surrogate loss: 0.0128
             Mean action noise std: 0.89
                       Mean reward: 11046.94
               Mean episode length: 420.06
                 Mean success rate: 82.50
                  Mean reward/step: 25.85
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 27664384
                    Iteration time: 2.55s
                        Total time: 8800.68s
                               ETA: 512415.0s

################################################################################
                    [1m Learning iteration 3377/200000 [0m

                       Computation: 3175 steps/s (collection: 0.484s, learning 2.096s)
               Value function loss: 113713.5142
                    Surrogate loss: 0.0093
             Mean action noise std: 0.89
                       Mean reward: 11319.86
               Mean episode length: 427.73
                 Mean success rate: 84.50
                  Mean reward/step: 25.25
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 27672576
                    Iteration time: 2.58s
                        Total time: 8803.26s
                               ETA: 512410.8s

################################################################################
                    [1m Learning iteration 3378/200000 [0m

                       Computation: 3175 steps/s (collection: 0.465s, learning 2.115s)
               Value function loss: 97338.7012
                    Surrogate loss: 0.0119
             Mean action noise std: 0.89
                       Mean reward: 11374.88
               Mean episode length: 431.33
                 Mean success rate: 86.00
                  Mean reward/step: 25.12
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 27680768
                    Iteration time: 2.58s
                        Total time: 8805.84s
                               ETA: 512406.7s

################################################################################
                    [1m Learning iteration 3379/200000 [0m

                       Computation: 3246 steps/s (collection: 0.445s, learning 2.078s)
               Value function loss: 105553.6168
                    Surrogate loss: 0.0095
             Mean action noise std: 0.89
                       Mean reward: 11597.66
               Mean episode length: 436.35
                 Mean success rate: 87.50
                  Mean reward/step: 25.59
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 27688960
                    Iteration time: 2.52s
                        Total time: 8808.37s
                               ETA: 512399.3s

################################################################################
                    [1m Learning iteration 3380/200000 [0m

                       Computation: 3194 steps/s (collection: 0.464s, learning 2.101s)
               Value function loss: 82106.3814
                    Surrogate loss: 0.0100
             Mean action noise std: 0.89
                       Mean reward: 11583.61
               Mean episode length: 437.57
                 Mean success rate: 87.00
                  Mean reward/step: 25.45
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 27697152
                    Iteration time: 2.56s
                        Total time: 8810.93s
                               ETA: 512394.2s

################################################################################
                    [1m Learning iteration 3381/200000 [0m

                       Computation: 3129 steps/s (collection: 0.491s, learning 2.127s)
               Value function loss: 101860.0338
                    Surrogate loss: 0.0096
             Mean action noise std: 0.89
                       Mean reward: 11549.09
               Mean episode length: 437.02
                 Mean success rate: 87.50
                  Mean reward/step: 25.68
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 27705344
                    Iteration time: 2.62s
                        Total time: 8813.55s
                               ETA: 512392.3s

################################################################################
                    [1m Learning iteration 3382/200000 [0m

                       Computation: 3113 steps/s (collection: 0.474s, learning 2.157s)
               Value function loss: 54177.9396
                    Surrogate loss: 0.0119
             Mean action noise std: 0.89
                       Mean reward: 11281.66
               Mean episode length: 428.90
                 Mean success rate: 86.00
                  Mean reward/step: 25.89
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 27713536
                    Iteration time: 2.63s
                        Total time: 8816.18s
                               ETA: 512391.2s

################################################################################
                    [1m Learning iteration 3383/200000 [0m

                       Computation: 3191 steps/s (collection: 0.464s, learning 2.103s)
               Value function loss: 90244.8321
                    Surrogate loss: 0.0082
             Mean action noise std: 0.89
                       Mean reward: 11307.64
               Mean episode length: 429.65
                 Mean success rate: 87.00
                  Mean reward/step: 26.61
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 2.57s
                        Total time: 8818.75s
                               ETA: 512386.3s

################################################################################
                    [1m Learning iteration 3384/200000 [0m

                       Computation: 3154 steps/s (collection: 0.463s, learning 2.134s)
               Value function loss: 90784.4160
                    Surrogate loss: 0.0102
             Mean action noise std: 0.89
                       Mean reward: 11415.72
               Mean episode length: 433.53
                 Mean success rate: 87.50
                  Mean reward/step: 26.55
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 27729920
                    Iteration time: 2.60s
                        Total time: 8821.34s
                               ETA: 512383.2s

################################################################################
                    [1m Learning iteration 3385/200000 [0m

                       Computation: 3158 steps/s (collection: 0.478s, learning 2.116s)
               Value function loss: 68202.2703
                    Surrogate loss: 0.0102
             Mean action noise std: 0.89
                       Mean reward: 11673.98
               Mean episode length: 440.70
                 Mean success rate: 89.00
                  Mean reward/step: 26.57
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 27738112
                    Iteration time: 2.59s
                        Total time: 8823.94s
                               ETA: 512379.9s

################################################################################
                    [1m Learning iteration 3386/200000 [0m

                       Computation: 3103 steps/s (collection: 0.521s, learning 2.119s)
               Value function loss: 77070.6803
                    Surrogate loss: 0.0096
             Mean action noise std: 0.89
                       Mean reward: 11606.79
               Mean episode length: 441.08
                 Mean success rate: 89.00
                  Mean reward/step: 26.89
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 27746304
                    Iteration time: 2.64s
                        Total time: 8826.58s
                               ETA: 512379.2s

################################################################################
                    [1m Learning iteration 3387/200000 [0m

                       Computation: 3089 steps/s (collection: 0.486s, learning 2.166s)
               Value function loss: 109958.5217
                    Surrogate loss: 0.0084
             Mean action noise std: 0.89
                       Mean reward: 11722.11
               Mean episode length: 445.46
                 Mean success rate: 89.00
                  Mean reward/step: 26.82
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 27754496
                    Iteration time: 2.65s
                        Total time: 8829.23s
                               ETA: 512379.2s

################################################################################
                    [1m Learning iteration 3388/200000 [0m

                       Computation: 3143 steps/s (collection: 0.477s, learning 2.129s)
               Value function loss: 116239.3777
                    Surrogate loss: 0.0120
             Mean action noise std: 0.89
                       Mean reward: 11704.93
               Mean episode length: 445.46
                 Mean success rate: 89.00
                  Mean reward/step: 26.04
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 27762688
                    Iteration time: 2.61s
                        Total time: 8831.83s
                               ETA: 512376.7s

################################################################################
                    [1m Learning iteration 3389/200000 [0m

                       Computation: 3152 steps/s (collection: 0.468s, learning 2.130s)
               Value function loss: 72146.0160
                    Surrogate loss: 0.0110
             Mean action noise std: 0.89
                       Mean reward: 11482.24
               Mean episode length: 438.00
                 Mean success rate: 87.50
                  Mean reward/step: 25.94
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 27770880
                    Iteration time: 2.60s
                        Total time: 8834.43s
                               ETA: 512373.6s

################################################################################
                    [1m Learning iteration 3390/200000 [0m

                       Computation: 3116 steps/s (collection: 0.487s, learning 2.141s)
               Value function loss: 111709.7108
                    Surrogate loss: 0.0092
             Mean action noise std: 0.89
                       Mean reward: 11565.25
               Mean episode length: 440.98
                 Mean success rate: 88.50
                  Mean reward/step: 26.45
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 27779072
                    Iteration time: 2.63s
                        Total time: 8837.06s
                               ETA: 512372.3s

################################################################################
                    [1m Learning iteration 3391/200000 [0m

                       Computation: 3124 steps/s (collection: 0.496s, learning 2.126s)
               Value function loss: 126355.1137
                    Surrogate loss: 0.0103
             Mean action noise std: 0.89
                       Mean reward: 11642.54
               Mean episode length: 442.74
                 Mean success rate: 89.00
                  Mean reward/step: 25.99
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 27787264
                    Iteration time: 2.62s
                        Total time: 8839.68s
                               ETA: 512370.6s

################################################################################
                    [1m Learning iteration 3392/200000 [0m

                       Computation: 3236 steps/s (collection: 0.477s, learning 2.055s)
               Value function loss: 94304.8601
                    Surrogate loss: 0.0110
             Mean action noise std: 0.89
                       Mean reward: 11607.88
               Mean episode length: 441.43
                 Mean success rate: 88.50
                  Mean reward/step: 25.47
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 27795456
                    Iteration time: 2.53s
                        Total time: 8842.21s
                               ETA: 512363.7s

################################################################################
                    [1m Learning iteration 3393/200000 [0m

                       Computation: 3306 steps/s (collection: 0.438s, learning 2.039s)
               Value function loss: 97505.5047
                    Surrogate loss: 0.0097
             Mean action noise std: 0.89
                       Mean reward: 11838.14
               Mean episode length: 450.07
                 Mean success rate: 90.00
                  Mean reward/step: 25.23
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 27803648
                    Iteration time: 2.48s
                        Total time: 8844.69s
                               ETA: 512353.6s

################################################################################
                    [1m Learning iteration 3394/200000 [0m

                       Computation: 3274 steps/s (collection: 0.443s, learning 2.059s)
               Value function loss: 98138.6082
                    Surrogate loss: 0.0082
             Mean action noise std: 0.89
                       Mean reward: 11718.88
               Mean episode length: 445.53
                 Mean success rate: 89.50
                  Mean reward/step: 25.98
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 27811840
                    Iteration time: 2.50s
                        Total time: 8847.19s
                               ETA: 512345.0s

################################################################################
                    [1m Learning iteration 3395/200000 [0m

                       Computation: 3297 steps/s (collection: 0.453s, learning 2.031s)
               Value function loss: 117537.1014
                    Surrogate loss: 0.0085
             Mean action noise std: 0.89
                       Mean reward: 11546.06
               Mean episode length: 442.11
                 Mean success rate: 88.50
                  Mean reward/step: 26.41
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 2.48s
                        Total time: 8849.68s
                               ETA: 512335.4s

################################################################################
                    [1m Learning iteration 3396/200000 [0m

                       Computation: 3241 steps/s (collection: 0.450s, learning 2.078s)
               Value function loss: 108893.0088
                    Surrogate loss: 0.0097
             Mean action noise std: 0.89
                       Mean reward: 11324.61
               Mean episode length: 433.74
                 Mean success rate: 87.50
                  Mean reward/step: 26.06
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 27828224
                    Iteration time: 2.53s
                        Total time: 8852.21s
                               ETA: 512328.2s

################################################################################
                    [1m Learning iteration 3397/200000 [0m

                       Computation: 3276 steps/s (collection: 0.425s, learning 2.075s)
               Value function loss: 75366.9233
                    Surrogate loss: 0.0099
             Mean action noise std: 0.89
                       Mean reward: 11120.62
               Mean episode length: 428.99
                 Mean success rate: 86.50
                  Mean reward/step: 25.64
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 27836416
                    Iteration time: 2.50s
                        Total time: 8854.71s
                               ETA: 512319.5s

################################################################################
                    [1m Learning iteration 3398/200000 [0m

                       Computation: 3281 steps/s (collection: 0.428s, learning 2.068s)
               Value function loss: 63774.1616
                    Surrogate loss: 0.0071
             Mean action noise std: 0.89
                       Mean reward: 11015.95
               Mean episode length: 425.01
                 Mean success rate: 86.00
                  Mean reward/step: 26.22
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 27844608
                    Iteration time: 2.50s
                        Total time: 8857.20s
                               ETA: 512310.5s

################################################################################
                    [1m Learning iteration 3399/200000 [0m

                       Computation: 3258 steps/s (collection: 0.454s, learning 2.060s)
               Value function loss: 121848.1604
                    Surrogate loss: 0.0101
             Mean action noise std: 0.89
                       Mean reward: 11232.57
               Mean episode length: 430.17
                 Mean success rate: 86.00
                  Mean reward/step: 26.60
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 27852800
                    Iteration time: 2.51s
                        Total time: 8859.72s
                               ETA: 512302.6s

################################################################################
                    [1m Learning iteration 3400/200000 [0m

                       Computation: 3272 steps/s (collection: 0.445s, learning 2.059s)
               Value function loss: 83232.4200
                    Surrogate loss: 0.0095
             Mean action noise std: 0.89
                       Mean reward: 11046.25
               Mean episode length: 424.48
                 Mean success rate: 85.00
                  Mean reward/step: 26.58
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 27860992
                    Iteration time: 2.50s
                        Total time: 8862.22s
                               ETA: 512294.1s

################################################################################
                    [1m Learning iteration 3401/200000 [0m

                       Computation: 3253 steps/s (collection: 0.430s, learning 2.088s)
               Value function loss: 56792.3444
                    Surrogate loss: 0.0092
             Mean action noise std: 0.89
                       Mean reward: 11135.44
               Mean episode length: 425.76
                 Mean success rate: 85.00
                  Mean reward/step: 27.23
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 27869184
                    Iteration time: 2.52s
                        Total time: 8864.74s
                               ETA: 512286.4s

################################################################################
                    [1m Learning iteration 3402/200000 [0m

                       Computation: 3297 steps/s (collection: 0.439s, learning 2.045s)
               Value function loss: 74258.0213
                    Surrogate loss: 0.0106
             Mean action noise std: 0.89
                       Mean reward: 11112.39
               Mean episode length: 426.81
                 Mean success rate: 85.00
                  Mean reward/step: 27.83
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 27877376
                    Iteration time: 2.48s
                        Total time: 8867.22s
                               ETA: 512276.8s

################################################################################
                    [1m Learning iteration 3403/200000 [0m

                       Computation: 3303 steps/s (collection: 0.442s, learning 2.038s)
               Value function loss: 124960.4240
                    Surrogate loss: 0.0117
             Mean action noise std: 0.89
                       Mean reward: 11331.25
               Mean episode length: 434.24
                 Mean success rate: 86.50
                  Mean reward/step: 28.12
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 27885568
                    Iteration time: 2.48s
                        Total time: 8869.70s
                               ETA: 512266.9s

################################################################################
                    [1m Learning iteration 3404/200000 [0m

                       Computation: 3284 steps/s (collection: 0.424s, learning 2.070s)
               Value function loss: 93938.1733
                    Surrogate loss: 0.0102
             Mean action noise std: 0.89
                       Mean reward: 11368.95
               Mean episode length: 433.19
                 Mean success rate: 86.00
                  Mean reward/step: 26.85
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 27893760
                    Iteration time: 2.49s
                        Total time: 8872.20s
                               ETA: 512257.9s

################################################################################
                    [1m Learning iteration 3405/200000 [0m

                       Computation: 3216 steps/s (collection: 0.452s, learning 2.095s)
               Value function loss: 81188.4732
                    Surrogate loss: 0.0111
             Mean action noise std: 0.89
                       Mean reward: 11320.58
               Mean episode length: 433.52
                 Mean success rate: 86.00
                  Mean reward/step: 26.87
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 27901952
                    Iteration time: 2.55s
                        Total time: 8874.74s
                               ETA: 512251.9s

################################################################################
                    [1m Learning iteration 3406/200000 [0m

                       Computation: 3059 steps/s (collection: 0.516s, learning 2.162s)
               Value function loss: 107106.9589
                    Surrogate loss: 0.0098
             Mean action noise std: 0.89
                       Mean reward: 11368.79
               Mean episode length: 433.54
                 Mean success rate: 87.00
                  Mean reward/step: 27.17
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 27910144
                    Iteration time: 2.68s
                        Total time: 8877.42s
                               ETA: 512253.5s

################################################################################
                    [1m Learning iteration 3407/200000 [0m

                       Computation: 3136 steps/s (collection: 0.471s, learning 2.140s)
               Value function loss: 105044.2458
                    Surrogate loss: 0.0122
             Mean action noise std: 0.89
                       Mean reward: 11719.76
               Mean episode length: 445.12
                 Mean success rate: 89.00
                  Mean reward/step: 27.07
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 2.61s
                        Total time: 8880.03s
                               ETA: 512251.2s

################################################################################
                    [1m Learning iteration 3408/200000 [0m

                       Computation: 3228 steps/s (collection: 0.462s, learning 2.075s)
               Value function loss: 127487.0381
                    Surrogate loss: 0.0112
             Mean action noise std: 0.89
                       Mean reward: 12017.56
               Mean episode length: 453.87
                 Mean success rate: 91.00
                  Mean reward/step: 25.98
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 27926528
                    Iteration time: 2.54s
                        Total time: 8882.57s
                               ETA: 512244.7s

################################################################################
                    [1m Learning iteration 3409/200000 [0m

                       Computation: 3258 steps/s (collection: 0.440s, learning 2.074s)
               Value function loss: 71179.5982
                    Surrogate loss: 0.0104
             Mean action noise std: 0.89
                       Mean reward: 12156.79
               Mean episode length: 457.84
                 Mean success rate: 91.50
                  Mean reward/step: 25.65
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 27934720
                    Iteration time: 2.51s
                        Total time: 8885.08s
                               ETA: 512236.8s

################################################################################
                    [1m Learning iteration 3410/200000 [0m

                       Computation: 3128 steps/s (collection: 0.499s, learning 2.119s)
               Value function loss: 97060.3237
                    Surrogate loss: 0.0099
             Mean action noise std: 0.89
                       Mean reward: 12010.57
               Mean episode length: 453.33
                 Mean success rate: 90.50
                  Mean reward/step: 25.92
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 27942912
                    Iteration time: 2.62s
                        Total time: 8887.70s
                               ETA: 512234.9s

################################################################################
                    [1m Learning iteration 3411/200000 [0m

                       Computation: 3115 steps/s (collection: 0.486s, learning 2.143s)
               Value function loss: 126975.9807
                    Surrogate loss: 0.0102
             Mean action noise std: 0.89
                       Mean reward: 11835.49
               Mean episode length: 448.75
                 Mean success rate: 89.50
                  Mean reward/step: 25.59
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 27951104
                    Iteration time: 2.63s
                        Total time: 8890.33s
                               ETA: 512233.7s

################################################################################
                    [1m Learning iteration 3412/200000 [0m

                       Computation: 3113 steps/s (collection: 0.515s, learning 2.116s)
               Value function loss: 105754.2922
                    Surrogate loss: 0.0105
             Mean action noise std: 0.89
                       Mean reward: 11814.47
               Mean episode length: 445.57
                 Mean success rate: 89.50
                  Mean reward/step: 24.66
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 27959296
                    Iteration time: 2.63s
                        Total time: 8892.96s
                               ETA: 512232.6s

################################################################################
                    [1m Learning iteration 3413/200000 [0m

                       Computation: 3200 steps/s (collection: 0.478s, learning 2.082s)
               Value function loss: 92361.6367
                    Surrogate loss: 0.0101
             Mean action noise std: 0.89
                       Mean reward: 11554.44
               Mean episode length: 435.68
                 Mean success rate: 88.00
                  Mean reward/step: 25.72
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 27967488
                    Iteration time: 2.56s
                        Total time: 8895.52s
                               ETA: 512227.3s

################################################################################
                    [1m Learning iteration 3414/200000 [0m

                       Computation: 3189 steps/s (collection: 0.447s, learning 2.122s)
               Value function loss: 79808.4062
                    Surrogate loss: 0.0097
             Mean action noise std: 0.89
                       Mean reward: 11662.73
               Mean episode length: 439.36
                 Mean success rate: 89.00
                  Mean reward/step: 26.51
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 27975680
                    Iteration time: 2.57s
                        Total time: 8898.09s
                               ETA: 512222.6s

################################################################################
                    [1m Learning iteration 3415/200000 [0m

                       Computation: 3138 steps/s (collection: 0.507s, learning 2.104s)
               Value function loss: 107988.3749
                    Surrogate loss: 0.0084
             Mean action noise std: 0.89
                       Mean reward: 11467.54
               Mean episode length: 433.01
                 Mean success rate: 88.00
                  Mean reward/step: 26.63
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 27983872
                    Iteration time: 2.61s
                        Total time: 8900.70s
                               ETA: 512220.2s

################################################################################
                    [1m Learning iteration 3416/200000 [0m

                       Computation: 3268 steps/s (collection: 0.457s, learning 2.050s)
               Value function loss: 91737.3999
                    Surrogate loss: 0.0075
             Mean action noise std: 0.89
                       Mean reward: 11559.80
               Mean episode length: 436.14
                 Mean success rate: 88.00
                  Mean reward/step: 26.09
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 27992064
                    Iteration time: 2.51s
                        Total time: 8903.21s
                               ETA: 512211.9s

################################################################################
                    [1m Learning iteration 3417/200000 [0m

                       Computation: 3229 steps/s (collection: 0.437s, learning 2.100s)
               Value function loss: 94455.7983
                    Surrogate loss: 0.0100
             Mean action noise std: 0.89
                       Mean reward: 11600.52
               Mean episode length: 436.14
                 Mean success rate: 88.00
                  Mean reward/step: 26.28
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 28000256
                    Iteration time: 2.54s
                        Total time: 8905.74s
                               ETA: 512205.4s

################################################################################
                    [1m Learning iteration 3418/200000 [0m

                       Computation: 3277 steps/s (collection: 0.433s, learning 2.067s)
               Value function loss: 74881.6493
                    Surrogate loss: 0.0099
             Mean action noise std: 0.89
                       Mean reward: 11615.29
               Mean episode length: 436.14
                 Mean success rate: 88.00
                  Mean reward/step: 26.97
       Mean episode length/episode: 30.91
--------------------------------------------------------------------------------
                   Total timesteps: 28008448
                    Iteration time: 2.50s
                        Total time: 8908.24s
                               ETA: 512196.7s

################################################################################
                    [1m Learning iteration 3419/200000 [0m

                       Computation: 3208 steps/s (collection: 0.507s, learning 2.046s)
               Value function loss: 118030.2980
                    Surrogate loss: 0.0159
             Mean action noise std: 0.89
                       Mean reward: 11385.03
               Mean episode length: 429.64
                 Mean success rate: 86.00
                  Mean reward/step: 26.67
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 2.55s
                        Total time: 8910.80s
                               ETA: 512191.0s

################################################################################
                    [1m Learning iteration 3420/200000 [0m

                       Computation: 3223 steps/s (collection: 0.462s, learning 2.079s)
               Value function loss: 77076.5455
                    Surrogate loss: 0.0096
             Mean action noise std: 0.89
                       Mean reward: 11363.89
               Mean episode length: 427.87
                 Mean success rate: 86.00
                  Mean reward/step: 25.33
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 28024832
                    Iteration time: 2.54s
                        Total time: 8913.34s
                               ETA: 512184.7s

################################################################################
                    [1m Learning iteration 3421/200000 [0m

                       Computation: 3263 steps/s (collection: 0.466s, learning 2.044s)
               Value function loss: 123520.4311
                    Surrogate loss: 0.0121
             Mean action noise std: 0.89
                       Mean reward: 11334.34
               Mean episode length: 430.50
                 Mean success rate: 86.50
                  Mean reward/step: 26.09
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 28033024
                    Iteration time: 2.51s
                        Total time: 8915.85s
                               ETA: 512176.6s

################################################################################
                    [1m Learning iteration 3422/200000 [0m

                       Computation: 3258 steps/s (collection: 0.455s, learning 2.059s)
               Value function loss: 93531.6335
                    Surrogate loss: 0.0108
             Mean action noise std: 0.89
                       Mean reward: 11501.93
               Mean episode length: 436.51
                 Mean success rate: 87.50
                  Mean reward/step: 26.02
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 28041216
                    Iteration time: 2.51s
                        Total time: 8918.36s
                               ETA: 512168.8s

################################################################################
                    [1m Learning iteration 3423/200000 [0m

                       Computation: 3229 steps/s (collection: 0.490s, learning 2.047s)
               Value function loss: 109600.7543
                    Surrogate loss: 0.0104
             Mean action noise std: 0.89
                       Mean reward: 11671.27
               Mean episode length: 445.79
                 Mean success rate: 89.00
                  Mean reward/step: 25.60
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 28049408
                    Iteration time: 2.54s
                        Total time: 8920.90s
                               ETA: 512162.2s

################################################################################
                    [1m Learning iteration 3424/200000 [0m

                       Computation: 3254 steps/s (collection: 0.460s, learning 2.057s)
               Value function loss: 109138.5487
                    Surrogate loss: 0.0109
             Mean action noise std: 0.89
                       Mean reward: 11806.89
               Mean episode length: 451.31
                 Mean success rate: 90.00
                  Mean reward/step: 25.20
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 28057600
                    Iteration time: 2.52s
                        Total time: 8923.42s
                               ETA: 512154.6s

################################################################################
                    [1m Learning iteration 3425/200000 [0m

                       Computation: 3269 steps/s (collection: 0.443s, learning 2.062s)
               Value function loss: 79773.5132
                    Surrogate loss: 0.0120
             Mean action noise std: 0.89
                       Mean reward: 11895.04
               Mean episode length: 453.68
                 Mean success rate: 90.00
                  Mean reward/step: 25.79
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 28065792
                    Iteration time: 2.51s
                        Total time: 8925.92s
                               ETA: 512146.2s

################################################################################
                    [1m Learning iteration 3426/200000 [0m

                       Computation: 3278 steps/s (collection: 0.451s, learning 2.048s)
               Value function loss: 120785.1600
                    Surrogate loss: 0.0128
             Mean action noise std: 0.89
                       Mean reward: 11930.96
               Mean episode length: 452.90
                 Mean success rate: 89.00
                  Mean reward/step: 26.75
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 28073984
                    Iteration time: 2.50s
                        Total time: 8928.42s
                               ETA: 512137.5s

################################################################################
                    [1m Learning iteration 3427/200000 [0m

                       Computation: 3176 steps/s (collection: 0.469s, learning 2.110s)
               Value function loss: 119718.8181
                    Surrogate loss: 0.0091
             Mean action noise std: 0.89
                       Mean reward: 12137.47
               Mean episode length: 462.30
                 Mean success rate: 90.00
                  Mean reward/step: 25.68
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 28082176
                    Iteration time: 2.58s
                        Total time: 8931.00s
                               ETA: 512133.4s

################################################################################
                    [1m Learning iteration 3428/200000 [0m

                       Computation: 3264 steps/s (collection: 0.434s, learning 2.075s)
               Value function loss: 83776.9771
                    Surrogate loss: 0.0088
             Mean action noise std: 0.89
                       Mean reward: 12142.50
               Mean episode length: 462.30
                 Mean success rate: 90.00
                  Mean reward/step: 25.44
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 28090368
                    Iteration time: 2.51s
                        Total time: 8933.51s
                               ETA: 512125.3s

################################################################################
                    [1m Learning iteration 3429/200000 [0m

                       Computation: 3129 steps/s (collection: 0.498s, learning 2.120s)
               Value function loss: 80595.6626
                    Surrogate loss: 0.0094
             Mean action noise std: 0.89
                       Mean reward: 11502.27
               Mean episode length: 447.56
                 Mean success rate: 86.50
                  Mean reward/step: 26.09
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 28098560
                    Iteration time: 2.62s
                        Total time: 8936.13s
                               ETA: 512123.4s

################################################################################
                    [1m Learning iteration 3430/200000 [0m

                       Computation: 3196 steps/s (collection: 0.475s, learning 2.088s)
               Value function loss: 92975.3204
                    Surrogate loss: 0.0086
             Mean action noise std: 0.89
                       Mean reward: 11364.87
               Mean episode length: 442.37
                 Mean success rate: 85.50
                  Mean reward/step: 26.46
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 28106752
                    Iteration time: 2.56s
                        Total time: 8938.69s
                               ETA: 512118.4s

################################################################################
                    [1m Learning iteration 3431/200000 [0m

                       Computation: 3184 steps/s (collection: 0.467s, learning 2.106s)
               Value function loss: 99399.1009
                    Surrogate loss: 0.0129
             Mean action noise std: 0.89
                       Mean reward: 11436.78
               Mean episode length: 442.30
                 Mean success rate: 85.00
                  Mean reward/step: 26.15
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 2.57s
                        Total time: 8941.26s
                               ETA: 512113.9s

################################################################################
                    [1m Learning iteration 3432/200000 [0m

                       Computation: 3209 steps/s (collection: 0.457s, learning 2.096s)
               Value function loss: 67540.0577
                    Surrogate loss: 0.0109
             Mean action noise std: 0.89
                       Mean reward: 11522.81
               Mean episode length: 443.65
                 Mean success rate: 86.00
                  Mean reward/step: 26.25
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 28123136
                    Iteration time: 2.55s
                        Total time: 8943.81s
                               ETA: 512108.2s

################################################################################
                    [1m Learning iteration 3433/200000 [0m

                       Computation: 3277 steps/s (collection: 0.453s, learning 2.046s)
               Value function loss: 67291.6921
                    Surrogate loss: 0.0121
             Mean action noise std: 0.89
                       Mean reward: 11338.69
               Mean episode length: 435.93
                 Mean success rate: 84.00
                  Mean reward/step: 26.68
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 28131328
                    Iteration time: 2.50s
                        Total time: 8946.31s
                               ETA: 512099.6s

################################################################################
                    [1m Learning iteration 3434/200000 [0m

                       Computation: 3258 steps/s (collection: 0.447s, learning 2.067s)
               Value function loss: 102149.2301
                    Surrogate loss: 0.0080
             Mean action noise std: 0.89
                       Mean reward: 11197.29
               Mean episode length: 429.68
                 Mean success rate: 82.50
                  Mean reward/step: 27.18
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 28139520
                    Iteration time: 2.51s
                        Total time: 8948.83s
                               ETA: 512091.8s

################################################################################
                    [1m Learning iteration 3435/200000 [0m

                       Computation: 3207 steps/s (collection: 0.477s, learning 2.077s)
               Value function loss: 90196.2000
                    Surrogate loss: 0.0132
             Mean action noise std: 0.89
                       Mean reward: 11133.93
               Mean episode length: 427.32
                 Mean success rate: 82.00
                  Mean reward/step: 26.87
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 28147712
                    Iteration time: 2.55s
                        Total time: 8951.38s
                               ETA: 512086.2s

################################################################################
                    [1m Learning iteration 3436/200000 [0m

                       Computation: 3196 steps/s (collection: 0.486s, learning 2.076s)
               Value function loss: 63998.7167
                    Surrogate loss: 0.0125
             Mean action noise std: 0.89
                       Mean reward: 10796.85
               Mean episode length: 418.11
                 Mean success rate: 80.50
                  Mean reward/step: 26.77
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 28155904
                    Iteration time: 2.56s
                        Total time: 8953.94s
                               ETA: 512081.2s

################################################################################
                    [1m Learning iteration 3437/200000 [0m

                       Computation: 3211 steps/s (collection: 0.493s, learning 2.058s)
               Value function loss: 142628.6672
                    Surrogate loss: 0.0112
             Mean action noise std: 0.89
                       Mean reward: 10900.54
               Mean episode length: 419.23
                 Mean success rate: 81.50
                  Mean reward/step: 27.13
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 28164096
                    Iteration time: 2.55s
                        Total time: 8956.50s
                               ETA: 512075.5s

################################################################################
                    [1m Learning iteration 3438/200000 [0m

                       Computation: 3151 steps/s (collection: 0.480s, learning 2.120s)
               Value function loss: 90315.0368
                    Surrogate loss: 0.0115
             Mean action noise std: 0.89
                       Mean reward: 10427.17
               Mean episode length: 406.31
                 Mean success rate: 79.00
                  Mean reward/step: 25.80
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 28172288
                    Iteration time: 2.60s
                        Total time: 8959.09s
                               ETA: 512072.6s

################################################################################
                    [1m Learning iteration 3439/200000 [0m

                       Computation: 3239 steps/s (collection: 0.464s, learning 2.065s)
               Value function loss: 109810.1705
                    Surrogate loss: 0.0117
             Mean action noise std: 0.89
                       Mean reward: 10918.22
               Mean episode length: 418.25
                 Mean success rate: 82.00
                  Mean reward/step: 25.55
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 28180480
                    Iteration time: 2.53s
                        Total time: 8961.62s
                               ETA: 512065.6s

################################################################################
                    [1m Learning iteration 3440/200000 [0m

                       Computation: 3239 steps/s (collection: 0.455s, learning 2.074s)
               Value function loss: 91606.7195
                    Surrogate loss: 0.0089
             Mean action noise std: 0.89
                       Mean reward: 11088.56
               Mean episode length: 422.36
                 Mean success rate: 83.00
                  Mean reward/step: 25.42
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 28188672
                    Iteration time: 2.53s
                        Total time: 8964.15s
                               ETA: 512058.7s

################################################################################
                    [1m Learning iteration 3441/200000 [0m

                       Computation: 3170 steps/s (collection: 0.470s, learning 2.114s)
               Value function loss: 98275.8992
                    Surrogate loss: 0.0088
             Mean action noise std: 0.89
                       Mean reward: 11216.53
               Mean episode length: 424.08
                 Mean success rate: 84.00
                  Mean reward/step: 26.56
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 28196864
                    Iteration time: 2.58s
                        Total time: 8966.74s
                               ETA: 512054.8s

################################################################################
                    [1m Learning iteration 3442/200000 [0m

                       Computation: 3187 steps/s (collection: 0.460s, learning 2.110s)
               Value function loss: 137355.8005
                    Surrogate loss: 0.0109
             Mean action noise std: 0.89
                       Mean reward: 11276.67
               Mean episode length: 426.36
                 Mean success rate: 84.50
                  Mean reward/step: 26.53
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 28205056
                    Iteration time: 2.57s
                        Total time: 8969.31s
                               ETA: 512050.3s

################################################################################
                    [1m Learning iteration 3443/200000 [0m

                       Computation: 3182 steps/s (collection: 0.489s, learning 2.085s)
               Value function loss: 112130.3455
                    Surrogate loss: 0.0138
             Mean action noise std: 0.89
                       Mean reward: 11404.99
               Mean episode length: 429.91
                 Mean success rate: 85.00
                  Mean reward/step: 25.97
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 2.57s
                        Total time: 8971.88s
                               ETA: 512045.9s

################################################################################
                    [1m Learning iteration 3444/200000 [0m

                       Computation: 3200 steps/s (collection: 0.485s, learning 2.074s)
               Value function loss: 122401.5809
                    Surrogate loss: 0.0121
             Mean action noise std: 0.89
                       Mean reward: 11541.44
               Mean episode length: 433.94
                 Mean success rate: 86.00
                  Mean reward/step: 25.71
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 28221440
                    Iteration time: 2.56s
                        Total time: 8974.44s
                               ETA: 512040.7s

################################################################################
                    [1m Learning iteration 3445/200000 [0m

                       Computation: 3233 steps/s (collection: 0.449s, learning 2.085s)
               Value function loss: 89286.5454
                    Surrogate loss: 0.0109
             Mean action noise std: 0.89
                       Mean reward: 11404.58
               Mean episode length: 430.95
                 Mean success rate: 85.50
                  Mean reward/step: 26.00
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 28229632
                    Iteration time: 2.53s
                        Total time: 8976.97s
                               ETA: 512034.0s

################################################################################
                    [1m Learning iteration 3446/200000 [0m

                       Computation: 3162 steps/s (collection: 0.494s, learning 2.096s)
               Value function loss: 159590.2504
                    Surrogate loss: 0.0103
             Mean action noise std: 0.89
                       Mean reward: 11835.39
               Mean episode length: 443.08
                 Mean success rate: 87.50
                  Mean reward/step: 26.21
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 28237824
                    Iteration time: 2.59s
                        Total time: 8979.56s
                               ETA: 512030.5s

################################################################################
                    [1m Learning iteration 3447/200000 [0m

                       Computation: 3196 steps/s (collection: 0.468s, learning 2.094s)
               Value function loss: 119200.9530
                    Surrogate loss: 0.0102
             Mean action noise std: 0.89
                       Mean reward: 11926.33
               Mean episode length: 445.57
                 Mean success rate: 88.00
                  Mean reward/step: 25.34
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 28246016
                    Iteration time: 2.56s
                        Total time: 8982.13s
                               ETA: 512025.5s

################################################################################
                    [1m Learning iteration 3448/200000 [0m

                       Computation: 3184 steps/s (collection: 0.511s, learning 2.062s)
               Value function loss: 77253.3933
                    Surrogate loss: 0.0121
             Mean action noise std: 0.89
                       Mean reward: 11747.81
               Mean episode length: 439.71
                 Mean success rate: 87.50
                  Mean reward/step: 24.89
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 28254208
                    Iteration time: 2.57s
                        Total time: 8984.70s
                               ETA: 512021.1s

################################################################################
                    [1m Learning iteration 3449/200000 [0m

                       Computation: 3214 steps/s (collection: 0.472s, learning 2.077s)
               Value function loss: 88996.3987
                    Surrogate loss: 0.0101
             Mean action noise std: 0.89
                       Mean reward: 11762.44
               Mean episode length: 438.57
                 Mean success rate: 87.50
                  Mean reward/step: 26.04
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 28262400
                    Iteration time: 2.55s
                        Total time: 8987.25s
                               ETA: 512015.3s

################################################################################
                    [1m Learning iteration 3450/200000 [0m

                       Computation: 3207 steps/s (collection: 0.486s, learning 2.068s)
               Value function loss: 122638.5810
                    Surrogate loss: 0.0101
             Mean action noise std: 0.89
                       Mean reward: 11835.67
               Mean episode length: 441.06
                 Mean success rate: 88.00
                  Mean reward/step: 27.18
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 28270592
                    Iteration time: 2.55s
                        Total time: 8989.80s
                               ETA: 512009.7s

################################################################################
                    [1m Learning iteration 3451/200000 [0m

                       Computation: 3179 steps/s (collection: 0.502s, learning 2.074s)
               Value function loss: 81256.0178
                    Surrogate loss: 0.0097
             Mean action noise std: 0.89
                       Mean reward: 11483.71
               Mean episode length: 433.61
                 Mean success rate: 86.50
                  Mean reward/step: 26.80
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 28278784
                    Iteration time: 2.58s
                        Total time: 8992.38s
                               ETA: 512005.5s

################################################################################
                    [1m Learning iteration 3452/200000 [0m

                       Computation: 3229 steps/s (collection: 0.469s, learning 2.067s)
               Value function loss: 108021.6946
                    Surrogate loss: 0.0100
             Mean action noise std: 0.89
                       Mean reward: 11657.16
               Mean episode length: 441.88
                 Mean success rate: 88.00
                  Mean reward/step: 27.00
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 28286976
                    Iteration time: 2.54s
                        Total time: 8994.92s
                               ETA: 511999.0s

################################################################################
                    [1m Learning iteration 3453/200000 [0m

                       Computation: 3180 steps/s (collection: 0.499s, learning 2.076s)
               Value function loss: 150693.6224
                    Surrogate loss: 0.0116
             Mean action noise std: 0.89
                       Mean reward: 11726.16
               Mean episode length: 442.49
                 Mean success rate: 88.00
                  Mean reward/step: 26.52
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 28295168
                    Iteration time: 2.58s
                        Total time: 8997.49s
                               ETA: 511994.7s

################################################################################
                    [1m Learning iteration 3454/200000 [0m

                       Computation: 3220 steps/s (collection: 0.472s, learning 2.072s)
               Value function loss: 107113.1365
                    Surrogate loss: 0.0092
             Mean action noise std: 0.89
                       Mean reward: 11792.46
               Mean episode length: 445.35
                 Mean success rate: 89.00
                  Mean reward/step: 26.88
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 28303360
                    Iteration time: 2.54s
                        Total time: 9000.03s
                               ETA: 511988.6s

################################################################################
                    [1m Learning iteration 3455/200000 [0m

                       Computation: 3187 steps/s (collection: 0.499s, learning 2.071s)
               Value function loss: 99891.0757
                    Surrogate loss: 0.0139
             Mean action noise std: 0.89
                       Mean reward: 11793.12
               Mean episode length: 446.20
                 Mean success rate: 89.00
                  Mean reward/step: 26.82
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 2.57s
                        Total time: 9002.60s
                               ETA: 511984.1s

################################################################################
                    [1m Learning iteration 3456/200000 [0m

                       Computation: 3226 steps/s (collection: 0.479s, learning 2.060s)
               Value function loss: 58243.1746
                    Surrogate loss: 0.0140
             Mean action noise std: 0.89
                       Mean reward: 11639.16
               Mean episode length: 441.75
                 Mean success rate: 88.00
                  Mean reward/step: 27.07
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 28319744
                    Iteration time: 2.54s
                        Total time: 9005.14s
                               ETA: 511977.7s

################################################################################
                    [1m Learning iteration 3457/200000 [0m

                       Computation: 3226 steps/s (collection: 0.462s, learning 2.077s)
               Value function loss: 111986.9631
                    Surrogate loss: 0.0117
             Mean action noise std: 0.89
                       Mean reward: 11789.68
               Mean episode length: 446.94
                 Mean success rate: 89.50
                  Mean reward/step: 27.00
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 28327936
                    Iteration time: 2.54s
                        Total time: 9007.68s
                               ETA: 511971.3s

################################################################################
                    [1m Learning iteration 3458/200000 [0m

                       Computation: 3184 steps/s (collection: 0.488s, learning 2.084s)
               Value function loss: 123068.9250
                    Surrogate loss: 0.0111
             Mean action noise std: 0.89
                       Mean reward: 11536.77
               Mean episode length: 439.58
                 Mean success rate: 88.00
                  Mean reward/step: 25.72
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 28336128
                    Iteration time: 2.57s
                        Total time: 9010.25s
                               ETA: 511966.9s

################################################################################
                    [1m Learning iteration 3459/200000 [0m

                       Computation: 3246 steps/s (collection: 0.468s, learning 2.056s)
               Value function loss: 106695.4277
                    Surrogate loss: 0.0160
             Mean action noise std: 0.89
                       Mean reward: 11825.69
               Mean episode length: 451.26
                 Mean success rate: 89.50
                  Mean reward/step: 25.49
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 28344320
                    Iteration time: 2.52s
                        Total time: 9012.78s
                               ETA: 511959.7s

################################################################################
                    [1m Learning iteration 3460/200000 [0m

                       Computation: 3236 steps/s (collection: 0.467s, learning 2.064s)
               Value function loss: 89386.3545
                    Surrogate loss: 0.0117
             Mean action noise std: 0.89
                       Mean reward: 11767.19
               Mean episode length: 451.33
                 Mean success rate: 89.00
                  Mean reward/step: 26.50
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 28352512
                    Iteration time: 2.53s
                        Total time: 9015.31s
                               ETA: 511952.9s

################################################################################
                    [1m Learning iteration 3461/200000 [0m

                       Computation: 3233 steps/s (collection: 0.442s, learning 2.091s)
               Value function loss: 98725.1119
                    Surrogate loss: 0.0119
             Mean action noise std: 0.89
                       Mean reward: 11555.25
               Mean episode length: 446.13
                 Mean success rate: 87.50
                  Mean reward/step: 26.56
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 28360704
                    Iteration time: 2.53s
                        Total time: 9017.84s
                               ETA: 511946.2s

################################################################################
                    [1m Learning iteration 3462/200000 [0m

                       Computation: 3200 steps/s (collection: 0.487s, learning 2.072s)
               Value function loss: 139694.4579
                    Surrogate loss: 0.0123
             Mean action noise std: 0.89
                       Mean reward: 11864.44
               Mean episode length: 449.88
                 Mean success rate: 88.50
                  Mean reward/step: 26.53
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 28368896
                    Iteration time: 2.56s
                        Total time: 9020.40s
                               ETA: 511941.0s

################################################################################
                    [1m Learning iteration 3463/200000 [0m

                       Computation: 3164 steps/s (collection: 0.503s, learning 2.086s)
               Value function loss: 83242.3095
                    Surrogate loss: 0.0126
             Mean action noise std: 0.89
                       Mean reward: 11727.34
               Mean episode length: 445.71
                 Mean success rate: 88.00
                  Mean reward/step: 25.73
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 28377088
                    Iteration time: 2.59s
                        Total time: 9022.99s
                               ETA: 511937.5s

################################################################################
                    [1m Learning iteration 3464/200000 [0m

                       Computation: 3225 steps/s (collection: 0.461s, learning 2.079s)
               Value function loss: 94357.5826
                    Surrogate loss: 0.0100
             Mean action noise std: 0.89
                       Mean reward: 11808.06
               Mean episode length: 449.06
                 Mean success rate: 89.00
                  Mean reward/step: 25.96
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 28385280
                    Iteration time: 2.54s
                        Total time: 9025.53s
                               ETA: 511931.2s

################################################################################
                    [1m Learning iteration 3465/200000 [0m

                       Computation: 3271 steps/s (collection: 0.442s, learning 2.062s)
               Value function loss: 108885.7900
                    Surrogate loss: 0.0118
             Mean action noise std: 0.89
                       Mean reward: 11851.12
               Mean episode length: 449.06
                 Mean success rate: 89.00
                  Mean reward/step: 26.62
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 28393472
                    Iteration time: 2.50s
                        Total time: 9028.03s
                               ETA: 511922.9s

################################################################################
                    [1m Learning iteration 3466/200000 [0m

                       Computation: 3266 steps/s (collection: 0.463s, learning 2.045s)
               Value function loss: 121640.3449
                    Surrogate loss: 0.0104
             Mean action noise std: 0.89
                       Mean reward: 11619.05
               Mean episode length: 443.32
                 Mean success rate: 88.00
                  Mean reward/step: 26.00
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 28401664
                    Iteration time: 2.51s
                        Total time: 9030.54s
                               ETA: 511914.8s

################################################################################
                    [1m Learning iteration 3467/200000 [0m

                       Computation: 3201 steps/s (collection: 0.487s, learning 2.071s)
               Value function loss: 78940.2067
                    Surrogate loss: 0.0115
             Mean action noise std: 0.89
                       Mean reward: 11690.60
               Mean episode length: 443.07
                 Mean success rate: 88.50
                  Mean reward/step: 25.38
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 2.56s
                        Total time: 9033.10s
                               ETA: 511909.6s

################################################################################
                    [1m Learning iteration 3468/200000 [0m

                       Computation: 3219 steps/s (collection: 0.477s, learning 2.068s)
               Value function loss: 118856.4546
                    Surrogate loss: 0.0096
             Mean action noise std: 0.89
                       Mean reward: 11538.01
               Mean episode length: 440.94
                 Mean success rate: 88.00
                  Mean reward/step: 26.00
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 28418048
                    Iteration time: 2.54s
                        Total time: 9035.65s
                               ETA: 511903.6s

################################################################################
                    [1m Learning iteration 3469/200000 [0m

                       Computation: 3208 steps/s (collection: 0.469s, learning 2.084s)
               Value function loss: 100286.7509
                    Surrogate loss: 0.0090
             Mean action noise std: 0.89
                       Mean reward: 11177.31
               Mean episode length: 431.90
                 Mean success rate: 85.50
                  Mean reward/step: 25.38
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 28426240
                    Iteration time: 2.55s
                        Total time: 9038.20s
                               ETA: 511898.1s

################################################################################
                    [1m Learning iteration 3470/200000 [0m

                       Computation: 3234 steps/s (collection: 0.480s, learning 2.053s)
               Value function loss: 82628.8456
                    Surrogate loss: 0.0100
             Mean action noise std: 0.90
                       Mean reward: 11065.38
               Mean episode length: 427.31
                 Mean success rate: 84.50
                  Mean reward/step: 25.62
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 28434432
                    Iteration time: 2.53s
                        Total time: 9040.73s
                               ETA: 511891.4s

################################################################################
                    [1m Learning iteration 3471/200000 [0m

                       Computation: 3248 steps/s (collection: 0.462s, learning 2.059s)
               Value function loss: 89690.0952
                    Surrogate loss: 0.0091
             Mean action noise std: 0.90
                       Mean reward: 11480.79
               Mean episode length: 437.98
                 Mean success rate: 87.00
                  Mean reward/step: 26.09
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 28442624
                    Iteration time: 2.52s
                        Total time: 9043.25s
                               ETA: 511884.1s

################################################################################
                    [1m Learning iteration 3472/200000 [0m

                       Computation: 3235 steps/s (collection: 0.482s, learning 2.050s)
               Value function loss: 98439.6852
                    Surrogate loss: 0.0106
             Mean action noise std: 0.90
                       Mean reward: 10964.51
               Mean episode length: 424.43
                 Mean success rate: 83.50
                  Mean reward/step: 26.09
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 28450816
                    Iteration time: 2.53s
                        Total time: 9045.79s
                               ETA: 511877.4s

################################################################################
                    [1m Learning iteration 3473/200000 [0m

                       Computation: 3272 steps/s (collection: 0.453s, learning 2.051s)
               Value function loss: 119486.0426
                    Surrogate loss: 0.0124
             Mean action noise std: 0.90
                       Mean reward: 10911.18
               Mean episode length: 423.54
                 Mean success rate: 83.50
                  Mean reward/step: 25.82
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 28459008
                    Iteration time: 2.50s
                        Total time: 9048.29s
                               ETA: 511869.1s

################################################################################
                    [1m Learning iteration 3474/200000 [0m

                       Computation: 3204 steps/s (collection: 0.483s, learning 2.073s)
               Value function loss: 83627.0274
                    Surrogate loss: 0.0101
             Mean action noise std: 0.90
                       Mean reward: 10578.24
               Mean episode length: 412.47
                 Mean success rate: 81.00
                  Mean reward/step: 25.38
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 28467200
                    Iteration time: 2.56s
                        Total time: 9050.85s
                               ETA: 511863.7s

################################################################################
                    [1m Learning iteration 3475/200000 [0m

                       Computation: 3215 steps/s (collection: 0.462s, learning 2.086s)
               Value function loss: 94405.4469
                    Surrogate loss: 0.0108
             Mean action noise std: 0.90
                       Mean reward: 10544.22
               Mean episode length: 410.82
                 Mean success rate: 81.50
                  Mean reward/step: 25.45
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 28475392
                    Iteration time: 2.55s
                        Total time: 9053.39s
                               ETA: 511857.9s

################################################################################
                    [1m Learning iteration 3476/200000 [0m

                       Computation: 3207 steps/s (collection: 0.477s, learning 2.077s)
               Value function loss: 80970.9956
                    Surrogate loss: 0.0101
             Mean action noise std: 0.90
                       Mean reward: 10693.84
               Mean episode length: 415.12
                 Mean success rate: 82.00
                  Mean reward/step: 26.14
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 28483584
                    Iteration time: 2.55s
                        Total time: 9055.95s
                               ETA: 511852.5s

################################################################################
                    [1m Learning iteration 3477/200000 [0m

                       Computation: 3246 steps/s (collection: 0.479s, learning 2.044s)
               Value function loss: 82263.1477
                    Surrogate loss: 0.0117
             Mean action noise std: 0.90
                       Mean reward: 10869.36
               Mean episode length: 422.17
                 Mean success rate: 83.00
                  Mean reward/step: 26.30
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 28491776
                    Iteration time: 2.52s
                        Total time: 9058.47s
                               ETA: 511845.3s

################################################################################
                    [1m Learning iteration 3478/200000 [0m

                       Computation: 3141 steps/s (collection: 0.515s, learning 2.093s)
               Value function loss: 76663.3540
                    Surrogate loss: 0.0092
             Mean action noise std: 0.90
                       Mean reward: 11054.28
               Mean episode length: 429.07
                 Mean success rate: 84.00
                  Mean reward/step: 26.11
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 28499968
                    Iteration time: 2.61s
                        Total time: 9061.08s
                               ETA: 511842.8s

################################################################################
                    [1m Learning iteration 3479/200000 [0m

                       Computation: 3282 steps/s (collection: 0.468s, learning 2.028s)
               Value function loss: 82567.1477
                    Surrogate loss: 0.0119
             Mean action noise std: 0.90
                       Mean reward: 11128.78
               Mean episode length: 428.65
                 Mean success rate: 84.00
                  Mean reward/step: 26.52
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 2.50s
                        Total time: 9063.57s
                               ETA: 511834.1s

################################################################################
                    [1m Learning iteration 3480/200000 [0m

                       Computation: 3483 steps/s (collection: 0.396s, learning 1.955s)
               Value function loss: 75765.0674
                    Surrogate loss: 0.0081
             Mean action noise std: 0.90
                       Mean reward: 11114.19
               Mean episode length: 425.70
                 Mean success rate: 83.50
                  Mean reward/step: 26.72
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 28516352
                    Iteration time: 2.35s
                        Total time: 9065.93s
                               ETA: 511817.2s

################################################################################
                    [1m Learning iteration 3481/200000 [0m

                       Computation: 3344 steps/s (collection: 0.425s, learning 2.024s)
               Value function loss: 102836.2932
                    Surrogate loss: 0.0153
             Mean action noise std: 0.90
                       Mean reward: 11040.55
               Mean episode length: 425.98
                 Mean success rate: 83.50
                  Mean reward/step: 26.30
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 28524544
                    Iteration time: 2.45s
                        Total time: 9068.38s
                               ETA: 511805.9s

################################################################################
                    [1m Learning iteration 3482/200000 [0m

                       Computation: 3266 steps/s (collection: 0.464s, learning 2.044s)
               Value function loss: 110671.6188
                    Surrogate loss: 0.0147
             Mean action noise std: 0.90
                       Mean reward: 11058.79
               Mean episode length: 425.25
                 Mean success rate: 83.50
                  Mean reward/step: 26.19
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 28532736
                    Iteration time: 2.51s
                        Total time: 9070.88s
                               ETA: 511797.8s

################################################################################
                    [1m Learning iteration 3483/200000 [0m

                       Computation: 3256 steps/s (collection: 0.476s, learning 2.039s)
               Value function loss: 76684.5372
                    Surrogate loss: 0.0144
             Mean action noise std: 0.90
                       Mean reward: 11160.02
               Mean episode length: 428.35
                 Mean success rate: 84.00
                  Mean reward/step: 26.12
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 28540928
                    Iteration time: 2.52s
                        Total time: 9073.40s
                               ETA: 511790.2s

################################################################################
                    [1m Learning iteration 3484/200000 [0m

                       Computation: 3268 steps/s (collection: 0.472s, learning 2.035s)
               Value function loss: 136227.8107
                    Surrogate loss: 0.0112
             Mean action noise std: 0.90
                       Mean reward: 11413.27
               Mean episode length: 437.89
                 Mean success rate: 86.00
                  Mean reward/step: 26.13
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 28549120
                    Iteration time: 2.51s
                        Total time: 9075.90s
                               ETA: 511782.1s

################################################################################
                    [1m Learning iteration 3485/200000 [0m

                       Computation: 3275 steps/s (collection: 0.463s, learning 2.038s)
               Value function loss: 92552.7680
                    Surrogate loss: 0.0115
             Mean action noise std: 0.90
                       Mean reward: 11611.77
               Mean episode length: 444.92
                 Mean success rate: 87.50
                  Mean reward/step: 26.11
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 28557312
                    Iteration time: 2.50s
                        Total time: 9078.41s
                               ETA: 511773.6s

################################################################################
                    [1m Learning iteration 3486/200000 [0m

                       Computation: 3313 steps/s (collection: 0.439s, learning 2.034s)
               Value function loss: 83866.2809
                    Surrogate loss: 0.0087
             Mean action noise std: 0.90
                       Mean reward: 11692.45
               Mean episode length: 447.98
                 Mean success rate: 87.50
                  Mean reward/step: 25.83
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 28565504
                    Iteration time: 2.47s
                        Total time: 9080.88s
                               ETA: 511763.6s

################################################################################
                    [1m Learning iteration 3487/200000 [0m

                       Computation: 3199 steps/s (collection: 0.511s, learning 2.049s)
               Value function loss: 86671.3091
                    Surrogate loss: 0.0095
             Mean action noise std: 0.90
                       Mean reward: 11504.01
               Mean episode length: 441.22
                 Mean success rate: 86.00
                  Mean reward/step: 26.07
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 28573696
                    Iteration time: 2.56s
                        Total time: 9083.44s
                               ETA: 511758.5s

################################################################################
                    [1m Learning iteration 3488/200000 [0m

                       Computation: 3196 steps/s (collection: 0.524s, learning 2.039s)
               Value function loss: 119846.5391
                    Surrogate loss: 0.0103
             Mean action noise std: 0.90
                       Mean reward: 11174.25
               Mean episode length: 429.19
                 Mean success rate: 84.00
                  Mean reward/step: 25.74
       Mean episode length/episode: 28.95
--------------------------------------------------------------------------------
                   Total timesteps: 28581888
                    Iteration time: 2.56s
                        Total time: 9086.00s
                               ETA: 511753.6s

################################################################################
                    [1m Learning iteration 3489/200000 [0m

                       Computation: 3319 steps/s (collection: 0.434s, learning 2.034s)
               Value function loss: 98723.9050
                    Surrogate loss: 0.0097
             Mean action noise std: 0.90
                       Mean reward: 11245.52
               Mean episode length: 431.31
                 Mean success rate: 84.50
                  Mean reward/step: 25.31
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 28590080
                    Iteration time: 2.47s
                        Total time: 9088.47s
                               ETA: 511743.3s

################################################################################
                    [1m Learning iteration 3490/200000 [0m

                       Computation: 3189 steps/s (collection: 0.477s, learning 2.092s)
               Value function loss: 92406.2567
                    Surrogate loss: 0.0097
             Mean action noise std: 0.90
                       Mean reward: 11384.60
               Mean episode length: 435.21
                 Mean success rate: 85.50
                  Mean reward/step: 25.64
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 28598272
                    Iteration time: 2.57s
                        Total time: 9091.04s
                               ETA: 511738.6s

################################################################################
                    [1m Learning iteration 3491/200000 [0m

                       Computation: 3260 steps/s (collection: 0.455s, learning 2.057s)
               Value function loss: 101952.2292
                    Surrogate loss: 0.0109
             Mean action noise std: 0.90
                       Mean reward: 11345.91
               Mean episode length: 435.60
                 Mean success rate: 85.50
                  Mean reward/step: 26.37
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 2.51s
                        Total time: 9093.55s
                               ETA: 511730.9s

################################################################################
                    [1m Learning iteration 3492/200000 [0m

                       Computation: 3285 steps/s (collection: 0.455s, learning 2.039s)
               Value function loss: 79806.0422
                    Surrogate loss: 0.0104
             Mean action noise std: 0.90
                       Mean reward: 11440.18
               Mean episode length: 434.19
                 Mean success rate: 86.50
                  Mean reward/step: 26.74
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 28614656
                    Iteration time: 2.49s
                        Total time: 9096.04s
                               ETA: 511722.0s

################################################################################
                    [1m Learning iteration 3493/200000 [0m

                       Computation: 3293 steps/s (collection: 0.454s, learning 2.033s)
               Value function loss: 94367.9110
                    Surrogate loss: 0.0134
             Mean action noise std: 0.90
                       Mean reward: 11266.15
               Mean episode length: 430.13
                 Mean success rate: 85.50
                  Mean reward/step: 26.97
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 28622848
                    Iteration time: 2.49s
                        Total time: 9098.53s
                               ETA: 511712.8s

################################################################################
                    [1m Learning iteration 3494/200000 [0m

                       Computation: 3293 steps/s (collection: 0.463s, learning 2.024s)
               Value function loss: 79909.9999
                    Surrogate loss: 0.0109
             Mean action noise std: 0.90
                       Mean reward: 11414.95
               Mean episode length: 433.63
                 Mean success rate: 86.50
                  Mean reward/step: 26.33
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 28631040
                    Iteration time: 2.49s
                        Total time: 9101.02s
                               ETA: 511703.7s

################################################################################
                    [1m Learning iteration 3495/200000 [0m

                       Computation: 3297 steps/s (collection: 0.447s, learning 2.038s)
               Value function loss: 102774.0903
                    Surrogate loss: 0.0116
             Mean action noise std: 0.90
                       Mean reward: 11350.77
               Mean episode length: 429.95
                 Mean success rate: 85.50
                  Mean reward/step: 26.49
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 28639232
                    Iteration time: 2.48s
                        Total time: 9103.50s
                               ETA: 511694.4s

################################################################################
                    [1m Learning iteration 3496/200000 [0m

                       Computation: 3264 steps/s (collection: 0.488s, learning 2.022s)
               Value function loss: 84556.7908
                    Surrogate loss: 0.0078
             Mean action noise std: 0.90
                       Mean reward: 10994.99
               Mean episode length: 418.65
                 Mean success rate: 83.00
                  Mean reward/step: 26.85
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 28647424
                    Iteration time: 2.51s
                        Total time: 9106.01s
                               ETA: 511686.5s

################################################################################
                    [1m Learning iteration 3497/200000 [0m

                       Computation: 3333 steps/s (collection: 0.439s, learning 2.018s)
               Value function loss: 124176.9537
                    Surrogate loss: 0.0108
             Mean action noise std: 0.90
                       Mean reward: 10945.01
               Mean episode length: 419.00
                 Mean success rate: 83.00
                  Mean reward/step: 26.36
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 28655616
                    Iteration time: 2.46s
                        Total time: 9108.47s
                               ETA: 511675.6s

################################################################################
                    [1m Learning iteration 3498/200000 [0m

                       Computation: 3259 steps/s (collection: 0.485s, learning 2.028s)
               Value function loss: 99090.8116
                    Surrogate loss: 0.0124
             Mean action noise std: 0.90
                       Mean reward: 11112.24
               Mean episode length: 424.21
                 Mean success rate: 84.50
                  Mean reward/step: 25.96
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 28663808
                    Iteration time: 2.51s
                        Total time: 9110.98s
                               ETA: 511667.9s

################################################################################
                    [1m Learning iteration 3499/200000 [0m

                       Computation: 3225 steps/s (collection: 0.456s, learning 2.084s)
               Value function loss: 119878.5971
                    Surrogate loss: 0.0109
             Mean action noise std: 0.90
                       Mean reward: 11164.08
               Mean episode length: 428.25
                 Mean success rate: 85.50
                  Mean reward/step: 26.31
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 28672000
                    Iteration time: 2.54s
                        Total time: 9113.52s
                               ETA: 511661.7s

################################################################################
                    [1m Learning iteration 3500/200000 [0m

                       Computation: 3266 steps/s (collection: 0.499s, learning 2.009s)
               Value function loss: 105458.3293
                    Surrogate loss: 0.0109
             Mean action noise std: 0.90
                       Mean reward: 11073.22
               Mean episode length: 426.29
                 Mean success rate: 85.00
                  Mean reward/step: 26.68
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 28680192
                    Iteration time: 2.51s
                        Total time: 9116.03s
                               ETA: 511653.7s

################################################################################
                    [1m Learning iteration 3501/200000 [0m

                       Computation: 3281 steps/s (collection: 0.465s, learning 2.032s)
               Value function loss: 94940.8080
                    Surrogate loss: 0.0116
             Mean action noise std: 0.90
                       Mean reward: 11044.76
               Mean episode length: 423.44
                 Mean success rate: 84.50
                  Mean reward/step: 26.94
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 28688384
                    Iteration time: 2.50s
                        Total time: 9118.52s
                               ETA: 511645.1s

################################################################################
                    [1m Learning iteration 3502/200000 [0m

                       Computation: 3279 steps/s (collection: 0.451s, learning 2.047s)
               Value function loss: 100709.7268
                    Surrogate loss: 0.0117
             Mean action noise std: 0.90
                       Mean reward: 11164.91
               Mean episode length: 427.73
                 Mean success rate: 85.00
                  Mean reward/step: 27.32
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 28696576
                    Iteration time: 2.50s
                        Total time: 9121.02s
                               ETA: 511636.5s

################################################################################
                    [1m Learning iteration 3503/200000 [0m

                       Computation: 3271 steps/s (collection: 0.480s, learning 2.024s)
               Value function loss: 108049.7086
                    Surrogate loss: 0.0104
             Mean action noise std: 0.90
                       Mean reward: 11306.49
               Mean episode length: 433.25
                 Mean success rate: 85.50
                  Mean reward/step: 26.62
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 2.50s
                        Total time: 9123.53s
                               ETA: 511628.4s

################################################################################
                    [1m Learning iteration 3504/200000 [0m

                       Computation: 3313 steps/s (collection: 0.444s, learning 2.029s)
               Value function loss: 117135.3695
                    Surrogate loss: 0.0146
             Mean action noise std: 0.90
                       Mean reward: 11361.94
               Mean episode length: 433.25
                 Mean success rate: 85.50
                  Mean reward/step: 25.50
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 28712960
                    Iteration time: 2.47s
                        Total time: 9126.00s
                               ETA: 511618.4s

################################################################################
                    [1m Learning iteration 3505/200000 [0m

                       Computation: 3313 steps/s (collection: 0.434s, learning 2.039s)
               Value function loss: 79615.1310
                    Surrogate loss: 0.0112
             Mean action noise std: 0.90
                       Mean reward: 11329.38
               Mean episode length: 431.43
                 Mean success rate: 85.50
                  Mean reward/step: 25.41
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 28721152
                    Iteration time: 2.47s
                        Total time: 9128.47s
                               ETA: 511608.4s

################################################################################
                    [1m Learning iteration 3506/200000 [0m

                       Computation: 3325 steps/s (collection: 0.438s, learning 2.026s)
               Value function loss: 78364.0968
                    Surrogate loss: 0.0120
             Mean action noise std: 0.90
                       Mean reward: 11743.01
               Mean episode length: 443.75
                 Mean success rate: 88.50
                  Mean reward/step: 25.71
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 28729344
                    Iteration time: 2.46s
                        Total time: 9130.94s
                               ETA: 511598.0s

################################################################################
                    [1m Learning iteration 3507/200000 [0m

                       Computation: 3336 steps/s (collection: 0.428s, learning 2.028s)
               Value function loss: 86140.6807
                    Surrogate loss: 0.0123
             Mean action noise std: 0.90
                       Mean reward: 11266.56
               Mean episode length: 429.25
                 Mean success rate: 85.50
                  Mean reward/step: 26.29
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 28737536
                    Iteration time: 2.46s
                        Total time: 9133.39s
                               ETA: 511587.1s

################################################################################
                    [1m Learning iteration 3508/200000 [0m

                       Computation: 3260 steps/s (collection: 0.481s, learning 2.031s)
               Value function loss: 73600.8099
                    Surrogate loss: 0.0098
             Mean action noise std: 0.90
                       Mean reward: 11361.49
               Mean episode length: 431.70
                 Mean success rate: 86.00
                  Mean reward/step: 26.59
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 28745728
                    Iteration time: 2.51s
                        Total time: 9135.90s
                               ETA: 511579.4s

################################################################################
                    [1m Learning iteration 3509/200000 [0m

                       Computation: 3276 steps/s (collection: 0.453s, learning 2.047s)
               Value function loss: 120508.9123
                    Surrogate loss: 0.0112
             Mean action noise std: 0.90
                       Mean reward: 11233.26
               Mean episode length: 426.94
                 Mean success rate: 84.00
                  Mean reward/step: 26.08
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 28753920
                    Iteration time: 2.50s
                        Total time: 9138.40s
                               ETA: 511571.0s

################################################################################
                    [1m Learning iteration 3510/200000 [0m

                       Computation: 3271 steps/s (collection: 0.453s, learning 2.052s)
               Value function loss: 79527.7870
                    Surrogate loss: 0.0099
             Mean action noise std: 0.90
                       Mean reward: 11214.43
               Mean episode length: 424.27
                 Mean success rate: 83.50
                  Mean reward/step: 26.30
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 28762112
                    Iteration time: 2.50s
                        Total time: 9140.91s
                               ETA: 511562.8s

################################################################################
                    [1m Learning iteration 3511/200000 [0m

                       Computation: 3182 steps/s (collection: 0.516s, learning 2.059s)
               Value function loss: 87437.1622
                    Surrogate loss: 0.0096
             Mean action noise std: 0.90
                       Mean reward: 11348.28
               Mean episode length: 427.15
                 Mean success rate: 84.50
                  Mean reward/step: 26.53
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 28770304
                    Iteration time: 2.57s
                        Total time: 9143.48s
                               ETA: 511558.6s

################################################################################
                    [1m Learning iteration 3512/200000 [0m

                       Computation: 3219 steps/s (collection: 0.504s, learning 2.040s)
               Value function loss: 123011.0916
                    Surrogate loss: 0.0124
             Mean action noise std: 0.90
                       Mean reward: 11179.19
               Mean episode length: 421.94
                 Mean success rate: 83.00
                  Mean reward/step: 26.62
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 28778496
                    Iteration time: 2.54s
                        Total time: 9146.03s
                               ETA: 511552.7s

################################################################################
                    [1m Learning iteration 3513/200000 [0m

                       Computation: 3138 steps/s (collection: 0.545s, learning 2.065s)
               Value function loss: 120390.0172
                    Surrogate loss: 0.0131
             Mean action noise std: 0.90
                       Mean reward: 11462.14
               Mean episode length: 429.30
                 Mean success rate: 84.50
                  Mean reward/step: 25.98
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 28786688
                    Iteration time: 2.61s
                        Total time: 9148.64s
                               ETA: 511550.5s

################################################################################
                    [1m Learning iteration 3514/200000 [0m

                       Computation: 3172 steps/s (collection: 0.515s, learning 2.068s)
               Value function loss: 112172.1533
                    Surrogate loss: 0.0088
             Mean action noise std: 0.90
                       Mean reward: 11518.60
               Mean episode length: 428.92
                 Mean success rate: 84.50
                  Mean reward/step: 25.38
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 28794880
                    Iteration time: 2.58s
                        Total time: 9151.22s
                               ETA: 511546.7s

################################################################################
                    [1m Learning iteration 3515/200000 [0m

                       Computation: 3167 steps/s (collection: 0.493s, learning 2.093s)
               Value function loss: 96958.8586
                    Surrogate loss: 0.0137
             Mean action noise std: 0.90
                       Mean reward: 11216.68
               Mean episode length: 420.77
                 Mean success rate: 83.00
                  Mean reward/step: 25.75
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 2.59s
                        Total time: 9153.81s
                               ETA: 511543.1s

################################################################################
                    [1m Learning iteration 3516/200000 [0m

                       Computation: 3280 steps/s (collection: 0.458s, learning 2.039s)
               Value function loss: 86680.5015
                    Surrogate loss: 0.0112
             Mean action noise std: 0.90
                       Mean reward: 11300.16
               Mean episode length: 424.10
                 Mean success rate: 83.50
                  Mean reward/step: 25.87
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 28811264
                    Iteration time: 2.50s
                        Total time: 9156.30s
                               ETA: 511534.6s

################################################################################
                    [1m Learning iteration 3517/200000 [0m

                       Computation: 3165 steps/s (collection: 0.509s, learning 2.078s)
               Value function loss: 76201.5083
                    Surrogate loss: 0.0133
             Mean action noise std: 0.90
                       Mean reward: 11646.72
               Mean episode length: 432.36
                 Mean success rate: 85.50
                  Mean reward/step: 26.51
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 28819456
                    Iteration time: 2.59s
                        Total time: 9158.89s
                               ETA: 511531.1s

################################################################################
                    [1m Learning iteration 3518/200000 [0m

                       Computation: 3178 steps/s (collection: 0.516s, learning 2.061s)
               Value function loss: 83626.1557
                    Surrogate loss: 0.0104
             Mean action noise std: 0.90
                       Mean reward: 11421.17
               Mean episode length: 431.17
                 Mean success rate: 85.50
                  Mean reward/step: 26.59
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 28827648
                    Iteration time: 2.58s
                        Total time: 9161.47s
                               ETA: 511527.0s

################################################################################
                    [1m Learning iteration 3519/200000 [0m

                       Computation: 3194 steps/s (collection: 0.504s, learning 2.060s)
               Value function loss: 142548.4527
                    Surrogate loss: 0.0102
             Mean action noise std: 0.90
                       Mean reward: 11553.35
               Mean episode length: 435.15
                 Mean success rate: 86.00
                  Mean reward/step: 26.82
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 28835840
                    Iteration time: 2.56s
                        Total time: 9164.03s
                               ETA: 511522.2s

################################################################################
                    [1m Learning iteration 3520/200000 [0m

                       Computation: 3306 steps/s (collection: 0.443s, learning 2.034s)
               Value function loss: 73831.6030
                    Surrogate loss: 0.0116
             Mean action noise std: 0.90
                       Mean reward: 11475.83
               Mean episode length: 435.96
                 Mean success rate: 86.50
                  Mean reward/step: 25.93
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 28844032
                    Iteration time: 2.48s
                        Total time: 9166.51s
                               ETA: 511512.6s

################################################################################
                    [1m Learning iteration 3521/200000 [0m

                       Computation: 3255 steps/s (collection: 0.457s, learning 2.059s)
               Value function loss: 84606.9593
                    Surrogate loss: 0.0097
             Mean action noise std: 0.90
                       Mean reward: 11561.95
               Mean episode length: 438.62
                 Mean success rate: 87.00
                  Mean reward/step: 26.71
       Mean episode length/episode: 30.57
--------------------------------------------------------------------------------
                   Total timesteps: 28852224
                    Iteration time: 2.52s
                        Total time: 9169.03s
                               ETA: 511505.2s

################################################################################
                    [1m Learning iteration 3522/200000 [0m

                       Computation: 3130 steps/s (collection: 0.492s, learning 2.125s)
               Value function loss: 86906.2604
                    Surrogate loss: 0.0080
             Mean action noise std: 0.90
                       Mean reward: 11848.50
               Mean episode length: 447.92
                 Mean success rate: 89.00
                  Mean reward/step: 27.48
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 28860416
                    Iteration time: 2.62s
                        Total time: 9171.64s
                               ETA: 511503.3s

################################################################################
                    [1m Learning iteration 3523/200000 [0m

                       Computation: 3166 steps/s (collection: 0.535s, learning 2.052s)
               Value function loss: 106579.2363
                    Surrogate loss: 0.0109
             Mean action noise std: 0.90
                       Mean reward: 11635.01
               Mean episode length: 443.73
                 Mean success rate: 88.50
                  Mean reward/step: 27.73
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 28868608
                    Iteration time: 2.59s
                        Total time: 9174.23s
                               ETA: 511499.8s

################################################################################
                    [1m Learning iteration 3524/200000 [0m

                       Computation: 3225 steps/s (collection: 0.478s, learning 2.062s)
               Value function loss: 121569.6816
                    Surrogate loss: 0.0120
             Mean action noise std: 0.90
                       Mean reward: 11543.94
               Mean episode length: 440.87
                 Mean success rate: 88.00
                  Mean reward/step: 27.08
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 28876800
                    Iteration time: 2.54s
                        Total time: 9176.77s
                               ETA: 511493.6s

################################################################################
                    [1m Learning iteration 3525/200000 [0m

                       Computation: 3136 steps/s (collection: 0.480s, learning 2.132s)
               Value function loss: 99568.1648
                    Surrogate loss: 0.0133
             Mean action noise std: 0.90
                       Mean reward: 11489.28
               Mean episode length: 441.38
                 Mean success rate: 88.50
                  Mean reward/step: 26.49
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 28884992
                    Iteration time: 2.61s
                        Total time: 9179.38s
                               ETA: 511491.5s

################################################################################
                    [1m Learning iteration 3526/200000 [0m

                       Computation: 3082 steps/s (collection: 0.531s, learning 2.126s)
               Value function loss: 86264.2460
                    Surrogate loss: 0.0198
             Mean action noise std: 0.90
                       Mean reward: 11573.54
               Mean episode length: 447.22
                 Mean success rate: 89.50
                  Mean reward/step: 26.35
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 28893184
                    Iteration time: 2.66s
                        Total time: 9182.04s
                               ETA: 511491.9s

################################################################################
                    [1m Learning iteration 3527/200000 [0m

                       Computation: 3057 steps/s (collection: 0.535s, learning 2.144s)
               Value function loss: 87474.9893
                    Surrogate loss: 0.0098
             Mean action noise std: 0.90
                       Mean reward: 11588.22
               Mean episode length: 447.22
                 Mean success rate: 89.50
                  Mean reward/step: 27.44
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 2.68s
                        Total time: 9184.72s
                               ETA: 511493.5s

################################################################################
                    [1m Learning iteration 3528/200000 [0m

                       Computation: 3195 steps/s (collection: 0.466s, learning 2.097s)
               Value function loss: 129696.1975
                    Surrogate loss: 0.0099
             Mean action noise std: 0.90
                       Mean reward: 11485.87
               Mean episode length: 441.94
                 Mean success rate: 88.50
                  Mean reward/step: 27.02
       Mean episode length/episode: 29.15
--------------------------------------------------------------------------------
                   Total timesteps: 28909568
                    Iteration time: 2.56s
                        Total time: 9187.28s
                               ETA: 511488.7s

################################################################################
                    [1m Learning iteration 3529/200000 [0m

                       Computation: 3175 steps/s (collection: 0.451s, learning 2.128s)
               Value function loss: 84116.6420
                    Surrogate loss: 0.0111
             Mean action noise std: 0.90
                       Mean reward: 11975.50
               Mean episode length: 453.61
                 Mean success rate: 91.00
                  Mean reward/step: 26.17
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 28917760
                    Iteration time: 2.58s
                        Total time: 9189.86s
                               ETA: 511484.7s

################################################################################
                    [1m Learning iteration 3530/200000 [0m

                       Computation: 3088 steps/s (collection: 0.506s, learning 2.146s)
               Value function loss: 105898.9496
                    Surrogate loss: 0.0114
             Mean action noise std: 0.90
                       Mean reward: 11730.38
               Mean episode length: 446.92
                 Mean success rate: 90.00
                  Mean reward/step: 26.30
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 28925952
                    Iteration time: 2.65s
                        Total time: 9192.51s
                               ETA: 511484.9s

################################################################################
                    [1m Learning iteration 3531/200000 [0m

                       Computation: 3156 steps/s (collection: 0.517s, learning 2.079s)
               Value function loss: 123573.8295
                    Surrogate loss: 0.0124
             Mean action noise std: 0.90
                       Mean reward: 11661.06
               Mean episode length: 441.18
                 Mean success rate: 88.50
                  Mean reward/step: 25.86
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 28934144
                    Iteration time: 2.60s
                        Total time: 9195.11s
                               ETA: 511481.8s

################################################################################
                    [1m Learning iteration 3532/200000 [0m

                       Computation: 3218 steps/s (collection: 0.460s, learning 2.085s)
               Value function loss: 62132.6316
                    Surrogate loss: 0.0103
             Mean action noise std: 0.90
                       Mean reward: 11491.15
               Mean episode length: 436.71
                 Mean success rate: 87.50
                  Mean reward/step: 25.27
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 28942336
                    Iteration time: 2.55s
                        Total time: 9197.65s
                               ETA: 511476.0s

################################################################################
                    [1m Learning iteration 3533/200000 [0m

                       Computation: 3181 steps/s (collection: 0.462s, learning 2.112s)
               Value function loss: 84701.0056
                    Surrogate loss: 0.0085
             Mean action noise std: 0.90
                       Mean reward: 11466.46
               Mean episode length: 436.12
                 Mean success rate: 87.00
                  Mean reward/step: 26.07
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 28950528
                    Iteration time: 2.57s
                        Total time: 9200.23s
                               ETA: 511471.8s

################################################################################
                    [1m Learning iteration 3534/200000 [0m

                       Computation: 3294 steps/s (collection: 0.461s, learning 2.025s)
               Value function loss: 68740.7259
                    Surrogate loss: 0.0076
             Mean action noise std: 0.90
                       Mean reward: 11579.94
               Mean episode length: 441.61
                 Mean success rate: 88.50
                  Mean reward/step: 26.74
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 28958720
                    Iteration time: 2.49s
                        Total time: 9202.71s
                               ETA: 511462.7s

################################################################################
                    [1m Learning iteration 3535/200000 [0m

                       Computation: 3302 steps/s (collection: 0.448s, learning 2.033s)
               Value function loss: 134493.4475
                    Surrogate loss: 0.0101
             Mean action noise std: 0.90
                       Mean reward: 11720.66
               Mean episode length: 444.48
                 Mean success rate: 89.00
                  Mean reward/step: 26.33
       Mean episode length/episode: 29.05
--------------------------------------------------------------------------------
                   Total timesteps: 28966912
                    Iteration time: 2.48s
                        Total time: 9205.20s
                               ETA: 511453.3s

################################################################################
                    [1m Learning iteration 3536/200000 [0m

                       Computation: 3321 steps/s (collection: 0.464s, learning 2.003s)
               Value function loss: 66587.8311
                    Surrogate loss: 0.0114
             Mean action noise std: 0.90
                       Mean reward: 11801.51
               Mean episode length: 444.48
                 Mean success rate: 89.00
                  Mean reward/step: 26.24
       Mean episode length/episode: 30.68
--------------------------------------------------------------------------------
                   Total timesteps: 28975104
                    Iteration time: 2.47s
                        Total time: 9207.66s
                               ETA: 511443.0s

################################################################################
                    [1m Learning iteration 3537/200000 [0m

                       Computation: 3309 steps/s (collection: 0.432s, learning 2.044s)
               Value function loss: 68561.3529
                    Surrogate loss: 0.0097
             Mean action noise std: 0.90
                       Mean reward: 11449.96
               Mean episode length: 433.43
                 Mean success rate: 86.00
                  Mean reward/step: 27.08
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 28983296
                    Iteration time: 2.48s
                        Total time: 9210.14s
                               ETA: 511433.4s

################################################################################
                    [1m Learning iteration 3538/200000 [0m

                       Computation: 3207 steps/s (collection: 0.497s, learning 2.057s)
               Value function loss: 106677.1262
                    Surrogate loss: 0.0106
             Mean action noise std: 0.90
                       Mean reward: 11401.59
               Mean episode length: 432.20
                 Mean success rate: 85.00
                  Mean reward/step: 27.56
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 28991488
                    Iteration time: 2.55s
                        Total time: 9212.69s
                               ETA: 511428.0s

################################################################################
                    [1m Learning iteration 3539/200000 [0m

                       Computation: 3258 steps/s (collection: 0.510s, learning 2.004s)
               Value function loss: 90195.6345
                    Surrogate loss: 0.0110
             Mean action noise std: 0.90
                       Mean reward: 11339.50
               Mean episode length: 431.84
                 Mean success rate: 85.00
                  Mean reward/step: 27.16
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 2.51s
                        Total time: 9215.21s
                               ETA: 511420.5s

################################################################################
                    [1m Learning iteration 3540/200000 [0m

                       Computation: 3295 steps/s (collection: 0.438s, learning 2.049s)
               Value function loss: 133865.2363
                    Surrogate loss: 0.0119
             Mean action noise std: 0.90
                       Mean reward: 11587.16
               Mean episode length: 437.50
                 Mean success rate: 86.50
                  Mean reward/step: 26.79
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 29007872
                    Iteration time: 2.49s
                        Total time: 9217.69s
                               ETA: 511411.4s

################################################################################
                    [1m Learning iteration 3541/200000 [0m

                       Computation: 3297 steps/s (collection: 0.463s, learning 2.021s)
               Value function loss: 88366.4606
                    Surrogate loss: 0.0124
             Mean action noise std: 0.90
                       Mean reward: 11736.99
               Mean episode length: 442.76
                 Mean success rate: 87.50
                  Mean reward/step: 26.12
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 29016064
                    Iteration time: 2.48s
                        Total time: 9220.18s
                               ETA: 511402.2s

################################################################################
                    [1m Learning iteration 3542/200000 [0m

                       Computation: 3305 steps/s (collection: 0.447s, learning 2.031s)
               Value function loss: 79323.7894
                    Surrogate loss: 0.0125
             Mean action noise std: 0.90
                       Mean reward: 11740.74
               Mean episode length: 444.01
                 Mean success rate: 88.00
                  Mean reward/step: 26.39
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 29024256
                    Iteration time: 2.48s
                        Total time: 9222.65s
                               ETA: 511392.7s

################################################################################
                    [1m Learning iteration 3543/200000 [0m

                       Computation: 3280 steps/s (collection: 0.468s, learning 2.029s)
               Value function loss: 126159.1518
                    Surrogate loss: 0.0144
             Mean action noise std: 0.90
                       Mean reward: 11933.27
               Mean episode length: 448.48
                 Mean success rate: 89.00
                  Mean reward/step: 27.10
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 29032448
                    Iteration time: 2.50s
                        Total time: 9225.15s
                               ETA: 511384.2s

################################################################################
                    [1m Learning iteration 3544/200000 [0m

                       Computation: 3317 steps/s (collection: 0.462s, learning 2.007s)
               Value function loss: 110664.1283
                    Surrogate loss: 0.0113
             Mean action noise std: 0.90
                       Mean reward: 11860.73
               Mean episode length: 443.48
                 Mean success rate: 87.50
                  Mean reward/step: 26.38
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 29040640
                    Iteration time: 2.47s
                        Total time: 9227.62s
                               ETA: 511374.1s

################################################################################
                    [1m Learning iteration 3545/200000 [0m

                       Computation: 3250 steps/s (collection: 0.463s, learning 2.058s)
               Value function loss: 85657.8662
                    Surrogate loss: 0.0137
             Mean action noise std: 0.90
                       Mean reward: 11716.59
               Mean episode length: 440.94
                 Mean success rate: 87.00
                  Mean reward/step: 26.06
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 29048832
                    Iteration time: 2.52s
                        Total time: 9230.14s
                               ETA: 511367.0s

################################################################################
                    [1m Learning iteration 3546/200000 [0m

                       Computation: 3158 steps/s (collection: 0.469s, learning 2.125s)
               Value function loss: 133095.4411
                    Surrogate loss: 0.0131
             Mean action noise std: 0.90
                       Mean reward: 11359.81
               Mean episode length: 430.98
                 Mean success rate: 84.50
                  Mean reward/step: 26.14
       Mean episode length/episode: 28.74
--------------------------------------------------------------------------------
                   Total timesteps: 29057024
                    Iteration time: 2.59s
                        Total time: 9232.73s
                               ETA: 511363.9s

################################################################################
                    [1m Learning iteration 3547/200000 [0m

                       Computation: 3202 steps/s (collection: 0.537s, learning 2.021s)
               Value function loss: 116272.6729
                    Surrogate loss: 0.0128
             Mean action noise std: 0.90
                       Mean reward: 11640.90
               Mean episode length: 439.07
                 Mean success rate: 86.50
                  Mean reward/step: 25.35
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 29065216
                    Iteration time: 2.56s
                        Total time: 9235.29s
                               ETA: 511358.8s

################################################################################
                    [1m Learning iteration 3548/200000 [0m

                       Computation: 3270 steps/s (collection: 0.458s, learning 2.047s)
               Value function loss: 69940.4952
                    Surrogate loss: 0.0120
             Mean action noise std: 0.90
                       Mean reward: 11900.24
               Mean episode length: 446.46
                 Mean success rate: 88.50
                  Mean reward/step: 25.92
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 29073408
                    Iteration time: 2.50s
                        Total time: 9237.80s
                               ETA: 511350.7s

################################################################################
                    [1m Learning iteration 3549/200000 [0m

                       Computation: 3142 steps/s (collection: 0.479s, learning 2.127s)
               Value function loss: 125103.8158
                    Surrogate loss: 0.0128
             Mean action noise std: 0.90
                       Mean reward: 11807.04
               Mean episode length: 441.17
                 Mean success rate: 87.50
                  Mean reward/step: 26.62
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 29081600
                    Iteration time: 2.61s
                        Total time: 9240.40s
                               ETA: 511348.3s

################################################################################
                    [1m Learning iteration 3550/200000 [0m

                       Computation: 3180 steps/s (collection: 0.509s, learning 2.066s)
               Value function loss: 138493.9844
                    Surrogate loss: 0.0185
             Mean action noise std: 0.90
                       Mean reward: 11706.72
               Mean episode length: 437.23
                 Mean success rate: 86.50
                  Mean reward/step: 25.95
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 29089792
                    Iteration time: 2.58s
                        Total time: 9242.98s
                               ETA: 511344.2s

################################################################################
                    [1m Learning iteration 3551/200000 [0m

                       Computation: 3209 steps/s (collection: 0.501s, learning 2.052s)
               Value function loss: 107468.6888
                    Surrogate loss: 0.0131
             Mean action noise std: 0.90
                       Mean reward: 11849.58
               Mean episode length: 441.50
                 Mean success rate: 87.00
                  Mean reward/step: 24.03
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 2.55s
                        Total time: 9245.53s
                               ETA: 511338.8s

################################################################################
                    [1m Learning iteration 3552/200000 [0m

                       Computation: 3223 steps/s (collection: 0.470s, learning 2.071s)
               Value function loss: 73423.7717
                    Surrogate loss: 0.0136
             Mean action noise std: 0.90
                       Mean reward: 11685.54
               Mean episode length: 436.78
                 Mean success rate: 86.00
                  Mean reward/step: 24.23
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 29106176
                    Iteration time: 2.54s
                        Total time: 9248.07s
                               ETA: 511332.8s

################################################################################
                    [1m Learning iteration 3553/200000 [0m

                       Computation: 3226 steps/s (collection: 0.476s, learning 2.063s)
               Value function loss: 74892.1014
                    Surrogate loss: 0.0109
             Mean action noise std: 0.90
                       Mean reward: 10974.05
               Mean episode length: 417.92
                 Mean success rate: 82.00
                  Mean reward/step: 25.58
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 29114368
                    Iteration time: 2.54s
                        Total time: 9250.61s
                               ETA: 511326.6s

################################################################################
                    [1m Learning iteration 3554/200000 [0m

                       Computation: 3144 steps/s (collection: 0.488s, learning 2.117s)
               Value function loss: 101823.1669
                    Surrogate loss: 0.0116
             Mean action noise std: 0.90
                       Mean reward: 10887.31
               Mean episode length: 420.17
                 Mean success rate: 83.00
                  Mean reward/step: 25.64
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 29122560
                    Iteration time: 2.60s
                        Total time: 9253.22s
                               ETA: 511324.2s

################################################################################
                    [1m Learning iteration 3555/200000 [0m

                       Computation: 3271 steps/s (collection: 0.459s, learning 2.046s)
               Value function loss: 102856.3053
                    Surrogate loss: 0.0119
             Mean action noise std: 0.90
                       Mean reward: 11048.59
               Mean episode length: 423.00
                 Mean success rate: 83.50
                  Mean reward/step: 25.61
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 29130752
                    Iteration time: 2.50s
                        Total time: 9255.72s
                               ETA: 511316.1s

################################################################################
                    [1m Learning iteration 3556/200000 [0m

                       Computation: 3297 steps/s (collection: 0.459s, learning 2.025s)
               Value function loss: 112033.9368
                    Surrogate loss: 0.0116
             Mean action noise std: 0.90
                       Mean reward: 11094.86
               Mean episode length: 425.56
                 Mean success rate: 84.50
                  Mean reward/step: 25.17
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 29138944
                    Iteration time: 2.48s
                        Total time: 9258.21s
                               ETA: 511307.0s

################################################################################
                    [1m Learning iteration 3557/200000 [0m

                       Computation: 3173 steps/s (collection: 0.511s, learning 2.070s)
               Value function loss: 102698.9334
                    Surrogate loss: 0.0108
             Mean action noise std: 0.90
                       Mean reward: 10673.73
               Mean episode length: 414.52
                 Mean success rate: 82.50
                  Mean reward/step: 25.13
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 29147136
                    Iteration time: 2.58s
                        Total time: 9260.79s
                               ETA: 511303.2s

################################################################################
                    [1m Learning iteration 3558/200000 [0m

                       Computation: 3215 steps/s (collection: 0.456s, learning 2.092s)
               Value function loss: 57509.2264
                    Surrogate loss: 0.0114
             Mean action noise std: 0.90
                       Mean reward: 10680.45
               Mean episode length: 414.38
                 Mean success rate: 82.50
                  Mean reward/step: 26.34
       Mean episode length/episode: 30.80
--------------------------------------------------------------------------------
                   Total timesteps: 29155328
                    Iteration time: 2.55s
                        Total time: 9263.33s
                               ETA: 511297.6s

################################################################################
                    [1m Learning iteration 3559/200000 [0m

                       Computation: 3229 steps/s (collection: 0.477s, learning 2.059s)
               Value function loss: 124889.1521
                    Surrogate loss: 0.0103
             Mean action noise std: 0.90
                       Mean reward: 10762.31
               Mean episode length: 418.50
                 Mean success rate: 83.00
                  Mean reward/step: 26.33
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 29163520
                    Iteration time: 2.54s
                        Total time: 9265.87s
                               ETA: 511291.3s

################################################################################
                    [1m Learning iteration 3560/200000 [0m

                       Computation: 3244 steps/s (collection: 0.444s, learning 2.081s)
               Value function loss: 92727.7004
                    Surrogate loss: 0.0113
             Mean action noise std: 0.90
                       Mean reward: 10499.94
               Mean episode length: 410.38
                 Mean success rate: 81.50
                  Mean reward/step: 26.10
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 29171712
                    Iteration time: 2.53s
                        Total time: 9268.40s
                               ETA: 511284.4s

################################################################################
                    [1m Learning iteration 3561/200000 [0m

                       Computation: 3222 steps/s (collection: 0.469s, learning 2.073s)
               Value function loss: 103763.4422
                    Surrogate loss: 0.0127
             Mean action noise std: 0.90
                       Mean reward: 10358.10
               Mean episode length: 406.95
                 Mean success rate: 81.00
                  Mean reward/step: 26.01
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 29179904
                    Iteration time: 2.54s
                        Total time: 9270.94s
                               ETA: 511278.5s

################################################################################
                    [1m Learning iteration 3562/200000 [0m

                       Computation: 3179 steps/s (collection: 0.495s, learning 2.082s)
               Value function loss: 126319.2352
                    Surrogate loss: 0.0136
             Mean action noise std: 0.90
                       Mean reward: 10537.70
               Mean episode length: 412.20
                 Mean success rate: 83.00
                  Mean reward/step: 25.76
       Mean episode length/episode: 29.36
--------------------------------------------------------------------------------
                   Total timesteps: 29188096
                    Iteration time: 2.58s
                        Total time: 9273.52s
                               ETA: 511274.4s

################################################################################
                    [1m Learning iteration 3563/200000 [0m

                       Computation: 3210 steps/s (collection: 0.476s, learning 2.076s)
               Value function loss: 70945.8894
                    Surrogate loss: 0.0141
             Mean action noise std: 0.90
                       Mean reward: 10825.46
               Mean episode length: 419.74
                 Mean success rate: 84.50
                  Mean reward/step: 25.24
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 2.55s
                        Total time: 9276.07s
                               ETA: 511269.0s

################################################################################
                    [1m Learning iteration 3564/200000 [0m

                       Computation: 3245 steps/s (collection: 0.452s, learning 2.072s)
               Value function loss: 75819.6088
                    Surrogate loss: 0.0138
             Mean action noise std: 0.90
                       Mean reward: 10771.04
               Mean episode length: 417.50
                 Mean success rate: 83.50
                  Mean reward/step: 26.15
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 29204480
                    Iteration time: 2.52s
                        Total time: 9278.59s
                               ETA: 511262.1s

################################################################################
                    [1m Learning iteration 3565/200000 [0m

                       Computation: 3285 steps/s (collection: 0.444s, learning 2.049s)
               Value function loss: 105894.3456
                    Surrogate loss: 0.0118
             Mean action noise std: 0.90
                       Mean reward: 10799.89
               Mean episode length: 417.94
                 Mean success rate: 83.50
                  Mean reward/step: 26.33
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 29212672
                    Iteration time: 2.49s
                        Total time: 9281.08s
                               ETA: 511253.4s

################################################################################
                    [1m Learning iteration 3566/200000 [0m

                       Computation: 3209 steps/s (collection: 0.488s, learning 2.064s)
               Value function loss: 115369.6241
                    Surrogate loss: 0.0079
             Mean action noise std: 0.90
                       Mean reward: 10877.27
               Mean episode length: 422.38
                 Mean success rate: 84.00
                  Mean reward/step: 26.03
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 29220864
                    Iteration time: 2.55s
                        Total time: 9283.64s
                               ETA: 511248.1s

################################################################################
                    [1m Learning iteration 3567/200000 [0m

                       Computation: 3217 steps/s (collection: 0.478s, learning 2.068s)
               Value function loss: 80144.7769
                    Surrogate loss: 0.0097
             Mean action noise std: 0.90
                       Mean reward: 11047.11
               Mean episode length: 426.93
                 Mean success rate: 84.50
                  Mean reward/step: 26.04
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 29229056
                    Iteration time: 2.55s
                        Total time: 9286.18s
                               ETA: 511242.3s

################################################################################
                    [1m Learning iteration 3568/200000 [0m

                       Computation: 3241 steps/s (collection: 0.469s, learning 2.059s)
               Value function loss: 93775.4625
                    Surrogate loss: 0.0102
             Mean action noise std: 0.90
                       Mean reward: 11155.46
               Mean episode length: 431.52
                 Mean success rate: 85.50
                  Mean reward/step: 27.08
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 29237248
                    Iteration time: 2.53s
                        Total time: 9288.71s
                               ETA: 511235.6s

################################################################################
                    [1m Learning iteration 3569/200000 [0m

                       Computation: 3237 steps/s (collection: 0.467s, learning 2.064s)
               Value function loss: 80507.7625
                    Surrogate loss: 0.0112
             Mean action noise std: 0.90
                       Mean reward: 11115.80
               Mean episode length: 431.27
                 Mean success rate: 86.00
                  Mean reward/step: 27.93
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 29245440
                    Iteration time: 2.53s
                        Total time: 9291.24s
                               ETA: 511229.0s

################################################################################
                    [1m Learning iteration 3570/200000 [0m

                       Computation: 3280 steps/s (collection: 0.452s, learning 2.044s)
               Value function loss: 90618.6141
                    Surrogate loss: 0.0087
             Mean action noise std: 0.90
                       Mean reward: 11100.11
               Mean episode length: 431.12
                 Mean success rate: 86.00
                  Mean reward/step: 27.59
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 29253632
                    Iteration time: 2.50s
                        Total time: 9293.74s
                               ETA: 511220.6s

################################################################################
                    [1m Learning iteration 3571/200000 [0m

                       Computation: 3199 steps/s (collection: 0.496s, learning 2.064s)
               Value function loss: 124715.5697
                    Surrogate loss: 0.0092
             Mean action noise std: 0.90
                       Mean reward: 11358.01
               Mean episode length: 439.63
                 Mean success rate: 87.50
                  Mean reward/step: 27.05
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 29261824
                    Iteration time: 2.56s
                        Total time: 9296.30s
                               ETA: 511215.7s

################################################################################
                    [1m Learning iteration 3572/200000 [0m

                       Computation: 3260 steps/s (collection: 0.473s, learning 2.039s)
               Value function loss: 106899.6475
                    Surrogate loss: 0.0100
             Mean action noise std: 0.90
                       Mean reward: 11483.27
               Mean episode length: 443.46
                 Mean success rate: 88.00
                  Mean reward/step: 25.89
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 29270016
                    Iteration time: 2.51s
                        Total time: 9298.81s
                               ETA: 511208.1s

################################################################################
                    [1m Learning iteration 3573/200000 [0m

                       Computation: 3278 steps/s (collection: 0.442s, learning 2.056s)
               Value function loss: 103408.1990
                    Surrogate loss: 0.0104
             Mean action noise std: 0.90
                       Mean reward: 11677.66
               Mean episode length: 448.44
                 Mean success rate: 88.50
                  Mean reward/step: 25.52
       Mean episode length/episode: 30.12
--------------------------------------------------------------------------------
                   Total timesteps: 29278208
                    Iteration time: 2.50s
                        Total time: 9301.31s
                               ETA: 511199.8s

################################################################################
                    [1m Learning iteration 3574/200000 [0m

                       Computation: 3221 steps/s (collection: 0.464s, learning 2.078s)
               Value function loss: 90347.9697
                    Surrogate loss: 0.0108
             Mean action noise std: 0.90
                       Mean reward: 11622.80
               Mean episode length: 446.88
                 Mean success rate: 88.00
                  Mean reward/step: 26.20
       Mean episode length/episode: 30.34
--------------------------------------------------------------------------------
                   Total timesteps: 29286400
                    Iteration time: 2.54s
                        Total time: 9303.85s
                               ETA: 511193.9s

################################################################################
                    [1m Learning iteration 3575/200000 [0m

                       Computation: 3184 steps/s (collection: 0.514s, learning 2.059s)
               Value function loss: 102283.3400
                    Surrogate loss: 0.0100
             Mean action noise std: 0.90
                       Mean reward: 12011.46
               Mean episode length: 457.94
                 Mean success rate: 90.50
                  Mean reward/step: 26.68
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 2.57s
                        Total time: 9306.42s
                               ETA: 511189.7s

################################################################################
                    [1m Learning iteration 3576/200000 [0m

                       Computation: 3196 steps/s (collection: 0.475s, learning 2.088s)
               Value function loss: 90316.0528
                    Surrogate loss: 0.0122
             Mean action noise std: 0.90
                       Mean reward: 12117.00
               Mean episode length: 460.11
                 Mean success rate: 91.00
                  Mean reward/step: 26.20
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 29302784
                    Iteration time: 2.56s
                        Total time: 9308.99s
                               ETA: 511184.9s

################################################################################
                    [1m Learning iteration 3577/200000 [0m

                       Computation: 3308 steps/s (collection: 0.453s, learning 2.023s)
               Value function loss: 124781.8034
                    Surrogate loss: 0.0113
             Mean action noise std: 0.90
                       Mean reward: 12186.12
               Mean episode length: 460.57
                 Mean success rate: 91.50
                  Mean reward/step: 26.27
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 29310976
                    Iteration time: 2.48s
                        Total time: 9311.46s
                               ETA: 511175.4s

################################################################################
                    [1m Learning iteration 3578/200000 [0m

                       Computation: 3327 steps/s (collection: 0.441s, learning 2.021s)
               Value function loss: 113997.3976
                    Surrogate loss: 0.0083
             Mean action noise std: 0.90
                       Mean reward: 12299.48
               Mean episode length: 462.08
                 Mean success rate: 92.00
                  Mean reward/step: 25.82
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 29319168
                    Iteration time: 2.46s
                        Total time: 9313.93s
                               ETA: 511165.1s

################################################################################
                    [1m Learning iteration 3579/200000 [0m

                       Computation: 3189 steps/s (collection: 0.479s, learning 2.090s)
               Value function loss: 80824.6504
                    Surrogate loss: 0.0121
             Mean action noise std: 0.90
                       Mean reward: 12340.73
               Mean episode length: 463.45
                 Mean success rate: 92.50
                  Mean reward/step: 26.32
       Mean episode length/episode: 30.45
--------------------------------------------------------------------------------
                   Total timesteps: 29327360
                    Iteration time: 2.57s
                        Total time: 9316.49s
                               ETA: 511160.6s

################################################################################
                    [1m Learning iteration 3580/200000 [0m

                       Computation: 3227 steps/s (collection: 0.501s, learning 2.037s)
               Value function loss: 94615.2490
                    Surrogate loss: 0.0114
             Mean action noise std: 0.90
                       Mean reward: 12502.86
               Mean episode length: 467.70
                 Mean success rate: 93.00
                  Mean reward/step: 27.45
       Mean episode length/episode: 30.23
--------------------------------------------------------------------------------
                   Total timesteps: 29335552
                    Iteration time: 2.54s
                        Total time: 9319.03s
                               ETA: 511154.5s

################################################################################
                    [1m Learning iteration 3581/200000 [0m

                       Computation: 3265 steps/s (collection: 0.473s, learning 2.036s)
               Value function loss: 101128.8207
                    Surrogate loss: 0.0088
             Mean action noise std: 0.90
                       Mean reward: 12557.76
               Mean episode length: 469.39
                 Mean success rate: 93.50
                  Mean reward/step: 27.08
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 29343744
                    Iteration time: 2.51s
                        Total time: 9321.54s
                               ETA: 511146.7s

################################################################################
                    [1m Learning iteration 3582/200000 [0m

                       Computation: 3273 steps/s (collection: 0.436s, learning 2.066s)
               Value function loss: 96380.8566
                    Surrogate loss: 0.0109
             Mean action noise std: 0.90
                       Mean reward: 12530.84
               Mean episode length: 469.80
                 Mean success rate: 94.00
                  Mean reward/step: 26.00
       Mean episode length/episode: 29.57
--------------------------------------------------------------------------------
                   Total timesteps: 29351936
                    Iteration time: 2.50s
                        Total time: 9324.04s
                               ETA: 511138.7s

################################################################################
                    [1m Learning iteration 3583/200000 [0m

                       Computation: 3256 steps/s (collection: 0.490s, learning 2.026s)
               Value function loss: 74153.0957
                    Surrogate loss: 0.0079
             Mean action noise std: 0.90
                       Mean reward: 12289.61
               Mean episode length: 462.08
                 Mean success rate: 93.50
                  Mean reward/step: 26.15
       Mean episode length/episode: 30.01
--------------------------------------------------------------------------------
                   Total timesteps: 29360128
                    Iteration time: 2.52s
                        Total time: 9326.56s
                               ETA: 511131.3s

################################################################################
                    [1m Learning iteration 3584/200000 [0m

                       Computation: 3249 steps/s (collection: 0.478s, learning 2.043s)
               Value function loss: 124701.2594
                    Surrogate loss: 0.0098
             Mean action noise std: 0.90
                       Mean reward: 12084.04
               Mean episode length: 454.75
                 Mean success rate: 92.00
                  Mean reward/step: 27.05
       Mean episode length/episode: 29.26
--------------------------------------------------------------------------------
                   Total timesteps: 29368320
                    Iteration time: 2.52s
                        Total time: 9329.08s
                               ETA: 511124.3s

################################################################################
                    [1m Learning iteration 3585/200000 [0m

                       Computation: 3280 steps/s (collection: 0.468s, learning 2.029s)
               Value function loss: 102651.9744
                    Surrogate loss: 0.0154
             Mean action noise std: 0.90
                       Mean reward: 11381.85
               Mean episode length: 435.48
                 Mean success rate: 87.50
                  Mean reward/step: 26.52
       Mean episode length/episode: 29.47
--------------------------------------------------------------------------------
                   Total timesteps: 29376512
                    Iteration time: 2.50s
                        Total time: 9331.58s
                               ETA: 511115.9s

################################################################################
                    [1m Learning iteration 3586/200000 [0m

                       Computation: 3306 steps/s (collection: 0.454s, learning 2.023s)
               Value function loss: 72247.2257
                    Surrogate loss: 0.0106
             Mean action noise std: 0.90
                       Mean reward: 11185.50
               Mean episode length: 427.69
                 Mean success rate: 86.00
                  Mean reward/step: 25.90
       Mean episode length/episode: 29.90
--------------------------------------------------------------------------------
                   Total timesteps: 29384704
                    Iteration time: 2.48s
                        Total time: 9334.05s
                               ETA: 511106.5s

################################################################################
                    [1m Learning iteration 3587/200000 [0m

                       Computation: 3206 steps/s (collection: 0.513s, learning 2.042s)
               Value function loss: 112656.0047
                    Surrogate loss: 0.0103
             Mean action noise std: 0.90
                       Mean reward: 11111.92
               Mean episode length: 426.05
                 Mean success rate: 85.50
                  Mean reward/step: 26.35
       Mean episode length/episode: 29.68
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 2.55s
                        Total time: 9336.61s
                               ETA: 511101.3s

################################################################################
                    [1m Learning iteration 3588/200000 [0m

                       Computation: 3266 steps/s (collection: 0.447s, learning 2.061s)
               Value function loss: 96523.0197
                    Surrogate loss: 0.0082
             Mean action noise std: 0.90
                       Mean reward: 11082.67
               Mean episode length: 424.30
                 Mean success rate: 84.50
                  Mean reward/step: 25.95
       Mean episode length/episode: 29.79
--------------------------------------------------------------------------------
                   Total timesteps: 29401088
                    Iteration time: 2.51s
                        Total time: 9339.12s
                               ETA: 511093.5s
Traceback (most recent call last):
  File "tools/train_ppo.py", line 51, in <module>
    train()
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/home/tb1/anaconda3/envs/ccmf/lib/python3.8/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "tools/train_ppo.py", line 47, in train
    ppo.run(num_learning_iterations=max_iterations, log_interval=cfg.train.learn.save_interval)
  File "/home/tb1/ccmfinal/Computational-Cognitive-Modeling/mvp/ppo/ppo.py", line 267, in run
    mean_value_loss, mean_surrogate_loss = self.update(it, num_learning_iterations)
  File "/home/tb1/ccmfinal/Computational-Cognitive-Modeling/mvp/ppo/ppo.py", line 415, in update
    mean_value_loss += value_loss.item()
KeyboardInterrupt
